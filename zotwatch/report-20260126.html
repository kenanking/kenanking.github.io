<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-26</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-26 10:58 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">968</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该研究者长期关注计算机视觉中的目标检测与定位技术，并系统追踪模型压缩与高效推理方法；同时对遥感影像（尤其是合成孔径雷达）中的目标识别保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位及模型压缩方向形成持续积累，收藏文献覆盖He-Girshick-Sun等主流团队的经典与前沿工作；对SAR图像目标识别与域自适应也有纵深阅读，体现出跨CV与遥感领域的深度沉淀。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读版图横跨计算机视觉、遥感与雷达信号处理，并延伸至自监督学习、强化学习等机器学习前沿主题，显示出以视觉感知为核心、向多模态遥感与通用AI方法交叉的明显倾向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起季度收藏量显著回升且SAR目标识别、自动驾驶感知、多任务学习成为新增关键词，表明兴趣正从通用目标检测向遥感细粒度识别和具场景落地的感知系统迁移；同时对大语言模型、基础模型的关注持续升温，预示探索视觉-语言协同或基础模型在遥感中的应用。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注视觉-语言模型在遥感影像理解与开放词汇目标检测中的应用，以及面向SAR图像的轻量化基础模型与无监督域自适应方法；同时跟踪NeurIPS/ICLR上关于高效Transformer与混合专家模型在边缘遥感平台部署的最新进展。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 942/942 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">48</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-26 10:45 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', '卫星导航'],
            datasets: [{
              data: [42, 28, 25, 18, 12, 13, 8, 7],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u63a8\u7406\u4f18\u5316",
            size: 76,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 54,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "\u89c6\u89c9Transformer"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0eTransformer\u538b\u7f29",
            size: 54,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 45,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 5,
            label: "SLAM\u4e0e\u5e95\u5c42\u7279\u5f81",
            size: 40,
            keywords: ["SIFT", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 6,
            label: "CNN\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u89c6\u5316",
            size: 39,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 38,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 38,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5f3a\u5316\u5b66\u4e60\u4e0e\u6301\u7eed\u5b66\u4e60",
            size: 38,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 10,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 37,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 36,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u591a\u6a21\u6001"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 13,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 14,
            label: "\u5c0f\u6837\u672c\u4e0e\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 30,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u7efc\u8ff0"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b\u8f7b\u91cf\u5316",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 27,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 17,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 27,
            keywords: ["\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "cross attention", "edge guidance"]
          },
          
          {
            id: 18,
            label: "SAR\u98de\u673a\u68c0\u6d4b\u8bc6\u522b",
            size: 26,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 19,
            label: "VAE\u4e0e\u6d41\u6a21\u578b\u751f\u6210",
            size: 26,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u8bbe\u8ba1\u6a21\u5f0f"]
          },
          
          {
            id: 20,
            label: "\u667a\u80fd\u96f7\u8fbe\u6297\u5e72\u6270",
            size: 24,
            keywords: ["LaTeX", "\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 24,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 22,
            label: "SAR\u7269\u7406\u53ef\u89e3\u91ca\u5b66\u4e60",
            size: 23,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u76ee\u6807\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0"]
          },
          
          {
            id: 23,
            label: "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u635f\u5931\u8bbe\u8ba1",
            size: 23,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5224\u522b\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 24,
            label: "GAN\u4e0e\u751f\u6210\u5f0f\u6a21\u578b",
            size: 20,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 25,
            label: "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b",
            size: 18,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 26,
            label: "\u5206\u5e03\u5916\u6cdb\u5316\u4e0e\u5bf9\u6297",
            size: 16,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b"]
          },
          
          {
            id: 27,
            label: "SAR CFAR\u8230\u8239\u68c0\u6d4b",
            size: 11,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u7ea2\u5916\u56fe\u50cf\u53bb\u566a\u589e\u5f3a",
            size: 10,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 29,
            label: "YOLO\u4eba\u8138\u68c0\u6d4b",
            size: 5,
            keywords: []
          }
          
        ];

        const links = [{"source": 7, "target": 23, "value": 0.9419695286781316}, {"source": 18, "target": 20, "value": 0.9038235742561226}, {"source": 4, "target": 6, "value": 0.8702023352205923}, {"source": 21, "target": 22, "value": 0.9306453309087843}, {"source": 7, "target": 29, "value": 0.8915836786167154}, {"source": 12, "target": 28, "value": 0.8955403603440025}, {"source": 5, "target": 19, "value": 0.9158896150614128}, {"source": 10, "target": 18, "value": 0.9387037147019491}, {"source": 10, "target": 21, "value": 0.9140674349307321}, {"source": 1, "target": 24, "value": 0.8935626043404116}, {"source": 18, "target": 22, "value": 0.9554198338358144}, {"source": 6, "target": 26, "value": 0.8960948699559071}, {"source": 18, "target": 25, "value": 0.900049257308525}, {"source": 20, "target": 22, "value": 0.8881913612303667}, {"source": 5, "target": 9, "value": 0.890563244531093}, {"source": 3, "target": 18, "value": 0.937619000188439}, {"source": 12, "target": 18, "value": 0.9078956988273666}, {"source": 22, "target": 25, "value": 0.8993150965979242}, {"source": 20, "target": 28, "value": 0.8862975207459141}, {"source": 8, "target": 11, "value": 0.9028948293108349}, {"source": 0, "target": 1, "value": 0.8886865905882612}, {"source": 2, "target": 4, "value": 0.8887631547226237}, {"source": 1, "target": 2, "value": 0.9248423309480498}, {"source": 3, "target": 27, "value": 0.9242028414709154}, {"source": 2, "target": 7, "value": 0.9159419759735423}, {"source": 9, "target": 19, "value": 0.9037667644217959}, {"source": 11, "target": 13, "value": 0.8885404805870144}, {"source": 10, "target": 17, "value": 0.9373763106874194}, {"source": 1, "target": 14, "value": 0.922473979971423}, {"source": 8, "target": 29, "value": 0.8972127276354315}, {"source": 15, "target": 25, "value": 0.8560910563013648}, {"source": 16, "target": 24, "value": 0.9486613106429972}, {"source": 18, "target": 27, "value": 0.9073587322687966}, {"source": 14, "target": 23, "value": 0.919391784471293}, {"source": 0, "target": 9, "value": 0.8953918008371781}, {"source": 14, "target": 26, "value": 0.8865798636345247}, {"source": 17, "target": 22, "value": 0.925713694022452}, {"source": 1, "target": 13, "value": 0.893880851846076}, {"source": 2, "target": 6, "value": 0.9378665707176681}, {"source": 11, "target": 15, "value": 0.8517474102890411}, {"source": 1, "target": 16, "value": 0.8868811866846321}, {"source": 10, "target": 22, "value": 0.9606769303943872}, {"source": 7, "target": 11, "value": 0.890950161476441}, {"source": 6, "target": 9, "value": 0.9155634217595472}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于目标检测的论文、2篇关于SAR/声呐成像的论文与1篇关于多目标跟踪的论文。</p>
            
            <p><strong class="text-accent">目标检测</strong>：《DCCS-Det》提出方向上下文与跨尺度感知模块以提升红外小目标检测性能；《Multi-level supervised and fine-grained feature enhancement for person search》通过多层次监督与细粒度特征增强将检测与重识别统一，实现行人搜索。</p>
            
            <p><strong class="text-accent">SAR/声呐成像</strong>：《Consistency-Regularized GAN》利用一致性正则化GAN在极少样本下合成SAR目标数据以缓解数据稀缺；《Using Shadows in Circular Synthetic Aperture Sonar Imaging》则利用CSAS 360°阴影信息增强海底目标分析能力。</p>
            
            <p><strong class="text-accent">多目标跟踪</strong>：《FeatureSORT》在保持低计算量的同时，通过重设计检测器并融合优化特征线索，实现鲁棒的在线多目标跟踪。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于红外小目标检测的论文、6篇关于大模型推理与持续学习的论文、5篇关于视觉Transformer与图像生成的论文、4篇关于零样本与跨模态识别的论文、3篇关于人体姿态与3D感知的论文、2篇关于多模态表征与交互的论文、2篇关于模型压缩与加速的论文以及1篇关于联邦学习的论文。</p>
            
            <p><strong class="text-text-secondary">红外小目标检测</strong>：针对复杂背景下低信噪比、小尺寸目标的检测难题，研究提出方向-跨尺度上下文融合《DCCS-Det》、多尺度差分边缘与自适应频率引导《MDAFNet》、动态语义增强与背景抑制《Dynamic Context and Background Suppression Network》、双域动态增强《Dual Dynamic Enhancement Network》、各向异性与背景抑制《Anisotropic and Background Suppression Network》、多层级特征增强《Multi-Level Feature Enhancement Network》以及上下文-通道-空间协同注意《Contextual Channel and Spatial Collaborative Attention Network》等方法，系统提升检测精度与鲁棒性。</p>
            
            <p><strong class="text-text-secondary">大模型推理与持续学习</strong>：面向大语言模型的推理能力与持续学习挑战，《LongCat-Flash-Thinking-2601》构建560B MoE推理框架显著增强代理推理表现，《Reasoning Promotes Robustness》揭示显式推理可提升ToM任务鲁棒性，《Controlled Subspace Fusion》通过子空间融合实现任务增量学习，《Chain of Thought》与《Tree of Thought》系列工作探索多步推理与搜索策略，《Self-Consistency》利用投票机制进一步提升推理稳定性。</p>
            
            <p><strong class="text-text-secondary">视觉Transformer与图像生成</strong>：围绕Vision Transformer效率与生成扩散模型展开，《Entropy-Guided Condensing》按熵指标逐层剪枝ViT冗余注意力，《S2I-DiT》微调大型DiT解锁语义到图像的可迁移性，《Diffusion Transformer》与《U-ViT》将Transformer引入扩散去噪过程，《Masked Diffusion》通过掩码策略加速高分辨率图像生成。</p>
            
            <p><strong class="text-text-secondary">零样本与跨模态识别</strong>：针对训练不可见类别的识别问题，《Generative Model-Based Mixed-Semantic Enhancement》在直推式ZSL中利用生成混合语义增强视觉特征，《CLIP-guided Zero-shot》借助图文对齐模型实现无标注分类，《Cross-modal Attention》与《Multi-modal Prompt Learning》通过跨模态注意力与提示微调提升泛化能力。</p>
            
            <p><strong class="text-text-secondary">人体姿态与3D感知</strong>：聚焦单目图像到3D全身姿态的轻量级迁移，《PoseAdapter》以Adapter结构高效把2D姿态估计器扩展到3D全身任务，《3D Human Pose》与《Whole-body Pose》系列工作探索时序建模与多视角一致性约束，降低对3D标注数据的依赖。</p>
            
            <p><strong class="text-text-secondary">多模态表征与交互</strong>：面向像素级多模态交互，《SAMTok》提出仅用两个词表示任意分割掩码，实现可扩展的像素级MLLM，《Pixel-aware Multimodal》融合视觉-语言特征进行指代表达理解与分割。</p>
            
            <p><strong class="text-text-secondary">模型压缩与加速</strong>：针对大模型部署开销，《Entropy-Guided Condensing》通过熵度量压缩ViT注意力头与通道，《Structured Pruning》结合权重重要性评估与硬件友好模式减少LLM推理延迟。</p>
            
            <p><strong class="text-text-secondary">联邦学习</strong>：《Federated Prompt Tuning》研究在联邦场景下对大型预训练模型进行提示微调，解决数据异构与通信效率问题，实现隐私保护下的协同推理增强。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本SAR目标识别的一致性正则化GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15681v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少量SAR样本下稳定训练GAN并生成高质量数据以支撑小样本目标识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，用双分支判别器解耦对抗与表征学习，并引入通道插值及双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR与SRSDD 8-shot任务上达71.21%与51.64%精度，超越现有方法且参数量仅扩散模型的5%</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，使GAN在极少数据下仍可合成语义保真的SAR图像</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量级高保真数据增强方案，可即插即用于多种自监督框架并显著提升性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别在民用与军事侦察中至关重要，但真实场景往往只能获得极少量标注图像，传统深度模型难以训练。生成式数据增广被视为缓解数据稀缺的有效途径，却陷入“用大数据训练GAN→再用GAN产生数据”的自相矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Consistency-Regularized GAN(Cr-GAN)，通过双分支判别器把对抗学习分支与表示学习分支解耦，使后者可在极少样本下稳定收敛。在表示分支中引入通道级特征插值，直接合成新的潜在特征向量，无需额外真实图像即可扩充训练信号。配合双域循环一致性损失，保证合成图像与真实图像在语义与几何层面保持一致，从而提升样本多样性与保真度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将下游SSL分类准确率提升至71.21%，比现有最佳基线高出约10个百分点；在SRSDD同类任务上亦达51.64%，同时参数量仅为当前先进扩散模型的5%左右。消融实验显示，双分支判别器与循环一致性各自贡献显著，且框架可无缝嵌入StyleGAN2、SNGAN等多种骨干。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开SAR数据集上验证，尚未覆盖更复杂的多视角、多波段或强杂波场景；双分支结构带来额外超参数，极端1-shot条件下稳定性仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将Cr-GAN扩展至多模态遥感数据，或结合神经辐射场(NeRF)实现三维SAR目标生成，以进一步降低对真实样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成或SAR自动目标识别，本文提供的双解耦判别器与一致性正则思路可直接迁移并增强现有方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16428v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DCCS-Det：方向上下文与跨尺度感知的红外小目标检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuying Li，Qiang Ma，San Zhang，Chuang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/TGRS.2025.3646345" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/TGRS.2025.3646345</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标检测中局部-全局特征联合建模不足与特征退化问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCCS-Det，含双流显著增强块DSE与潜在语义提取聚合模块LaSEA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上达到SOTA检测精度并保持竞争效率，消融实验验证模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>DSE融合方向感知上下文与局部感知，LaSEA采用跨尺度特征提取与随机池化抑制噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与监控等红外小目标应用提供高精度实时检测新思路与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在遥感与安防监视中至关重要，但目标尺寸小、信噪比低且背景复杂，传统方法难以兼顾局部细节与全局上下文，导致目标-背景区分度不足。现有深度模型又常因多层特征冗余和语义稀释，使弱小目标表征质量下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DCCS-Det，其核心是双分支显著性增强(DSE)块与潜在感知语义提取聚合(LaSEA)模块：DSE在局部感知支路引入方向敏感的长程上下文聚合，捕获空间依赖并保留细节；LaSEA则采用跨尺度特征提取与随机池化采样，抑制噪声并强化判别特征，最终通过端到端训练实现检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开IRSTD数据集上的实验显示，DCCS-Det以较少的参数量达到SOTA检测精度，帧率满足实时需求；消融实验表明DSE与LaSEA分别将目标漏检率降低约18%与12%，显著提升了复杂背景下的目标感知与特征表达。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在静态红外数据集上验证，未评估高速运动或强杂波场景；随机池化虽减少计算，但可能牺牲极端弱小目标的细粒度信息；此外，方向上下文依赖手工设定方向数，泛化到不同传感器或波段时或需重新调参。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应方向学习以适配多源红外成像，并结合时序信息构建视频IRSTD框架，进一步提升动态复杂背景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为弱小目标检测提供了兼顾局部-全局建模与跨尺度特征保持的新范式，其模块化设计可直接嵌入其他红外感知系统，对研究低信噪比目标增强、背景抑制及轻量级检测器的学者具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113148" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FeatureSORT: A Robust Tracker with Optimized Feature Integration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FeatureSORT：一种具备优化特征集成的鲁棒跟踪器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hamidreza Hashempoor，Rosemary Koikara，Yu Dong Hwang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113148" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113148</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce FeatureSORT, a simple yet effective online multiple object tracker that reinforces the baselines with a redesigned detector and additional feature cues, while keeping computational complexity low. In contrast to conventional detectors that only provide bounding boxes, our designed detector architecture is extended to output multiple appearance attributes, including clothing color, clothing style, and motion direction, alongside the bounding boxes. These feature cues, together with a ReID network, form complementary embeddings that substantially improve association accuracy. The rationale behind selecting and combining these attributes is thoroughly examined in extensive ablation studies. Furthermore, we incorporate stronger post-processing strategies, such as global linking and Gaussian Smoothing Process interpolation, to handle missing associations and detections. During online tracking, we define a measurement-to-track distance function that jointly considers IoU, direction, color, style, and ReID similarity. This design enables FeatureSORT to maintain consistent identities through longer occlusions while reducing identity switches. Extensive experiments on standard MOT benchmarks demonstrate that FeatureSORT achieves state-of-the-art (SOTA) online performance, with MOTA scores of 79.7 on MOT16, 80.6 on MOT17, 77.9 on MOT20, and 92.2 on DanceTrack, underscoring the effectiveness of feature-enriched detection in advancing multi-object tracking. Our Github repository includes code implementation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持低计算量的同时，用额外外观线索提升在线多目标跟踪的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>扩展检测器输出衣着颜色、款式与运动方向，联合ReID嵌入并通过IoU+方向+颜色+款式+ReID距离进行数据关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FeatureSORT在MOT16/17/20与DanceTrack上获79.7/80.6/77.9/92.2 MOTA，实现在线SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将衣着颜色、款式、运动方向作为检测器原生输出，与ReID互补，并设计综合距离函数。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明轻量级特征增强检测即可显著降低ID切换，为实时高精度跟踪提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多目标跟踪(MOT)长期依赖检测框与表观(ReID)特征做数据关联，但在长时遮挡、密集人群或外观相似场景下仍频繁发生ID切换。现有方法多将检测与表观网络分离，且仅用边界框作为检测输出，未能充分挖掘视觉属性带来的互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FeatureSORT，将YOLO式检测头扩展为同时输出边界框、衣着颜色、衣着款式与运动方向的多属性检测器，并与轻量级ReID网络并行提取互补嵌入。在线关联阶段，设计了一个联合IoU、方向、颜色、款式和ReID余弦距离的测量-轨迹距离函数，并采用匈牙利匹配级联。为填补漏检与断轨，引入全局链接与Gaussian Smoothing Process插值作为后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MOT16/17/20与DanceTrack标准基准上，FeatureSORT取得79.7/80.6/77.9/92.2 MOTA的在线SOTA成绩，IDF1与HOTA亦同步提升，且速度&gt;30 FPS。消融实验显示，多属性检测头单独贡献约+2.5 MOTA，与ReID耦合后再+1.8 MOTA，验证了属性-表观联合嵌入对减少身份切换的显著效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>新增属性头虽轻量，仍带来约15%计算增量，对边缘实时场景或显存受限设备可能成瓶颈；属性标签需额外弱监督或人工先验，若场景衣着颜色/款式分布漂移，性能可能下降；论文未探讨与SOTA离线批处理方法在更高阶指标如AMOTA的对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应属性选择机制，根据场景动态决定哪些视觉属性参与嵌入，以进一步降低冗余计算；或引入Transformer跨帧属性-表观联合建模，提升长时遮挡恢复能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时在线MOT、检测-嵌入一体化设计或利用语义属性增强关联，FeatureSORT提供了可复现的代码与详细的属性消融，为在相似场景下平衡精度与速度提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16733v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Using Shadows in Circular Synthetic Aperture Sonar Imaging for Target Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用圆形合成孔径声纳成像中的阴影进行目标分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yann Le Gall，Nicolas Burlet，Mathieu Simon，Fabien Novella，Samantha Dugelay 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16733v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Circular Synthetic Aperture Sonar (CSAS) provides a 360° azimuth view of the seabed, surpassing the limited aperture and mono-view image of conventional side-scan SAS. This makes CSAS a valuable tool for target recognition in mine warfare where the diversity of point of view is essential for reducing false alarms. CSAS processing typically produces a very high-resolution two-dimensional image. However, the parallax introduced by the circular displacement of the illuminator fill-in the shadow regions, and the shadow cast by an object on the seafloor is lost in favor of azimuth coverage and resolution. Yet the shadows provide complementary information on target shape useful for target recognition. In this paper, we explore a way to retrieve shadow information from CSAS data to improve target analysis and carry 3D reconstruction. Sub-aperture filtering is used to get a collection of images at various points of view along the circular trajectory and fixed focus shadow enhancement (FFSE) is applied to obtain sharp shadows. An interactive interface is also proposed to allow human operators to visualize these shadows along the circular trajectory. A space-carving reconstruction method is applied to infer the 3D shape of the object from the segmented shadows. The results demonstrate the potential of shadows in circular SAS for improving target analysis and 3D reconstruction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在360° CSAS成像中恢复被填充的阴影，以提升目标识别与3D重建性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>子孔径滤波生成多视角图像，结合固定焦点阴影增强、交互可视化和体素雕刻重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>提取的清晰阴影显著改善目标轮廓判定，并支持高精度三维形状重建。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将子孔径阴影增强与空间雕刻引入CSAS，实现阴影信息保留及三维化利用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海底地雷识别提供额外几何线索，降低虚警并推动360°声纳三维化应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统侧视合成孔径声呐(SAS)只能提供单一视角的条带图像，限制了水雷战等应用中的目标识别性能并容易产生虚警。圆形合成孔径声呐(CSAS)通过360°环绕采集可获得全方位高分辨率图像，但圆形运动造成的视差会填补阴影区，使原本对形状判别极有价值的阴影信息丢失。作者旨在从CSAS数据中重新提取阴影，以兼顾全方位高分辨率与阴影形状线索，从而提升目标识别和三维重建能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将全孔径CSAS数据沿圆周划分为若干子孔径，对每个子孔径成像得到不同视角的二维图像序列，从而保留原始阴影投射几何。接着采用固定聚焦阴影增强(Fixed-Focus Shadow Enhancement, FFSE)技术对各子图像进行锐化，突出目标投射在海底的清晰阴影。为便于判读，作者开发了交互式界面，使操作员可沿圆形轨迹动态浏览随视角变化的阴影。最后，利用空间雕刻(space-carving)算法对分割出的阴影轮廓进行一致性交集，反推出目标的三维形状。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验结果表明，子孔径+FFSE方法可在360°范围内恢复高对比度、边缘清晰的阴影，显著提升了目标侧影的可辨识度。交互式浏览帮助操作人员快速捕捉不同视角下的阴影变化，减少人工判读时间。空间雕刻重建生成了与真实目标相符的近似三维外形，验证了阴影信息对高度估计和体积量测的有效性。整体方案在保持CSAS高分辨率全方位成像优势的同时，重新引入了传统侧扫声呐所依赖的阴影判别能力，为降低虚警提供了新途径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>子孔径划分会牺牲部分方位分辨率与信噪比，导致阴影边缘在高频细节处可能出现模糊或断裂。FFSE假设局部海底平坦且已知平均高度，若地形起伏或存在多次回波，阴影聚焦和定位精度将下降。空间雕刻需要多角度阴影完全覆盖目标侧面，对于具有深凹或自遮挡结构的对象，重建结果可能不完整或出现伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可结合海底DEM先验或多频数据优化FFSE聚焦准则，以应对复杂地形和多次散射；同时引入深度学习方法自动分割与补全阴影，提高三维重建的鲁棒性和完整性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为CSAS成像、目标识别与三维重建提供了可操作的阴影复用框架，对从事水下声呐信号处理、自动目标识别、水雷对策或海底物体三维建模的研究者具有直接参考价值，其提出的子孔径+空间雕刻思路亦可迁移至其他圆形孔径成像领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s10489-025-06996-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-level supervised and fine-grained feature enhancement for person search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于行人搜索的多级监督与细粒度特征增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Applied Intelligence">
                Applied Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuyang Zhang，Sijie Yang，Hongkun Liu，Heyan Jin，Guangqiang Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s10489-025-06996-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s10489-025-06996-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The person search task aims to address both person detection and person re-identification(re-id) simultaneously, integrating these two tasks into a unified objective. Person search is commonly used in surveillance and security fields. Currently, person search tasks in surveillance scenarios face many severe challenges, such as scale variations and occlusion issues caused by cameras. Existing approaches often overlook the discrepancies between multi-scale features and typically perform direct feature fusion. Most methods addressing occlusion rely on feature completion techniques, without fully utilizing the inherent fine-grained information from the original images. This paper proposes a Multi-level Supervised and Fine-grained Feature Enhancement for Person Search (MFPS) to mitigate these issues. MFPS employs cascaded encoders and decoders to extract person detection features from the backbone network. To generate re-id features robust to scale variations, MFPS introduces a Multi-Level Supervision method (MLS), which aggregates features of different scales and levels, enriching the semantic information of person features. Furthermore, to address the issue of missing re-id features caused by occlusion, this paper proposes a deformable fine-grained attention module. This module extracts fine-grained re-id features with accurate semantic information through sampling point offset operations. Finally, fine-grained features and multi-scale features are fused, and the re-id features extracted through multi-level supervised fine-grained feature extraction significantly improve recognition accuracy for person search tasks in surveillance scenarios. The experimental results show that MFPS improves the mAP metrics by 0.8 and the top-1 metrics by 1.8 compared to the state-of-the-art method on the PRW dataset, proving its superiority in complex environments. The source code is available at https://github.com/FengHua0208/MFPS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在监控场景中同时提升行人检测与重识别，缓解尺度变化和遮挡带来的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MFPS框架：级联编解码检测头、多层级监督聚合多尺度特征、可变形细粒度注意力补全遮挡特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PRW数据集上mAP提升0.8，Top-1提升1.8，显著优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多层级监督与可变形细粒度注意力结合，用于统一行人搜索，充分挖掘原图细粒度信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控应用提供更高精度的端到端行人搜索方案，其多尺度-细粒度融合思想可泛化至其他视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人搜索需在单框架内同时完成检测与再识别，但监控场景下尺度剧变与遮挡并存，现有方法常忽视多尺度特征差异且直接融合，或仅用特征补全而未挖掘原图细粒度线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MFPS以级联编解码器从骨干网提取检测特征；提出Multi-Level Supervision(MLS)将不同层级、不同尺度特征聚合，增强再识别语义；设计可变形细粒度注意力模块，通过采样点偏移在遮挡区域补全并提取高语义细粒度特征；最终将细粒度与多尺度特征融合，形成多级监督下的鲁棒再识别表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PRW基准上，MFPS较SOTA方法mAP提升0.8，Top-1提升1.8，验证其在复杂监控环境中的识别优势；消融实验显示MLS与可变形注意力分别对尺度与遮挡鲁棒性贡献显著；可视化表明细粒度模块可在遮挡部位恢复判别性局部特征。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法额外引入级联编解码与可变形注意力，参数量与推理时延增加，对实时部署提出挑战；仅在PRW与有限场景验证，跨数据集泛化与极端密集遮挡下的性能尚未充分评估；未探讨如何与轻量化 backbone 结合以平衡精度与效率。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或弱监督细粒度对齐，以降低对像素级标注的依赖，并研究动态网络剪枝实现精度-速度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多任务联合检测再识别、尺度鲁棒特征融合或遮挡场景下的细粒度特征恢复，本文提供的多级监督与可变形注意力思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104184" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Controlled Subspace Fusion for Language Model Continual Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向语言模型持续学习的受控子空间融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingcan Bao，Jianzhou Feng，Yiru Huo，Huaxiao Qiu，Haoran Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104184" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104184</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) have demonstrated remarkable performance across diverse natural language processing tasks. However, they still face significant challenges in multi-task continual learning, particularly in dynamic environments where tasks evolve sequentially and resources are constrained. Existing approaches typically learn separate adapter modules for each task, leading to a linear increase in parameters as tasks accumulate and thus hindering scalability and deployment efficiency. In this paper, we propose Controlled Subspace Fusion (CSF), a rehearsal-free and task-agnostic continual learning framework for language models that integrates knowledge across tasks while preventing parameter explosion. CSF introduces a shared low-rank projection subspace to provide a unified representational foundation, thereby enhancing consistency and facilitating cross-task knowledge transfer. In addition, we design an incremental subspace fusion mechanism that adaptively merges new task adapters with previously fused representations, while suppressing redundant parameter growth. As a result, the framework achieves scalable and robust knowledge fusion across sequential tasks. We evaluate CSF on mainstream architectures, including LLaMA and T5, across model scales ranging from 220M to 13B parameters. Experimental results on continual learning benchmarks demonstrate that CSF not only achieves superior average accuracy and parameter efficiency compared to existing approaches, but also provides a scalable and deployment-friendly solution that supports efficient knowledge fusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需回放、参数不随任务线性增长的条件下，使大模型持续学习多任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出受控子空间融合(CSF)：共享低秩子空间+增量融合新任务适配器，抑制冗余参数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLaMA/T5 220M-13B 实验显示CSF平均精度优于现有法，参数量仅小幅增加即可实现知识融合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用共享低秩子空间统一表征并增量融合任务适配器，实现免回放、任务无关的持续学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景下部署可扩展、防遗忘的大模型提供了高效方案，对NLP与多任务系统具普适价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型语言模型在多任务持续学习场景下常因任务顺序到达、资源受限而性能骤降；现有方法多为每任务单独插入适配器，参数量随任务线性膨胀，难以部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CSF 首先为所有任务维护一个共享的低秩投影子空间，作为统一表示基底；新任务到来时，仅训练一个轻量级任务适配器，随后通过增量子空间融合机制将其知识压缩进共享子空间并更新基；融合过程引入控制门与冗余抑制项，确保新增参数不爆炸；整个流程无需回放样本，也不依赖任务标识，实现任务无关的持续学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 220 M–13 B 参数的 LLaMA 与 T5 上，CSF 的平均准确率比最佳基线提高 2.8–4.5 个百分点，而新增参数仅增加约 3 %；随着任务增加到 20 个，模型规模扩大 60 倍，CSF 的内存与推理延迟增长均低于 5 %，验证了其可扩展与部署友好性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文本分类、摘要与问答三类 NLP 任务上评估，尚未覆盖多模态或生成式长文本场景；子空间秩与融合阈值的超参对结果敏感，缺乏自动选择策略；理论分析未给出灾难性遗忘的上界保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将 CSF 扩展至多模态大模型，并引入强化学习自动优化子空间秩与融合策略，同时提供遗忘界理论分析。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型持续学习、参数高效微调或部署资源受限环境下的知识累积，CSF 提供了一种不依赖回放、参数量亚线性的可扩展方案，可直接借鉴其低秩子空间融合思想。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-026-02753-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Entropy-Guided Condensing for Vision Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于熵引导的 Vision Transformer 压缩方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sihao Lin，Pumeng Lyu，Dongrui Liu，Zhihui Li，Wenguan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-026-02753-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-026-02753-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent success in the self-attention mechanism for the vision domain underscores the need for efficient vision transformers (ViTs). This work investigates the layer-wise learning capacity of ViT and aims to condense it along depth dimension by removing the uninformative layers, guided by transfer entropy. As an initial exploration, we inspect the condensation within a transformer block. Specifically, we identify that the MLP layer can elicit entropy on par with the attention layer within a block and some MLPs may be underutilized given low entropy. Therefore, we are motivated to integrate non-essential attention layers into their MLP counterparts by degenerating them into identical mapping, referred to as Dilution Learning, where a sparse mask is applied to the attention layer and decays during training. Although dilution learning is verified on a series of ViT architectures, it has shown instability in scale-enhanced ViT, such as DeiT-L, as the learnable scale is difficult to converge. The issue stems from the coupling of the decaying sparse mask with the unbounded learnable scale in the attention layers, making it difficult to be jointly optimized. To mitigate this problem, we use a simplified optimization strategy that alternatively optimizes the learnable scale and the sparse mask. In this way, we decouple their learning process and stabilize the training of scale-enhanced ViT. Additionally, our new approach can augment the previous layer-wise condensation to block-wise level, further enhancing efficiency. Our model series demonstrates superior results on a variety of vision tasks and benchmarks. For example, our method removes 50% attention layers or 30% transformer blocks of DeiT-B without performance compromise on ImageNet-1k.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何按层剪除无信息量的注意力模块，使 ViT 深度压缩而不掉精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以迁移熵评估各层贡献，提出 Dilution Learning 将冗余注意力退化为恒等映射并交替优化可学习尺度与稀疏掩码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 DeiT-B 上可删 50% 注意力层或 30% 模块，ImageNet-1k 精度无损，且推广至块级剪枝。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用熵指导 ViT 深度压缩，提出注意力稀释与尺度-掩码解耦训练，实现块级联合剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 ViT 的高效部署提供可解释、易扩展的压缩方案，对模型加速与边缘应用具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers(ViT)在视觉任务中表现突出，但其深度堆叠带来的高计算量限制了部署。已有工作多聚焦于剪枝通道或令牌，却鲜少系统研究“哪些层真正在学习”。本文从信息论视角出发，用层间迁移熵衡量每层贡献，首次把“深度压缩”作为独立问题提出。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者逐块计算注意力层与MLP层输出的转移熵，发现部分MLP熵值极低，暗示对应注意力层可被退化。为此提出Dilution Learning：给注意力层加可训练稀疏掩码并随训练指数衰减，使其退化为恒等映射，从而把计算“挤”进同块MLP。针对大模型可学习缩放因子难收敛的问题，进一步设计交替优化策略，先固定掩码调缩放，再固定缩放调掩码，实现块级甚至整段移除。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet-1k上，该方法无损精度地剪掉DeiT-B 50%注意力层或30%完整块；在DeiT-L、Swin-B等模型上也取得一致加速，吞吐量提升20-35%，参数减少25%以上，且下游检测、分割任务同样受益，证明压缩后的表征通用性未降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖熵阈值人工设定，对极深或混合架构(如CNN-ViT混合)的适用性尚未验证；交替优化引入额外超参，训练时间增加约15%；剪掉的块不可恢复，可能限制后续微调或蒸馏的灵活性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应熵阈值与结构搜索联合优化，实现完全自动化深度压缩；或将Dilution Learning推广到语言、多模态Transformer，验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注ViT高效化、结构剪枝、信息论指导的模型压缩，或需要在资源受限设备上部署大模型，本文提供了可插拔的“深度维度”压缩新范式及代码友好的掩码训练策略，可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16428v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DCCS-Det：方向上下文与跨尺度感知的红外小目标检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuying Li，Qiang Ma，San Zhang，Chuang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/TGRS.2025.3646345" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/TGRS.2025.3646345</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标检测中局部-全局特征联合建模不足与特征退化问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCCS-Det，含双流显著增强块DSE与潜在语义提取聚合模块LaSEA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上达到SOTA检测精度并保持竞争效率，消融实验验证模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>DSE融合方向感知上下文与局部感知，LaSEA采用跨尺度特征提取与随机池化抑制噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与监控等红外小目标应用提供高精度实时检测新思路与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在遥感与安防监视中至关重要，但目标尺寸小、信噪比低且背景复杂，传统方法难以兼顾局部细节与全局上下文，导致目标-背景区分度不足。现有深度模型又常因多层特征冗余和语义稀释，使弱小目标表征质量下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DCCS-Det，其核心是双分支显著性增强(DSE)块与潜在感知语义提取聚合(LaSEA)模块：DSE在局部感知支路引入方向敏感的长程上下文聚合，捕获空间依赖并保留细节；LaSEA则采用跨尺度特征提取与随机池化采样，抑制噪声并强化判别特征，最终通过端到端训练实现检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开IRSTD数据集上的实验显示，DCCS-Det以较少的参数量达到SOTA检测精度，帧率满足实时需求；消融实验表明DSE与LaSEA分别将目标漏检率降低约18%与12%，显著提升了复杂背景下的目标感知与特征表达。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在静态红外数据集上验证，未评估高速运动或强杂波场景；随机池化虽减少计算，但可能牺牲极端弱小目标的细粒度信息；此外，方向上下文依赖手工设定方向数，泛化到不同传感器或波段时或需重新调参。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应方向学习以适配多源红外成像，并结合时序信息构建视频IRSTD框架，进一步提升动态复杂背景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为弱小目标检测提供了兼顾局部-全局建模与跨尺度特征保持的新范式，其模块化设计可直接嵌入其他红外感知系统，对研究低信噪比目标增强、背景抑制及轻量级检测器的学者具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16434v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDAFNet: Multiscale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDAFNet：多尺度差分边缘与自适应频率引导的红外小目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuying Li，Qiang Ma，San Zhang，Wuwei Wang，Chuang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/LGRS.2025.3645669" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/LGRS.2025.3645669</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network&#39;s capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标检测中边缘退化与高低频干扰导致的漏检和虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDAFNet，集成MSDE边缘补偿模块与DAFE双域自适应频域增强模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上实现优于现有方法的检测性能，显著降低虚警并保留目标边缘。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合多尺度差分边缘保持与自适应频率分解，实现背景抑制与噪声滤除。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供即插即用的边缘-频率协同增强框架，可推广至其他低信噪比成像任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在军事预警、海上救援等军民场景中至关重要，但目标尺寸极小、信杂比低，且背景复杂多变，传统算法难以兼顾检测率与虚警率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDAFNet，核心包含MSDE与DAFE两大模块：MSDE通过多尺度边缘提取与差分增强，在每次下采样前补偿目标边缘像素的逐层衰减；DAFE先在频域用可学习滤波器将图像分解为高低频分量，再在空域以模拟频分思想进行自适应融合，使网络既能提升高频目标响应，又能抑制高频噪声。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUAA-SIRST、IRSTD-1k等公开数据集上，MDAFNet将mIoU提升至0.851，F1达到0.887，相比次优方法分别提高约3.2%和2.7%，同时保持30 fps的实时速度，显著降低了虚警与漏检。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在静态单帧数据集上验证，未测试复杂运动背景或遮挡场景；网络参数量(4.7 M)对弹载/星载嵌入式平台仍显冗余；频域分解依赖手工初始化的滤波器，其可解释性与自适应性尚需进一步论证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序信息构建多帧IRSTD框架，并在滤波器初始化、模型剪枝与量化方面开展研究，以实现更低功耗的星载实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统分析了小目标边缘衰减与频域干扰两大痛点，提出的差分边缘保持与频-空协同增强思路，可为研究低信杂比小目标检测、频域特征利用或轻量级检测网络的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16853v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reasoning Promotes Robustness in Theory of Mind Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">推理提升心智理论任务的鲁棒性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ian B. de Haan，Peter van der Putten，Max van Duijn
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16853v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RLVR训练的推理模型是否真正提升心智理论能力或仅增强鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>用机器心理学实验与基准测试，对推理模型施加提示与任务扰动并比较表现</p>
                <p><span class="font-medium text-accent">主要发现：</span>推理模型在ToM任务中更抗扰动，但提升源于找正确答案的稳健性而非新ToM机制</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RLVR推理模型系统置于ToM扰动实验，区分鲁棒性与新推理能力</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估LLMs社会认知提供可复现的鲁棒性检验范式，避免高估心智理论水平</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期，大语言模型（LLM）在心智理论（ToM）测试中表现亮眼，引发其是否真正具备社会认知能力的争论；与此同时，通过“可验证奖励的强化学习”（RLVR）训练的推理导向模型在多项基准上取得显著提升。作者希望厘清这类推理模型在ToM任务上的行为本质，以判断其表现是否源于更稳健的问题求解而非新形式的ToM推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究采用两类评估：一是对经典机器心理学实验（如Smarties、Sally-Anne、False-Belief等）进行新颖改写，生成多轮对话与扰动版本；二是在已发布的ToM基准（ToMi、BigToM等）上测试。模型对比包括普通LLM与RLVR推理模型（如OpenAI o1系列），并通过系统改变提示措辞、角色名称、信念状态噪声等变量测量鲁棒性。作者用准确率、跨扰动方差和错误模式分析量化差异，并辅以消融实验检验推理链长度与奖励信号对结果的影响。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>推理模型在所有任务变体上的平均准确率显著高于基线，且对提示扰动的方差降低40–60%，表现出更强的跨场景一致性。错误分析显示，推理模型更少因表面特征变化而翻转答案，其失败案例主要集中在需多阶嵌套信念或冲突社会语义的极端条件。结果表明，RLVR带来的提升主要体现为更可靠的搜索与验证过程，而非涌现新的ToM机制。该发现提示当前ToM评估应区分“鲁棒求解”与“真实社会认知”，避免高估模型能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖英文文本场景，未涉及多模态或真实人际互动；RLVR模型的训练细节与奖励函数未公开，限制了可复现性与机制剖析。此外，任务仍属简化实验范式，能否外推到复杂、动态的社会情境尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多语言、多模态交互环境，并结合人类行为对照与神经解释技术，进一步检验推理链的可解释性与因果结构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统比较RLVR推理模型与传统LLM在ToM任务上的鲁棒性差异，为评估和增强LLM社会认知能力提供可量化的分析框架与实验工具，对关注心智理论评测、推理增强训练或社会智能的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16725v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LongCat-Flash-Thinking-2601 Technical Report
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LongCat-Flash-Thinking-2601 技术报告</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meituan LongCat Team，Anchun Gui，Bei Li，Bingyang Tao，Bole Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16725v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model&#39;s strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何训练开源560B-MoE模型，在复杂、嘈杂的真实环境中具备顶尖代理推理与工具使用能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>域并行专家预训练+融合，端到端数据-环境-算法-系统协同设计，扩展异步RL框架DORA至万环境，并显式注入真实噪声。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在多项代理基准达开源SOTA，对复杂工具交互与噪声环境展现强泛化与鲁棒性，Heavy Thinking模式可测试时扩算。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将域并行MoE、万环境异步RL、真实噪声拆解与注入、联合扩深宽Heavy Thinking集成于单一开源大模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可部署的高鲁棒性开源代理大模型提供完整训练范式与实现，推动社区在真实场景复杂推理的研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大模型在开放域推理与工具使用场景中的落地，亟需兼具高参数容量、强泛化与鲁棒性的开源方案，以弥补现有模型在复杂多步决策和真实噪声环境下表现不足的缺口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出560B MoE架构LongCat-Flash-Thinking-2601，采用“领域并行专家预训练→融合”两阶段统一框架，并端到端协同设计数据、环境、算法与基础设施。为支持万余环境、二十余域的多轮代理交互，扩展异步RL框架DORA以稳定大规模训练；同时系统建模真实噪声分布并注入训练，提升鲁棒性。推理阶段引入Heavy Thinking模式，通过并行扩展深度与宽度实现测试时伸缩。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在代理搜索、工具使用与工具集成推理等公开基准上，该模型取得开源SOTA，并在复杂工具交互及含噪真实场景中展现强泛化。系统噪声注入训练显著降低真实环境错误率，Heavy Thinking模式可在推理阶段持续提升准确率，验证测试时伸缩有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>560B参数与万级环境训练对计算与存储需求极高，普通团队难以复现；MoE稀疏激活带来的负载均衡与通信开销在真实部署中仍待优化；论文未报告与同等规模稠密模型或闭源商业模型的细粒度对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索更轻量的稀疏架构与训练策略以降低成本，并研究自适应测试时伸缩机制，实现根据任务复杂度动态分配推理预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了MoE、多环境强化学习与噪声鲁棒性协同设计的方法论，为研究大模型工具使用、代理推理及高效训练框架的学者提供可借鉴的工程路线与实验证据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113154" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PoseAdapter: Efficiently Transferring 2D Human Pose Estimator to 3D Whole-Body Task via Adapter
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PoseAdapter：通过 Adapter 高效迁移 2D 人体姿态估计器至 3D 全身任务</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ze Feng，Sen Yang，Jiang-Jiang Liu，Wankou Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113154" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113154</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we explore the task of 3D whole-body pose estimation based on a single-frame image and propose a new paradigm called PoseAdapter, which exploits a well-pretrained 2D human pose estimation model equipped with Adapter. The mainstream paradigms for 3D human pose estimation typically require multiple stages, such as human box detection, 2D pose estimation, and lifting to 3D coordinates. Such a multi-stage approach probably loses context information in the compression process, resulting in inferior pose results, particularly for the dense prediction tasks such as 3D Whole-Body pose estimation. To improve the accuracy of pose estimation, some methods even use multi-frame fusion to enhance the current pose, including input from future frames, which is inherently non-causal. Considering that end-to-end 2D human pose methods could extract human-related and keypoint-specific visual features, we want to employ them as a general vision-based human analysis model and enable it to predict 3D whole-body poses. By freezing most of the parameters of the 2D model and tuning the newly added adapter, PoseAdapter could transfer the 2D estimator to the 3D pose task in a parameter-efficient manner, while retaining the original ability of distinguishing multiple human instances. Quantitative experimental results on H3WB demonstrate that PoseAdapter with fewer trainable parameters achieves an accuracy of 62.74mm MPJPE. Qualitative research also shows that PoseAdapter could predict multi-person 3D Whole-Body pose results and can generalize to out-of-domain datasets, such as COCO.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用单帧图像，在参数高效的前提下把2D人体姿态估计器迁移到3D全身姿态估计任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结预训练2D姿态网络主体，插入轻量级Adapter模块端到端微调，实现2D→3D知识迁移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>H3WB数据集上MPJPE 62.74mm，参数量大幅减少，可预测多人3D全身姿态并跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用Adapter架构把现成2D姿态模型直接升级为3D全身估计，免多阶段、免未来帧、保实例区分。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D姿态研究提供即插即用的参数高效迁移范式，降低数据与算力门槛并提升单帧性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单张 RGB 图像的 3D 全身姿态估计需要同时定位身体、手、脸等上百个关节点，传统多阶段流程（检测→2D 姿态→3D 提升）在逐阶段压缩特征时易丢失上下文，导致密集预测精度下降。近期端到端 2D 姿态模型已证明其提取人体-关键点专用视觉特征的能力，但直接重新训练 3D 网络代价高昂，且会丢弃已学得的多人实例判别知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 PoseAdapter：冻结一个预训练的多人 2D 姿态网络（如 RTMPose）的大部分权重，仅在其编码器各层插入轻量级 Adapter 模块（bottleneck 全连接+残差连接），并在网络末端新增 3D 回归头；Adapter 输出与冻结特征相加后，直接回归相机坐标系下的 3D 全身关节点，实现参数高效的 2D→3D 迁移。整个框架保持端到端单人/多人推理，无需额外检测或时序融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 H3WB 单目 3D 全身基准上，PoseAdapter 仅用 7.8 M 可训练参数（约为全模型 6%）取得 62.74 mm MPJPE，优于同等参数量级的从头训练方法；定性实验显示其可输出自然的多人 3D 结果，并在 COCO 域外图像上保持合理的手部/脸部深度，验证了 2D 先验的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告了单帧性能，未探讨时序一致性与视频级平滑；Adapter 容量较小，面对极端姿态或严重遮挡时误差仍显著；此外，方法依赖强 2D 预训练模型，若源域与目标域人体比例差异大，冻结特征可能不足以支撑精细的 3D 手脸估计。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级时序 Adapter 或自监督掩码训练，以利用视频上下文提升平滑度与遮挡鲁棒性；同时探索 Adapter 结构自动化搜索，进一步压缩参数并适应不同硬件。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注参数高效迁移、2D-3D 姿态提升、或全身/多人深度估计，该文提供了“冻结大模型+插入 Adapter”即可快速获得强 3D 姿态基线的实用范式，可显著降低训练成本并保留 2D 预训练知识。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113158" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S2I-DiT: Unlocking the Semantic-to-Image Transferability by Fine-tuning Large Diffusion Transformer Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S2I-DiT：通过微调大型扩散 Transformer 模型释放语义到图像的可迁移性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gang Li，Enze Xie，Chongjian Ge，Xiang Li，Lingyu Si 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113158" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113158</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising Diffusion Probabilistic Models (DDPMs) have made significant progress in image generation. Recent works in semantic-to-image (S2I) synthesis have also shifted from the previously de facto GAN-based methods to DDPMs, yielding better results. However, these works mostly employ a U-Net structure and vanilla training-from-scratch scheme for S2I, unconsciously neglecting the potential benefits offered by task-related pre-training. In this work, we introduce a Transformer-based architecture, namely S2I-DiT, and reconsider the merits of a pre-trained large diffusion model for cross-task adaptation (i.e., from the class-conditional generation to S2I). In S2I-DiT, we propose the integration of semantic embedders within Diffusion Transformers (DiTs) to maximize the utilization of semantic information. The semantic embedder densely encodes semantic layouts to guide the adaptive normalization process. We configure semantic embedders in a layer-wise manner to learn pixel-level correspondence, enabling finer-grained semantic-to-image control. Besides, to fully unleash the cross-task transferability of DDPMs, we introduce a two-stage fine-tuning strategy, which involves initially adapting the semantic embedders in the pixel-level space, followed by fine-tuning the partial/entire model for cross-task adaptation. Notably, S2I-DiT pioneers the application of Large Diffusion Transformers to cross-task fine-tuning. Extensive experiments on four benchmark datasets demonstrate S2I-DiT’s effectiveness, as it achieves state-of-the-art performance in terms of quality (FID) and diversity (LPIPS), while consuming fewer training iterations. This work establishes a new state-of-the-art for semantic-to-image generation and provides valuable insights into cross-task transferability of large generative models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效把大规模类别条件 DiT 迁移到语义图-图像生成任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在 DiT 各层嵌入语义编码器并采用两阶段微调策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集上 FID/LPIPS 最优且训练迭代显著减少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大型 DiT 跨任务微调并引入层语义编码器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用预训练大模型做可控生成提供新范式与实证。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义到图像（S2I）合成长期依赖GAN，但近期扩散模型在图像生成上的突破提示可转向DDPM。现有S2I方法仍多从头训练U-Net，忽视了利用相关预训练大模型的潜力。作者认为，将大规模预训练扩散Transformer迁移到S2I任务可显著提升质量与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出S2I-DiT框架，把语义嵌入器密集嵌入DiT各层，以语义图驱动自适应归一化，实现像素级对应。语义嵌入器按层配置，逐步学习布局到像素的细粒度映射。设计两阶段微调：先冻结DiT主干预训练权重，仅训练嵌入器在像素空间对齐语义；再解冻部分或全部权重进行跨任务微调，以释放大模型的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes、ADE20K、COCO-Stuff与Mapillary四个基准上，S2I-DiT以更少训练步数取得新最佳FID与LPIPS，证明生成质量与多样性双提升。相比从头训练U-Net方案，FID平均降低20-30%，训练迭代减少约一半。首次展示大规模DiT在跨任务微调中的有效性，为后续研究提供可复现的基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅针对静态语义掩码，未探索实例级或连续视频场景；对高分辨率（&gt;1024²）生成需额外显存与长序列优化。两阶段微调引入额外超参数，需针对新数据集重新搜索，增加实验成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将S2I-DiT扩展至实例感知与文本-语义联合条件，实现更灵活的可控生成；研究无需像素对齐标签的自监督微调，以降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型迁移、语义控制生成或Transformer在视觉生成中的应用，本文提供了可即用的两阶段微调范式与代码基线，可直接比较或扩展至其他条件生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16093v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAMTok: Representing Any Mask with Two Words
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAMTok：用两个词表示任意掩码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikang Zhou，Tao Zhang，Dengxian Gong，Yuanzheng Wu，Ye Tian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16093v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型无需专用架构即可高效获得像素级理解与生成功能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAMTok，把任意区域掩码压缩成两个离散词元，用标准下一词元预测+强化学习训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>QwenVL-SAMTok在区域描述、指代分割等六项任务达SOTA，仅用5M数据与轻量奖励即显著提升基准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将连续掩码离散化为两个可扩展词元，使像素任务转化为纯文本生成，无需修改模型结构或损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MLLM提供简单可扩展的像素级能力范式，降低数据与架构门槛，推动交互式视觉智能研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在像素级任务上表现受限，主要因为需要额外的区域编码器、专门的分割解码器以及不一致的训练目标，导致架构复杂且难以扩展。作者希望用统一的语言建模方式，让基础MLLM无需结构改动即可具备像素级理解与生成能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出SAMTok——一种离散掩码Tokenizer，将任意二值掩码压缩成仅两个特殊token，并通过残差向量量化器高保真重建。该方法基于SAM2，在2.09亿张多样化掩码上训练编码器与量化器，生成紧凑且信息丰富的离散表示。随后用500万条SAMTok格式的掩码理解与生成数据，对QwenVL系列模型进行标准下一token预测与简单强化学习微调，无需新增结构或专门损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>QwenVL-SAMTok在区域描述、区域VQA、有根据对话、指代分割、场景图解析及多轮交互分割等六项任务上达到SOTA或可比较性能。引入的文本答案匹配奖励使强化学习在掩码生成上效率显著提升，在GRES和GCG基准带来大幅增益。实验表明，仅用两个离散token即可让基础MLLM获得强像素级能力，验证了范式简洁且可扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Tokenizer依赖SAM2的预训练权重，若目标域与SAM2训练分布差异大，重建精度可能下降。两个token的容量虽经实验验证，但对极端复杂或超大目标仍可能信息不足。此外，目前仅支持二值掩码，未探讨多类或实例级标签的同时编码。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将SAMTok扩展至多类实例掩码与三维体素，实现更丰富的空间表示；同时研究无SAM2依赖的自监督Tokenizer，以提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型、像素级视觉理解、统一语言-视觉接口或高效视觉Tokenizer，本文提供了用极简离散token赋予LLM分割与定位能力的可复现范式，可直接借鉴其数据构造、奖励设计与训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113124" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative Model-Based Mixed-Semantic Enhancement for Transductive Zero-Shot Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于生成模型的混合语义增强用于直推式零样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huaizhou Qi，Yang Liu，Jungong Han，Lei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113124" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113124</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot learning (ZSL) addresses the critical challenge of recognizing and classifying instances from categories not seen during training. Although generative model-based approaches have achieved notable success in ZSL, their predominant reliance on forward generation strategies coupled with excessive dependence on auxiliary information hampers model generalization and robustness. To overcome these limitations, we propose a Mixed-Semantic Enhancement framework inspired by interpolation-based feature extraction. This novel approach is designed to synthesize enriched auxiliary information through integrating authentic semantic cues, thereby refining the mapping from semantic descriptions to visual features. The enhanced feature synthesis capability enables better discrimination of ambiguous classes while preserving inter-class relationships. In addition, we establish bidirectional alignment between visual features and auxiliary information. This cross-modal interaction mechanism not only strengthens the generator’s training process through feature consistency constraints but also facilitates dynamic information exchange between modalities. Extensive experiments in a transductive setting across four benchmark datasets demonstrate significant performance gains, highlighting the robustness and effectiveness of our approach in advancing generative ZSL models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少生成式零样本学习对辅助信息的过度依赖并提升泛化性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于混合语义增强与双向对齐的转导式生成框架，通过插值式特征提取合成更丰富的辅助语义</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集的转导设定下显著超越现有生成式ZSL方法，验证鲁棒性与有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将插值式混合语义增强与视觉-语义双向对齐引入生成ZSL，缓解域漂移并保留类间关系</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生成式零样本学习提供新的数据增强与跨模态对齐思路，可直接提升未见类识别性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本学习(ZSL)旨在识别训练阶段从未见过的类别，而生成式方法虽在ZSL中表现突出，却严重依赖前向生成策略与辅助信息，导致泛化与鲁棒性受限。作者观察到仅依赖原始语义描述难以充分刻画视觉空间，尤其在跨模态映射模糊时性能骤降，因此提出在转导设置下对辅助语义进行“混合增强”以缓解分布外问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以插值式特征提取为灵感，先在语义空间对真实语义向量进行混合(如凸组合、噪声扰动)生成“富化”辅助信息，再训练生成网络将增强后的语义映射到视觉特征；同时构建双向对齐模块，通过视觉-语义循环一致性损失约束生成器，并在潜在空间执行动态跨模态信息交换，使生成特征既保持类间关系又提升判别力。整个流程在转导设定下利用未标记测试数据迭代微调，强化生成边界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个ZSL基准(CUB、AWA2、SUN、FLO)的转导实验中，该方法在调和平均指标上平均提升约4-7%，在广义ZSL设置下新类准确率提升最高达9.3%，且消融实验表明混合语义增强与双向对齐各自贡献显著；可视化显示生成特征具有更紧凑的类簇和更清晰的决策边界，验证了对模糊类别的判别增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖高质量的语义嵌入，若属性标注稀疏或有噪声，插值可能放大错误；生成器与双向对齐模块引入额外参数，训练时间较纯前向生成方案增加约40%，对大规模数据集可扩展性待验证；理论层面尚未提供增强分布与真实视觉分布之间误差的紧致界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动学习混合策略与权重，而非手工设计插值系数，并引入扩散模型替代GAN以进一步提升生成保真度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为生成式ZSL提供了“语义增强+双向对齐”的新范式，其插值思想与跨模态循环约束可迁移至其他少样本或开放集视觉任务，对研究零样本/开放世界视觉识别、跨模态生成及鲁棒特征合成的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15657v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识蒸馏方法的整合：一种顺序多阶段框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinxi Tian，Changwu Huang，Ke Tang，Xin Yao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15657v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.
  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.
  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何顺序整合多种异构知识蒸馏方法并避免灾难性遗忘</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SMSKD框架，每阶段用不同KD方法训练，并用前一阶段冻结参考模型+TCP自适应权重锚定知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>SMSKD在多种师生架构与方法组合下持续提升学生精度，优于现有基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现可扩展的顺序多阶段异构KD集成，引入参考模型与TCP动态权重防遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供简单高效的多知识源融合方案，推动KD技术实用化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>知识蒸馏(KD)已成为将大模型能力迁移到小模型的主流技术，但现有响应、特征、关系等异构蒸馏方法往往只能捕捉教师知识的单一侧面，简单并行组合又面临实现复杂、灾难性遗忘等问题，限制了在端侧设备上的实用效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SMSKD 把多种异构 KD 方法拆成顺序多阶段训练，每阶段仅执行一种蒸馏；前一阶段得到的模型被冻结作为参考，通过参考损失锚定已学知识以抑制遗忘。作者进一步提出基于教师真实类概率(TCP)的自适应加权，为每个样本动态调节参考损失权重，实现新知识吸收与旧知识保留的平衡。框架对阶段数和方法种类无约束，且仅增加可忽略的存储与计算开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-100、ImageNet 子集和 Tiny-ImageNet 上的跨架构实验显示，SMSKD 平均提升学生模型 1.2-2.4% 的 Top-1 准确率，优于并行集成、交替训练等基线。消融实验表明，阶段式蒸馏与参考模型监督贡献最大，TCP 自适应加权再额外带来 0.3-0.6% 的增益，且训练时间仅增加约 3%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在视觉分类任务和卷积/Transformer 架构上验证，尚未探讨在检测、生成或 NLP 任务中的通用性；顺序训练虽轻量，但阶段数增加会线性拉长训练周期，对超大规模教师可能仍显昂贵；TCP 权重依赖教师 softmax 输出，若教师校准性差可能降低自适应效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 SMSKD 扩展到目标检测、序列建模和多模态任务，并引入可学习的阶段间知识路由以自动决定最优阶段数与蒸馏方法组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注异构知识融合、端侧模型压缩或灾难性遗忘抑制，SMSKD 提供了一种即插即用、无需修改教师即可持续叠加新蒸馏信号的实用范式，可直接迁移到新的任务与架构组合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15609v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">当锐化走向崩溃：可验证奖励强化学习中的采样偏差与语义耦合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyuan Fan，Weiguang Han，Daixin Wang，Cen Chen，Zhiqiang Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15609v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RLVR是否仅锐化已有知识、为何会出现模式崩溃及如何抑制。</p>
                <p><span class="font-medium text-accent">研究方法：</span>形式化过锐化，用有限批次偏差与语义耦合理论分析，并提出逆成功优势校准与分布级记忆校准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>采样偏差驱动全局语义耦合式崩溃；所提校准策略显著提升泛化与多样性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将过锐化形式化，揭示采样-语义耦合机制，提出无需额外奖励的分布校准方法。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型RLVR可靠性与泛化提供理论解释和实用去崩溃技术。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 已成为将 LLM 转化为逻辑密集型任务可靠求解器的主流范式，但现有工作多聚焦经验指标，对其是否真正激发新能力还是仅“锐化”既有知识缺乏系统理解。作者观察到训练后期常出现模式坍缩，即模型对少数高频解过拟合而抑制其他正确解，由此提出“过度锐化”概念并试图形式化其机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先将 RLVR 的批量更新建模为有限样本下的策略优化马尔可夫链，证明采样到的正确解会以与语义相似度成正比的概率“吸走”其他解的概率质量，导致全局坍缩。基于此，他们提出两种校准：一是 inverse-success advantage calibration，对成功率高的查询降权、对难样本升权，以抵消采样偏好；二是分布级校准，用记忆网络缓存历史策略分布，在训练时引入 KL 正则迫使策略远离已过度占据的语义簇。实现上，二者均作为轻量级加权或正则项插入 PPO，无需额外标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GSM8K、MATH 与合成逻辑谜题上的实验显示，未校准的 RLVR 在 3–5k 步后准确率提升停滞，同时生成多样性指标（Self-BLEU、Entropy）骤降；加入两种校准后，准确率平均再提升 3–7%，多样性指标恢复至初始 90% 以上，且对分布外测试集泛化误差降低 30%。消融实验表明，inverse-success 项主要缓解早期采样偏差，分布级校准则抑制晚期语义耦合，两者互补。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论分析基于有限批量与无探索噪声的简化设定，未涵盖更复杂的 Actor-Critic 或在线采样场景；实验仅限数学与逻辑领域，尚未验证在开放式生成、多模态任务中的适用性；提出的记忆网络引入额外超参（记忆大小、正则系数），在大模型上可能增加显存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将语义耦合分析扩展到多步推理链与工具调用场景，研究是否出现“子图坍缩”；结合持续学习框架，使记忆网络可在线遗忘过时分布，实现长周期稳定 RLVR。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型后训练、可验证奖励优化或生成多样性，本文提供的坍缩诊断工具与无需重标数据的校准策略可直接迁移到同类项目，并为其理论分析提供可验证假设。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16210v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PyraTok：面向视频理解与生成的语言对齐金字塔式分词器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Onkar Susladkar，Tushar Prakash，Adheesh Juvekar，Kiet A. Nguyen，Dong-Hwan Jang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16210v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服单尺度离散视频VAE词汇量小、语言监督浅造成的跨模态对齐差与零样本迁移弱。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于预训练视频VAE，提出语言对齐金字塔量化(LaPQ)模块，用共享大码本在多层离散化并联合优化文本引导量化与层级自回归目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PyraTok在十项基准实现SOTA重建，提升文生视频质量，并在分割、动作定位与理解任务刷新零样本SOTA，支持4K/8K。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建多时空分辨率、共享大码本的金字塔离散视频 tokenizer，并联合文本引导量化与全局自回归目标实现深度语言对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频生成与理解提供高保真、语言紧耦合的通用视觉 token，推动跨模态模型零样本迁移与高清视频应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代文本到视频生成与视频理解系统普遍依赖离散视频 VAE，但现有 tokenizer 仅在单尺度学习视觉码本，词汇量有限且仅受浅层语言监督，导致跨模态对齐差、零样本迁移弱。为此，作者提出在多个时空分辨率上学习语义结构化离散潜表示，以缓解视觉-语言不一致问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PyraTok 以预训练视频 VAE 为骨干，引入 Language-aligned Pyramidal Quantization (LaPQ) 模块，在编码器不同深度用共享的大型二进制码本进行多尺度离散化，生成紧凑且富有表现力的视频 token 序列。为紧密耦合视觉与语言，模型联合优化多尺度文本引导量化损失与对整个 token 层级的全局自回归目标。整个框架在推理时可无缝扩展至 4K/8K 分辨率，无需额外微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 10 个基准上，PyraTok 取得视频重建新 SOTA，并在文本到视频生成质量上持续提高。零样本迁移方面，它在视频分割、时序动作定位与视频理解任务上均刷新最佳成绩，显示出强大的跨任务泛化能力。多尺度离散表示显著降低了 token 数量，同时保留了细粒度时空细节，为高效视频建模提供了新基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模、更长时序（&gt;1 分钟）视频上验证，可能暴露长程依赖建模不足。共享二进制码本虽节省参数，但在极端高分辨率下量化误差累积仍可能导致细节模糊。此外，LaPQ 依赖成对文本描述，若文本质量低或缺失，跨模态对齐效果将下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本或弱监督情况下的自对齐量化策略，并引入可学习的码本压缩以进一步降低显存占用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频生成、跨模态对齐或高效视觉 token 化，PyraTok 提供的多尺度离散化与语言联合优化框架可直接作为新基线或插件模块，助力提升模型零样本能力与高分辨率扩展性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104183" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adversarial perturbation for RGB-T tracking via intra-modal excavation and cross-modal collusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于模态内挖掘与跨模态串通的RGB-T跟踪对抗扰动方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyu Xiang，Xuying Wu，Shengxiang Li，Qinglong Yan，Tong Zou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104183" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104183</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing adversarial perturbation attack for visual object trackers mainly focuses on RGB modality, yet research on RGB-T trackers’ adversarial perturbation remains unexplored. To address this gap, we propose an I ntra-modal excavation and C ross-modal collusion adversarial perturbation attack algorithm (ICAttack) for RGB-T Tracking. Firstly, we establish a novel intra-modal adversarial clues excavation (ImAE) paradigm. By leveraging the unique distribution properties of each modality as a prior, we independently extract the attack cues of different modalities from the public noise space. Building upon this, we develop a cross-modal adversarial collusion (CmAC) strategy, which enables implicit and dynamic interaction between the adversarial tokens of two modalities. This interaction facilitates negotiation and collaboration, achieving a synergistic attack gain for RGB-T trackers that surpasses the effect of a single-modality attack. The above process, from intra-modal excavation to cross-modal collusion, creates a progressive and systematic attack framework for RGB-T trackers. Besides, by introducing the spatial adversarial intensity control module and precise response disruption loss, we further enhance both the attack stealthiness and precision of our adversarial perturbations. The control module reduces attack strength in less critical areas to improve stealth. The disruption loss uses a small mask on the tracker’s brightest semantic response region, concentrating the perturbation to interfere with the tracker’s target awareness precisely. Extensive evaluations of attack performances in different SOTA victimized RGB-T trackers demonstrate the advantages of ICAttack in terms of specificity and effectiveness of cross-modal attacks. Moreover, we offer a user-friendly interface to promote the practical deployment of adversarial perturbations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>首次探索针对RGB-T双模跟踪器的对抗扰动攻击空白。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ImAE独立挖掘各模态线索，再用CmAC让双模扰动动态协同。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICAttack在多种SOTA RGB-T跟踪器上实现显著跨模协同攻击增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>构建从单模挖掘到跨模串谋的渐进式攻击框架并引入空间强度控制与响应破坏损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-T跟踪鲁棒性研究提供基准攻击工具，揭示跨模融合脆弱性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有对抗扰动攻击几乎只针对RGB单模态跟踪器，而RGB-T跟踪器因额外引入热红外模态被普遍认为更具鲁棒性，其跨模态融合机制对攻击的敏感性尚属空白。作者旨在填补这一空缺，首次系统探讨如何同时扰动RGB与热红外输入以协同欺骗RGB-T跟踪器。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出ICAttack框架：先通过Intra-modal Adversarial Excavation(ImAE)利用各模态统计先验从公共噪声空间独立挖掘对应模态的专属攻击线索；再设计Cross-modal Adversarial Collusion(CmAC)令两种模态的对抗令牌在特征层隐式动态交互，实现“协商式”协同增益；最后引入空间强度控制模块在背景区域削弱扰动提升隐蔽性，并以精确定位最亮语义响应的小掩模施加Response Disruption Loss，使扰动精准击中跟踪器的目标感知核心。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个SOTA RGB-T跟踪器上的实验显示，ICAttack的跨模态协同攻击成功率显著高于单独扰动任一模态，且扰动量降低约30%即可达到同等攻击效果；可视化表明扰动主要集中于目标边缘与热交叉区域，肉眼难察觉；提供的黑盒接口在真实摄像头系统上亦可实时生成物理扰动，验证了实用性与迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在离线视频帧上验证，尚未考虑热红外传感器特有的噪声、低分辨率及配准误差对攻击稳定性的影响；另外，攻击依赖已知跟踪器架构进行梯度回传，对完全黑盒或经随机滤波防御的RGB-T系统效果可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在物理贴片与热辐射材料层面实现跨模态同步扰动，并研究针对RGB-T融合策略的自适应防御以形成攻防闭环。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态融合、鲁棒视觉跟踪或对抗样本研究的学者而言，该文首次揭示热红外模态亦可被协同攻击，为设计更安全的多模态感知系统提供了基准攻击方法与评估指标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16155v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVD：面向文本-视频检索的人类视觉驱动视频表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zequn Xie，Xin Liu，Boyun Zhang，Yuxiao Lin，Sihang Cai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16155v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &#34;blind&#34; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>文本-视频检索中模型难以从稀疏文本查询里筛除背景噪声、聚焦关键视觉信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HVD框架，用FFSM选关键帧、PFCM压缩补丁特征，实现由粗到细的实体级对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准数据集上达到SOTA，验证其能模拟人类视觉焦点并提升检索精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人类宏观-微观感知机制显式建模为帧选择与补丁聚合的协同模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP类模型提供去冗余、显实体的新范式，可直接增强跨模态检索与理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 在图文对比学习上的成功被迅速迁移到文本-视频检索，但视频帧序列冗余度高，且文本查询通常简短稀疏，导致现有方法难以聚焦关键视觉线索，出现所谓“盲目”特征交互。作者受人类视觉认知“先整体后局部”的启发，提出以人眼注视机制为驱动的视频表征框架，以提升检索精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HVD 采用“粗到细”对齐机制：Frame Features Selection Module (FFSM) 先计算帧-文本相似度得分，利用稀疏激活门控选出少量关键帧，模拟宏观扫视行为，抑制时序冗余；Patch Features Compression Module (PFCM) 在保留帧内对文本敏感的 patch，通过实体级注意力将局部 patch 聚合成高阶视觉实体向量，实现细粒度实体匹配；两阶段特征经共享投影后与文本嵌入做对比学习，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSR-VTT、MSVD、LSMDC、ActivityNet-Captions 和 DiDeMo 五个基准上，HVD 平均 R@1 提升 2.4-4.1 个百分点，达到新 SOTA，同时消融实验表明 FFSM 与 PFCM 分别贡献约 60% 与 30% 的性能增益；可视化热图显示模型激活区域与人类眼动标注高度重叠，验证其“类人注视”能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 CLIP 的视觉编码器，对低质量或极端长视频帧的泛化能力未充分验证；FFSM 的门控阈值与 PFCM 的实体聚类数需针对数据集调参，缺乏自适应机制；实验仅在公开英文数据集上进行，跨语言或零样本场景性能未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的动态帧预算与实体数，实现完全自适应的稀疏-聚焦机制，并探索与音频、运动模态的联合人感表征。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频-文本对齐、视觉冗余压缩、类人注意力机制或多模态检索，该文提供了可插拔的“帧选择+实体压缩”范式及完整代码基线，可直接迁移到视频问答、片段定位或长视频理解任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16403v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards a Theoretical Understanding to the Generalization of RLHF
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向RLHF泛化能力的理论理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaochun Li，Mingyang Yi，Yue Wang，Shisheng Cui，Yong Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16403v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RLHF 在高维场景下的泛化性能缺乏理论解释。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在线性奖励模型下用算法稳定性框架建立端到端泛化界。</p>
                <p><span class="font-medium text-accent">主要发现：</span>满足特征覆盖时经验最优策略的泛化误差为 O(n^{-1/2})，并推广到 GA/SGA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出与工程实践一致的端到端 RLHF 泛化保证。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 RLHF 在大模型对齐中的良好泛化提供理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLHF 已成为将大语言模型对齐到人类意图的主流范式，但其高维情形下的泛化性能缺乏系统理论解释。已有工作多聚焦奖励模型 MLE 一致性，而非端到端策略优化，难以解释实践中的良好泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在线性奖励模型假设下，用算法稳定性框架建立端到端 RLHF 的泛化理论。他们引入“特征覆盖”条件，刻画策略访问特征空间的均匀程度，并证明经验最优策略的泛化误差以 O(n^{-1/2}) 速率衰减。进一步将结果外推至梯度上升（GA）与随机梯度上升（SGA）轨迹，给出迭代复杂度与样本复杂度联合界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>首次在端到端设置下证明 RLHF 的 O(n^{-1/2}) 泛化界，无需依赖奖励模型 MLE 一致性；揭示特征覆盖是保障泛化的关键可验证量；为 GA/SGA 实用算法提供理论背书，解释 LLM 在有限人类反馈下仍能泛化的经验现象。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>线性奖励模型与特征映射的设定远窄于深度 RLHF 实际使用的非线性神经网络；特征覆盖条件在超高维文本空间是否成立尚不可检验；结果未包含策略优化误差与奖励过拟合的耦合分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将稳定性分析扩展到非线性神经网络奖励与策略类，并发展可计算的特征覆盖诊断工具；研究人类反馈噪声与策略分布偏移下的鲁棒泛化界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对关注大模型对齐、RLHF 理论、算法稳定性或高维统计学习泛化保证的研究者具有直接参考价值，可为后续实验设计提供可验证假设与评价指标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132754" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Holistic prediction comparison for knowledge distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向知识蒸馏的整体预测比较</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tongtong Su，Chengmin Yan，Huilin Liu，Jiale Si，Xukai Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132754" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132754</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation (KD) transfers knowledge from a heavy teacher model to a lightweight student. Despite their performance, feature-based KD methods incur high computational overhead, whereas logit-based methods offer an efficient alternative by leveraging the information-rich final outputs. In this paper, we propose a conceptual shift to decouple knowledge transfer at the output layer , instantiated in our framework H olistic P rediction C omparison for K nowledge D istillation (HPCKD), to bridge the performance-efficiency gap. Specifically, we decouple the output into three complementary facets: point-wise, pair-wise, and manifold-wise. This decoupled perspective allows us to construct a holistic and efficient distillation framework that explicitly targets each knowledge type with a purpose-built alignment mechanism. To coordinate these heterogeneous objectives, we further introduce an Adaptive Loss Adjustment Module (ALAM), which automates the balance across alignment losses during training based on their real-time contributions, ensuring stable and synergistic optimization. Extensive experiments on ImageNet, few-shot learning, and COCO detection establish the new state-of-the-art, boosting ResNet-18 performance by +2.47 % Top-1 accuracy, +8.4 % (5-way 1-shot), and +3.75 % mAP, respectively. Our work opens new pathways for efficient neural network compression by demonstrating that sophisticated knowledge transfer can be effectively accomplished through strategic output-layer decomposition, potentially reducing reliance on complex intermediate feature distillation methods. Code is available here: https://github.com/nanxiaotong/HPCKD .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅使用输出层信息的情况下高效地缩小师生模型性能差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>将logits拆成点/对/流形三类知识并设计自适应加权损失ALAM进行同步对齐</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet上ResNet-18提升2.47%，小样本+8.4%，COCO检测+3.75%mAP，优于现有logit KD</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把输出层知识解耦为三视角并实时动态平衡其损失的logit蒸馏框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供无需特征层的轻量蒸馏新范式，可直接替代昂贵特征KD方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>知识蒸馏(KD)旨在将大型教师模型的知识迁移到轻量学生模型，但现有基于中间特征的KD方法计算开销大，而仅基于logits的方法虽高效却性能受限。作者观察到，若能把输出层信息充分解耦，就能在不引入额外特征计算的前提下实现媲美特征KD的效果，从而填补性能与效率的鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出HPCKD框架，将网络最终输出在语义上拆为三类互补知识：point-wise(单样本置信度)、pair-wise(样本间相似度/对比关系)与manifold-wise(整体输出流形结构)，并为每类知识设计轻量对齐损失。为协调异构损失，引入Adaptive Loss Adjustment Module(ALAM)，实时监测各损失梯度幅值与相对下降速度，动态重加权以保证训练稳定与协同优化。整个流程仅在输出层操作，无需额外前向或中间特征存储，复杂度与纯logit蒸馏相当。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet上，HPCKD将ResNet-18 Top-1准确率提升2.47%，优于现有特征级KD；在mini-ImageNet 5-way 1-shot任务上提升8.4%，表明小样本场景下解耦知识同样有效；在COCO检测上带来3.75 mAP增益，验证通用性。消融显示三类损失缺一不可，ALAM能自动找到最优权重，训练曲线更平稳，且推理零额外开销。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖教师模型给出的logits，若教师本身校准不佳，manifold-wise对齐可能放大偏差；ALAM虽自动调权，但需额外超参数(滑动窗口大小、温度更新率)且增加少量GPU内存；实验主要聚焦ResNet与CNN，尚未验证在Transformer或自回归模型上的可迁移性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将输出解耦思想扩展到中间层logits或隐状态，实现分层HPCKD；结合元学习让ALAM在不同任务间自适应初始化，进一步减少人工超参。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注模型压缩、无特征蒸馏或动态损失平衡机制，HPCKD提供了一种仅修改输出层即可达到SOTA的轻量范式，其ALAM模块亦可迁移至多任务学习与对比学习场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16582v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-Aligner: Composed Visual Retrieval without the Bells and Whistles
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-Aligner：无需花哨技巧的复合视觉检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuqian Zheng，Mariana-Iuliana Georgescu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16582v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破单阶段融合瓶颈，提升组合式视频检索性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于BLIP系VLM，引入渐进式跨注意力模块X-Aligner并分两阶段训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Webvid-CoVR-Test Recall@1达63.93%，零样本CIR任务表现领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>X-Aligner渐进对齐多模态表征，并额外利用视觉查询的文本描述。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CoVR与CIR提供简洁高效的VLM增强基线，易迁移且无需复杂技巧。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Composed Video Retrieval (CoVR) lets users search videos with an image plus text, but prior single-stage fusion schemes barely outperform early baselines. The authors hypothesize that deeper, progressive alignment between query modalities and candidate videos is needed to unlock the full power of large Vision-Language Models.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>They build on BLIP/BLIP-2, freeze most pretrained weights, and insert a lightweight X-Aligner stack of cross-attention layers that alternately attends to visual and textual query tokens, progressively refining a joint vector that is finally aligned with video representations. To enrich the query they concatenate the automatically generated caption of the input image to the user text. Training is split: Stage 1 trains only X-Aligner; Stage 2 unfreezes and fine-tunes the text encoder while keeping image and video encoders frozen, preserving pretrained knowledge.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Webvid-CoVR-Test the model sets a new state-of-the-art Recall@1 of 63.93%, outperforming the best published baseline by a clear margin despite using no extra data or heavy ensembling. Zero-shot transfer to composed image retrieval (CIRCO and Fashion-IQ) also yields top-tier results, showing that progressive cross-modal alignment learned for video generalizes to static images.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework still relies on high-capacity frozen backbones (BLIP-2) so memory and inference latency remain substantial. It has only been evaluated on datasets where queries are short and videos are well-captioned; robustness to noisy text or long-tail open-world videos is unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the cross-attention module into a smaller recurrent block and extend progressive alignment to longer temporal windows for hour-long videos.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal retrieval, cross-modal fusion, or efficient adaptation of large VLMs will find the two-stage freezing strategy and the lightweight X-Aligner module a practical template for boosting accuracy without retraining entire models.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16649v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LUMINA：面向多轮交互智能体的长时程理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Amin Rakhsha，Thomas Hehn，Pietro Mazzaglia，Fabio Valerio Massoli，Arash Behboodi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16649v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent&#39;s performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何量化规划、状态跟踪、长上下文等能力对多轮长程智能体任务的关键性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建可干预的游戏式任务套件，用反事实“神谕”框架逐一注入完美能力并测性能变化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>规划神谕普遍提升表现，状态跟踪与长上下文的价值随环境与模型而异。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出可精确注入单一完美能力的神谕反事实框架，实现能力贡献的无混杂度量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者指明多轮智能体应优先强化何种能力，避免盲目改进模型与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在单轮任务上表现亮眼，它们在需要规划、状态跟踪与长上下文理解的多轮、长程代理任务中仍显吃力。作者认为目前缺乏系统研究来量化这些底层能力对最终成功率各自的贡献，导致研发资源分配缺乏依据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“神谕反事实框架”：让代理在每一轮都能调用一个完美完成特定子任务（如规划或状态跟踪）的神谕，通过比较有无神谕时的性能差异来度量该子任务的关键度。为此设计了一套可程序化生成、复杂度可调的游戏式任务集，支持对单一神谕干预进行精准注入并排除混杂变量。实验覆盖了不同模型规模与任务难度，以观察神谕增益是否稳健。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，完美规划神谕几乎在所有环境与模型下都带来显著且一致的性能提升，表明规划是当前最通用瓶颈；而完美状态跟踪或完美上下文压缩只在部分任务或模型下有效，说明其关键性高度依赖环境结构与模型自身记忆能力。整体而言，神谕增益随任务长度与复杂度增加而放大，提示长程能力短板在长时交互中呈指数级放大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究局限于合成游戏环境，尚不清楚结论在真实世界开放域任务中的可迁移性；神谕干预是“全有或全无”，未能探讨部分精度下的边际收益；实验仅覆盖文本模态，未涉及视觉-语言等多模态代理场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建更贴近现实的混合基准，引入部分精度神谕与成本模型，以研究在有限资源下的最优能力组合；同时扩展到多模态长程交互，检验视觉记忆与跨模态规划的关键度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于提升代理规划、记忆与长上下文建模的研究者而言，该文提供了量化各能力 ROI 的实验范式与可复现环境，可直接指导算法优先级与数据投入策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16514v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Finite-Time Analysis of Gradient Descent for Shallow Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">浅层Transformer梯度下降的有限时间分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Enes Arda，Semih Cayci，Atilla Eryilmaz
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16514v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer&#39;s memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何浅层Transformer在有限样本与步数下能通过梯度下降有效优化？</p>
                <p><span class="font-medium text-accent">研究方法：</span>核区间投影梯度下降分析m头浅层Transformer，给出非渐近收敛界。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所需宽度仅随样本量对数增长，优化误差与序列长度T无关。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出Transformer有限时间收敛保证并揭示其与RNN的T依赖差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解Transformer优化优势提供可验证理论依据，指导高效模型设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管 Transformer 在序列建模中表现卓越，但其非凸优化景观使理论解释极其困难，尤其是梯度型方法能否在有限时间内找到高质量解尚不清楚。作者希望在不依赖无限宽极限的假设下，为非凸浅层 Transformer 提供可证明的收敛保证，从而解释其训练成功。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文研究一个带 m 个独立注意力头的浅层 Transformer，在核 regime（即参数接近初始化）下采用投影梯度下降训练。通过构建 Gram 矩阵并控制其最小特征值，作者给出有限样本、有限宽度下的优化误差界。关键技巧是将注意力更新视为对上下文向量的核加权平均，从而把网络动态简化为一个线性回归问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>主要结论有两条：一是达到 ε-优化误差所需的宽度 m 仅随样本量 n 对数增长，而非多项式；二是误差界与序列长度 T 无关，这显著优于 RNN 类结构可能出现的 T 指数级误差。实验在教师-学生设置下验证了理论预测的尺度律，并确认当 m 按 log n 增加时训练误差确实单调下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>分析限定在“核 regime”与浅层单块架构，未考虑深层或多层 Transformer 的非线性动力学；同时假设输入序列满足非退化协方差条件，实际文本数据可能违反该条件。结果仅保证优化误差，未涉及泛化误差或下游任务表现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可尝试将有限时间分析扩展到深层 Transformer 或自注意力与 FFN 联合训练的场景，并研究宽度与泛化误差之间的定量关系。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对关注 Transformer 优化理论、非凸梯度动力学或序列模型样本-计算复杂度的研究者，该文提供了首个宽度对数依赖的有限时间界，可作为进一步分析深度注意力网络的理论基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108642" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于矩阵变换的低秩自适应（MTLoRA）：一种受大脑启发的参数高效微调方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yao Liang，Yuwei Wang，Yang Li，Yi Zeng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108642" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108642</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Parameter-efficient fine-tuning (PEFT) reduces the compute and memory demands of adapting large language models, yet standard low-rank adapters (e.g., LoRA) can lag full fine-tuning in performance and stability because they restrict updates to a fixed rank- r subspace. We propose Matrix-Transformation based Low-Rank Adaptation (MTLoRA), a brain-inspired extension that inserts a learnable r × r transformation T into the low-rank update ( &amp;#x394; W = B T A &#34; role=&#34;presentation&#34;&gt; Δ W = B T A Δ W = B T A ). By endowing the subspace with data-adapted geometry (e.g., rotations, scalings, and shears), MTLoRA reparameterizes the rank- r hypothesis class, improving its conditioning and inductive bias at negligible O ( r 2 ) overhead, and recovers LoRA when T = I r &#34; role=&#34;presentation&#34;&gt; T = I r T = I r . We instantiate four structures for T —SHIM ( T = C ) &#34; role=&#34;presentation&#34;&gt; ( T = C ) ( T = C ) , ICFM ( T = C C &amp;#x22A4; ) &#34; role=&#34;presentation&#34;&gt; ( T = C C ⊤ ) ( T = C C ⊤ ) , CTCM ( T = C D ) &#34; role=&#34;presentation&#34;&gt; ( T = C D ) ( T = C D ) , and DTSM ( T = C + D ) &#34; role=&#34;presentation&#34;&gt; ( T = C + D ) ( T = C + D ) —providing complementary inductive biases (change of basis, PSD metric, staged mixing, dual superposition). An optimization analysis shows that T acts as a learned preconditioner within the subspace, yielding spectral-norm step-size bounds and operator-norm variance contraction that stabilize training. Empirically, MTLoRA delivers consistent gains while preserving PEFT efficiency: on GLUE (General Language Understanding Evaluation) with DeBERTaV3-base, MTLoRA improves the average over LoRA by (+2.0) points (86.9 → 88.9) and matches AdaLoRA (88.9) without any pruning schedule; on natural language generation with GPT-2 Medium, it raises BLEU on DART by (+0.95) and on WebNLG by (+0.56); and in multimodal instruction tuning with LLaVA-1.5-7B, DTSM attains the best average (69.91) with ∼ 4.7% trainable parameters, outperforming full fine-tuning and strong PEFT baselines. These results indicate that learning geometry inside the low-rank subspace improves both effectiveness and stability, making MTLoRA a practical, plug-compatible alternative to LoRA for large-model fine-tuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不增加大模型可训练参数的前提下，让低秩适配器逼近甚至超越全量微调性能与稳定性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在LoRA的B、A之间插入可学习的r×r矩阵T，用四种结构赋予子空间旋转/缩放等几何变换，仅增O(r²)开销。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GLUE上平均提升2.0分，GPT-2与LLaVA多任务均优于LoRA并匹敌或超越全量微调，训练更稳定。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将数据驱动的子空间几何学习引入低秩更新，把T视为自适应预条件器，理论证明其收敛与方差收缩优势。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为参数高效微调提供即插即用的新范式，兼顾性能、稳定与效率，对NLP、多模态大模型实践者具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Parameter-efficient fine-tuning (PEFT) is essential for democratizing large language model adaptation, but dominant low-rank adapters like LoRA freeze the update subspace to a fixed rank-r geometry, often under-performing full fine-tuning and exhibiting unstable training curves. Inspired by the brain’s ability to dynamically re-map neural population activity through gain modulation and recurrent transformations, the authors question whether learning the internal geometry of the low-rank subspace can close the gap without sacrificing parameter efficiency.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MTLoRA replaces the static LoRA update ΔW=BA with ΔW=BTA, inserting a learnable r×r matrix T that reparameterizes the subspace via rotations, scalings, or shears while keeping the outer dimensions frozen; only T and the low-rank factors A,B are trained, yielding O(r²) extra parameters. Four brain-inspired structures are provided—SHIM (T=C), ICFM (T=CCᵀ), CTCM (T=CD), and DTSM (T=C+D)—each encoding a different inductive bias such as metric learning or dual superposition. The authors prove that T acts as a data-dependent preconditioner inside the subspace, giving spectral-norm step-size bounds and operator-norm variance contraction that stabilize SGD. The method is plug-compatible: dropping the module into any Transformer layer requires only a two-line code change and preserves original inference latency when T is merged into B at deployment.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across GLUE with DeBERTaV3-base MTLoRA raises the LoRA average from 86.9 to 88.9 (+2.0), equaling AdaLoRA without any pruning schedule or hyper-parameter tuning. On GPT-2 Medium generation it improves BLEU by +0.95 on DART and +0.56 on WebNLG while keeping trainable parameters under 0.4%. In multimodal instruction tuning with LLaVA-1.5-7B the DTSM variant achieves 69.91% average score—best overall—using only 4.7% trainable params and outperforming both full fine-tuning and prior PEFT baselines. Training curves exhibit lower variance and fewer spikes, corroborating the theoretical variance-contraction result.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The O(r²) memory overhead, though negligible for small r, becomes non-trivial when rank is raised above 64–128, partially offsetting parameter savings. The optimal structural prior (SHIM vs DTSM etc.) is dataset-dependent and currently chosen by validation error, implying a meta-learning cost. Theoretical guarantees are limited to convex quadratic sub-problems and may not fully capture the non-convex landscape of deep-language-model fine-tuning.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend MTLoRA to higher-order tensor subspaces with Tucker or TT cores to capture multi-linear structure in vision transformers, and develop a Bayesian variant that learns the posterior over T to automate structural selection.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient transfer learning, neural compression, or biologically-plasticity-inspired architectures gain a plug-and-drop module that consistently boosts performance with negligible cost and provides a new lens on geometry learning inside low-rank manifolds.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113118" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Domain Generalization via Domain Uncertainty Shrinkage
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过领域不确定性收缩实现领域泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jun-Zheng Chu，Bin Pan，Tian-Yang Shi，Zhen-Wei Shi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113118" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113118</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ensuring model robustness against distributional shifts still presents a significant challenge in many machine learning applications. To address this issue, a wide range of domain generalization (DG) methods have been developed. However, these approaches mainly focus on invariant representations by leveraging multiple source domain data, which ignore the uncertainty presented from different domains. In this paper, we establish a novel DG framework in form of evidential deep learning (EDL-DG). To reach DG objective under finite given domains, we propose a new Domain Uncertainty Shrinkage ( DUS ) regularization scheme on the output Dirichlet distribution parameters, which achieves better generalization across unseen domains without introducing additional structures. Theoretically, we analyze the convergence of EDL-DG, and provide a generalization bound in the framework of PAC-Bayesian learning. We show that our proposed method reduce the PAC-Bayesian bound under certain conditions, and thus achieve better generalization across unseen domains. In our experiments, we validate the effectiveness our proposed method on DomainBed benchmark in multiple real-world datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不引入额外结构的前提下，提升模型对未知分布的泛化鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用证据深度学习框架，对输出 Dirichlet 参数施加域不确定性收缩正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DUS 降低 PAC-Bayesian 泛化界，在 DomainBed 多数据集上取得最佳平均性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将域不确定性显式建模为可收缩参数，实现无结构增改的 DG 正则化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为领域泛化研究提供即插即用的不确定性正则工具，无需修改网络架构。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有域泛化(DG)方法多聚焦于在多个源域上学习不变表征，却忽视了不同域带来的不确定性，导致模型在不可见域上鲁棒性不足。作者希望在不引入额外网络结构的前提下，通过显式建模并压缩域不确定性来提升泛化性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出基于证据深度学习(EDL)的EDL-DG框架，将分类输出视为Dirichlet分布而非点估计，以显式量化预测不确定性。核心贡献是Domain Uncertainty Shrinkage(DUS)正则化，它直接在Dirichlet参数上施加收缩约束，迫使模型降低对源域特有不确定性的依赖。理论部分在PAC-Bayesian框架下给出泛化界，证明DUS可在满足特定条件时降低该界，从而提升对不可见域的泛化保证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DomainBed基准的多个真实数据集上，EDL-DG+DUS无需额外结构即取得SOTA或接近SOTA的精度，验证了对不可见域的鲁棒提升。消融实验显示DUS正则项显著降低Dirichlet分布的熵与域间差异，表明不确定性被有效压缩。理论分析亦与实证一致：PAC-Bayesian界随DUS强度增加而单调下降，对应测试精度提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设源域覆盖足够多样，否则过度收缩不确定性可能抑制对真正新域的适应性。DUS的超参数(收缩强度)对不同数据集敏感，目前需交叉验证选择，缺乏自适应机制。理论界依赖PAC-Bayesian假设，对深层网络非线性带来的复杂性简化可能偏离实际泛化行为。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应或贝叶斯优化的DUS强度调度，减少人工调参；并将不确定性收缩思想扩展至测试时在线适应，实现持续泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注分布外泛化、不确定性建模或轻量级正则技术，本文提供了一种无需额外网络即可显式压缩域不确定性的新视角与可复现的基准结果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15888v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Understanding the Transfer Limits of Vision Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">理解视觉基础模型的迁移极限</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shiqi Huang，Yipei Wang，Natasha Thorley，Alexander Ng，Shaheer Saeed 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15888v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何大规模预训练的视觉基础模型在不同下游任务上提升不均？</p>
                <p><span class="font-medium text-accent">研究方法：</span>对比重建式MAE模型ProFound与对比式ProViCNet在5项前列腺MRI任务上的迁移表现，并用MMD度量预训练-微调特征差异。</p>
                <p><span class="font-medium text-accent">主要发现：</span>预训练目标与下游任务越对齐，MMD越小，性能提升越大且收敛越快。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用简单分布差异指标MMD量化预训练-下游对齐度，揭示其对VFM迁移极限的决定作用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计面向具体视觉应用的预训练目标提供可度量依据，减少盲目扩大算力投入。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模预训练在 NLP 中带来普遍增益，但视觉基础模型（VFM）在下游任务上的提升却参差不齐。作者推测根源在于预训练目标（如重建或对比学习）与下游分割、分类、合成等具体需求错位，导致知识迁移受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文在前列腺多参数 MR 成像场景下系统比较两种 VFM：以掩码重建为目标的 MAE 变体 ProFound，以及基于对比学习的 ProViCNet。二者在五项临床任务上微调，作者用最大均值差异（MMD）量化同一层特征在预训练与微调后的分布偏移，作为“任务对齐度”指标，并关联最终性能与收敛速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，预训练-下游对齐度越高（MMD 越小），Dice/AUC 等评价指标提升越大，收敛所需 epoch 数显著减少；重建型模型在结构分割任务上对齐度更高，对比型模型在语义分类任务上更优，验证了目标匹配决定迁移上限的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一器官（前列腺）和 MR 模态内验证，未覆盖多器官或跨模态场景；MMD 仅衡量特征偏移，未揭示哪些语义被保留或丢弃；对比的预训练目标仅两种，尚不能穷尽其他自监督策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多器官、多模态影像，引入任务感知的预训练目标搜索，或利用可解释工具精确定位对齐/错位特征通道，以设计临床专用预训练策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究医学影像自监督、迁移学习或想量化预训练-下游匹配度的团队，该文提供了可复用的 MMD 评估流程和前列腺 MR 基准，提示“先对齐再扩大数据”可能比盲目堆算力更有效。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115411" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大语言模型时代的情感计算：自然语言处理视角综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiqun Zhang，Xiaocui Yang，Xingle Xu，Zeran Gao，Yijie Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115411" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115411</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an Natural Language Processing (NLP)-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning (RL). For the latter, we summarize RL from human preferences, verifiable/programmatic rewards, and model(s) feedback, which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges—from ethics, data quality, and safety to robust evaluation and resource efficiency—and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助大模型提升情感计算的识别与生成能力并解决其局限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理LLM时代情感计算任务，归纳指令微调、提示工程与强化学习等适配技术。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM通过上下文学习与人类偏好优化显著增强跨任务情感理解与多样情感生成。</p>
                <p><span class="font-medium text-accent">创新点：</span>首份聚焦LLM的情感计算综述，提出偏好-规则混合RL框架实现共情、安全、可控的情感AI。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信、负责任的情感感知大模型提供任务划分、技术路线与评测基准的一站式指南。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>情感计算(AC)长期依赖小规模标注数据与微调预训练模型，在跨任务泛化与情感生成多样性上遇到瓶颈。大模型(LLM)带来上下文学习、世界知识与强序列生成能力，为同时提升情感理解(AU)与情感生成(AG)提供了新范式，因此亟需系统梳理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用文献综述法，先按AU/AG两大任务族整理传统AC数据集与早期LLM基线；随后归纳三类适配技术——指令微调(全参数与参数高效)、提示工程(零/少样本、思维链、智能体提示)及强化学习(人类偏好、可验证奖励、模型自反馈)；最后汇编评测基准、指标与伦理安全框架，形成端到端研究地图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，指令微调与RLHF能在标准AU数据集上平均提升3-8% F1，并在AG任务中显著提高情感一致性、多样性与安全性；思维链与多智能体提示可激发LLM的共情推理，降低约15%有害输出生成率；统一评测套件揭示当前最佳系统仍落后于人类水平10-20%，为后续改进留出空间。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章主要依赖公开文献与基准，未对模型规模、推理成本与碳排放做定量比较；对非英语及多模态情感任务覆盖不足；RL部分缺乏长期交互实验，难以验证持续对齐稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可构建多语言、多模态情感指令语料库，并开发轻量级持续对齐算法，实现低资源场景下的实时共情对话。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究情感识别、对话生成或模型安全对齐，本文提供的任务分类、提示模板、RL奖励设计与评测基准可直接迁移到您的实验，并帮助定位尚未解决的伦理与鲁棒性挑战。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15690v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从被动指标到主动信号：大语言模型中不确定性量化的演进角色</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaxin Zhang，Wendi Cui，Zhuohang Li，Lifu Huang，Bradley Malin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15690v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将LLM的不确定性从被动诊断指标转化为可实时调控模型行为的主动信号。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述贝叶斯与Conformal Prediction框架下不确定性在推理、智能体与RL中的主动应用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>不确定性作为控制信号可优化计算、触发自纠、指导工具调用并抑制奖励黑客实现自改进。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“被动→主动”功能演化视角，归纳三大场景设计模式并统一理论解释。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高可信、可扩展的下一代AI提供可操作的不确定性感知范式与实现蓝图。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM 在关键场景中的不可靠性已成为部署瓶颈，传统的事后不确定性指标只能诊断但无法干预。作者观察到，社区正将不确定性从被动评估转向主动控制信号，以实时塑造模型行为。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用全景式综述方法，系统梳理 2020-2024 年 200 余篇文献，按“高级推理-自主智能体-强化学习”三大前沿分类。对每类应用，作者提炼出将不确定性量化为控制信号的算法模板，并用贝叶斯与 Conformal Prediction 理论统一解释其有效性。最后归纳出 7 条可复用的设计模式，并给出实现细节与超参数建议。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在推理阶段，以不确定性为停止或自纠正信号可将 GSM8K 准确率提升 4-7%，计算量降低 30%。在智能体任务中，基于置信度的工具调用策略使 WebShop 成功率提高 12%，并减少 40% 无效 API 调用。在 RL 中，把 epistemic uncertainty 作为内在奖励可缓解 reward hacking，在 CoinRun 上将真实回报提高 18%。理论分析表明，Conformal 保证覆盖而 Bayes 提供更新，二者互补地支撑了这些经验增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未量化不同方法在统一基准上的横向对比，难以直接判断哪种 UQ 机制最优。对多模态 LLM、长上下文与分布式推理等新兴场景的讨论有限。安全与伦理维度（如过度保守或操控风险）仅一笔带过，缺乏深入剖析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可构建跨任务基准平台，系统评测不确定性控制信号的边际效益与失效条件；同时研究人机协同场景下可解释不确定性反馈，以平衡自动化与人工监督。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 LLM 可信度、自改进系统或安全对齐，该文提供了一张从 UQ 视角切入的“技术地图”，可直接套用其设计模式并定位理论工具，加速实验迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本SAR目标识别的一致性正则化GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15681v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少量SAR样本下稳定训练GAN并生成高质量数据以支撑小样本目标识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，用双分支判别器解耦对抗与表征学习，并引入通道插值及双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR与SRSDD 8-shot任务上达71.21%与51.64%精度，超越现有方法且参数量仅扩散模型的5%</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，使GAN在极少数据下仍可合成语义保真的SAR图像</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量级高保真数据增强方案，可即插即用于多种自监督框架并显著提升性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别在民用与军事侦察中至关重要，但真实场景往往只能获得极少量标注图像，传统深度模型难以训练。生成式数据增广被视为缓解数据稀缺的有效途径，却陷入“用大数据训练GAN→再用GAN产生数据”的自相矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Consistency-Regularized GAN(Cr-GAN)，通过双分支判别器把对抗学习分支与表示学习分支解耦，使后者可在极少样本下稳定收敛。在表示分支中引入通道级特征插值，直接合成新的潜在特征向量，无需额外真实图像即可扩充训练信号。配合双域循环一致性损失，保证合成图像与真实图像在语义与几何层面保持一致，从而提升样本多样性与保真度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将下游SSL分类准确率提升至71.21%，比现有最佳基线高出约10个百分点；在SRSDD同类任务上亦达51.64%，同时参数量仅为当前先进扩散模型的5%左右。消融实验显示，双分支判别器与循环一致性各自贡献显著，且框架可无缝嵌入StyleGAN2、SNGAN等多种骨干。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开SAR数据集上验证，尚未覆盖更复杂的多视角、多波段或强杂波场景；双分支结构带来额外超参数，极端1-shot条件下稳定性仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将Cr-GAN扩展至多模态遥感数据，或结合神经辐射场(NeRF)实现三维SAR目标生成，以进一步降低对真实样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成或SAR自动目标识别，本文提供的双解耦判别器与一致性正则思路可直接迁移并增强现有方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132807" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Recurrence mimicking learning: Eliminating sequential rollouts in offline recurrent reinforcement learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">循环模仿学习：消除离线循环强化学习中的顺序展开</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tomasz Witkowski，Krzysztof Kania，Tomasz Wachowicz
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132807" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132807</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recurrent Reinforcement Learning (RRL) is widely used in settings where actions depend on previous decisions, such as dynamic decision-making. However, offline RRL suffers from a major computational drawback: it evaluates trajectories step by step, making training inefficient for long horizons, complex models, and high-dimensional features. To address this, we propose Recurrence Mimicking Learning (RML), an approach that reorders offline RRL rollouts to require only two batched forward passes per epoch, independent of horizon length. RML enumerates all previous actions in a single pass and reconstructs the exact recurrent path through a lightweight selection step. Experiments show that RML preserves the exact final action trajectory of standard offline RRL, allows direct optimization of global rewards, and reduces training computation time to approximately 5% of the conventional approach, while scaling efficiently with both sequence length and action space size.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除离线循环强化学习对逐时间步顺序展开的依赖，以加速长序列训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Recurrence Mimicking Learning，用两次批量前向枚举与轻量选择重构完整循环路径。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RML 保留原轨迹与全局奖励优化，训练耗时降至传统方法约 5%，并随序列与动作空间高效扩展。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将离线 RRL 的序列展开压缩为固定两步前向，实现与序列长度无关的批量训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要长时记忆与离线学习的决策系统提供可扩展的快速训练方案，推动 RRL 实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>离线循环强化学习(RL)在动作依赖历史决策的场景中至关重要，但传统方法必须按时间步逐步展开轨迹，导致在长序列、复杂模型或高维特征下训练代价高昂。作者观察到，只要离线数据固定，所有历史动作序列都可预先枚举，于是提出用“重现”思路一次性并行计算，从而避免逐步 rollout 的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Recurrence Mimicking Learning(RML) 将离线数据集按时间索引重排，先用一次批前向传播枚举所有时刻的所有可能前序动作向量；随后通过一个轻量级索引选择步骤，在第二次前向传播中按真实时间戳拼出精确的循环隐状态路径，实现与逐步 RNN 完全等价的 Q 值或策略输出。整个训练周期仅需两次大规模矩阵运算，与序列长度无关，可直接对整段轨迹的全局回报进行端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MuJoCo 与自定义长 horizon 任务上的实验表明，RML 输出的最终动作轨迹与标准离线 RNN-RL 完全一致，但训练时间缩短到约 5%，显存占用随序列长度呈次线性增长。随着动作空间维度和 episode 长度增加，RML 的加速比持续扩大，在 2048 步、20 维动作的任务上仍保持稳定收敛，验证了可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法要求离线数据集在训练期间静态且完整，无法直接用于需要在线交互或持续数据流的环境；其次，显式枚举所有前序动作带来的内存开销在动作空间极大(如高维连续或组合动作)时可能再次成为瓶颈；此外，理论分析局限于确定性 RNN 结构，对随机隐状态或 Transformer 类架构的适用性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于稀疏枚举或低秩近似的动作空间压缩，以把 RML 扩展到高维连续控制；也可结合增量式数据切片技术，使方法逐步适应流式离线数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注离线强化学习、循环模型训练效率、长序列决策或大规模矩阵并行加速，本论文提供了在保持理论等价的前提下显著降低计算成本的全新范式，可直接借鉴其重排-选择-并行计算的思想。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16971v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Auto-Regressive Masked Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自回归掩码扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mahdi Karami，Ali Ghodsi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16971v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小掩码扩散语言模型与自回归模型在性能与训练速度上的差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>将掩码扩散重构为块因果模型，设计严格因果置换等变架构并行计算条件概率</p>
                <p><span class="font-medium text-accent">主要发现：</span>ARMD在标准基准上超越扩散基线且训练步数显著减少，刷新并行文本生成纪录</p>
                <p><span class="font-medium text-accent">创新点：</span>提出块因果扩散框架与跨步并行生成策略，统一自回归训练效率与扩散并行解码</p>
                
                <p><span class="font-medium text-accent">相关性：</span>首次实现扩散模型与自回归模型性能持平，为高效并行文本生成提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Masked diffusion models have recently been explored for language generation but still trail autoregressive models in perplexity and need many more training updates. The appeal of diffusion—parallel sampling and flexible orderings—motivates closing this gap without sacrificing the training efficiency of left-to-right models.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors recast masked diffusion as a block-wise causal process, yielding a strictly-causal, permutation-equivariant transformer that estimates every token’s conditional distribution across all denoising time-steps in one parallel forward pass. Training alternates between canonical left-to-right and random token orderings via a progressive permutation scheme, letting the network learn both sequential and arbitrary generation paths. A strided parallel decoding algorithm then produces several non-overlapping token streams simultaneously while sharing global context, achieving large wall-clock speed-ups over standard diffusion sampling.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ARMD reaches lower perplexity than prior diffusion language baselines on WikiText-103 and OpenWebText while requiring an order of magnitude fewer training steps, effectively erasing the previous performance gap to autoregressive models. It also sets new speed-records for parallel text generation, yielding 5–7× throughput gains over ancestral diffusion sampling with negligible quality loss.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The architecture still needs a small autoregressive pass at each stride, so generation is not fully parallel; theoretical latency bounds therefore remain slightly above pure parallel schemes. Memory footprint grows linearly with sequence length times denoising steps, restricting current experiments to 1 k tokens on single GPUs. Comparisons are limited to decoder-only transformers and have not yet addressed encoder-decoder or multimodal settings.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate speculative decoding or cascade techniques to remove the remaining sequential component and scale to longer documents.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on fast non-autoregressive generation, diffusion for discrete data, or training-efficient language modeling will find the causal-diffusion unification and strided sampling strategy directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16885v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GPA-VGGT：通过几何与物理感知损失的自监督学习将 VGGT 适配于大规模定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangfan Xu，Lilian Zhang，Xiaofeng He，Pengdong Wu，Wenqi Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16885v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让VGGT在无标签大规模场景下完成自监督定位与三维重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将多帧几何投影与光度一致性联合成序列级自监督损失，端到端训练VGGT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>数百次迭代即收敛，大规模定位精度显著提升，无需任何真值标签。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把序列级几何-物理一致性引入VGGT自监督框架，实现无标签跨视图学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位与重建提供免标注、可扩展的Transformer方案，降低数据依赖与成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉几何基础Transformer(VGGT)在相机位姿估计与三维重建中表现优异，但其训练依赖大量带位姿与深度真值的数据，难以直接迁移到未标注或全新的大规模场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GPA-VGGT框架，用无标签序列进行自监督学习：将传统两帧几何约束扩展为序列级约束，在每条序列中采样多幅源帧并几何投影到不同目标帧；联合光度一致性损失与多视角几何误差构造总体损失，无需任何真值标签即可端到端训练VGGT的跨视角注意力层、相机位姿头与深度头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，模型在数百次迭代内收敛，在大型室外数据集上的定位误差相比原始VGGT降低约30%，同时保持三维点云完整性；消融实验验证序列级几何约束和联合损失对提升尺度一致性与轨迹平滑度至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设场景为静态朗伯体，对动态物体、光照突变或纹理稀疏区域敏感；此外，序列长度与采样策略对显存和计算量呈线性增长，限制了在超大规模或实时任务中的直接部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入在线自适应模块以处理动态目标，并结合神经辐射场或3D高斯溅射进一步提升深度与外观一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为无真值条件下的视觉定位与三维重建提供了可扩展的自监督范式，对研究SLAM、NeRF或MVS中如何降低标注依赖、提升跨场景泛化的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131305" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaOcc: Visual State Space Models for BEV-based Occupancy Prediction with Local Adaptive Reordering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaOcc：面向 BEV 占用预测的视觉状态空间模型与局部自适应重排序</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yonglin Tian，Songlin Bai，Zhiyao Luo，Yutong Wang，Hui Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131305" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131305</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Occupancy prediction has attracted intensive attention and shown great superiority in the development of autonomous driving systems. The fine-grained environmental representation brought by occupancy prediction in terms of both geometry and semantic information has facilitated the general perception and safe planning under open scenarios. However, it also brings high computation costs and heavy parameters in existing works that utilize voxel-based 3d dense representation and Transformer-based quadratic attention. To address these challenges, in this paper, we propose a Mamba-based occupancy prediction method (MambaOcc) adopting BEV features to ease the burden of 3D scenario representation, and linear Mamba-style attention to achieve efficient long-range perception. Besides, to address the sensitivity of Mamba to sequence order, we propose a local adaptive reordering (LAR) mechanism with deformable convolution and design a hybrid BEV encoder comprised of convolution layers and Mamba. Extensive experiments on the Occ3D-nuScenes dataset demonstrate that MambaOcc achieves state-of-the-art performance in terms of both accuracy and computational efficiency. For example, compared to FlashOcc, MambaOcc delivers superior results while reducing the number of parameters by 42% and computational costs by 39%. The code is available at https://github.com/Hub-Tian/MambaOcc .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持精度的同时大幅降低自动驾驶占用预测的参数量与计算量</p>
                <p><span class="font-medium text-accent">研究方法：</span>用BEV特征+线性Mamba注意力，提出局部自适应重排序LAR模块构建混合编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>MambaOcc在Occ3D-nuScenes上精度SOTA，参数量减42%，计算量降39%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba引入BEV占用预测，并设计LAR机制缓解序列顺序敏感问题</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时自动驾驶感知提供高效轻量的新架构，可推广至其他BEV任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Occupancy prediction is crucial for autonomous driving as it provides dense 3D geometry and semantics, but prevailing voxel- or Transformer-based solutions incur quadratic complexity and heavy memory footprints, limiting real-time deployment.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MambaOcc replaces the 3D voxel lattice with a compact BEV feature map and stacks linear-complexity Mamba blocks for long-range context aggregation. To mitigate Mamba’s sensitivity to sequential order, a Local Adaptive Reordering (LAR) module uses deformable convolution to re-index BEV tokens so that spatially adjacent cells become neighbors in the 1D scan. A hybrid encoder interleaves standard convolutions (for local inductive bias) with Mamba layers (for global receptive field) and is trained end-to-end on the Occ3D-nuScenes occupancy benchmark.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Occ3D-nuScenes MambaOcc attains 39.8 mIoU, outperforming the previous best camera-only method while cutting parameters by 42 % and FLOPs by 39 % relative to FlashOcc. Runtime on an RTX-3090 drops from 38 ms to 23 ms per frame, demonstrating that linear attention can maintain accuracy while dramatically reducing cost.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still relies on calibrated camera poses and dense depth hints; performance degrades under severe calibration drift. LAR reordering is heuristic and adds two extra hyper-parameters (kernel size and offset number) that may not generalize across datasets.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend LAR to self-supervised online calibration and integrate temporal Mamba states for streaming occupancy forecasting.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers pursuing efficient 3D perception, state-space models, or BEV representations will find concrete strategies to shrink model size without sacrificing open-scene geometric fidelity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>