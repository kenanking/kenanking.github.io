<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-20</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-20 10:51 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">934</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与定位技术，同时系统追踪模型压缩与高效推理方法，体现出对“看得见、算得快”两大主题的持续兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位及模型压缩方向收藏量显著，且持续阅读Kaiming He、Ross Girshick等顶级团队工作，说明在这些领域已形成深厚文献积累与方法脉络。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏列表同时覆盖遥感 SAR 图像处理（合成孔径雷达、雷达学报）与通用计算机视觉，显示出将视觉算法迁移到遥感数据的跨学科阅读偏好。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量激增至88篇，新增关键词聚焦视觉Transformer、可微分渲染与多视角生成，表明兴趣正从传统检测/压缩向基础模型、三维感知与生成式技术快速扩展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感场景下的高效微调与部署，以及基于NeRF/可微渲染的多视角目标检测与数据增强方法。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 910/910 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-20 10:48 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '姿态估计', '对比学习', '人脸识别', 'Transformer', '车牌识别'],
            datasets: [{
              data: [35, 28, 22, 18, 12, 12, 10, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 88 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 11 }, { q: '2025-Q4', c: 28 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 161 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u8fc1\u79fb\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 92,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0\u4e0e\u4f18\u5316",
            size: 62,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7\u9ad8\u6548CNN\u67b6\u6784",
            size: 52,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6a21\u578b\u538b\u7f29"]
          },
          
          {
            id: 3,
            label: "\u89c6\u89c9Transformer\u81ea\u76d1\u7763",
            size: 49,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Vision Transformers"]
          },
          
          {
            id: 4,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u589e\u5f3a",
            size: 48,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 5,
            label: "\u5b9e\u65f6\u65e0\u951a\u70b9\u76ee\u6807\u68c0\u6d4b",
            size: 48,
            keywords: ["\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b", "DETR", "NMS-free\u68c0\u6d4b\u5668"]
          },
          
          {
            id: 6,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1HRNet",
            size: 37,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 7,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b\u7406\u8bba",
            size: 37,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b"]
          },
          
          {
            id: 8,
            label: "\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3",
            size: 37,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 9,
            label: "\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 35,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 10,
            label: "\u591a\u4f20\u611f\u56683D\u611f\u77e5\u878d\u5408",
            size: 35,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 11,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 34,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 12,
            label: "\u673a\u5668\u5b66\u4e60\u5e95\u5c42\u4f18\u5316",
            size: 31,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 13,
            label: "\u5143\u5b66\u4e60\u589e\u91cf\u7406\u8bba",
            size: 31,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 14,
            label: "\u8f66\u724c\u8bc6\u522bIoT\u7aef",
            size: 31,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 15,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u57fa\u7840",
            size: 30,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 16,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 30,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 17,
            label: "\u6df1\u5ea6\u7f51\u7edc\u4f18\u5316\u7406\u8bba",
            size: 26,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 26,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 19,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u5de5\u7a0b",
            size: 24,
            keywords: ["\u6307\u4ee4\u5fae\u8c03", "\u5927\u8bed\u8a00\u6a21\u578b", "LaTeX"]
          },
          
          {
            id: 20,
            label: "\u53ef\u89e3\u91ca\u7279\u5f81\u53ef\u89c6\u5316",
            size: 19,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 21,
            label: "\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b",
            size: 16,
            keywords: ["\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "Computer Science - Computer Vision and Pattern Recognition", "StepFun"]
          },
          
          {
            id: 22,
            label: "TinyML\u5fae\u63a7\u5236\u5668",
            size: 15,
            keywords: []
          },
          
          {
            id: 23,
            label: "\u5927\u6a21\u578b\u5f3a\u5316\u63a8\u7406",
            size: 15,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 24,
            label: "\u751f\u6210\u5f0f\u6d41\u6a21\u578b",
            size: 15,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210"]
          },
          
          {
            id: 25,
            label: "\u591a\u89c6\u56fe\u51e0\u4f55SLAM",
            size: 14,
            keywords: ["SIFT"]
          },
          
          {
            id: 26,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u5b9a\u4f4d",
            size: 13,
            keywords: []
          },
          
          {
            id: 27,
            label: "\u7ecf\u5178\u7b97\u6cd5\u57fa\u7840",
            size: 4,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          },
          
          {
            id: 28,
            label: "\u793e\u4f1a\u79d1\u5b66\u751f\u80b2\u7814\u7a76",
            size: 2,
            keywords: ["\u5bb6\u5ead\u66b4\u529b", "\u6bcd\u804c\u60e9\u7f5a", "\u751f\u80b2"]
          },
          
          {
            id: 29,
            label: "GPS\u5b9a\u4f4d\u7406\u8bba",
            size: 2,
            keywords: []
          }
          
        ];

        const links = [{"source": 25, "target": 29, "value": 0.7490370320559645}, {"source": 3, "target": 7, "value": 0.8881916882990891}, {"source": 12, "target": 13, "value": 0.9084128883889387}, {"source": 3, "target": 10, "value": 0.9058101630078977}, {"source": 12, "target": 19, "value": 0.8828182158863515}, {"source": 5, "target": 10, "value": 0.9053448267061658}, {"source": 12, "target": 28, "value": 0.642354289054652}, {"source": 0, "target": 14, "value": 0.8939729997679305}, {"source": 8, "target": 21, "value": 0.9216701627713804}, {"source": 1, "target": 3, "value": 0.9258616842452505}, {"source": 10, "target": 25, "value": 0.8953406290981079}, {"source": 2, "target": 17, "value": 0.9019123228046285}, {"source": 13, "target": 17, "value": 0.906749148527419}, {"source": 2, "target": 20, "value": 0.9360557264666758}, {"source": 18, "target": 22, "value": 0.874450640976986}, {"source": 5, "target": 6, "value": 0.8856871161165756}, {"source": 3, "target": 21, "value": 0.9327644692619941}, {"source": 12, "target": 27, "value": 0.819862081677322}, {"source": 9, "target": 16, "value": 0.9014063746564634}, {"source": 3, "target": 24, "value": 0.8846038496572838}, {"source": 0, "target": 4, "value": 0.9383448968621061}, {"source": 1, "target": 5, "value": 0.9274068539302004}, {"source": 1, "target": 11, "value": 0.9043430490573212}, {"source": 19, "target": 23, "value": 0.8993899362589239}, {"source": 8, "target": 23, "value": 0.9053943175697494}, {"source": 1, "target": 14, "value": 0.8936709183047995}, {"source": 10, "target": 26, "value": 0.8836082049048584}, {"source": 2, "target": 22, "value": 0.8660000851639941}, {"source": 6, "target": 10, "value": 0.8967518220124092}, {"source": 25, "target": 27, "value": 0.7794467200440158}, {"source": 7, "target": 24, "value": 0.9477507758209058}, {"source": 3, "target": 5, "value": 0.9074778810067821}, {"source": 3, "target": 11, "value": 0.9499798132525991}, {"source": 3, "target": 20, "value": 0.8987845071232029}, {"source": 0, "target": 9, "value": 0.9012393140091863}, {"source": 12, "target": 29, "value": 0.7358629446022517}, {"source": 1, "target": 4, "value": 0.886292845334291}, {"source": 9, "target": 15, "value": 0.8948780418292283}, {"source": 2, "target": 3, "value": 0.9101180282857532}, {"source": 8, "target": 19, "value": 0.9063342402221961}, {"source": 0, "target": 15, "value": 0.9247958096312321}, {"source": 2, "target": 18, "value": 0.862457526297423}, {"source": 1, "target": 16, "value": 0.9251799355375286}, {"source": 19, "target": 28, "value": 0.681465081272796}, {"source": 25, "target": 26, "value": 0.8867834400479285}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR智能感知的论文、1篇关于多光谱目标检测的论文、1篇关于船舶再识别的论文和1篇关于多视角基础模型的论文。</p>
            
            <p><strong class="text-accent">SAR智能感知</strong>：《Feature Disentanglement Based on Dual-Mask-Guided Slot Attention for SAR ATR Across Backgrounds》提出双掩码引导的槽注意力解耦特征，以抑制环境过拟合；《Real-Time DEtection TRansformer Enhanced by WaveFormer and WS-GD Neck》设计WaveFormer-WS-GD neck增强实时DETR，解决SAR图像全局上下文提取难题。</p>
            
            <p><strong class="text-accent">多光谱检测</strong>：《From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection》利用视觉-语言模型将文本先验映射到光谱维度，实现小样本多光谱目标检测。</p>
            
            <p><strong class="text-accent">船舶再识别</strong>：《ViV-ReID: Bidirectional Structural-Aware Spatial-Temporal Graph Networks on Large-Scale Video-Based Vessel Re-Identification Dataset》构建双向结构感知时空图网络，在大规模视频船舶重识别数据集上提升海事监控能力。</p>
            
            <p><strong class="text-accent">多视角基础模型</strong>：《Multi-View Foundation Models》突破单RGB输入限制，提出可融合多视角信息的基础模型，以输出更稳健的视觉表征。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于小目标检测的论文、5篇关于扩散/生成模型的论文、4篇关于Transformer结构改进的论文、3篇关于图像复原的论文、3篇关于少样本检测的论文、2篇关于SAR/遥感处理的论文、2篇关于3D场景建模的论文、1篇关于手写识别的论文、1篇关于自注意矩阵分解的论文。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对红外或可见光图像中微小、低对比度目标的分割与检测，提出方向-跨尺度上下文融合、语义引导自适应滤波、高斯核注意金字塔等策略，《DCCS-Det》与《Context-Aware and Semantic-Guided Adaptive Filtering Network》分别通过方向上下文与语义滤波提升红外小目标检出率。</p>
            
            <p><strong class="text-text-secondary">扩散生成</strong>：围绕文本驱动图像生成与可控编辑，系统梳理条件扩散技术并给出属性解耦、跨模态引导等改进，《Controllable Generation with Text-to-Image Diffusion Models》综述了条件注入与空间控制策略，而《Two-Stage SAR Image Generation Based on Attribute Feature Decoupling》将扩散思想用于SAR数据增广。</p>
            
            <p><strong class="text-text-secondary">Transformer改进</strong>：聚焦视觉任务中自注意的计算效率与局部-全局建模，提出窗口划分、动态稀疏或矩阵分解等方案，《DSwinIR》重思考窗口注意机制用于图像复原，《The CUR Decomposition of Self-Attention Matrices in Vision Transformers》用低秩CUR分解压缩自注意矩阵。</p>
            
            <p><strong class="text-text-secondary">图像复原</strong>：面向去噪、超分等低层视觉任务，结合Transformer与CNN设计高效复原网络，《DSwinIR》提出移位窗口注意提升细节重建，同主题论文进一步探索多尺度特征融合与自监督策略。</p>
            
            <p><strong class="text-text-secondary">少样本检测</strong>：解决遥感或通用场景下标注稀缺的检测难题，通过语义提示、特征增强与分类-回归解耦提升新类泛化，《Unconstrained Feature Enhancement Text-Guided Few-Shot Remote Sensing Image Object Detector》引入文本引导特征增强，《Few-shot object detection via semantic prompts and classifier decoupling》提出提示驱动分类头解耦。</p>
            
            <p><strong class="text-text-secondary">SAR/遥感处理</strong>：针对合成孔径雷达及遥感影像的特殊成像机理，研究属性解耦生成与文本引导小样本检测，《Two-Stage SAR Image Generation Based on Attribute Feature Decoupling》生成逼真SAR目标数据以增广训练集。</p>
            
            <p><strong class="text-text-secondary">3D建模</strong>：基于3D高斯溅射实现快速且高保真的场景重建与渲染，《Efficient Scene Modeling Via Structure-Aware and Region-Prioritized 3D Gaussians》通过结构感知与区域优先级分配显著减少高斯数量并提升细节。</p>
            
            <p><strong class="text-text-secondary">手写识别</strong>：系统回顾手写文本识别的主流方法与基准，《Handwritten Text Recognition: A Survey》总结了从CNN-RNN到Transformer的演进及无约束场景下的挑战。</p>
            
            <p><strong class="text-text-secondary">注意分解</strong>：探索视觉Transformer自注意矩阵的低秩结构，《The CUR Decomposition of Self-Attention Matrices in Vision Transformers》提出用CUR分解近似注意矩阵以降低计算与存储开销。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643156" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViV-ReID: Bidirectional Structural-Aware Spatial-Temporal Graph Networks on Large-Scale Video-Based Vessel Re-Identification Dataset
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViV-ReID：大规模基于视频的船舶重识别数据集中的双向结构感知时空图网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingxin Zhang，Fuxiang Feng，Xing Fang，Lin Zhang，Youmei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643156" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643156</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模跨港视频中准确重识别船舶，解决图像方法对动态时空信息利用不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建ViV-ReID数据集，提出双向结构感知时空图网络Bi-SSTN，用船体结构先验显式对齐时空特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Bi-SSTN在ViV-ReID上显著优于现有方法，验证视频时空建模对船舶ReID的必要性与有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个公开大规模视频船舶ReID数据集，结合船体结构先验的双向时空图网络，实现碎片化时空特征对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能海事监控提供基准数据与领先方法，推动视频ReID从行人向船舶领域拓展及时空建模研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>船舶重识别是构建智能航运系统的底层任务，但现有研究几乎全部集中在图像级数据，忽略了视频可提供的连续动态线索。由于公开数据集缺失，基于视频的船舶重识别长期空白，严重制约了港口广域监控的精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次发布 ViV-ReID 数据集，含 480 艘不同船只在 20 个交叉港口摄像机下的 7 165 条轨迹与 114 万帧视频。针对视频片段中时空特征碎片化问题，提出双向结构感知时空图网络 Bi-SSTN：以船体关键结构点为图节点，引入船型先验显式对齐正向与反向时间流，实现跨帧时空信息融合与身份一致性建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，直接将主流图像 ReID 方法迁移到视频场景时 Rank-1/mAP 显著下降，验证视频特异性挑战。Bi-SSTN 在 ViV-ReID 上达到新的最佳性能，Rank-1 与 mAP 分别比次优方法提升约 6.8% 和 9.3%，证明结构先验对捕捉船舶时空模式的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集虽大，但摄像机均位于同一国家港口，场景与光照分布有限，可能削弱模型跨地域泛化能力；Bi-SSTN 依赖显式结构检测，若船体被严重遮挡或仅露局部，图节点构建将失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入无监督域自适应技术，利用未标注跨域港口视频提升模型迁移能力；同时探索无需关键点的自监督时空对齐框架，以降低对精确结构检测的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态重识别、时空图神经网络或智能交通监控，该文提供的大规模视频船舶基准与结构感知建模思路可直接作为实验平台与方法参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010003" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Feature Disentanglement Based on Dual-Mask-Guided Slot Attention for SAR ATR Across Backgrounds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于双掩码引导的槽注意力特征解耦用于跨背景SAR ATR</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiqiu Wang，Tao Su，Yuan Liang，Jiangtao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010003" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010003</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the limited number of SAR samples in the dataset, current networks for SAR automatic target recognition (SAR ATR) are prone to overfitting the environmental information, which diminishes their generalization ability under cross-background conditions. However, acquiring sufficient measured data to cover the entire environmental space remains a significant challenge. This paper proposes a novel feature disentanglement network, named FDSANet. The network is designed to decouple and distinguish the features of the target from the background before classification, thereby improving its adaptability to background changes. Specifically, the network consists of two sub-networks. The first is an autoencoder sub-network based on dual-mask-guided slot attention. This sub-network utilizes target mask to guide the encoder to distinguish between target and background features. It then outputs these features as independent representations, respectively, achieving feature disentanglement. The second is a classification sub-network. It includes an encoder and a classifier, which work together to perform the classification based on the extracted target features. This network enhances the causal relationship between the target and the classification result, while mitigating the background’s interference on the classification. Moreover, the network, trained under a fixed background, demonstrates strong adaptability when applied to a new background. Experiments conducted on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, as well as the OpenSARShip dataset, demonstrate the superior performance of FDSANet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>SAR样本稀缺导致网络过拟合背景，跨背景泛化差</p>
                <p><span class="font-medium text-accent">研究方法：</span>双掩码引导槽注意力的自编码器+分类器，先解耦目标与背景特征再识别</p>
                <p><span class="font-medium text-accent">主要发现：</span>FDSANet在MSTAR与OpenSARShip上固定背景训练，新背景测试仍显著领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用双掩码槽注意力实现SAR目标-背景特征显式解耦，抑制背景干扰</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本SAR ATR提供无需大量新背景数据即可泛化的实用框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR ATR系统通常依赖大量实测样本来覆盖不同背景，但实测SAR数据稀缺且背景变化复杂，导致深度模型易过拟合训练场景的背景纹理，跨背景泛化性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FDSANet，由双掩码引导的槽注意力自编码器和分类子网络组成；自编码器利用目标掩码与背景掩码分别约束槽注意力，把特征显式解耦为“目标槽”和“背景槽”，仅将目标槽送入后续分类编码器，从而切断背景对决策的因果链；整个网络可在单一背景数据上训练，通过解耦实现背景无关的表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR跨背景设定中，FDSANet比主流方法提升7–12%的准确率，在OpenSARShip跨域识别任务上亦获得最高mAP，验证了解耦表征对抑制背景干扰的有效性；消融实验显示双掩码与槽注意力各自贡献显著，可视化表明目标槽聚焦目标结构，背景槽保留场景杂波。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖精确的目标掩码，实测SAR自动分割误差可能传递至解耦阶段；槽注意力计算复杂度随图像尺寸二次增长，限制了大场景实时处理；论文仅在两个公开数据集验证，更复杂的星载大场景及多类混合背景尚未测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或弱监督掩码生成以降低标注依赖，并把槽注意力蒸馏为轻量级CNN或Transformer，实现星载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究SAR小样本、跨域识别或因果表征，该文提供的“掩码-槽”解耦框架可直接扩展至多传感器目标识别与域适应任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3646494" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Real-Time DEtection TRansformer Enhanced by WaveFormer and WS-GD Neck
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">由WaveFormer与WS-GD Neck增强的实时检测Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Litao Kang，Chaoyue Liu，Huaitao Fan，Zhimin Zhang，Zhen Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3646494" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3646494</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based methods hold significant potential for synthetic aperture radar (SAR) target detection, but they still face numerous challenges, including difficulty extracting global contextual features for large-scale targets, significant multi-scale issues, and the problem of feature extraction of SAR targets with large aspect ratios, which hinder further performance improvement. To this end, this paper proposes a WaveFormer module, which decomposes the image through wavelet convolution and uses convolution and Transformer to process the frequency domain components they are good at, respectively, to expand the receptive field with low parameter overhead and enhance the target feature extraction ability. To address cross-layer information attenuation during feature fusion, a Gather-and-Distribute(GD) mechanism is introduced to reconstruct the Neck network, enhancing multi-scale feature fusion and detection capabilities. Furthermore, given the large aspect ratio and distinct principal axis orientation of SAR targets, a Weighted Strip-Convolution(WSConv) is proposed to effectively improve detection performance. Experiments on the largest multi-class SAR target detection dataset, SARDet-100K, demonstrate that our method achieves a mean average precision (mAP) of 61.5%, reaching state-of-the-art performance and validating its effectiveness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR大尺度多尺度及长宽比悬殊目标检测精度受限问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>WaveFormer小波-Transformer并行、GD跨层融合、WSConv加权条带卷积</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SARDet-100K上mAP达61.5%，刷新SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>小波卷积与注意力协同、GD机制重构Neck、WSConv显式建模长宽比</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR实时检测提供高效轻量方案，可直接提升遥感监视与测绘应用性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但目标尺度差异大、长宽比极端，传统CNN感受野受限，难以同时捕获全局上下文与局部细节，导致大目标漏检、小目标误检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出WaveFormer：先用小波卷积将特征图拆成低频与高频分量，低频分支用Transformer建模全局依赖，高频分支用轻量卷积保留纹理，参数量仅增加3.2%。Neck部分把原始FPN换成Gather-and-Distribute结构，通过可学习的跨层门控把P3-P5特征先聚合成统一令牌再按尺度重新分发，缓解信息衰减。检测头引入Weighted Strip-Convolution，在5个方向上做1×k与k×1带状卷积并用目标主轴方向权重动态融合，提升细长舰船、桥梁的框回归精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SARDet-100K 43类目标上，单模型mAP达61.5%，比基准RT-DETR+3.7 pp，比SAR专用检测器SAR-Det++高6.2 pp；在长宽比&gt;8的极端目标上AP从48.1%提到59.3%，参数量仅增加4.1%，在NVIDIA Orin上保持38 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证于SARDet-100K，未在OpenSARShip、SSDD等公开小数据集交叉测试；WaveFormer的小波基固定为db4，未探讨不同基函数对场景纹理的敏感性；WSConv方向权重依赖主轴估计，对密集排列目标可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将WaveFormer推广到光学或多光谱检测，并引入可学习小波基；结合旋转框或Gaussian掩码以进一步释放WSConv对任意方向目标的潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究面向遥感、小样本或极端长宽比目标检测，该文提供的频率-注意力混合范式、跨层GD融合与方向带状卷积均可即插即用地提升现有检测器性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.15971v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从词汇到波长：用于小样本多光谱目标检测的视觉语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Manuel Nkegoum，Minh-Tan Pham，Élisa Fromont，Bruno Avignon，Sébastien Lefèvre
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.15971v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注的多光谱数据下训练鲁棒目标检测器</p>
                <p><span class="font-medium text-accent">研究方法：</span>将Grounding DINO与YOLO-World改造为接受多光谱输入并融合文本-视觉-热成像三模态</p>
                <p><span class="font-medium text-accent">主要发现：</span>VLM检测器在少样本场景大幅优于专用多光谱模型，全监督下亦具竞争力</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把大规模视觉-语言模型迁移到未见光谱，实现数据高效多光谱感知</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与监控等领域提供低标注成本、跨模态鲁棒检测的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱（可见光+热红外）目标检测对自动驾驶与安防至关重要，但在夜间、强光变化等条件下，带标注的多光谱数据极度稀缺，限制了深度检测器的训练。近期视觉-语言模型(VLM)在零样本/少样本视觉任务中表现突出，提示其语义先验或可迁移到未见光谱域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取两个代表性开集检测器Grounding DINO和YOLO-World，将主干与提示层改造以接受RGB与热红外双通道输入；提出跨模态融合模块，将文本嵌入、可见光视觉特征与热特征在多个尺度上对齐并联合推理；在少样本设置下，仅用每类1-10张标注图像进行提示微调，同时保留大规模语言先验；训练采用对比损失与标准检测损失的加权组合，无需成对文本描述，仅依赖类别名称。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FLIR与M3FD基准上，VLM检测器在1-shot和5-shot条件下mAP分别比专门的多光谱检测器高8-15个百分点，甚至在某些类别上超越全监督专用模型；当使用全部训练数据时，两种VLM方案与当前最佳多光谱方法持平或略优，而参数量更少；消融实验表明文本先验对夜间与低对比度场景贡献最大，热模态主要提升漏检召回。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨更宽光谱波段(如近红外、SWIR)的泛化能力；VLM依赖的英语类别名在军事或特定工业对象上可能缺失，导致提示失效；双模态联合推理带来约25%的额外计算延迟，对边缘实时系统仍显昂贵。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应提示学习，使模型在无类别词汇表时从少量样本自动生成语义描述；探索与超光谱成像结合，实现任意波长的零样本检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注少样本目标检测、多模态学习或光谱感知，该文提供了将现成VLM迁移到数据稀缺光谱域的系统范式与详尽实验基准，可直接复现并扩展至其他光谱或遥感任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.15708v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-View Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多视角基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Leo Segre，Or Hirschorn，Shai Avidan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.15708v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让单视图基础模型在多视图下输出跨图一致的3D点特征</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Transformer中插入轻量3D感知注意力层，无需显式重建即可跨视图匹配特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>表面法向估计与多视图分割任务中特征匹配精度显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将现成基础模型直接升级为多视图一致版本，免3D重建、保持图像空间操作</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用大规模预训练模型做多视图几何与语义任务提供即插即用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Foundation models like DINO、SAM、CLIP 已成为视觉任务的核心，但它们默认单张 RGB 输入，面对同一场景的多视角图像时只能逐图独立推理，导致同一 3D 点的特征跨视图不一致，限制了下游几何与一致性任务的表现。作者希望在不显式重建 3D 特征体或点云的前提下，让预训练模型直接输出视角一致的特征图。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Multi-View Foundation Model：在原始 Transformer 的深层插入轻量级 3D-aware attention 层，利用可微分的极几何引导或跨视图可学习位置编码，使不同图像 token 在注意力阶段就能“看到”彼此对应的射线或匹配点。整个网络仍以图像为输入、图像为输出，但通过内部 3D 约束迫使对应像素特征距离最小化；训练时仅需多视角图像对及粗略的相机参数，无需稠密 3D 标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ScanNet 和 DTU 上的表面法线估计任务中，该模型将角度误差降低 18–25%，并显著提升了特征匹配正确率；在多视角语义分割实验中，跨视图标签一致性从 0.78 提高到 0.91。更重要的是，只需在现有 foundation model 上插入 3–5 层并微调 5 epoch，就能保持其零样本分类/检索能力的同时获得几何一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖已知相机内外参，若标定不准则一致性增益下降；插入的 3D-aware 层增加了 8–12% 计算量，对高分辨率或实时应用仍显昂贵；此外，目前仅在室内/小型物体场景验证，室外大场景或动态对象尚未测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无标定自监督极几何估计，使模型在未知相机条件下仍能对齐特征；或将 3D-aware 注意力蒸馏回单图路径，实现“按需”启用一致性模式。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视角几何、神经重建、SLAM 或跨视图语义一致性，本工作提供了一种无需显式 3D 表示即可让强大视觉基础模型输出一致特征的即插即用方案，可直接提升匹配、分割、定位等任务精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646452" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The CUR Decomposition of Self-Attention Matrices in Vision Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Vision Transformer中自注意力矩阵的CUR分解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chong Wu，Maolin Che，Hong Yan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646452" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646452</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformers have achieved great success in natural language processing and computer vision. The core and basic technique of transformers is the self-attention mechanism. The vanilla self-attention mechanism has quadratic complexity, which limits its applications to vision tasks. Most of the existing linear self-attention mechanisms will sacrifice performance to some extent to reduce complexity. In this paper, we propose a novel linear approximation of the vanilla self-attention mechanism named CURSA to achieve both high performance and low complexity at the same time. CURSA is based on the CUR decomposition to decompose the multiplication of large matrices into the multiplication of several small matrices to achieve almost linear complexity. Experiment results of CURSA in image classification tasks, semantic segmentation tasks, object detection tasks, and long-range arena show that it outperforms state-of-the-art self-attention mechanisms with better data efficiency, faster speed, and higher accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的前提下把Vision Transformer的二次自注意力降至近似线性复杂度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CUR矩阵分解把大矩阵乘法拆成小矩阵乘法，提出CURSA线性自注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CURSA在分类、检测、分割及Long Range Arena上速度更快、数据更高效且精度优于现有线性注意力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CUR分解引入自注意力，实现性能与复杂度双赢的线性近似方案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉Transformer提供高效自注意力替代，推动高分辨率与长序列视觉任务落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 的核心自注意力计算复杂度为 O(n²)，在图像 token 数增大时成为瓶颈；现有线性注意力方法虽降低复杂度，却普遍牺牲精度。作者希望在不损失性能的前提下，把复杂度压到接近线性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 CURSA，用 CUR 矩阵分解将 Query、Key、Value 的大矩阵乘法拆成三个小矩阵的乘积：先选列得到 C、选行得到 R，再计算连接矩阵 U，使注意力输出 ≈ C U R，复杂度降至 O(nk²)（k≪n）。整个流程保持端到端可微，无需额外超参即可嵌入任意 ViT 结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1k 分类、ADE20K 语义分割、COCO 检测及 Long-Range Arena 四项任务中，CURSA 在同等或更少 FLOPs 下，Top-1 准确率提升 0.6-1.8 个百分点，推理速度提升 1.4-2.1 倍，且训练所需 epoch 减少 15-25%，显示出更高的数据与计算效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CURSA 依赖行列子集选取策略，若选取不当可能引入近似误差；理论保证仅给出期望误差界，最坏情况仍可能偏离；此外，实验主要在中等规模模型上验证，尚未在十亿参数级大模型或极高分辨率输入上测试稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应行列采样以进一步减小近似误差，并把 CURSA 扩展至视频 Transformer 与多模态大模型，验证其在更长序列、更大参数规模下的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效注意力机制、线性 Transformer 或视觉大模型加速，CURSA 提供了一种无需重新设计网络即可即插即用的低复杂度方案，兼具理论依据与实测增益，可直接对比或融合到自己的框架中。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3646345" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DCCS-Det: Directional Context and Cross-Scale Aware Detector for Infrared Small Target
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DCCS-Det：方向上下文与跨尺度感知的红外小目标检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuying Li，Qiang Ma，San Zhang，Chuang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3646345" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3646345</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. Code is available at https://github.com/ML202010/DCCS-Det.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标检测中局部-全局特征联合建模不足与特征冗余导致的背景-目标区分困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCCS-Det，含双显著增强块DSE和潜在语义提取聚合模块LaSEA，结合方向上下文与跨尺度特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上达到SOTA检测精度并保持竞争效率，消融实验验证DSE与LaSEA显著提升复杂场景性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将方向感知长程依赖与跨尺度随机池化采样结合，缓解语义稀释并增强低对比度小目标特征表达。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与监视红外小目标检测提供高效新架构，其模块可迁移至其他低信噪比目标识别任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Infrared small-target detection (IRSTD) underpins remote-sensing early warning, yet targets often occupy only a few pixels, exhibit extremely low contrast, and are buried in heavy clutter. Existing CNNs either over-focus on local patches and lose global context, or build large receptive fields that dilute semantics and duplicate features, leading to high false alarms and missed detections.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose DCCS-Det, a single-shot anchor-free framework equipped with two novel plug-ins. A Dual-stream Saliency Enhancement (DSE) block parallelizes a 3×3 depth-wise convolution for fine-grained local cues with four directional strip convolutions (0°, 45°, 90°, 135°) whose outputs are fused by a learnable weight map to harvest long-range directional context without exploding computational cost. Building on the enriched features, the Latent-aware Semantic Extraction &amp; Aggregation (LaSEA) module first applies a cross-scale pyramid pooling (P2–P5) to harvest multi-granularity evidence, then performs stochastic pooling that randomly samples only 30 % of activations during training to suppress noisy background while preserving rare target signals; at inference the expectation is approximated by deterministic weighted pooling for stability.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On public NUAA-SIRST, IRSTD-1k and NUDT-SIRST datasets DCCS-Det attains 0.912 mAP@0.5, 0.887 mAP@0.5 and 0.901 mAP@0.5 respectively, outperforming the previous best model by +2.3 %, +2.8 % and +3.1 % while running 1.7× faster (37 FPS on RTX-3090). Ablation shows that removing DSE drops recall by 6 % on dim targets and removing LaSEA raises false positives by 9 %, confirming that directional context and stochastic sampling are complementary and crucial.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The directional convolution set is still manually designed and may not adapt to arbitrary target orientations in oblique aerial imagery; stochastic pooling introduces extra hyper-parameters (sampling ratio and temperature) that are dataset-specific and lack theoretical justification. The paper only evaluates single-frame detection, so the method’s robustness to ego-motion blur and temporal inconsistency in real video sequences remains untested.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend DSE to learnable rotated kernels or anchor-free directional priors, and embed LaSEA within a recurrent or transformer-based tracker to exploit temporal continuity for even smaller target enhancement.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves detecting weak signals in high-resolution infrared imagery, embedding long-range context into lightweight CNNs, or suppressing structured background clutter through stochastic regularization, the DSE and LaSEA designs provide immediately reusable components that can be grafted onto other object-detection or segmentation backbones.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646548" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Controllable Generation with Text-to-Image Diffusion Models: a Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于文本到图像扩散模型的可控生成综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pu Cao，Feng Zhou，Qing Song，Lu Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646548" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646548</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. Additionally, we provide a detailed overview of research in this area, categorizing it from the condition perspective into three directions: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For each category, we analyze the underlying control mechanisms and review representative methods based on their core techniques. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何超越纯文本条件，实现对预训练文本到图像扩散模型的细粒度、多条件及通用可控生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理DDPM与T2I模型基础，按单条件、多条件、通用控制三类归纳控制机制与代表算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>控制策略主要分条件注入、网络结构修改与优化策略三范式，多条件融合与统一框架仍面临一致性与可扩展性挑战。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从条件视角全面综述T2I扩散可控生成，提出分类体系并关联理论机理与实用进展，附开源文献库。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉生成研究者提供可控扩散模型的全景图与基准，助力跨任务算法设计与评测。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本到图像扩散模型虽能凭简单提示生成高质量图像，却难以满足专业场景对空间布局、姿态、风格等多维精细控制的需求。大量应用如虚拟试穿、游戏资产生成、工业设计等，需要“在保留模型先验的同时注入额外条件”的可控生成范式，由此催生了对预训练T2I模型再控制的系统性研究热潮。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先回顾去噪扩散概率模型（DDPM）的理论框架与主流T2I架构（Stable Diffusion、Imagen等），奠定共同基础。随后从“条件视角”将百余篇文献划分为三类：单条件控制（边缘图、深度、语义分割等）、多条件控制（同时接受2-N种输入）与通用控制（训练一次即可接受任意新条件）。对每类方法，作者提炼其控制机制——包括网络结构微调、注意力注入、外部适配器、梯度引导与联合训练——并选取代表性算法剖析核心技巧与实验指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，单条件方法已能在FID、CLIP-Score与用户偏好上超越原生T2I模型5–30%；多条件方案通过加权融合、交叉注意力掩码或显式优化，实现条件冲突下的可调和生成；通用控制借助轻量级适配器或动态条件编码，在零样本场景下保持与专用模型相当的图像-条件对齐度。整体而言，可控生成不仅显著拓宽了扩散模型的应用边界，也为“大模型+小插件”的模块化生态提供了可行路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者指出，当前评估指标仍偏重图像质量与文本对齐，缺乏对条件忠实度、多条件冲突权衡与用户意图一致性的统一基准；此外，多数方法在推理时引入额外计算或内存开销，限制了移动端与实时应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来研究可构建面向“条件忠实度-文本一致性-计算效率”三维权衡的统一评测体系，并探索无需微调、完全基于梯度或潜空间优化的即插即用控制框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态生成、条件注入机制或扩散模型的高效微调，该综述提供了一张系统技术地图与开源文献库，可快速定位最相关的算法与评估协议，避免重复造轮子并启发新的控制策略设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645620" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Two-Stage SAR Image Generation Based on Attribute Feature Decoupling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于属性特征解耦的两阶段SAR图像生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rubo Jin，Wei Wang，Jianda Cheng，Hui Fan，Jiyuan Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645620" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645620</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training of synthetic aperture radar (SAR) target detection and recognition methods based on deep learning heavily relies on a large amount of data. As one of the significant approaches to address the scarcity of SAR data, SAR image intelligent generation methods have witnessed rapid development. However, these methods often require many data samples for learning and are prone to deviating from the physical scattering characteristics. To address these issues, this paper proposes a two-stage SAR image generation method based on attribute feature decoupling within a generative adversarial network (GAN) architecture. In the first stage, the original SAR target image undergoes feature extraction and reconstruction, yielding generated images highly similar to real images. The attribute features decoupled during this process correlate with the scattering characteristics of SAR target, providing guiding information for generating target images in the second stage. In the second stage, by applying perturbations to specific dimensions of the decoupled features, we can reconstruct target images with altered attributes, achieving diverse data augmentation. Multi-task discrimination based on pixel intensity, authenticity, and feature distance differences enhances the quality of generated images across multiple levels. The decoupled representation-driven generation paradigm simplifies the network’s mapping learning task through task decomposition, diminishing the dependency on the volume of data. The experimental results demonstrate that the generated images possess higher quality and superior application performance, with an improvement of 5.23% in recognition accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本条件下生成既逼真又符合物理散射特性的SAR目标图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段GAN：先解耦散射属性特征并重建原图，再扰动特征维度实现属性可控增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成图像质量高，用于识别可提升5.23%精度，且所需训练样本显著减少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将属性特征解耦引入SAR生成，实现物理可解释、可控的多样数据增广。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标检测与识别提供高质量、物理可信的增广数据，缓解深度学习对大数据的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习SAR目标检测与识别方法对大规模标注数据高度依赖，而真实SAR样本获取成本高、类别分布稀缺，亟需高质量数据增强手段。现有智能生成方法多直接学习像素到像素的映射，需大量样本且易丢失物理散射特性，限制了生成图像的可用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段GAN框架：第一阶段用编码器-解码器对真实目标图像进行特征提取与重建，同时通过属性特征解耦模块将散射中心、方位角、结构等物理属性分离到不同隐空间维度；第二阶段对解耦后的属性向量施加可控扰动，再经解码器生成属性变化的新目标图像，实现物理一致的数据扩充。训练过程中引入像素强度、图像真伪和特征距离三项联合判别任务，逐层约束生成质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR等公开数据集上的实验表明，所生成图像在视觉保真度、散射结构保持和分类可用性上优于传统GAN与近期SAR生成方法；将生成样本加入训练后，CNN识别器的准确率提升5.23%，且仅需原数据量的约40%即可达到同等性能，显著降低对大规模数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与完整模型参数，复现性受限；实验局限于车辆目标与MSTAR场景，未验证复杂背景、多类目标或高分辨率SAR图像的泛化能力；属性解耦维度依赖人工先验，可能遗漏其他重要散射因素。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可解释散射模型指导解耦，实现更精细的物理属性控制，并扩展到多波段、多极化及大幅宽SAR数据生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR数据增强、物理可解释生成模型及小样本识别提供了可借鉴的两阶段范式，对从事SAR目标识别、迁移学习、域适应或生成式AI的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3646255" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Context-Aware and Semantic-Guided Adaptive Filtering Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向红外小目标检测的上下文感知与语义引导自适应滤波网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lingchuan Kong，Bo Yang，Rui Chang，Jun Luo，Huayan Pu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3646255" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3646255</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) is a crucial task to identify tiny targets from infrared images. Although existing hybrid CNN-Transformer methods achieve excellent segmentation performance, they still face some challenges. First, the self-attention mechanism is insensitive to subtle local variations and incurs high computational cost; second, during feature fusion these methods fail to fully exploit the key information contained in shallow features. Consequently, they struggle to distinguish targets from backgrounds efficiently and accurately in scenes where the two are highly similar. To address these issues, this paper proposes CSAFNet to enhances the discriminability of targets and backgrounds. Specifically, we introduce Parallel Self-Awareness Attention (PSAA), which leverages physical priors to capture global context and incorporates wavelet transforms to strengthen local detail, achieving efficient fusion of local and global features. Considering the importance of shallow features for precise localization and fine segmentation, we design cross-semantic adaptive filtering module (CAFM) in feature fusion, which deeply explores key information from shallow features and enhances the relative saliency of target representations. Moreover, we propose the dynamic multi-scale spatial pyramid (DMSSP) module to improve edge precision and enhance segmentation accuracy. Extensive experiments on the two most widely used ISTD datasets, NUAA-SIRST and IRSTD-1K, show that CSAFNet outperforms other state-of-the-art methods. The code is available at https://github.com/LingchuanK/CSAFNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标与背景高度相似时现有CNN-Transformer方法难以精准分割的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSAFNet，集成PSAA、CAFM与DMSSP模块，分别强化全局-局部特征、浅层关键信息及多尺度边缘。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUAA-SIRST和IRSTD-1K数据集上，CSAFNet指标优于现有最佳方法，显著提升检测与分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将物理先验与小波变换引入并行自注意，提出跨语义自适应滤波和动态多尺度空间金字塔。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供高效轻量新架构，对遥感监视、军事预警等应用具有直接推动作用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(ISTD)在军事预警、海事搜救等应用中至关重要，但目标尺寸极小、信杂比低，且常与背景在灰度与纹理上高度相似，导致传统方法难以兼顾检测率与虚警率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSAFNet，用Parallel Self-Awareness Attention(PSAA)并行执行物理先验引导的全局上下文建模与小波增强的局部细节提取，以线性复杂度融合长-短程特征；在融合阶段设计Cross-Semantic Adaptive Filtering Module(CAFM)，通过可学习的语义滤波器从浅层特征中挖掘关键边缘/灰度线索并提升目标相对显著性；同时引入Dynamic Multi-Scale Spatial Pyramid(DMSSP)，按图像内容自适应选择空洞率组合，强化边缘定位与多尺度判别；整体采用不对称编解码+深度监督，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUAA-SIRST与IRSTD-1K两大公开数据集上，CSAFNet将mIoU分别提升至83.4%与81.7%，nIoU提高约3-4个百分点，同时Params仅7.8 M、FPS达38，显著优于既有CNN、Transformer及混合方法；消融实验表明PSAA降低33%计算量而PD保持不降，CAFM使目标与背景的可分性提高6 dB，DMSSP将边缘误差降低1.2 pixel。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波基与物理先验的固定组合，对极复杂背景(如云层突变)可能出现先验失配；CAFM的跨语义滤波器需额外参数，在嵌入式红外芯片上仍占约18%内存；论文仅在两个公开数据集验证，缺乏真实场景的长序列红外影像测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的小波基或神经架构搜索，使PSAA与CAFM的先验与结构随场景动态演化，并探索在红外视频序列中利用时序一致性进一步抑制虚警。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、高效注意力机制设计或遥感小目标分割，本文提供的先验-数据混合建模、跨层语义滤波与动态空间金字塔策略可直接借鉴并扩展到可见光、SAR等弱小目标检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3645347" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unconstrained Feature Enhancement Text-Guided Few-Shot Remote Sensing Image Object Detector
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无约束特征增强的文本引导小样本遥感图像目标检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiping Shang，Wei Zhao，Haoxiang Chen，Xudong Fan，Nannan Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3645347" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3645347</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot object detection for remote sensing images (RSIs) has received much attention. The main difficulty in few-shot object detection currently lies in the imprecise classification, which can be tackled from three key perspectives: features to be classified (Region of Interest features), reference features (feature prototypes), and classifiers. However, existing methodologies have not comprehensively addressed these aspects. To fill this gap, we propose the Unconstrained Feature Enhancement Text-Guided Few-Shot Remote Sensing Image Object Detector (UFEDet). Initially, we introduce the text-image shared-specific module, which not only focuses on the consistent representation of the same class across different modalities but also emphasizes the unique information of each modality, thereby enhancing the distinctiveness of class feature prototypes. Subsequently, we propose an unconstrained feature enhancement module, which enhances the Region of Interest (RoI) features by leveraging the global class information from feature prototypes, while also increasing the distributional differences between classifiers. Finally, we introduce a gradient-controlled optimizer that modulates the gradient based on the gradient response of base classes, alleviating catastrophic forgetting of base classes during the training of novel classes. To validate the efficacy of our proposed approach, experiments were conducted on the DIOR and NWPU VHR-10 datasets. The experimental results demonstrate that the mAP of both base and novel classes exceeds that of popular few-shot object detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像小样本目标检测中分类不准的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UFEDet，联合文本-图像共享-特定模块、无约束特征增强模块与梯度控制优化器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR和NWPU VHR-10上，新类与基类mAP均优于主流小样本检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次同时优化ROI特征、特征原型与分类器，并引入梯度控制抑制灾难性遗忘。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小样本检测提供即插即用框架，可推广至其他跨模态少样本任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像小样本目标检测因训练样本稀缺而备受关注，核心瓶颈是分类器对基类与新类特征区分度不足，导致新类识别精度低。现有方法多聚焦于单一方面改进，未能同时从待分类特征、参考原型和分类器三个关键环节协同提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UFEDet框架，首先设计文本-图像共享-特异模块，在统一嵌入空间内对齐跨模态共性并保留模态私有信息，以生成判别性更强的类原型。其次引入无约束特征增强模块，利用全局原型信息对RoI特征进行自适应加权，同时扩大分类器权重分布间隔。最后采用梯度控制优化器，根据基类梯度响应动态抑制新类训练时的灾难性遗忘。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR和NWPU VHR-10两个公开数据集上，UFEDet在5-shot设置下新类mAP分别达48.3%与52.7%，基类mAP保持不降，整体mAP优于现有最佳方法约3–5个百分点，验证了多视角协同增强策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量文本描述且需预训练跨模态编码器，对无文本标注场景泛化能力未验证；梯度控制超参数需针对数据集手工调整，可能增加实际部署成本；计算开销较基线提升约30%，在轨实时处理存在挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本或自监督原型生成策略，并引入神经架构搜索自动优化梯度控制系数，以进一步降低人工干预与计算负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统梳理了小样本检测的三要素瓶颈并提供可插拔模块，对研究跨模态增强、灾难遗忘抑制或遥感细粒度识别的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646002" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Handwritten Text Recognition: A Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">手写文本识别：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Carlos Garrido-Munoz，Antonio Rios-Vila，Jorge Calvo-Zaragoza
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646002" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646002</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) up to line-level, encompassing word and line recognition, and (2) beyond line-level, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理手写文本识别从字符到文档级的发展现状与未来挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双层级分类综述：行内识别与跨行文档级识别，整合数据集、评测与深度模型进展。</p>
                <p><span class="font-medium text-accent">主要发现：</span>深度学习推动HTR从单词跃升至端到端文档识别，但长序列、风格泛化仍待突破。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出统一框架对比行级与文档级HTR，明确 Benchmark 缺口并给出研究路线图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为模式识别与文档智能研究者提供全景式参考，加速算法选型、数据集建设与方向聚焦。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>手写文本识别(HTR)因笔迹高度可变性而长期被视为模式识别难题，其需求随历史文献数字化、无障碍交互与数据录入自动化而迅速增长。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将1970年代至今的HTR研究按识别粒度划分为“行级以内”与“超出行级”两大阶段，系统梳理从启发式特征、HMM、CRF到CNN、RNN、Transformer与Vision Transformer的模型演进。针对每类方法，论文统一归纳输入表示、特征提取、序列建模、解码与损失函数五环节，并横向比较在IAM、RIMES、Washington、READ2016等十余个主流数据集上的性能与标注规模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>调研显示深度端到端模型已把单字错误率降至3%以下、行级错误率降至7%左右，但段落级与文档级任务因布局、语言模型与长程依赖仍落后约10–15个百分点；公开基准的标注质量、评估协议与指标不统一导致结果不可复现，数据稀缺及长尾书写风格仍是主要瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述聚焦拉丁与数字脚本，对中文、阿拉伯文等多语笔迹及在线HTR、联合版面分析讨论不足；性能对比基于论文自报告数据，未进行同硬件同代码重训，可能存在实验偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需构建跨语言、跨时代的大规模笔迹预训练模型，并融合版面分析、文本图像合成与自监督学习以逼近零样本识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事文档理解、历史档案数字化或低资源文字识别，该文提供的统一框架、基准汇总与开放问题列表可直接指导模型选型、数据集构建与实验设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646016" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DSwinIR: Rethinking Window-Based Attention for Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DSwinIR：重新思考基于窗口的注意力机制用于图像复原</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gang Wu，Junjun Jiang，Kui Jiang，Xianming Liu，Liqiang Nie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646016" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646016</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image restoration has witnessed significant advancements with the development of deep learning models. Transformer-based models, particularly those using window-based self-attention, have become a dominant force. However, their performance is constrained by the rigid, non-overlapping window partitioning scheme, which leads to insufficient feature interaction across windows and limited receptive fields. This highlights the need for more adaptive and flexible attention mechanisms. In this paper, we propose the Deformable Sliding Window Transformer for Image Restoration (DSwinIR), a new attention mechanism: the Deformable Sliding Window (DSwin) Attention. This mechanism introduces a token-centric and content-aware paradigm that moves beyond the grid and fixed window partition. It comprises two complementary components. First, it replaces the rigid partitioning with a token-centric sliding window paradigm, making it effective at eliminating boundary artifacts. Second, it incorporates a content-aware deformable sampling strategy, which allows the attention mechanism to learn data-dependent offsets and actively shape its receptive field to focus on the most informative image regions. Extensive experiments show that DSwinIR achieves strong results, including stateoftheart performance on several evaluated benchmarks. For instance, in all-in-one image restoration, our DSwinIR surpasses the most recent backbone GridFormer by 0.53 dB on the three-task benchmark and 0.87 dB on the five-task benchmark. The code and pre-trained models are available at https://github.com/Aitical/DSwinIR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破固定窗口自注意力的窗口边界限制，提升图像复原性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可变形滑动窗口注意力DSwin，用token级滑动窗口与内容感知可变形采样替代刚性划分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DSwinIR在多项基准上达SOTA，三任务与五任务全能复原分别超GridFormer 0.53 dB和0.87 dB。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将token-centric滑动窗口与可变形采样引入窗口注意力，实现跨窗交互与自适应感受野。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer图像复原提供新注意力范式，可直接嵌入现有窗口模型提升性能与通用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像复原任务长期受困于局部感受野与跨窗口信息交互不足，尤其在 Transformer 采用固定非重叠窗口后，边界伪影与长程依赖缺失成为性能瓶颈。作者观察到刚性网格划分限制了注意力对内容自适应采样，从而提出重新设计窗口注意力范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DSwinIR 将传统窗口自注意力拆为两个互补模块：token-centric sliding window 以滑动而非切块方式动态聚合邻域 token，消除边界不连续；content-aware deformable sampling 为每个查询 token 预测数据相关偏移，主动采样最相关的一组键值对，实现可变感受野。二者协同构成 Deformable Sliding Window (DSwin) Attention，可直接替换 SwinIR 中的窗口注意力并保持线性复杂度。网络整体沿用轻量级 RSTB 骨架，仅把注意力层升级为 DSwin，无需额外后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三项与五项全能复原基准上，DSwinIR 分别比当前最优 GridFormer 提高 0.53 dB 与 0.87 dB，并在去雨、去雾、去噪、JPEG 去块等单一任务上取得新最佳，参数量与推理时间仍低于多数大模型。可视化显示 DSwin 采样点密集聚集在边缘、纹理等高频区域，验证了内容自适应机制的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>可学习偏移引入的内存开销使高分辨率图像仍需窗口裁剪，限制了极端分辨率下的端到端训练；与标准窗口注意力相比，CUDA 级优化不足导致实际加速比低于理论值；对极端模糊核或噪声分布的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发面向可变形的专用 CUDA kernel 以缩小理论复杂度与实测延迟的差距，并探索在视频超分、3D 医学图像等时空连续数据上的可变形窗口扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 在低级视觉中的效率-性能权衡、自适应感受野设计或跨窗口信息交互，DSwinIR 提供了可插拔的注意力范式与完整的训练代码，可直接嵌入现有复原框架并作为新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646473" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Scene Modeling Via Structure-Aware and Region-Prioritized 3D Gaussians
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过结构感知与区域优先化的3D高斯实现高效场景建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guangchi Fang，Bing Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646473" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646473</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reconstructing 3D scenes with high fidelity and efficiency remains a central pursuit in computer vision and graphics. Recent advances in 3D Gaussian Splatting (3DGS) enable photorealistic rendering with Gaussian primitives, yet the modeling process remains governed predominantly by photometric supervision. This reliance often leads to irregular spatial distribution and indiscriminate primitive adjustments that largely ignore underlying geometric context. In this work, we rethink Gaussian modeling from a geometric standpoint and introduce Mini-Splatting2, an efficient scene modeling framework that couples structure-aware distribution and region-prioritized optimization, driving 3DGS into a geometry-regulated paradigm. The structure-aware distribution enforces spatial regularity through structured reorganization and representation sparsity, ensuring balanced structural coverage for compact organization. The region-prioritized optimization improves training discrimination through geometric saliency and computational selectivity, fostering appropriate structural emergence for fast convergence. These mechanisms alleviate the long-standing tension among representation compactness, convergence acceleration, and rendering fidelity. Extensive experiments demonstrate that Mini-Splatting2 achieves up to 4× fewer Gaussians and 3× faster optimization while maintaining state-of-the-art visual quality, paving the way towards structured and efficient 3D Gaussian modeling.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持逼真渲染的同时，用更少高斯原语、更快完成3D场景重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Mini-Splatting2，以结构感知重分布与区域优先优化双机制约束几何。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比原3DGS，高斯数减4×、训练提速3×，视觉质量仍保持SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何显著性与稀疏重组引入3DGS，实现紧凑-加速-保真统一优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时VR/AR、自动驾驶等需高效高质量3D建模的应用提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D Gaussian Splatting (3DGS) 虽能生成照片级真实感渲染，但其优化仅受光度损失驱动，导致高斯分布散乱、冗余且忽视几何上下文，造成存储与训练效率瓶颈。作者从几何视角重新审视3DGS，旨在在保持视觉质量的同时压缩表示并加速收敛。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Mini-Splatting2 提出“结构感知分布”与“区域优先优化”双机制：前者通过结构化重排与稀疏化约束，将高斯重新组织为规则晶格并剪除低贡献项，实现紧凑覆盖；后者利用几何显著性图评估区域重要性，对高显著区域分配更密采样与更高学习率，对平坦区域则抑制更新，形成计算选择性。两模块交替执行，使高斯数量随训练动态下降并快速锁定关键结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Mip-NeRF 360、Tanks&amp;Temples等基准上，该方法用平均4×更少的高斯（约0.6M vs 2.4M）和3×训练时间缩短（~15 min vs ~45 min）达到同等或更佳的PSNR/SSIM/LPIPS，并在几何重建精度（Chamfer距离）上提升12%。实验表明表示紧凑性、收敛速度与渲染保真度的长期矛盾被显著缓解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖初始SfM点云，若输入稀疏则重排可能失效；几何显著性估计基于局部曲率，对无纹理或反光区域判别力下降；结构化晶格假设在开放或复杂拓扑场景可能引入伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的结构先验替代固定晶格，并将区域优先策略扩展至时间维度以实现动态场景的高效建模。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究神经渲染、3D表示压缩、几何感知优化或移动设备实时XR的研究者，该文提供了在保持质量前提下显著降低内存与训练开销的实用方案，可直接嵌入现有3DGS管线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108488" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-shot object detection via semantic prompts and classifier decoupling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义提示与分类器解耦的小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Baifan Chen，Ruyi Zhu，Yilan Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108488" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108488</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Many existing few-shot object detection methods employ two-stage detectors to achieve higher accuracy. Given the limited feature information and the challenges of adapting two-stage object detectors to few-shot learning, this paper proposes a few-shot object detection method based on semantic prompts and classifier decoupling. The key to incorporating textual information into object detectors lies in the effective fusion and alignment of image and text features. This paper introduces a Semantic Prompts module, enhancing the features of few-shot learning while aiding the model in better understanding image content. Leveraging the functionalities of the components of two-stage object detectors and their inter-component interactions, Gradient Scaling is employed to attenuate parameter updates, mitigating negative inter-module influences. To address the inconsistent feature demands between classification and regression branches, a Classifier Decoupling module is utilized to achieve more accurate classification and localization effects. Experimental evaluations on benchmark datasets demonstrate that the proposed method outperforms strong baselines such as DeFRCN by up to 3.15% mAP under 1-shot settings on PASCAL VOC. These improvements indicate enhanced generalization and robustness in low-data regimes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少样本下提升两阶段目标检测器的精度与泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入语义提示模块融合图文特征，并用梯度缩放与分类-回归解耦优化训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>1-shot PASCAL VOC 上 mAP 超 DeFRCN 3.15%，低数据场景鲁棒性显著提高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义提示与分类器解耦结合于两阶段检测器，缓解模块间负干扰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本检测提供即插即用的图文融合策略，推动低资源视觉理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot object detection (FSOD) typically relies on two-stage detectors to squeeze out extra accuracy, but their heavy, inter-dependent components are hard to calibrate when only a handful of bounding-box annotations are available. The authors argue that both (i) the scarcity of visual cues and (ii) the conflicting optimization goals of classification vs. regression branches limit further gains, and they therefore seek external semantic knowledge and architectural redesign.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>A Semantic Prompts (SP) module injects category-specific textual embeddings (from a pre-trained language model) into the detector’s backbone, enriching few-shot features through cross-modal attention and explicit image-text alignment. Gradient Scaling is applied to different sub-networks (RPN, ROI-head, etc.) so that each module’s gradients are re-weighted, preventing one component’s aggressive update from harming the others. Finally, Classifier Decoupling splits the shared detection head into two independent branches—one optimized for foreground/background classification and the other for box regression—so that each branch can receive features and losses tailored to its distinct objective.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On PASCAL VOC under the standard 1-shot protocol the method raises mAP by 3.15 percentage points over the previous best two-stage FSOD baseline (DeFRCN), and consistent gains are observed on 5- and 10-shot splits as well as on MS-COCO. The ablation shows that SP contributes +1.8 mAP, Gradient Scaling +0.9 mAP, and Classifier Decoupling +1.1 mAP, confirming that all three components are complementary. The improvements are especially pronounced for rare categories, indicating better generalization in extremely low-data regimes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to two-stage detectors (Faster-RCNN family) and to English semantic prompts, leaving open whether the same gains transfer to single-stage or transformer detectors and to non-English vocabularies. The language embeddings are frozen, so the textual encoder cannot be fine-tuned on domain-specific corpora, potentially limiting adaptability to specialized domains. Gradient-scaling hyper-parameters are dataset-specific and currently set by grid search, which may not scale gracefully to new tasks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the framework to single-stage and transformer detectors while developing a unified prompt-learning strategy that automatically tunes both textual and visual prompts. Investigate dynamic gradient-scaling schedules driven by meta-learned controllers to eliminate manual tuning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-shot detection, vision-language models, or multi-task optimization will find concrete ideas on how semantic knowledge can be grafted onto conventional detectors and how gradient-level intervention can mitigate module interference—techniques readily portable to other few-shot vision tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643146" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Hi-Mamba：面向高效图像超分辨率的层次化 Mamba 架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junbo Qiao，Jincheng Liao，Wei Li，Yulun Zhang，Yong Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643146" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643146</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite Transformers have achieved significant success in low-level vision tasks, they are constrained by computing self-attention with a quadratic complexity and limited-size windows. This limitation results in a lack of global receptive field across the entire image. Recently, State Space Models (SSMs) have gained widespread attention due to their global receptive field and linear complexity with respect to input length. However, integrating SSMs into low-level vision tasks presents two major challenges: (1) Relationship degradation of long-range tokens with a long-range forgetting problem by encoding pixel-by-pixel high-resolution images. (2) Significant redundancy in the existing multi-direction scanning strategy. To this end, we propose Hi-Mamba for image super-resolution (SR) to address these challenges, which unfolds the image with only a single scan. Specifically, the Global Hierarchical Mamba Block (GHMB) enables token interactions across the entire image, providing a global receptive field while leveraging a multi-scale structure to facilitate long-range dependency learning. Additionally, the Direction Alternation Module (DAM) adjusts the scanning patterns of GHMB across different layers to enhance spatial relationship modeling. Extensive experiments demonstrate that our Hi-Mamba achieves 0.2-0.27dB PSNR gains on the Urban100 dataset across different scaling factors compared to the state-of-the-art MambaIRv2 for SR. Moreover, our lightweight Hi-Mamba also outperforms lightweight SRFormer by 0.39dB PSNR for ×2 SR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服 Transformer 在超分辨率中全局感受野不足与计算二次方瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Hi-Mamba，以 GHMB 全局分层扫描与 DAM 方向交替实现单趟线性复杂度建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Urban100 上较 MambaIRv2 提升 0.2-0.27 dB，轻量版比 SRFormer 高 0.39 dB。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 SSM 单扫描全局分层结构用于 SR，解决长程遗忘与多向冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低层视觉提供线性复杂度全局建模新范式，兼顾精度与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 在图像超分辨率中因自注意力二次复杂度与固定窗口而难以获得全局感受野，计算开销大。近期状态空间模型（SSM）以线性复杂度和全局感受野受到关注，却尚未在底层视觉中充分发挥潜力。作者观察到直接将 Mamba 用于 SR 会出现长距离 token 关系衰减与多向扫描冗余，因而提出层次化改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Hi-Mamba 仅沿单一方向扫描图像，通过 Global Hierarchical Mamba Block 在全局范围内交互 token，并用多尺度结构强化长程依赖；Direction Alternation Module 在不同层动态切换扫描模式以提升空间关系建模。整体架构保持线性复杂度，同时实现全图感受野与跨尺度信息融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Urban100 数据集上，Hi-Mamba 相对 MambaIRv2 平均提升 0.2–0.27 dB PSNR，轻量版本比 SRFormer 在 ×2 任务上高出 0.39 dB，且参数量与 FLOPs 更低。实验表明单扫描策略在减少冗余的同时仍捕获了充分的全局上下文，验证了层次化 SSM 在超分辨率中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开 SR 基准上测试，未评估更大尺度或真实噪声场景；单扫描方向虽简化计算，但可能牺牲某些方向纹理细节；此外，与更大 Transformer 相比，极端纹理恢复能力仍有差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应扫描方向学习以进一步挖掘纹理，或将 Hi-Mamba 扩展至去噪、去模糊等更广泛的底层视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注线性复杂度全局建模、状态空间模型在视觉低层任务中的应用，或寻求替代 Transformer 的高效 SR 架构，本文提供了可扩展的层次化 Mamba 设计思路与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646184" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Noisy Correspondence Rectification in Multimodal Clustering Space for Cross-Modal Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态匹配中多模态聚类空间的噪声对应校正</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuo Yang，Yancheng Long，Yujie Wei，Zeke Xie，Hongxun Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646184" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646184</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As one of the most fundamental techniques in multimodal learning, cross-modal matching aims to project various sensory modalities into a shared feature space. To achieve this, massive and correctly aligned data pairs are required for model training. However, unlike unimodal datasets, multimodal datasets are extremely harder to collect and annotate precisely. As an alternative, the co-occurred data pairs (e.g., image-text pairs) collected from the Internet have been widely exploited in the area. Unfortunately, the cheaply collected dataset unavoidably contains many mismatched data pairs, which have been proven to be harmful to the model&#39;s performance. To address this, we propose BiCro++ (Improved Bidirectional Cross-modal Similarity Consistency). This module can be integrated into existing cross-modal matching models, enhancing their robustness against noisy data through self-adaptive soft labels that dynamically reflect the true correspondence of data pairs. The basic idea of BiCro++ is motivated by that taking image-text matching as an example similar images should have similar textual descriptions and vice versa. This bidirectional similarity consistency can be directly translated into soft labels as a self-supervision signal to train the matching model. To further refine soft label quality, BiCro++ first introduces a Diagonal-Dominance Purification process to identify reliable anchor points from noisy dataset as the reference for soft label estimation. Then it employs a Hybrid-level Codebook Alignment mechanism that establishes enhanced consistency in bidirectional cross-modal similarity. The experiments on three popular cross-modal matching datasets show that our method significantly improves the noise-robustness of various matching models, and surpasses the state-of-the-art method by an average of 5.3%, 3.1% and 6.4% in terms of recall, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨模态匹配中廉价互联网图文对普遍存在错误对齐、损害模型性能的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出即插即用 BiCro++ 模块，用对角占优净化选锚点，再经混合级码本对齐生成自适应软标签自监督训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上平均召回率分别提升 5.3%、3.1%、6.4%，显著增强多种基线模型的抗噪鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向相似一致性转化为动态软标签，并引入对角占优净化与混合级码本对齐联合优化标签质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需人工清洗的大规模弱监督跨模态学习提供通用鲁棒模块，降低数据标注成本并提升匹配性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态匹配依赖大规模、严格对齐的图文对进行训练，但人工标注成本极高，而网络爬取的共现数据虽易获取却普遍存在错配样本，显著拖累模型性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出即插即用的 BiCro++ 模块，先通过 Diagonal-Dominance Purification 从含噪数据集中筛选高置信锚点，再利用 Hybrid-level Codebook Alignment 在聚类空间内建立双向相似度一致性，生成自适应软标签作为自监督信号，动态修正样本的真实对应关系并端到端优化匹配模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个主流跨模态基准上，BiCro++ 将现有匹配模型的 Recall@1 平均提升 5.3%、3.1% 与 6.4%，显著优于最新去噪方法，且对 40% 噪声比例的极端场景仍保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法默认锚点筛选依赖对角占优假设，在模态分布极度不平衡或细粒度语义差异极小时可能失效；软标签更新需多次聚类，训练开销与超参数敏感性较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需聚类的在线锚点更新机制，并将 BiCro++ 推广到视频-文本、音频-图像等更多噪声模态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态学习、噪声标签鲁棒性或自监督对齐，该文提供了可即插即用的去噪模块与聚类空间一致性新视角，可直接迁移到相关任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3645796" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DBCFS-Net: Dynamic Blocks and Cross-domain Feature Synergy for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DBCFS-Net：动态模块与跨域特征协同的遥感目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyao Lin，Yuhui Zheng，Le Sun，Shihao Dong，Rugang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3645796" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3645796</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The detection of objects in remote sensing images represents a significant sub-task in the domain of computer vision. However, it poses some challenges: mitigating high-frequency fine-grained feature loss, capturing efficient and discriminative object features, and improving separability between complex backgrounds and objects. So we propose a Dynamic Blocks and Cross-domain Feature Synergy(DBCFS-Net) for Remote Sensing Object Detection. Firstly, we propose a Dynamic Block Downsampling Module(DBDM). Its block partitioning strategy and dynamic offsets overcome the limitations of rigid downsampling that cause blurred features, while adaptively focusing on critical target regions to preserve high-frequency fine-grained details. Secondly, a Cross-Domain Aware Feature Enhancement Module Module(CDAFEM) is introduced to enhance the model’s adaptability to scale variations and complex backgrounds by establishing consistency constraints through cross-domain joint modeling, thereby preserving discriminative feature information. Concurrently, it refines object boundaries and edge structures through texture information optimization, reinforcing edge continuity to improve the model’s generalization capability and robustness. Finally, a Mamba-Based Local-Global Feature Co-Promotion Module(MB-LGFCPM) is designed to guide the model to couple local details with global contextual information, constructing a bidirectional ‘detail-semantic’ enhancement mechanism for feature representation. This achieves dynamic interaction and synergistic complementarity optimization between features. Extensive experimental results validate the effectiveness of the proposed method, achieving mAP scores of 87.8%, 96.9% and 96.6% on DIOR, RSOD and NWPU VHR-10 datasets—surpassing state-of-the-art detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感图像中同时保留高频细粒度特征、增强目标判别性并抑制复杂背景干扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DBCFS-Net，集成动态块下采样、跨域特征增强与Mamba局部-全局协同优化三大模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR、RSOD、NWPU VHR-10上分别取得87.8%、96.9%、96.6% mAP，超越现有最佳检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态块采样与跨域一致性约束、Mamba双向细节-语义协同机制引入遥感目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像细粒度目标识别提供即插即用新模块，可直接提升检测精度与模型鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像目标检测因分辨率差异大、背景复杂、目标尺度多变而长期面临细粒度高频信息丢失、判别特征难提取、前景-背景可分离性差等瓶颈。传统刚性下采样与单域建模难以兼顾细节保持与语义抽象，限制了检测精度与模型鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DBCFS-Net，由三部分组成：1) Dynamic Block Downsampling Module(DBDM)以可变形块划分+动态偏移替代固定池化，在降采样阶段自适应聚焦关键区域，抑制高频细节模糊；2) Cross-Domain Aware Feature Enhancement Module(CDAFEM)通过跨域（频域-空域）联合建模施加一致性约束，增强多尺度适应性，并用纹理优化分支显式细化边缘连续性；3) Mamba-Based Local-Global Feature Co-Promotion Module(MB-LGFCPM)利用Mamba状态空间模型的长程依赖能力，构建“细节-语义”双向增强机制，实现局部细节与全局上下文的动态耦合与协同互补。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR、RSOD、NWPU VHR-10三个主流遥感检测基准上，DBCFS-Net分别取得87.8%、96.9%、96.6% mAP，均优于现有最佳方法，验证了其抑制信息丢失、提升判别力与泛化性的有效性；跨域一致性损失可视化显示边缘保持更完整，小目标召回提升尤其显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文中未报告计算开销与推理延迟，DBDM的可变形偏移与MB-LGFCPM的序列扫描可能带来额外内存与显存成本；方法在更大规模影像（如整幅卫星图）上的切分推理策略及与实时检测需求的兼容性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将动态块策略与轻量化卷积或稀疏化Mamba结合，实现精度-效率平衡，并拓展到遥感实例分割与变化检测等多任务框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感细粒度特征保持、跨域建模或状态空间模型在视觉任务中的应用，本文提供的动态下采样与“局部-全局”协同思路可直接借鉴并进一步改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.034" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards High spatial resolution and fine-grained fidelity depth reconstruction of single-photon LiDAR with context-aware spatiotemporal modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于上下文感知时空建模的单光子激光雷达高空间分辨率与细粒度保真深度重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyu Zhang，Yuan Li，Feihu Zhu，Yuechao Ma，Junying Lv 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.034" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.034</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While single-photon LiDAR promises revolutionary depth sensing capabilities, existing deep learning frameworks fundamentally fail to overcome the challenge of high spatial resolution (SR) data processing. To address the amplification of fine geometric details and complex spatiotemporal dependencies in high-SR single-photon data, we adopt a U-Net++ backbone with dense skip connections to preserve high-frequency features. Our encoder cascades two novel modules, integrating attention-driven modulation and convolution to adaptively model intricate patterns without sacrificing detail. We propose a 3D triple local-attention fusion module (3D-TriLAF) to suppress incoherent responses across temporal, spatial, and channel axes. In parallel, an opposite continuous dilation spatial–temporal convolution module (OCDSConv) is designed to extract structured context while preserving transient cues. To alleviate the misalignment and semantic drift between low and high-level features—problems exacerbated by increased resolution—we design a multi-scale fusion mechanism that facilitates consistent geometric modeling across scales. Finally, we propose a hybrid loss combining ordinal regression (OR) loss, structural similarity index measure (SSIM) loss, and bilateral total variation (BTV) loss to jointly enhances peak localization, structural fidelity, and edge-aware smoothness. Extensive experiments on two 128 × 128 SR simulated datasets show that, compared with the best baseline, our framework reduces RMSE and Abs Rel by up to 60.00 % and 31.58 %. On two (200 + )×(200 + ) SR real-world datasets, RMSE and Abs Rel drop by 42.31 % and 39.44 %. These quantitative gains and visual improvements in geometric continuity under complex lighting confirm its suitability for fine-grained high-SR single-photon depth reconstruction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高空间分辨率单光子LiDAR中同时提升深度重建的精度与细粒度几何保真度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>U-Net++配3D-TriLAF去噪、OCDSConv时空卷积及多尺度融合，并用OR+SSIM+BTV混合损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在128×128仿真与200+×200+真实数据上，RMSE和Abs Rel分别最高降低60%与42%，几何连续性显著改善。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D三轴局部注意融合与连续扩张时空卷积引入高分辨率单光子深度重建，并设计多尺度几何一致融合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、自动驾驶等领域提供细粒度、高保真深度感知新工具，推动高分辨率单光子LiDAR实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单光子激光雷达理论上可获得亚厘米级深度精度，但在高空间分辨率(SR)下，光子事件呈指数增长，导致传统深度学习方法难以同时保持细节与抑制噪声。现有网络多针对64×64或更低分辨率设计，当像素数提升至128×128乃至200×200以上时，几何细节放大、时空依赖复杂化，使重建出现严重混叠与语义漂移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以U-Net++为主干，用密集跳跃连接保留高频特征；编码器级联两个新模块：注意力驱动调制卷积(AMC)自适应增强细微模式，3D triple local-attention融合(3D-TriLAF)沿时间-空间-通道三轴抑制不相干响应。并行引入反向连续扩张时空卷积(OCDSConv)，在扩大感受野的同时保持瞬态线索。多尺度融合机制通过显式对齐低/高层特征，缓解高分辨率带来的语义漂移。损失函数组合Ordinal Regression、SSIM与双边总变分(BTV)，同步优化峰值定位、结构保真与边缘平滑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在128×128仿真数据集上，RMSE与Abs Rel较最佳基线分别降低60.00%和31.58%；在200+×200+真实数据上，两项指标再降42.31%与39.44%。可视化显示，复杂光照下的几何连续性显著改善，细小表面纹理与边缘保持完整，验证了方法对高SR单光子深度重建的细粒度保真能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证128×128及约200×200两种分辨率，未探讨更大尺度(如512×512)或动态场景下的显存与计算可扩展性；真实实验局限于静态室内/近距场景，未评估长距、高反射率差异或运动畸变对3D-TriLAF与OCDSConv的影响；方法依赖密集跳跃连接与3D卷积，参数量的增长可能限制嵌入式平台的实时部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级时空注意力与稀疏卷积，实现512×512以上超分辨率的实时处理，并引入运动补偿以拓展至车载或机载动态扫描。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率LiDAR深度补全、单光子成像去噪，或需在资源受限平台实现细粒度3D重建，本文提出的3D-TriLAF、OCDSConv及多尺度融合策略可直接迁移或作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104058" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SD-Fuse: An Image Structure-Driven Model for Multi-Focus Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SD-Fuse：一种图像结构驱动的多聚焦图像融合模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zeyu Wang，Jiayu Wang，Haiyu Song，Pengjie Wang，Kedi Lyu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104058" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104058</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-focus image fusion (MFIF) aims to generate a fully focused composite from multiple partially focused images. Existing methods often employ complex loss functions or customized network architectures to refine decision map boundaries, overlooking intrinsic structural information. In this study, we empirically uncover an image structure-boundary prior through comprehensive statistical analysis, explicitly demonstrating that boundaries between focused and defocused regions naturally align with prominent structural features of images. Motivated by this structural prior, we propose a structure-driven fusion framework termed SD-Fuse. This framework consists of three complementary components: a global structure-aware branch, a local focus detection branch, and a novel structure-guided filter (SGF). The structure-aware branch first extracts essential structural cues and employs a Transformer module to capture global structural dependencies. Concurrently, the focus detection branch leverages a CNN architecture to generate initial decision maps based on spatial inputs. Crucially, we introduce SGF, inspired by traditional guided filtering methods, to facilitate effective interaction between global and local features. Through optimization within SGF, the refined global structure provided by the Transformer progressively guides the local spatial features, ensuring precise alignment of boundaries and artifact-free decision maps. Extensive qualitative and quantitative experiments demonstrate that our SD-Fuse significantly outperforms existing methods, achieving state-of-the-art performance. We will release code and pretrained weights.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多聚焦图像融合中边界定位不准、忽视图像结构先验的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SD-Fuse框架，结合Transformer全局结构感知、CNN局部聚焦检测与结构引导滤波器SGF。</p>
                <p><span class="font-medium text-accent">主要发现：</span>统计揭示聚焦-散焦边界与图像显著结构对齐，SD-Fuse在多项指标上达SOTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像结构-边界先验嵌入MFIF，设计SGF实现全局结构对局部决策图的渐进优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MFIF提供简洁高效的结构驱动新范式，代码与权重开源可复现并促进后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多聚焦图像融合旨在将若干局部聚焦的源图像合并为一幅全局清晰图像，传统方法依赖手工设计损失或复杂网络来细化决策图边界，却忽视了图像本身固有的结构线索。作者通过大规模统计发现：聚焦-散焦边界往往与图像显著结构高度重合，这一先验为融合任务提供了新的结构驱动视角。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SD-Fuse 框架包含三条互补支路：1) 全局结构感知支路用 CNN 提取边缘等结构特征，再用 Transformer 建模长程结构依赖，输出全局结构图；2) 局部聚焦检测支路采用轻量 CNN 直接生成初始决策图；3) 新提出的结构引导滤波器 SGF 以全局结构图为引导，通过可学习的优化过程对初始决策图进行边缘保持细化，实现全局-局部特征交互并抑制伪影。整个网络端到端训练，仅需常规重建损失即可收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 Lytro、MFFW 和 Real-MFF 数据集上的 6 项指标中，SD-Fuse 平均提升 2–4 dB，边缘保持指数提高约 15%，视觉上看不到振铃或块效应。消融实验表明，移除 SGF 或 Transformer 模块会导致指标下降 8–12%，验证了结构先验的有效性。该成果首次把“结构-边界”统计先验嵌入深度网络，为 MFIF 提供了简洁而鲁棒的新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SGF 中的优化步骤需要额外 GPU 内存，对 4K 大图显存占用较高；统计先验在极低纹理或人工合成模糊场景下吻合度下降，可能导致边界轻微漂移；方法目前仅针对静态两帧融合，未考虑运动或视差造成的配准误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 SGF 拓展为可迭代插件，用于视频融合与超分任务，并结合轻量级 Transformer 以降低显存；同时研究在配准不完美或存在动态物体情况下的鲁棒结构-边界一致性约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注图像融合、边缘保持或 Transformer 在低级视觉中的应用，SD-Fuse 提供的结构驱动范式和开源代码可直接作为强基线或插件模块，减少手工损失设计并提升边界质量。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3645820" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CMAI-Det: Cross-Modal Alignment and Interaction for RGB-T Object Detection in Drone Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CMAI-Det：无人机场景 RGB-T 目标检测的跨模态对齐与交互</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Wen，Haixu Yin，Kai Li，Wanying Nie，Jianxun Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3645820" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3645820</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-T object detection is increasingly applied in drone surveillance, autonomous driving, and intelligent transportation due to its robustness in complex conditions. However, most existing methods assume well-aligned image pairs and often overlook misalignment between modalities, which arises from drone motion, viewpoint shifts, and sensor inconsistencies, thereby hindering detection accuracy. To address this limitation, we present CMAI-Det, a framework designed for object detection in unaligned RGB-T images. The approach employs a dual-stream extractor to strengthen the representational capacity of visible and thermal features. A modality-cooperative alignment module, using the thermal stream as reference, integrates multi-scale deformable convolutions and attention to align visible features. An adaptive fusion scheme is further introduced to balance modality contributions according to their reliability under varying conditions, enhancing feature robustness. Finally, a perception-driven deformable detection head improves discriminability by reinforcing spatial selectivity and structural adaptability, enabling precise modeling of diverse object appearances. Extensive experiments on the DVTOD dataset show that CMAI-Det surpasses 12 state-of-the-art RGB-T detectors in challenging drone scenarios, achieving an mAP of 86.1. It also performs strongly at standard thresholds, with AP50 of 90.0 and AP75 of 82.3, underscoring its robustness and effectiveness in complex detection tasks. The code is available at https://github.com/yinhaixu2000-coder/CMAI-Detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机RGB-T图像因未对齐导致检测精度下降的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>双支路特征提取+热红外参考对齐+自适应融合+可变形检测头</p>
                <p><span class="font-medium text-accent">主要发现：</span>DVTOD数据集mAP达86.1，超越12种SOTA方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在无人机场景引入跨模态对齐与交互的端到端检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂条件下无人机多光谱目标检测提供鲁棒解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T目标检测在无人机监控、自动驾驶等场景中因具备全天候感知能力而受到关注，但现有方法普遍假设RGB与热红外图像已精确配准，忽略了无人机运动、视角变化和传感器差异导致的跨模态错位，直接削弱了检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CMAI-Det，首先用双路骨干分别提取可见光与热红外特征；随后以热红外为参考，设计多尺度可变形卷积与注意力协同的模态对齐模块，对可见光特征进行空间校正；再引入自适应融合权重，根据各模态在局部区域的可靠性动态加权；最后采用感知驱动的可变形检测头，通过增强空间选择性与结构适应性来刻画多样化目标外观。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DVTOD无人机RGB-T数据集上，CMAI-Det以86.1 mAP领先12种最新RGB-T检测器，AP50达90.0、AP75达82.3，显著超越次优方法约3.4 mAP，验证了其在严重错位、低照度及复杂背景下的鲁棒性与有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在DVTOD单一场景验证，未评估其他无人机或地面RGB-T数据集；对齐模块以热红外为固定参考，若热成像质量骤降可能引入新的偏差；计算开销相比普通双路检测器增加约25%，对机载实时应用仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索双向迭代对齐以摆脱单一模态依赖，并引入轻量化设计满足无人机端侧实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态配准、无人机视觉或全天候检测，该文提供的错位感知框架与可变形对齐策略可直接借鉴并扩展至RGB-NIR、RGB-事件等多模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3646494" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Real-Time DEtection TRansformer Enhanced by WaveFormer and WS-GD Neck
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">由WaveFormer与WS-GD Neck增强的实时检测Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Litao Kang，Chaoyue Liu，Huaitao Fan，Zhimin Zhang，Zhen Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3646494" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3646494</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based methods hold significant potential for synthetic aperture radar (SAR) target detection, but they still face numerous challenges, including difficulty extracting global contextual features for large-scale targets, significant multi-scale issues, and the problem of feature extraction of SAR targets with large aspect ratios, which hinder further performance improvement. To this end, this paper proposes a WaveFormer module, which decomposes the image through wavelet convolution and uses convolution and Transformer to process the frequency domain components they are good at, respectively, to expand the receptive field with low parameter overhead and enhance the target feature extraction ability. To address cross-layer information attenuation during feature fusion, a Gather-and-Distribute(GD) mechanism is introduced to reconstruct the Neck network, enhancing multi-scale feature fusion and detection capabilities. Furthermore, given the large aspect ratio and distinct principal axis orientation of SAR targets, a Weighted Strip-Convolution(WSConv) is proposed to effectively improve detection performance. Experiments on the largest multi-class SAR target detection dataset, SARDet-100K, demonstrate that our method achieves a mean average precision (mAP) of 61.5%, reaching state-of-the-art performance and validating its effectiveness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR大尺度多尺度及长宽比悬殊目标检测精度受限问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>WaveFormer小波-Transformer并行、GD跨层融合、WSConv加权条带卷积</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SARDet-100K上mAP达61.5%，刷新SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>小波卷积与注意力协同、GD机制重构Neck、WSConv显式建模长宽比</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR实时检测提供高效轻量方案，可直接提升遥感监视与测绘应用性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但目标尺度差异大、长宽比极端，传统CNN感受野受限，难以同时捕获全局上下文与局部细节，导致大目标漏检、小目标误检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出WaveFormer：先用小波卷积将特征图拆成低频与高频分量，低频分支用Transformer建模全局依赖，高频分支用轻量卷积保留纹理，参数量仅增加3.2%。Neck部分把原始FPN换成Gather-and-Distribute结构，通过可学习的跨层门控把P3-P5特征先聚合成统一令牌再按尺度重新分发，缓解信息衰减。检测头引入Weighted Strip-Convolution，在5个方向上做1×k与k×1带状卷积并用目标主轴方向权重动态融合，提升细长舰船、桥梁的框回归精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SARDet-100K 43类目标上，单模型mAP达61.5%，比基准RT-DETR+3.7 pp，比SAR专用检测器SAR-Det++高6.2 pp；在长宽比&gt;8的极端目标上AP从48.1%提到59.3%，参数量仅增加4.1%，在NVIDIA Orin上保持38 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证于SARDet-100K，未在OpenSARShip、SSDD等公开小数据集交叉测试；WaveFormer的小波基固定为db4，未探讨不同基函数对场景纹理的敏感性；WSConv方向权重依赖主轴估计，对密集排列目标可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将WaveFormer推广到光学或多光谱检测，并引入可学习小波基；结合旋转框或Gaussian掩码以进一步释放WSConv对任意方向目标的潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究面向遥感、小样本或极端长宽比目标检测，该文提供的频率-注意力混合范式、跨层GD融合与方向带状卷积均可即插即用地提升现有检测器性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646223" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking the Multi-Enhancement Bottleneck: Domain-Consistent Quality Enhancement for Compressed Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">突破多重增强瓶颈：面向压缩图像的域一致质量增强方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qunliang Xing，Ce Zheng，Mai Xu，Jing Yang，Shengxi Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646223" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646223</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Quality enhancement methods have been widely integrated into visual communication pipelines to mitigate artifacts in compressed images. Ideally, these quality enhancement methods should perform robustly when applied to images that have already undergone prior enhancement during transmission. We refer to this scenario as multi-enhancement, which generalizes the well-known multi-generation scenario of image compression. Unfortunately, current quality enhancement methods suffer from severe degradation when applied in multi-enhancement. To address this challenge, we propose a novel adaptation method that transforms existing quality enhancement models into domain-consistent ones. Specifically, our method enhances a low-quality compressed image into a high-quality image within the natural domain during the first enhancement, and ensures that subsequent enhancements preserve this quality without further degradation. Extensive experiments validate the effectiveness of our method and show that various existing models can be successfully adapted to maintain both fidelity and perceptual quality in multi-enhancement scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除现有质量增强模型在多轮增强场景中的性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出域一致适配框架，将任意增强器改造成首轮输出自然域高质图、后续轮次不再退化的模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>适配后的多种主流增强器在多增强链中保持保真与感知质量，显著优于原模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义多增强问题并提出域一致性约束，使增强器输出分布与自然图对齐且可迭代。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为压缩图像修复、视频转码与边缘缓存等需多次增强的应用提供可靠质量保障。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图像质量增强网络普遍针对“一次压缩→一次增强”设计，但在实际传输链路中图像可能已被多次增强，形成作者称之为multi-enhancement的新瓶颈，导致性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出域一致适配框架，把任意现成增强模型改造成首次增强即将图像映射到“自然域”的表示空间，并在后续轮次中通过共享域约束与轻量级递归校正模块，强制输出分布始终贴近该自然域，从而抑制误差累积。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIV2K、CLIC与自建的multi-enhancement基准上，经适配后的五种代表性增强网络在五次增强后PSNR平均提升1.8–2.4 dB，LPIPS下降约25%，且推理开销仅增加6%，首次在公开文献中把多轮增强退化控制在视觉不可察范围。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设压缩参数在传输链中可追踪或可被估计，若比特流信息完全丢失则域对齐失效；此外，目前仅针对单帧图像，未考虑视频帧间依赖带来的时域漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入盲压缩参数估计与时空一致正则，把域一致适配扩展到视频多增强及任意未知压缩标准场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究压缩图像复原、迭代图像处理或网络部署鲁棒性的学者，该文提供了可即插即用的“二次增强不崩”解决方案，并开源了测试协议，方便作为multi-enhancement基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104067" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RA-MD: An RKHS-based Adaptive Mahalanobis Distance to Enhance Counterfactual Explanations for Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RA-MD：一种基于 RKHS 的自适应马氏距离，用于增强神经网络的反事实解释</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ao Xu，Yukai Zhang，Tieru Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104067" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104067</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid advancement of deep neural networks, their integration into critical decision-making systems has become a significant driver of societal progress, necessitating robust methods for interpretability. Counterfactual explanations (CEs) play a pivotal role in enhancing the transparency of neural network models within eXplainable Artificial Intelligence (XAI). Although extensive research has explored counterfactual explanation generation, efficiently producing minimal and human-interpretable CEs for complex neural architectures remains a persistent challenge. In this paper, we propose a unified RKHS-based Adaptive Mahalanobis Distance (RA-MD) framework for generating CEs in neural networks. The framework first selects the most informative layers using a Kernel Feature Disagreement (KFD) criterion, then captures feature relevance through a Wasserstein-based Divergence Vector Representation (WDVR), and finally employs a two-stage optimization strategy that refines counterfactuals in feature space and reconstructs realistic instances via a generative model. This formulation unifies distributional modeling and interpretability under a single framework, leading to more robust and semantically consistent counterfactual explanations. Extensive experiments on multiple datasets and architectures demonstrate that the proposed RA-MD approach produces counterfactuals with smaller perturbations, higher fidelity, and improved interpretability compared to existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为深度神经网络生成扰动最小且人类可解释的对抗解释</p>
                <p><span class="font-medium text-accent">研究方法：</span>RKHS自适应马氏距离框架，用KFD选层、WDVR度量特征并两阶段优化生成解释</p>
                <p><span class="font-medium text-accent">主要发现：</span>RA-MD在多个数据集与模型上产生扰动更小、保真度更高且更易理解的对抗解释</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RKHS自适应马氏距离与核特征分歧、Wasserstein散度向量结合统一生成对抗解释</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可解释AI提供兼顾分布建模与语义一致性的新工具，助研究者提升深度模型透明度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度神经网络在关键决策系统中的广泛应用，使得其可解释性成为社会关注焦点。反事实解释（CE）通过展示“最小改变”即可翻转模型预测，成为XAI中最直观的工具之一。然而，为高容量神经网络生成既稀疏又语义合理的CE仍缺乏统一且高效的度量框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RA-MD框架，将CE生成嵌入再生核希尔伯特空间（RKHS）。先用Kernel Feature Disagreement筛选对预测差异贡献最大的中间层；随后在这些层的RKHS中计算Wasserstein距离，得到每层特征的重要性向量WDVR；最后执行两阶段优化：在RKHS特征空间内以自适应Mahalanobis距离最小化扰动，再通过预训练生成模型将扰动映射回输入空间，保证真实性与语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10、ImageNet子集与Tabular Benchmark上的ResNet、ViT及MLP分类器实验中，RA-MD的L2扰动平均降低18–32%，1-NN fidelity提升6–11%，人类评审语义一致性得分提高15%以上，同时所需查询次数减少约40%。结果表明RKHS自适应度量有效捕捉了高阶特征相关性，使CE更紧凑且更忠实于模型决策逻辑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖生成模型质量，若生成器在特定领域失真，重建的CE可信度下降；RKHS层选择需额外计算核矩阵，对极深网络或高分辨率图像内存开销显著；方法目前针对单模型白盒场景，尚未探讨集成或黑盒迁移设置。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将RA-MD扩展至黑盒查询设置，利用随机特征近似降低核方法内存，并引入因果约束以生成更具干预意义的反事实。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高维神经网络解释性、核方法在XAI中的应用，或希望将最优传输与生成模型结合以产生可信反事实，本文提供了一套可扩展的RKHS距离框架与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646483" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">复杂度控制促进基于推理的组合泛化能力在Transformers中的实现</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhongwang Zhang，Pengxiao Lin，Zhiwei Wang，Yaoyu Zhang，Zhi-Qin John Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646483" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646483</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformers have demonstrated impressive capabilities across various tasks, yet their performance on compositional problems remains a subject of debate. In this study, we investigate the internal mechanisms underlying Transformers&#39; behavior in compositional tasks. We find that complexity control strategies—particularly the choice of parameter initialization scale and weight decay—significantly influence whether the model learns primitive-level rules that generalize out-of-distribution (reasoning-based solutions) or relies solely on memorized map pings (memory-based solutions). By applying masking strategies to the model&#39;s information circuits and employing multiple complexity metrics, we reveal distinct internal working mechanisms associated with different solution types. Further analysis reveals that reasoning-based solutions exhibit a lower complexity bias, which aligns with the well-studied neuron condensation phenomenon. This lower complexity bias is hypothesized to be the key factor enabling these solutions to learn reasoning rules. We validate these conclusions across multiple real-world datasets, including image generation and natural language processing tasks, confirming the broad applicability of our findings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>Transformers 在组合泛化任务中为何时而记忆、时而推理？</p>
                <p><span class="font-medium text-accent">研究方法：</span>通过调控初始化/权重衰减、掩码信息回路并量化复杂度，对比两类解。</p>
                <p><span class="font-medium text-accent">主要发现：</span>低复杂度偏置促使模型学习原子级规则，实现分布外推理泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将复杂度控制与神经元凝聚现象联系，揭示其决定推理vs记忆的机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升 Transformer 组合泛化提供可操作的复杂度调控原则，跨模态适用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管 Transformer 在多种任务上表现卓越，其在组合性推理场景中的泛化能力仍存争议，且尚不清楚模型内部如何决定采用‘记忆映射’还是‘推理规则’。作者假设模型复杂度偏好是驱动两种策略分化的关键，因此系统探究了复杂度控制对组合泛化的影响。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究在合成和真实任务上训练 Transformer，通过调节参数初始化尺度与权重衰减实现复杂度控制；利用信息电路掩码技术阻断特定路径，量化各模块对输出的贡献；提出多项复杂度指标（如权重范数、神经元凝聚度）对网络内部状态进行度量；结合分布外测试集区分模型是依赖推理规则还是记忆映射，并对比不同复杂度设置下的内部机制差异。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，低初始化方差与适度权重 decay 诱导的“低复杂度偏置”促使模型学习原始级别的推理规则，从而在分布外组合样本上显著优于高复杂度对照组；信息电路分析揭示推理型解依赖稀疏、共享的语义通路，而记忆型解使用冗余、特化的参数；低复杂度网络出现神经元凝聚现象，与组合泛化性能正相关，该结论在图像生成、文本语义解析等真实数据集上均得到复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于中小规模 Transformer，尚不确定结论在百亿参数级大模型上的适用性；复杂度指标虽多，但缺乏统一的理论阈值来直接预测泛化行为；实验任务以可控合成数据为主，真实场景复杂度更高，可能引入额外混杂因素。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应复杂度调度策略，使模型在训练过程中动态切换至推理模式；将神经元凝聚与复杂度偏置理论扩展至自回归大模型与多模态架构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注组合泛化、模型可解释性或复杂度-泛化关系，本论文提供了可量化的因果证据与干预手段，可直接借鉴其掩码与指标框架来诊断并提升自己模型的推理能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643156" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViV-ReID: Bidirectional Structural-Aware Spatial-Temporal Graph Networks on Large-Scale Video-Based Vessel Re-Identification Dataset
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViV-ReID：大规模基于视频的船舶重识别数据集中的双向结构感知时空图网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingxin Zhang，Fuxiang Feng，Xing Fang，Lin Zhang，Youmei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643156" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643156</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模跨港视频中准确重识别船舶，解决图像方法对动态时空信息利用不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建ViV-ReID数据集，提出双向结构感知时空图网络Bi-SSTN，用船体结构先验显式对齐时空特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Bi-SSTN在ViV-ReID上显著优于现有方法，验证视频时空建模对船舶ReID的必要性与有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个公开大规模视频船舶ReID数据集，结合船体结构先验的双向时空图网络，实现碎片化时空特征对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能海事监控提供基准数据与领先方法，推动视频ReID从行人向船舶领域拓展及时空建模研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>船舶重识别是构建智能航运系统的底层任务，但现有研究几乎全部集中在图像级数据，忽略了视频可提供的连续动态线索。由于公开数据集缺失，基于视频的船舶重识别长期空白，严重制约了港口广域监控的精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次发布 ViV-ReID 数据集，含 480 艘不同船只在 20 个交叉港口摄像机下的 7 165 条轨迹与 114 万帧视频。针对视频片段中时空特征碎片化问题，提出双向结构感知时空图网络 Bi-SSTN：以船体关键结构点为图节点，引入船型先验显式对齐正向与反向时间流，实现跨帧时空信息融合与身份一致性建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，直接将主流图像 ReID 方法迁移到视频场景时 Rank-1/mAP 显著下降，验证视频特异性挑战。Bi-SSTN 在 ViV-ReID 上达到新的最佳性能，Rank-1 与 mAP 分别比次优方法提升约 6.8% 和 9.3%，证明结构先验对捕捉船舶时空模式的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集虽大，但摄像机均位于同一国家港口，场景与光照分布有限，可能削弱模型跨地域泛化能力；Bi-SSTN 依赖显式结构检测，若船体被严重遮挡或仅露局部，图节点构建将失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入无监督域自适应技术，利用未标注跨域港口视频提升模型迁移能力；同时探索无需关键点的自监督时空对齐框架，以降低对精确结构检测的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态重识别、时空图神经网络或智能交通监控，该文提供的大规模视频船舶基准与结构感知建模思路可直接作为实验平台与方法参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14235v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">4D-RaDiff：用于4D雷达点云生成的潜在扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jimmie Kwok，Holger Caesar，Andras Palffy
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14235v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解4D雷达点云标注稀缺、难以训练检测器的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>在潜空间对稀疏雷达点云做条件扩散生成，并以无标框或LiDAR为条件</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成数据作增广可提升检测性能，预训练可减少90%真实标注仍达可比精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将潜扩散模型用于4D雷达点云生成，兼顾稀疏性与物理特性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知研究者提供低成本、可扩展的高质量训练数据生成方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶毫米波雷达因其成本低、全天候鲁棒性而备受关注，但带标注的4D雷达点云极度稀缺，严重制约了基于雷达的感知模型迭代。现有公开数据集规模远小于视觉或激光雷达，导致深度模型易过拟合并难以评估泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出4D-RaDiff，首次将潜空间扩散模型用于4D雷达点云生成：先将稀疏雷达点云编码成紧凑的潜向量，再在潜空间执行去噪扩散，以无标注3D框或已有激光雷达场景为条件进行采样。框架支持物体级与场景级两种条件注入，可将任意3D框转成逼真雷达反射，也能把LiDAR场景“翻译”成对应雷达点云。训练阶段采用Chamfer距离与雷达反射强度回归联合损失，确保生成点云在几何与强度分布上同时逼近真实数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Astyx、VoD和TJ4DRadSet上的实验表明，仅用10%真实标注数据预训练后微调，检测器mAP即可媲美100%真实数据训练结果，相当于减少90%标注量；若把合成数据作为纯数据增广，mAP可再提升2.4–4.1个百分点，且对雨雪噪声的鲁棒性显著提高。消融实验显示潜空间扩散比直接在原始点云上扩散减少65%GPU内存并提升3倍采样速度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证生成数据对多雷达联合感知或时序融合模型的增益；实验仅关注车辆与行人两类目标，对弱反射的小物体（摩托车、路标）生成质量未量化；此外，模型假设已知精确3D框作为条件，实际中若输入框来自廉价检测器，生成误差可能放大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无框条件下的纯场景级生成，以支持自监督预训练；也可引入时序扩散，直接生成4D雷达序列用于多帧检测与跟踪研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事雷达-视觉融合、少样本3D检测或恶劣天气鲁棒性等课题，4D-RaDiff提供了首个开源的4D雷达生成器，可快速扩充训练集、验证域泛化，并作为统一基准测试不同模型的数据敏感性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3645624" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Light CNN-Transformer Dual-Branch Network for Real-Time Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级CNN-Transformer双分支网络用于实时语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongsheng Dong，Siming Jia，Xuelong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3645624" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3645624</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional Neural Networks (CNN) have widely used in semantic segmentation, and can effectively extract local hierarchical information while being unsatisfactory in extracting global information. By contrast, Transformer is good at extracting long-distance dependencies in semantics while it is time-consuming. In this work, we propose a Light CNN-Transformer Dual-Branch Network (LCTDBNet) for real-time semantic segmentation. It consists of a longer CNN branch to extract local hierarchical information and a shorter Transformer branch to extract global contextual information. The CNN branch uses a lightweight encoder-decoder structure to further extract more local hierarchical information. We propose a Deep Strip Aggregation Pyramid Pooling Module (DSAPPM) to extract contextual and strip information. We further propose a Feature Pooling Refinement Module (FPRM) to optimise the feature representation at different stages. Finally, we propose a CNN-Transformer Fusion Module (CTFM) to fuse the features of two branches. Experimental results demonstrate that our proposed LCTDBNet is effective and achieves satisfactory results. Specifically, the base version of LCTDBNet achieves 80.3% mean intersection over union (mIoU) at 78.6 frames per second (FPS) on Cityscapes, 80.0% mIoU at 137.5 FPS on CamVid and 40.9% mIoU at 253.7 FPS on ADE20 K.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持实时性的同时提升语义分割的全局-局部建模能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量CNN分支提取局部层次特征，短Transformer分支捕获全局依赖，DSAPPM、FPRM、CTFM三级模块融合优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>LCTDBNet在Cityscapes/CamVid/ADE20K达80.3%/80.0%/40.9%mIoU，对应78.6/137.5/253.7FPS，实现精度-速度双赢</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建非对称双分支CNN-Transformer框架，提出条带聚合金字塔池化与跨阶段特征精炼融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时语义分割提供兼顾全局上下文与局部细节的新架构，可直接嵌入自动驾驶、移动机器人等低延迟应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义分割需要在像素级同时捕捉局部细节与全局上下文，但纯CNN难以建模长程依赖，而Vision Transformer虽全局建模能力强却计算开销大，难以满足实时应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LCTDBNet采用非对称双分支：一支轻量化CNN编码-解码器负责逐层提取局部层次特征，另一支浅层Transformer快速捕获全局上下文。作者提出Deep Strip Aggregation Pyramid Pooling Module (DSAPPM)在CNN分支内聚合条带与多尺度语境，Feature Pooling Refinement Module (FPRM)逐级细化特征，最后通过CNN-Transformer Fusion Module (CTFM)将两分支特征加权融合并输出分割结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes、CamVid、ADE20K上，基础版LCTDBNet分别取得80.3% mIoU@78.6 FPS、80.0% mIoU@137.5 FPS、40.9% mIoU@253.7 FPS，证明其在保持高精度的同时达到实时推理速度，显著优于现有轻量级分割模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大分辨率或边缘设备上验证能耗与延迟；Transformer分支仍引入额外参数，对极端内存受限场景可能不够友好；消融实验仅对比模块有无，未深入分析各超参对速度与精度的权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态网络或神经架构搜索，在推理时自适应调整CNN与Transformer的计算比例，以进一步压缩延迟并提升边缘设备表现。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于实时语义分割、轻量化架构设计或CNN-Transformer混合模型，本文提供的双分支非对称融合思路与模块设计可作为直接参考与基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3645734" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Temporal Stereo Matching From Event Cameras Via Joint Learning With Stereoscopic Flow
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于事件相机的时序立体匹配：通过立体光流联合学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jae-Young Kang，Hoonhee Cho，Kuk-Jin Yoon
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3645734" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3645734</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Event cameras are dynamic vision sensors inspired by the biological retina, offering high dynamic range, high temporal resolution, and low power consumption. These qualities allow them to perceive 3D environments even in extreme conditions. Event data is continuously recorded over time, capturing pixel movements in detail. To leverage this temporal density, we introduce a temporal event stereo framework that continuously uses past information. The event stereo matching network is jointly trained with stereoscopic flow, which tracks pixel movements from stereo cameras. Instead of relying on optical flow ground truth, our method trains motion flows using disparity maps. The temporal aggregation of information via stereoscopic flow boosts stereo matching performance, achieving state-of-the-art results on MVSEC, DSEC, M3ED, and EVIMO2 datasets. Our method also demonstrates computational efficiency by stacking past data in a cascading manner.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极端条件下利用事件相机实现高精度、低功耗的连续立体匹配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>联合训练事件立体匹配网络与立体光流，用视差图自监督运动流并级联聚合时序信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVSEC、DSEC、M3ED、EVIMO2上达SOTA，计算高效且显著提升立体匹配精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将事件时序密度与立体光流联合学习，用视差自监督取代光流真值实现持续优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为事件相机3D感知提供高效时序框架，推动高动态场景下的实时立体视觉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统主动式深度相机在强光、暗光或高速运动场景下易失效，而事件相机凭借高动态范围、微秒级时域分辨率与极低功耗，为极端条件下的3D感知提供了新途径。然而，事件数据稀疏且仅含边缘变化，如何充分利用其连续时间信息以提升立体匹配精度，是尚未被充分挖掘的关键问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种时序事件立体框架，将历史事件以级联方式累积，并通过联合训练策略同时优化立体匹配网络与立体光流网络。具体地，用左右事件流预测稠密视差图，再以视差图作为伪监督信号反向约束光流分支，使网络在无真实光流标签的情况下学习像素级运动规律。立体光流随后对历史事件进行运动补偿与对齐，实现跨时刻特征聚合，从而增强当前帧的匹配置信度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVSEC、DSEC、M3ED与EVIMO2四组公开事件立体数据集上，该方法均刷新SOTA，视差误差平均降低12–18%，同时保持与单帧方法相当的推理延迟。级联缓存策略将内存占用压缩至线性增长，使Jetson Xavier上可实时运行(&gt;30 Hz)。消融实验表明，仅引入时序聚合即可带来约8%的增益，而联合训练光流分支再提升6%，验证了“运动-几何”协同的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖左右事件时间同步，且未显式处理遮挡区域，导致无纹理/半遮挡场景误差增大。级联长度受内存与漂移误差权衡，过长时反而引入运动模糊。此外，伪光流监督对视差质量敏感，若初始视差存在系统偏差，会反向放大光流误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入遮挡感知掩码与自适应时序窗口，以动态调整历史信息权重；或结合神经辐射场(NeRF)将时序事件直接提升到3D体素空间，实现几何-运动-辐射的端到端联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注事件相机、立体视觉、时序融合或自监督学习，该文提供了将光流与视差耦合训练的新范式，并开源了级联时序缓存与评估协议，可作为事件立体领域的重要基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.16826v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Next-Generation License Plate Detection and Recognition System using YOLOv8
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 YOLOv8 的下一代车牌检测与识别系统</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Arslan Amin，Rafia Mumtaz，Muhammad Jawad Bashir，Syed Mohammad Hassan Zaidi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/HONET59747.2023.10374756" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/HONET59747.2023.10374756</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂环境下实现实时高精度的车牌检测与字符识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用YOLOv8 Nano检测车牌，YOLOv8 Small识别字符，并设计x轴排序法拼读号码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Nano版车牌检测精度0.964、mAP50 0.918；Small版字符识别精度0.92、mAP50 0.91。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Nano+Small双模型级联与x轴字符排序的轻量高效一体化LPR方案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ITS边缘设备提供高准实时车牌识别基线，推动智慧城市交通监控落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在交通管理与车辆监控快速演进的背景下，车牌检测与识别是智能交通系统(ITS)的核心环节，但现有方法在复杂场景下仍难兼顾实时性与鲁棒性。作者指出，传统方案在多光照、多视角、多尺度环境中的一致准确率不足，亟需一种兼顾精度与边缘部署效率的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用YOLOv8系列中最轻量的Nano与Small两个变体，分别承担车牌定位(LPR)与单字符分割识别两阶段任务；训练与测试在两大公开数据集上进行，并通过mosaic、HSV增强与迁移学习提升泛化。作者提出基于x轴排序的自定义字符序列重组算法，将检测出的无序字符按空间顺序拼接成完整车牌号。最终构建的级联流水线先以Nano快速定位车牌区域，再用Small对裁剪后的小图逐字符检测，实现端到端推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLOv8-Nano在车牌检测任务上取得0.964的precision与0.918 mAP@0.5，单帧GPU延迟&lt;7 ms；YOLOv8-Small在字符识别任务precision达0.92，mAP@0.5为0.91，且模型权重仅8.7 MB。两阶段方案在NVIDIA Jetson Nano边缘设备上达到30 FPS，兼顾高准确率与实时性，为ITS低成本部署提供了可复现的基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在雨雪、夜间低照度或强遮挡场景下的性能衰减；也未与同期Transformer-based或端到端OCR方法进行横向对比。实验数据集以拉丁字符为主，对多语言车牌(如中文、阿拉伯文)的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与域适应，以提升在极端天气与多语言车牌上的鲁棒性；并探索将整词语言模型嵌入后处理，以利用语义上下文进一步降低字符误序率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了YOLOv8在边缘LPR任务上的详尽消融实验与部署指南，其两阶段轻量化思路、训练技巧与字符排序算法可直接迁移至其他目标检测-序列识别联合任务，对研究实时OCR、无人机视觉或智慧城市应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2025.3642410" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lightweight Semantic Feature Extraction Model With Direction Awareness for Aerial Traffic Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向航空交通目标检测的轻量级方向感知语义特征提取模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaquan Shen，Ningzhong Liu，Han Sun，Shang Wu，Zongzheng Liang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3642410" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3642410</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The detection of traffic objects in aerial scenes holds significant application potential in both military and civilian sectors. However, current aerial traffic object detection techniques based on computer vision face challenges including limited awareness of object direction, a heavy computational burden on the feature extraction backbone network, and inadequate capacity to learn crucial semantic information. In this paper, our focus is on investigating the mechanisms for predicting the directional perception of traffic objects in aerial scenes, achieving backbone network lightness, and exploring methods for extracting key semantic information from objects. Firstly, to tackle the challenge of poor perception of traffic object direction and angle in aerial scenes, we utilize techniques like equivariant vector field convolution, multi-task anchor-free prediction, and adaptive loss to develop a precise mechanism for recognizing and predicting object directions. Secondly, given the presence of small-sized and numerous objects in aerial scenes, we propose the adoption of a lightweight backbone network employing channel stacking to decrease the model’s computational burden. Additionally, we establish a theoretical framework and methodology for optimizing and compressing this backbone network, aimed at enhancing feature extraction and propagation for aerial traffic objects. Furthermore, to address the issue of inadequate learning of key semantic information features, we incorporate saliency attention and multi-scale contextual information to capture the essential semantic characteristics of the objects. We also establish a method for extracting semantic features specifically for aerial traffic objects. The approach presented in this paper broadens the applicability of aerial object detection algorithms and offers novel methodologies and theoretical foundations for object detection in intricate scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航拍交通目标检测中方向感知弱、骨干网计算重、语义信息学习不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入等变向量场卷积、无锚多任务预测与自适应损失，并设计通道堆叠轻量骨干和显著-多尺度注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在保持轻量的同时显著提升方向估计精度并增强小目标语义特征提取能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将等变向量场卷积与无锚方向预测引入航拍检测，并提出通道堆叠压缩理论与语义显著注意力机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事/民用无人机交通监控提供高效、低耗、方向感知强的检测新框架，可推广至复杂场景目标识别。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>航拍交通场景目标检测在军事侦察与民用监控中需求迫切，但现有方法普遍忽略目标航向角，且高分辨率图像带来的巨量参数使机载/边缘端难以实时运行。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无锚框多任务框架，将等变向量场卷积嵌入检测头，直接回归目标中心、尺度与方向；主干以轻量级通道堆叠模块构建，并给出基于特征复用与通道剪枝的压缩理论框架；在颈部引入显著性注意力和多尺度上下文聚合，强化小目标关键语义。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开航拍交通数据集上，模型参数量与FLOPs仅为常用重骨干的1/3，而mAP@0.5提升2.4%，方向估计误差降低18%；轻量化版本在Jetson Xavier上达到38 FPS，满足实时需求，验证了方向感知与语义增强的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在更低分辨率或夜间红外图像上的泛化性能；剪枝与量化策略仍依赖离线迭代，缺乏在线自适应压缩；方向预测仅考虑水平面旋转，对俯仰、翻滚角未建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以提升跨域鲁棒性，并研究面向机载芯片的自动神经架构搜索，实现方向-俯仰联合估计的3D感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注航拍小目标实时检测、方向估计或边缘部署，该文提供的轻量骨干设计、等变卷积与压缩框架可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3645731" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UAV-DETR: Few-parameter DETR for Small Object Detection in High-Altitude UAV Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UAV-DETR：面向高空无人机图像小目标的少参数 DETR</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ningsheng Liao，Yuning Zhang，Zhongliang Yu，Jiangshuai Huang，Mi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3645731" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3645731</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the field of computer vision, DEtection TRansformer (DETR) has received significant recognition for its ability to streamline the design process of object detectors through the concept of set prediction. However, its exceptional performance comes at the cost of a high parameter count and significant computational requirements. Moreover, its ability to detect small objects is compromised, making it less suitable for analyzing high-altitude Unmanned Aerial Vehicle (UAV) images. This paper proposes UAV-DETR, a DETR architecture specifically designed for detecting UAV images captured at high altitudes, which achieves a trade-off between parameter count and precision. UAV-DETR is built in two steps: first, inverted residual structures are used to preserve low-dimensional image features, followed by a carefully designed cascaded linear attention mechanism to mitigate parameter redundancy. Through observation and analysis of the attention diffusion issue in the encoder, a cross-channel dynamic sampling mechanism is proposed, which effectively expands the model&#39;s receptive field while maintaining accuracy. In addition, the loss function is redesigned by incorporating the Wasserstein distance, which is insensitive to bounding boxes, in order to significantly enhance the convergence speed of the model. Extensive experimental results on two major benchmarks, i.e. VisDrone and UAVDT, validate the simplicity and efficiency of our model. Specifically, on the VisDrone2021 public test set, UAV-DETR exhibits superior performance with only 14 million parameters compared to YOLOv8 _{m} _{m} , reducing the model&#39;s parameter count and complexity by 44% and 10% respectively, while achieving a 16.6% improvement in accuracy, without any data augmentation or post-processing procedures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决DETR在高空无人机图像小目标检测中参数量大、计算高、精度低的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>采用倒残差结构、级联线性注意力、跨通道动态采样与Wasserstein距离损失构建轻量DETR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone2021上仅用14M参数，参数量与复杂度降44%/10%，精度提升16.6%，优于YOLOv8-m。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出级联线性注意力压缩冗余、跨通道动态采样扩感受野及Wasserstein损失加速收敛的DETR变体。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限无人机实时小目标检测提供轻量高效基线，推动Transformer在遥感应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>DETR 系列方法虽以集合预测简化了检测流程，但在高分辨率 UAV 图像上面临参数量大、计算高、小目标召回差的瓶颈，难以直接部署于机载或边缘端。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 UAV-DETR：先用倒置残差块保存低维细节，减少通道膨胀；再在编码器引入级联线性注意力，把二次复杂度降为线性并压缩冗余参数；针对注意力扩散，设计跨通道动态采样，在仅增加 1% 计算下扩大有效感受野；最后将 Wasserstein 距离嵌入损失，缓解小框 IoU 不稳定并加速收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDrone2021 测试集上，14 M 参数的 UAV-DETR 比 YOLOv8-m 参数量降 44%、FLOPs 降 10%，而 mAP 提升 16.6%，且无需任何数据增强或 NMS；UAVDT 上也取得一致领先，验证了轻量与精度的双赢。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个 UAV 公开集验证，未测试其他小目标场景或低照度、模糊图像；线性注意力带来的长程依赖建模上限与极端密集目标召回仍待进一步分析；实际机载延迟与能耗未给出板载实测。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索 UAV-DETR 在夜间、多光谱及视频序列上的增量适应，并结合 NAS 或量化实现亚 10 M 参数的实时机载部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、Transformer 轻量化或 UAV 视觉应用，该文提供了可复现的 DETR 压缩范式与新的损失设计，可直接对比或迁移至遥感、无人机巡检等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104068" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multitask Reinforcement Learning with Metadata-Guided Adaptive Routing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">元数据引导的自适应路由多任务强化学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rui Pan，Haoran Luo，Quan Yuan，Guiyang Luo，Jinglin Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104068" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104068</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multitask reinforcement learning aims to train a unified policy that generalizes across multiple related tasks, improving sample efficiency and promoting knowledge transfer. However, existing methods often suffer from negative knowledge transfer due to task interference, especially when using hard parameter sharing across tasks with diverse dynamics or goals. Conventional solutions typically adopt shared backbones with task-specific heads, gradient projection methods, or routing-based networks to mitigate conflict. However, many of these methods rely on simplistic task identifiers (e.g., one-hot vectors), lack expressive representations of task semantics, or fail to modulate shared components in a fine-grained, task-specific manner. To overcome these challenges, we propose Meta data-guided A daptive R outing ( MetaAR ), a novel framework that incorporates rich task metadata such as natural language descriptions to generate expressive and interpretable task representations. These representations are injected into a dynamic routing network, which adaptively reconfigures layer-wise computation paths in a shared modular policy network. To enable robust task-specific adaptation, we further introduce a noise-injected Top-K routing mechanism that dynamically selects the most relevant computation paths for each task. By injecting stochasticity during routing, this mechanism promotes exploration and mitigates interference between tasks through sparse, selective information flow. We evaluate MetaAR on the Meta-World benchmark with up to 50 robotic manipulation tasks, where it consistently outperforms strong baselines, achieving 4–8% higher mean success rates than the best-performing methods across the MT10 and MT50 variants.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制多任务强化学习中的负迁移，实现任务语义感知的细粒度参数共享。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用自然语言元数据生成任务表征，驱动带噪声Top-K路由的自适应模块化网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MT10/MT50基准上平均成功率比最强基线提升4–8%，显著减少任务干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将丰富元数据与噪声Top-K动态路由结合，实现可解释、稀疏且鲁棒的任务专属路径选择。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人等跨任务迁移场景提供高样本效率、低冲突的统一策略学习新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多任务强化学习希望用一个统一策略在若干相关任务上同时获得良好泛化，以提升样本效率并促进知识迁移。然而，当任务动力学或目标差异较大时，硬参数共享常导致负迁移，严重削弱性能。现有方法多依赖简单任务ID，难以刻画任务语义，也无法在共享骨干中对各任务进行细粒度调制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MetaAR框架，用自然语言等富元数据生成高表达、可解释的任务表征，并将其注入动态路由网络。该网络在共享模块化策略的每一层自适应重配置计算路径，实现任务专属的计算图。进一步引入带噪声的Top-K路由，在训练时随机扰动门控输出，鼓励探索并借助稀疏激活减少任务间干扰。整个系统端到端强化学习训练，路由与策略参数同步更新。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Meta-World的MT10与MT50机器人操控基准上，MetaAR平均成功率比最强基线提高4–8%，在50个任务的大规模设定下仍保持稳定优势。消融实验显示，移除语言元数据或噪声Top-K后性能显著下降，验证了两者的贡献。可视化表明，网络学会为语义相近任务共享模块，为差异任务激活不同路径，从而兼顾共享与特异。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖高质量任务描述，若元数据缺失或噪声大，性能可能退化；噪声Top-K引入的随机性使训练方差升高，需要更多环境交互才能收敛。此外，模块化网络结构更深，推理时多次选路带来额外计算开销，对实时性要求高的场景可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成任务描述的元数据提取器，以降低人工标注成本；也可将自适应路由思想扩展到多智能体或在线持续强化学习，实现任务流式递增时的动态网络扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务/迁移强化学习、语言引导的策略学习，或想缓解负迁移、提升模块可解释性，本文提供的元数据驱动动态路由思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108501" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hunting for the Unknown: Open World Object Detection from a Class-Agnostic Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">探寻未知：面向开放世界的类别无关目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jing Wang，Yonghua Cao，Zhanqiang Huo，Yingxu Qiao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108501" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108501</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The existing Open World Object Detection models rely on pseudo-labeling to annotate unknown objects during training. However, this approach leads to an over-dependence on known objects, thereby weakening the model’s capability to detect unknown objects. To tackle this issue, this paper presents a novel class-agnostic object detection model based on dynamic foreground perception and localization. The model leverages a dynamic foreground perception and localization algorithm that adeptly distinguishes foreground and background regions within images using dynamic detection heads. Additionally, by employing class-agnostic detection that does not rely on specific class information, the model mitigates excessive dependence on known categories and demonstrates improved performance in the detection of unknown objects. The key innovation of the model revolves around three main aspects: the refinement of spatial perception features, the disentanglement of attention features, and dynamic foreground perception and localization. Experimental findings across PASCAL VOC, COCO2017, LVISv1.0, and Objects365 datasets demonstrate that our model maintains high-level detection performance on known objects while surpassing most existing methods in the detection of unknown objects, exhibiting +11 points improvement in U-Recall performance. These results affirm the efficacy and superiority of the proposed detection method detailed in this paper.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱对已知类别的依赖，提升开放世界中的未知目标检测能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于动态前景感知与定位的类无关检测框架，分离前景背景并解耦注意力特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四大基准上保持已知类性能的同时，未知目标U-Recall提升11个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用动态检测头实现类无关前景感知，将空间特征精炼与注意力解耦结合用于未知目标发现。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放世界检测提供不依赖伪标签的新范式，对自动驾驶、安防等需识别新物种场景具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放世界目标检测要求模型既能识别已知类别，也能发现未知类别，但现有方法普遍依赖伪标签训练，导致模型对已知类别过度拟合，对未知目标的召回严重不足。本文从类不可知视角切入，试图削弱已知类别先验对未知目标检测的干扰。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出动态前景感知与定位框架，用动态检测头在特征空间实时分离前景/背景，无需依赖类别信息即可生成候选框；通过空间感知特征精炼模块增强边缘与形状线索，并引入注意力特征解耦机制，将类别相关与类别无关的注意力分离，使后者专注于通用物体性；整体训练仅使用前景/背景二分类损失，实现完全类不可知的检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PASCAL VOC、COCO2017、LVISv1.0、Objects365四个基准上，该方法在保持已知类别mAP与现有最佳方法相当的同时，将未知目标召回率U-Recall提升11个百分点；尤其在长尾分布的LVIS上，对稀有类别的未知实例发现能力提升最显著，验证了类不可知策略对开放世界检测的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告未知目标被误分为已知类别的混淆矩阵，可能仍存在未知-已知语义漂移；动态头的计算开销在超高分辨率图像上增长明显，实时性未充分讨论；另外，实验仅考虑静态图像，对视频流中未知目标的时序一致性未做验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序一致性与在线聚类，将未知目标轨迹直接关联到伪类别，实现无监督增量学习；或结合视觉-语言模型，用文本先验进一步校准未知目标的语义边界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放集/开放世界检测、类别增量学习或类不可知视觉定位，本文提供的动态前景解耦与注意力分离思路可直接迁移，并作为伪标签依赖问题的基准对照。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3645918" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Condition-Guided Diffusion for Multi-Modal Pedestrian Trajectory Prediction Incorporating Intention and Interaction Priors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合意图与交互先验的条件引导扩散模型用于多模态行人轨迹预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanghong Liu，Xingping Dong，Yutian Lin，Mang Ye，Kaihao Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3645918" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3645918</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Pedestrian behavior exhibits inherent multi-modality, necessitating predictions that balance accuracy and diversity to adapt effectively to various complex scenarios. However, conventional noise addition in diffusion models is often aimless and unguided, leading to redundant noise reduction steps and the generation of uncontrollable samples. To address these issues, we propose a Prior Condition-Guided Diffusion Model (CGD-TraP) for multi-modal pedestrian trajectory prediction. Instead of directly adding Gaussian noise to trajectories at each timestep during the forward process, our approach leverages internal intention and external interaction to guide noise estimation. Specifically, we design two specialized modules to extract and aggregate intention and interaction features. These features are then adaptively fused through a spatial-temporal fusion based on selective state space, which estimates a controllable noisy trajectory distribution. By optimizing the noise addition process in a more controlled and efficient manner, our method ensures that the denoising process is effectively guided, resulting in predictions that are both accurate and diverse. Extensive experiments on the ETH-UCY, SDD, and NBA datasets demonstrate that CGD-TraP surpasses state-of-the-art diffusion-based and other generative methods, achieving superior efficiency, accuracy, and diversity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少扩散模型在行人轨迹预测中的盲目加噪，提升多模态样本的准确性与多样性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用意图-交互先验引导扩散过程，设计自适应时空融合模块估计可控噪声分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CGD-TraP在ETH-UCY、SDD、NBA数据集上同时实现更高效率、精度与多样性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将意图与交互先验嵌入扩散噪声估计，实现条件可控且高效的多模态轨迹生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型在行为预测中的可控生成提供新范式，对自动驾驶与机器人导航研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人轨迹天然呈现多模态特性，同一历史轨迹可能对应多种合理未来，这对预测模型在准确性与多样性之间取得平衡提出挑战。现有扩散模型在正向过程中盲目添加高斯噪声，导致反向去噪步数冗余且难以注入语义控制，限制了其在复杂场景下的实用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Prior Condition-Guided Diffusion Trajectory Predictor (CGD-TraP)，用内部意图和外部交互作为先验来引导每一步的噪声估计，而非直接对轨迹加噪。网络包含意图提取模块与交互提取模块，分别编码目标行人的潜在目的和周围智能体的时空关系；两路特征通过基于选择性状态空间模型的时空融合块自适应整合，输出可控的噪声分布参数。整个前向扩散过程被重新建模为条件分布迭代，使反向去噪在意图-交互先验约束下高效生成多模态未来轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ETH-UCY、SDD 和 NBA 三个基准上的实验表明，CGD-TraP 的 minADE/minFDE 显著优于现有扩散及 VAE/GAN 方法，同时生成样本的多样性指标 (如 Sample Diversity Score) 提升 15% 以上，且去噪步数减少约 30%，推理速度提高近 2 倍。消融实验证实意图与交互先验各自贡献互补，去除任一模块均导致精度下降 8–12%。结果验证了条件引导扩散在保持多样性的同时降低冗余计算的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开行人/运动员数据集验证，未涉及车辆-行人混合交通或大规模城市场景，对更复杂动力学与长时预测（&gt;5 s）的泛化能力尚不明确。意图先验依赖目的地标注或聚类近似，在真实无标签场景下需额外估计，可能引入误差并影响条件质量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 CGD-TraP 扩展为意图-交互联合在线估计框架，并引入跨模态条件（如语义地图、交通信号）以实现更长时域、更复杂场景的可控轨迹生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为扩散模型在行为预测领域的可控生成提供了可复用的条件注入范式，其意图-交互解耦与融合策略对关注多智能体建模、生成式预测或扩散加速的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>