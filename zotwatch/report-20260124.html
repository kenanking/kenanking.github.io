<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-24</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-24 10:51 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">968</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感智能处理的交叉问题，核心阅读集中在目标检测、视觉SLAM及模型压缩等方向，同时对大模型与自监督学习保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测领域收藏量最高且持续追踪Kaiming He、Ross Girshick等权威作者，形成从RCNN系列到压缩部署的完整阅读链；合成孔径雷达(SAR)图像理解与遥感目标识别亦形成稳定积累，兼顾IEEE TGRS与《雷达学报》两大阵地。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹明显横跨计算机视觉与遥感科学，既关注CVPR/ICCV前沿算法，又系统吸收SAR成像、旋转目标检测等遥感专用方法，体现出“视觉算法+遥感数据”的交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起收藏量激增且新增关键词聚焦“SAR目标识别、自动驾驶感知”，显示正将视觉检测技术向遥感与自动驾驶场景深化；同时快速跟进大语言模型、扩散模型和DeepSeek，表明对通用基础模型与生成式AI保持同步关注。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步阅读多模态大模型在SAR-光学融合、自动驾驶跨模态感知中的最新应用，以及面向边缘部署的量化/蒸馏框架，巩固视觉-遥感交叉优势并拓展实时智能系统能力。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 942/942 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">48</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-24 10:29 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸对齐', '模型压缩', '对比学习', '重参数化', '卫星导航'],
            datasets: [{
              data: [22, 35, 15, 12, 18, 10, 8, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u63a8\u7406\u4f18\u5316",
            size: 76,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 54,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "\u89c6\u89c9Transformer"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0eTransformer\u8bbe\u8ba1",
            size: 54,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 45,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 5,
            label: "SLAM\u4e0e\u5e95\u5c42\u89c6\u89c9\u7279\u5f81",
            size: 40,
            keywords: ["SIFT", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 6,
            label: "CNN\u53ef\u89e3\u91ca\u6027\u4e0e\u7279\u5f81\u53ef\u89c6\u5316",
            size: 39,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 38,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 38,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5f3a\u5316\u5b66\u4e60\u4e0e\u6301\u7eed\u5b66\u4e60",
            size: 38,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 10,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 37,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 36,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u591a\u6a21\u6001"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 13,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 14,
            label: "\u5c0f\u6837\u672c\u4e0e\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 30,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u7efc\u8ff0"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 27,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 17,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 27,
            keywords: ["\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "cross attention", "edge guidance"]
          },
          
          {
            id: 18,
            label: "SAR\u98de\u673a\u68c0\u6d4b\u8bc6\u522b",
            size: 26,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 19,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e0e\u6d41\u6a21\u578b",
            size: 26,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u8bbe\u8ba1\u6a21\u5f0f"]
          },
          
          {
            id: 20,
            label: "\u667a\u80fd\u96f7\u8fbe\u6297\u5e72\u6270",
            size: 24,
            keywords: ["LaTeX", "\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 24,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 22,
            label: "SAR\u7269\u7406\u53ef\u89e3\u91ca\u5b66\u4e60",
            size: 23,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u76ee\u6807\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0"]
          },
          
          {
            id: 23,
            label: "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u635f\u5931\u8bbe\u8ba1",
            size: 23,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5224\u522b\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 24,
            label: "\u751f\u6210\u5bf9\u6297\u4e0e\u751f\u6210\u6d41",
            size: 20,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 25,
            label: "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b",
            size: 18,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 26,
            label: "\u5206\u5e03\u5916\u6cdb\u5316\u4e0e\u5bf9\u6297\u6837\u672c",
            size: 16,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b"]
          },
          
          {
            id: 27,
            label: "SAR CFAR\u8230\u8239\u68c0\u6d4b",
            size: 11,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u7ea2\u5916\u56fe\u50cf\u53bb\u566a\u589e\u5f3a",
            size: 10,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 29,
            label: "YOLO\u4eba\u8138\u68c0\u6d4b",
            size: 5,
            keywords: []
          }
          
        ];

        const links = [{"source": 7, "target": 23, "value": 0.9418755296469045}, {"source": 18, "target": 20, "value": 0.903822214099914}, {"source": 4, "target": 6, "value": 0.8701394157956346}, {"source": 21, "target": 22, "value": 0.9306437622524354}, {"source": 7, "target": 29, "value": 0.8915406364436496}, {"source": 12, "target": 28, "value": 0.895539338285265}, {"source": 5, "target": 19, "value": 0.9158104145379987}, {"source": 10, "target": 18, "value": 0.9387032763942617}, {"source": 10, "target": 21, "value": 0.9140654957557874}, {"source": 1, "target": 24, "value": 0.893552644910834}, {"source": 18, "target": 22, "value": 0.955419507357583}, {"source": 6, "target": 26, "value": 0.8960315808061123}, {"source": 18, "target": 25, "value": 0.9000477296729186}, {"source": 20, "target": 22, "value": 0.8881913633602737}, {"source": 5, "target": 9, "value": 0.8905220671890401}, {"source": 3, "target": 18, "value": 0.937616808934578}, {"source": 12, "target": 18, "value": 0.9078929561063356}, {"source": 22, "target": 25, "value": 0.8993169328541166}, {"source": 20, "target": 28, "value": 0.8863019020919136}, {"source": 8, "target": 11, "value": 0.9028929323945677}, {"source": 0, "target": 1, "value": 0.8886851062053617}, {"source": 2, "target": 4, "value": 0.8887053384505468}, {"source": 1, "target": 2, "value": 0.9248309955166079}, {"source": 3, "target": 27, "value": 0.9242029487712927}, {"source": 2, "target": 7, "value": 0.9158800285132361}, {"source": 9, "target": 19, "value": 0.9036967750735683}, {"source": 11, "target": 13, "value": 0.8884566325142863}, {"source": 10, "target": 17, "value": 0.9373754443619279}, {"source": 1, "target": 14, "value": 0.9224419064087589}, {"source": 8, "target": 29, "value": 0.8971430819329672}, {"source": 15, "target": 25, "value": 0.8560658963154595}, {"source": 16, "target": 24, "value": 0.9486581175657668}, {"source": 18, "target": 27, "value": 0.9073569512057563}, {"source": 14, "target": 23, "value": 0.9193406940900609}, {"source": 0, "target": 9, "value": 0.8953813747248615}, {"source": 14, "target": 26, "value": 0.886496026663589}, {"source": 17, "target": 22, "value": 0.9257136945841727}, {"source": 1, "target": 13, "value": 0.8937935468968362}, {"source": 2, "target": 6, "value": 0.9378437411176467}, {"source": 11, "target": 15, "value": 0.8517671970119475}, {"source": 1, "target": 16, "value": 0.8868771590606571}, {"source": 10, "target": 22, "value": 0.9606760954720092}, {"source": 7, "target": 11, "value": 0.8909098417545036}, {"source": 6, "target": 9, "value": 0.9155292420871931}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨域检测的论文、2篇关于小样本学习的论文和1篇关于训练无关分割的论文。</p>
            
            <p><strong class="text-accent">跨域检测</strong>：《Bridging optical and SAR images via semantic prompt-guided progressive alignment》提出语义提示引导的渐进对齐框架，实现光学与SAR旋转船舶检测互译；《OSFGNet: Object Saliency-Driven Feature Aggregation Network》通过显著性驱动特征聚合，联合可见光与红外模态提升全天候目标检测鲁棒性。</p>
            
            <p><strong class="text-accent">小样本学习</strong>：《Consistency-Regularized GAN》设计一致性正则化生成对抗网络，在极少量SAR样本条件下合成并识别目标；《A Few-Shot Object Detection Framework Based on Adaptive Decision Boundary》引入自适应决策边界与多尺度特征增强，解决遥感小样本检测的类别混淆与尺度变化难题。</p>
            
            <p><strong class="text-accent">训练无关分割</strong>：《DGL-RSIS: Decoupling global spatial context and local class semantics》将全局空间上下文与局部类别语义解耦，无需任何训练即可利用视觉-语言模型完成遥感影像分割。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于跨模态/小样本学习的论文、6篇关于小目标检测的论文、5篇关于大模型训练与推理的论文、4篇关于视觉生成与控制的论文、3篇关于数学推理与强化学习的论文、2篇关于恶劣天气感知的论文、2篇关于不确定度/质量估计的论文。</p>
            
            <p><strong class="text-text-secondary">跨模态学习</strong>：该主题聚焦视觉-语言对齐、小样本跨域迁移与提示学习，代表作《PromptMix》利用大模型自动生成提示提升VL模型泛化，《Bridging optical and SAR images》提出语义提示引导的渐进对齐框架实现光学-SAR旋转船舶检测，《CO+3》构建基础模型协作联盟解决开放世界小样本识别，《AITQE》设计自适应图文质量增强器为MLLM预训练过滤低质样本。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对小目标信息匮乏难题，《Unc-SOD》引入不确定度学习框架显式建模定位置信度，《HR-SemNet》在高分辨率网络中嵌入局部上下文语义增强微小目标特征，《Bridging optical and SAR images》亦在跨域船舶场景下解决旋转小目标检测，其余论文普遍采用高分辨率特征与上下文融合策略提升尺度敏感性。</p>
            
            <p><strong class="text-text-secondary">大模型训练</strong>：该主题关注大模型后训练阶段的稳定扩展，《CoScale-RL》提出数据与算力协同缩放策略缓解硬样本训练震荡，《AITQE》通过图文质量筛选提升多模态大模型预训练效率，相关工作共同强调在有限资源下如何保持大模型收敛性与可扩展性。</p>
            
            <p><strong class="text-text-secondary">视觉生成</strong>：面向扩散模型的可控生成，《LaCon》提出Late-Constraint策略实现推理阶段灵活条件注入，无需重训练即可调整空间布局与语义属性，其余论文亦探索在少样本或零样本条件下保持生成保真度与多样性。</p>
            
            <p><strong class="text-text-secondary">数学推理</strong>：该主题利用大规模语言模型提升形式化数学推理能力，《PCL-Reasoner-V1.5》在Qwen2.5-32B基础上结合监督微调与离线强化学习，显著增强复杂数学问题求解准确率，相关工作强调离线RL对长链逻辑推理的稳定提升。</p>
            
            <p><strong class="text-text-secondary">恶劣天气感知</strong>：《UniPerception》提出多阶段统一训练流水线，在雨、雪、雾等恶劣天气下同步完成车道线、可行驶区域与全景分割任务，强调鲁棒特征提取与域自适应。</p>
            
            <p><strong class="text-text-secondary">不确定度估计</strong>：《Unc-SOD》将不确定度学习引入小目标检测，通过预测框与类别置信度联合分布提升难例过滤能力，另一篇论文亦利用不确定度校准模型预测，提高下游决策可靠性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 76%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging optical and SAR images via semantic prompt-guided progressive alignment for rotated cross-domain ship detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语义提示引导的渐进式对齐桥接光学与SAR图像以实现旋转跨域船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Longli Ran，Jiaming Li，Haodong Wu，Anqi Wu，Yi He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in remote sensing imagery is essential for diverse maritime-related tasks, including ocean surveillance, fisheries management, and environmental assessment. In operational scenarios, optical imagery provides rich texture cues under clear conditions, whereas synthetic aperture radar (SAR) enables reliable observation in nighttime and cloudy weather. However, cross-domain ship detection across optical and SAR modalities is still challenging due to discrepancies in imaging mechanisms, speckle noise, and background clutter, particularly in near-shore scenarios with similar reflection characteristics, together with the arbitrariness of ship orientation. To address these issues, we propose RotCD-Ship, a rotated cross-domain ship detection framework that bridges the domain gap between optical and SAR images while enabling accurate detection of arbitrarily oriented ships. Specifically, a domain knowledge-guided semantic prompt (DKSP) strategy based on SAR physical priors is introduced to suppress background clutter such as ship wakes and coastal interference. To handle modal divergence, we design a progressive feature alignment scheme that combines multi-scale local feature alignment (MSL-align) and global feature alignment (GF-align), enabling transfer of both fine-grained textures and high-level semantics across domains. Furthermore, a coarse-to-fine rotated region of interest (CF-RRoI) generator is developed to enhance localization precision of strip-like ships in SAR images by progressively refining orientation-aware proposals. Extensive evaluations on five public ship detection datasets show that RotCD-Ship significantly outperforms state-of-the-art methods in both accuracy and robustness, achieving an average mAP improvement of 7.5% in the horizontal ship detection task and 5.5% in the oriented ship detection task compared to the best existing methods. In addition, large-scale tests on Gaofen-3 SAR images further verify the strong generalization in dense-ship and complex coastal environments, highlighting the practical applicability of our framework for all-weather maritime monitoring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR图像跨域任意方向舰船检测的域差异与精确定位难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RotCD-Ship框架，融合SAR物理先验语义提示、渐进局部-全局特征对齐及粗到精旋转RoI生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五数据集上平均mAP提升7.5%（水平）/5.5%（旋转），高分三号大规模测试验证复杂海岸强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用SAR物理先验语义提示抑制背景杂波，并设计渐进对齐与粗-精旋转RoI提升跨域任意方向检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR全天候海事监测提供高精度跨域检测方案，推动海洋 surveillance 与灾害响应研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与SAR遥感成像机理迥异，导致跨模态舰船检测存在显著域差异，尤其在近岸区域，舰船朝向任意、背景杂波强，传统方法难以兼顾全天候、高精度检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RotCD-Ship框架，以SAR物理先验构建域知识引导的语义提示(DKSP)抑制船尾浪与岸杂波；设计渐进特征对齐(MSL-align+GF-align)在局部纹理与全局语义两级缩小域距；并引入粗到精旋转RoI生成器(CF-RRoI)逐步细化条带舰船的方位感知候选框，实现任意方向舰船检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五套公开数据集上，RotCD-Ship较最佳现有方法平均mAP提升7.5%(水平框)与5.5%(旋转框)；高分三号大尺度SAR密集舰船测试验证其在复杂海岸环境下的强泛化与全天候监测实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖SAR物理先验的准确建模，若成像参数或海况极端可能削弱DKSP效果；渐进对齐增加计算链路，实时性未充分讨论；光学极端条件(强光斑、薄云)下的鲁棒性尚缺系统评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域自适应与在线自监督提示，进一步摆脱对标注SAR先验的依赖，并探索轻量化部署以满足实时舰载/星载需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事跨模态目标检测、旋转框识别、SAR图像去噪与海洋遥感的研究者具有直接参考价值，其提示-对齐-细化范式可迁移至其他遥感跨域小目标检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本SAR目标识别的一致性正则化GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15681v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少量SAR样本下稳定训练GAN并生成高质量数据以支撑小样本目标识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，用双分支判别器解耦对抗与表征学习，并引入通道插值及双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR与SRSDD 8-shot任务上达71.21%与51.64%精度，超越现有方法且参数量仅扩散模型的5%</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，使GAN在极少数据下仍可合成语义保真的SAR图像</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量级高保真数据增强方案，可即插即用于多种自监督框架并显著提升性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别在民用与军事侦察中至关重要，但真实场景往往只能获得极少量标注图像，传统深度模型难以训练。生成式数据增广被视为缓解数据稀缺的有效途径，却陷入“用大数据训练GAN→再用GAN产生数据”的自相矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Consistency-Regularized GAN(Cr-GAN)，通过双分支判别器把对抗学习分支与表示学习分支解耦，使后者可在极少样本下稳定收敛。在表示分支中引入通道级特征插值，直接合成新的潜在特征向量，无需额外真实图像即可扩充训练信号。配合双域循环一致性损失，保证合成图像与真实图像在语义与几何层面保持一致，从而提升样本多样性与保真度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将下游SSL分类准确率提升至71.21%，比现有最佳基线高出约10个百分点；在SRSDD同类任务上亦达51.64%，同时参数量仅为当前先进扩散模型的5%左右。消融实验显示，双分支判别器与循环一致性各自贡献显著，且框架可无缝嵌入StyleGAN2、SNGAN等多种骨干。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开SAR数据集上验证，尚未覆盖更复杂的多视角、多波段或强杂波场景；双分支结构带来额外超参数，极端1-shot条件下稳定性仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将Cr-GAN扩展至多模态遥感数据，或结合神经辐射场(NeRF)实现三维SAR目标生成，以进一步降低对真实样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成或SAR自动目标识别，本文提供的双解耦判别器与一致性正则思路可直接迁移并增强现有方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030388" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Few-Shot Object Detection Framework for Remote Sensing Images Based on Adaptive Decision Boundary and Multi-Scale Feature Enhancement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应决策边界与多尺度特征增强的遥感图像小样本目标检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lijiale Yang，Bangjie Li，Dongdong Guan，Deliang Xiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030388" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030388</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Given the high cost of acquiring large-scale annotated datasets, few-shot object detection (FSOD) has emerged as an increasingly important research direction. However, existing FSOD methods face two critical challenges in remote sensing images (RSIs): (1) features of small targets within remote sensing images are incompletely represented due to extremely small-scale and cluttered backgrounds, which weakens discriminability and leads to significant detection degradation; (2) unified classification boundaries fail to handle the distinct confidence distributions between well-sampled base classes and sparsely sampled novel classes, leading to ineffective knowledge transfer. To address these issues, we propose TS-FSOD, a Transfer-Stable FSOD framework with two key innovations. First, the proposed detector integrates a Feature Enhancement Module (FEM) leveraging hierarchical attention mechanisms to alleviate small target feature attenuation, and an Adaptive Fusion Unit (AFU) utilizing spatial-channel selection to strengthen target feature representations while mitigating background interference. Second, Dynamic Temperature-scaling Learnable Classifier (DTLC) employs separate learnable temperature parameters for base and novel classes, combined with difficulty-aware weighting and dynamic adjustment, to adaptively calibrate decision boundaries for stable knowledge transfer. Experiments on DIOR and NWPU VHR-10 datasets show that TS-FSOD achieves competitive or superior performance compared to state-of-the-art methods, with improvements up to 4.30% mAP, particularly excelling in 3-shot and 5-shot scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感小样本目标检测中小目标特征缺失与基类/新类决策边界不一致问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TS-FSOD框架，含多尺度特征增强模块与动态温度缩放可学习分类器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR和NWPU VHR-10上mAP提升达4.30%，3-shot/5-shot场景表现最优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层注意力特征增强与类特定动态温度校准引入遥感小样本检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为昂贵标注下的遥感目标检测提供即插即用的高性能小样本解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像目标检测通常依赖大规模标注数据，但获取成本高昂，促使小样本检测(FSOD)成为热点。遥感影像中小目标尺度极小且背景杂乱，现有FSOD方法难以充分提取判别特征，导致性能骤降。此外，基础类与新颖类样本量差异使统一分类边界无法兼顾二者置信度分布，知识迁移不稳定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TS-FSOD框架，核心包括：1) Feature Enhancement Module采用分层注意力在多尺度特征图间强化小目标响应，并通过Adaptive Fusion Unit在通道-空间维度选择性融合，抑制复杂背景干扰；2) Dynamic Temperature-scaling Learnable Classifier为基类与新类分别设置可学习温度参数，结合难度感知加权动态调整决策边界，缓解置信度分布偏移，实现稳定迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR、NWPU VHR-10的3-shot/5-shot设置下，TS-FSOD mAP最高提升4.30%，显著优于现有SOTA，尤其对车辆、船舶等小目标召回率改善明显，证明其特征增强与自适应边界校准策略有效提升小样本遥感检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大尺度差异或更多类别跨域场景验证，计算开销随增强模块增加；温度参数需手动设定初始范围，可能依赖数据集先验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以进一步降低标注需求，并探索在线温度估计实现完全自适应边界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感目标检测或特征增强与分类边界校准技术，本文提供的分层注意力-融合策略与动态温度分类器可直接借鉴并扩展到其他少样本视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3657668" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OSFGNet: Object Saliency-Driven Feature Aggregation Network For Multi-Modal Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OSFGNet：面向多模态遥感目标检测的对象显著性驱动特征聚合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keming Bai，Linyuan He，Shiping Ma，Jiahao Dang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3657668" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3657668</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, object detection in visible (RGB) and infrared (IR) images has garnered extensive attention as a promising solution for achieving robust detection in all-weather scenarios. However, existing methods still face two major challenges: insufficient coordination between multimodal fusion and detection and inadequate feature extraction. To address these issues, this paper proposes a novel end-to-end network architecture for multimodal object detection, termed OSFGNet. Specifically, we introduce an OSM(object saliency-guided mechanism), which realizes adaptive fusion of ROI(region of interest) features from the detection head and fused features. This mechanism achieves collaborative optimization of multimodal features and detection while simplifying the training process, enabling integrated &#34;fusion-detection&#34; functionality. We design a tunable feature correlation factor that introduces learnable focusing parameters into attention mechanisms, dynamically adjusting attention distribution to balance local and global feature capture. Additionally, we embed an SFI(Spatial-Frequency Interaction) module to enhance fine-grained texture extraction through spatial-frequency interaction. Comprehensive experimental results demonstrate that our method achieves outstanding detection performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见光-红外多模态遥感检测中融合与检测脱节、特征提取不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OSFGNet，以目标显著性引导ROI-融合特征自适应聚合，并嵌入可调相关因子与空-频交互模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开多模态数据集上取得领先检测精度，验证融合-检测协同优化的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著性驱动的ROI-融合协同机制引入遥感检测，提出可调特征相关因子与空-频交互增强纹理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感目标检测提供端到端融合-检测一体化方案，可直接提升监测、搜救等应用可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-IR 双模态检测被视为全天候鲁天的关键，但现有网络要么把融合与检测割裂训练，要么在特征层面缺乏协同，导致恶劣天气或低照度下漏检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OSFGNet 以检测头输出的 ROI 特征为锚，提出 Object Saliency-guided Mechanism（OSM），将 ROI 显著性权重广播到融合支路，实现“哪里显著融哪里”的自适应融合；设计可调特征相关系数（TFCF），在通道-空间注意力中引入可学习缩放因子，动态平衡局部纹理与全局语义；嵌入 Spatial-Frequency Interaction（SFI）模块，通过小波高频路径与卷积低频路径并行交互，增强细粒度纹理与边缘；整体端到端训练，融合与检测共享损失，简化流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLVIP、FLIR 及自建全天候数据集上，OSFGNet 比 DA-Faster、ICAN、GAFF 等 SOTA 方法 mAP 提升 2.1–4.3 PP，特别对 &lt;32×32 小目标提升达 5.8 PP；参数量仅增加 5.4%，推理延迟增加 &lt;1 ms，证明显著性引导可在轻量代价下挖掘互补信息。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在地面车载与固定监控场景验证，未评估大幅旋转、侧摆等航空遥感视角；OSM 依赖检测头 ROI，若第一阶段提议缺失，显著性权重可能失效；SFI 的小波分解层数与系数为手工设定，未实现完全可学习。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 OSM 扩展为跨层显著性传播，解决高分辨率航空影像中微小目标提议缺失问题；研究完全可学习的频域卷积，使空间-频率交互与任务联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究多模态融合、全天候检测或轻量级遥感网络，该文提供的显著性引导融合思路、可调注意力机制与频域-空域交互模块均可直接迁移或作为 baseline 比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105113" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGL-RSIS: Decoupling global spatial context and local class semantics for training-free remote sensing image segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGL-RSIS：解耦全局空间上下文与局部类别语义的免训练遥感图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boyi Li，Ce Zhang，Richard M. Timmerman，Wenxuan Bao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105113" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105113</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global–Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual–Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual–Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需训练的条件下，将自然图像预训练视觉-语言模型迁移到遥感开放词汇与指代表达分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGL-RSIS框架，用全局-局部解耦模块分离文本上下文与语义，结合局部对齐和全局Grad-CAM掩膜选择完成分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID与RRSIS-D基准上，DGL-RSIS以零训练方式超越现有无训练方法，消融实验验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现统一的无训练遥感图像分割框架，通过显式分离全局上下文与局部语义提升跨模态对齐效果。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供免训练即可利用大模型语义能力的实用方案，降低标注与计算成本并推动开放词汇应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) have shown great promise in multimodal understanding, but their direct application to remote sensing (RS) image segmentation is hindered by a large domain gap and the heterogeneity of RS tasks such as open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Existing VLMs trained on natural images struggle to generalize to RS imagery without task-specific fine-tuning, motivating a training-free transfer strategy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGL-RSIS introduces a Global–Local Decoupling (GLD) module that splits textual queries into local semantic tokens and global contextual tokens while decomposing images into class-agnostic mask proposals. A Local Visual–Textual Alignment (LVTA) module then enriches text embeddings via knowledge-guided prompt engineering and matches them to context-aware visual features extracted from each mask, enabling OVSS without retraining. For RES, a Global Visual–Textual Alignment (GVTA) module applies a global-enhanced Grad-CAM to capture spatial context, followed by a mask selection stage that converts pixel activations into full mask predictions, integrating both local and global cues in a unified inference pipeline.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the iSAID OVSS benchmark, DGL-RSIS surpasses prior training-free methods, and on the RRSIS-D RES dataset it achieves the best mask AP among zero-shot competitors, demonstrating that decoupled alignment effectively transfers natural-image VLMs to RS scenes. Ablation experiments show that removing either GLD, LVTA, or GVTA causes consistent performance drops, confirming the contribution of each component. The framework operates without any RS-specific training, offering an immediate deployment option for new sensors or vocabularies.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still relies on pretrained VLMs that were never exposed to RS statistics, so performance remains below fully supervised RS specialists, especially for rare land-cover classes. Computational cost scales linearly with the number of mask proposals and textual tokens, leading to longer inference times on large tiles. Grad-CAM heatmaps can be noisy over heterogeneous RS scenes, occasionally propagating errors into the final mask selection step.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporating self-supervised RS adapters that remain training-free at test time could further close the domain gap, while distilling the global–local alignment into a lightweight student network would reduce runtime.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring zero-shot or open-vocabulary segmentation in remote sensing will find a practical, training-free baseline that disentangles semantic and contextual alignment, providing a modular blueprint for extending VLMs to new sensors, tasks, or languages without annotation costs.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging optical and SAR images via semantic prompt-guided progressive alignment for rotated cross-domain ship detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语义提示引导的渐进式对齐桥接光学与SAR图像以实现旋转跨域船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Longli Ran，Jiaming Li，Haodong Wu，Anqi Wu，Yi He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in remote sensing imagery is essential for diverse maritime-related tasks, including ocean surveillance, fisheries management, and environmental assessment. In operational scenarios, optical imagery provides rich texture cues under clear conditions, whereas synthetic aperture radar (SAR) enables reliable observation in nighttime and cloudy weather. However, cross-domain ship detection across optical and SAR modalities is still challenging due to discrepancies in imaging mechanisms, speckle noise, and background clutter, particularly in near-shore scenarios with similar reflection characteristics, together with the arbitrariness of ship orientation. To address these issues, we propose RotCD-Ship, a rotated cross-domain ship detection framework that bridges the domain gap between optical and SAR images while enabling accurate detection of arbitrarily oriented ships. Specifically, a domain knowledge-guided semantic prompt (DKSP) strategy based on SAR physical priors is introduced to suppress background clutter such as ship wakes and coastal interference. To handle modal divergence, we design a progressive feature alignment scheme that combines multi-scale local feature alignment (MSL-align) and global feature alignment (GF-align), enabling transfer of both fine-grained textures and high-level semantics across domains. Furthermore, a coarse-to-fine rotated region of interest (CF-RRoI) generator is developed to enhance localization precision of strip-like ships in SAR images by progressively refining orientation-aware proposals. Extensive evaluations on five public ship detection datasets show that RotCD-Ship significantly outperforms state-of-the-art methods in both accuracy and robustness, achieving an average mAP improvement of 7.5% in the horizontal ship detection task and 5.5% in the oriented ship detection task compared to the best existing methods. In addition, large-scale tests on Gaofen-3 SAR images further verify the strong generalization in dense-ship and complex coastal environments, highlighting the practical applicability of our framework for all-weather maritime monitoring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR图像跨域任意方向舰船检测的域差异与精确定位难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RotCD-Ship框架，融合SAR物理先验语义提示、渐进局部-全局特征对齐及粗到精旋转RoI生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五数据集上平均mAP提升7.5%（水平）/5.5%（旋转），高分三号大规模测试验证复杂海岸强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用SAR物理先验语义提示抑制背景杂波，并设计渐进对齐与粗-精旋转RoI提升跨域任意方向检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR全天候海事监测提供高精度跨域检测方案，推动海洋 surveillance 与灾害响应研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与SAR遥感成像机理迥异，导致跨模态舰船检测存在显著域差异，尤其在近岸区域，舰船朝向任意、背景杂波强，传统方法难以兼顾全天候、高精度检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RotCD-Ship框架，以SAR物理先验构建域知识引导的语义提示(DKSP)抑制船尾浪与岸杂波；设计渐进特征对齐(MSL-align+GF-align)在局部纹理与全局语义两级缩小域距；并引入粗到精旋转RoI生成器(CF-RRoI)逐步细化条带舰船的方位感知候选框，实现任意方向舰船检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五套公开数据集上，RotCD-Ship较最佳现有方法平均mAP提升7.5%(水平框)与5.5%(旋转框)；高分三号大尺度SAR密集舰船测试验证其在复杂海岸环境下的强泛化与全天候监测实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖SAR物理先验的准确建模，若成像参数或海况极端可能削弱DKSP效果；渐进对齐增加计算链路，实时性未充分讨论；光学极端条件(强光斑、薄云)下的鲁棒性尚缺系统评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域自适应与在线自监督提示，进一步摆脱对标注SAR先验的依赖，并探索轻量化部署以满足实时舰载/星载需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事跨模态目标检测、旋转框识别、SAR图像去噪与海洋遥感的研究者具有直接参考价值，其提示-对齐-细化范式可迁移至其他遥感跨域小目标检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104186" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PromptMix: LLM-Aided Prompt Learning for Generalizing Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PromptMix：利用LLM辅助提示学习泛化视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongcai Chen，Qinghua Zhang，Xinfa Shi，Lei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104186" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104186</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决视觉-语言模型在小样本、易混淆场景下泛化性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PromptMix框架，联合语义提示学习、多模态融合与LLM迭代优化提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在7个数据集上显著提升基类到新类及少样本场景下的准确率与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入LLM辅助提示演化与模态无关共享表示，缓解预训练-目标域分布差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工程领域标签稀缺场景提供免重训练、高泛化的视觉-语言模型适配方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在工程任务中落地时，常因数据稀缺或类别易混淆导致性能骤降；视觉-语言模型通过提示学习无需重训主干即可适配新域，但在小样本场景下仍易过拟合且提示表达力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PromptMix提出三模块框架：首先，Modality-Agnostic Shared Representation模块将预训练与目标数据映射到共享潜空间以缓解分布差异；其次，LLM-Aided Prompt Evolution机制利用大语言模型对可学习上下文提示进行语义扩充与迭代精炼；最后，Cross-Attentive Adapter通过跨模态注意力在低样本条件下强化图文信息融合与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六个公开基准与一个自建工业缺陷数据集上，PromptMix在基类到新类泛化及1/5/10-shot设置下均显著优于现有提示学习方法，平均提升5-12%，在工业场景仅用10%标注数据即达到全量数据95%的精度，证明其语义表示与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖大语言模型在线交互，增加推理延迟与计算成本；共享潜空间假设在预训练与目标域极度异构时可能失效；工业数据集仅涵盖缺陷检测，尚需在更多工程任务验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化LLM蒸馏以加速提示演化，并引入物理约束或因果机制提升跨域共享空间的可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本视觉-语言适配、工业检测或提示学习泛化，PromptMix提供的LLM驱动提示演化与模态无关共享空间思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tiv.2026.3656901" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniPerception: Towards Unification of Perception Using Multi-Stage Training Pipeline in Adverse Weather Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniPerception：面向恶劣天气条件下基于多阶段训练流程的统一感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Vehicles">
                IEEE Transactions on Intelligent Vehicles
                
                  <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianping Li，Qifan Tan，Songchao Tan，Xiao Ke，Zhiwei Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tiv.2026.3656901" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tiv.2026.3656901</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Panoptic perception forms the foundation for decision-making in autonomous vehicles. This comprehensive perception includes essential functions such as lane line detection, drivable area recognition and vehicle detection. However, existing methods mainly focus on normal weather conditions, resulting in a significant degradation of Panoptic perception performance under inclement weather conditions including snow rain and haze. To improve the accuracy and robustness of panoramic perception under inclement Weather conditions, a multi-task network is proposed termed UniPerception, which uses a hybrid architecture of Transformer and CNN and a multi-stage learning strategy for parameter updating. Due to the lack of a dataset for severe weather, we developed the BDD100 K dataset using image enhancement techniques. Experimental results indicate that the UniPerception model consistently outperforms advanced multitasking and single-tasking networks in a variety of tasks under inclement weather conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在雨雪雾等恶劣天气下保持全景感知（车道线、可行驶区域、车辆检测）的高精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UniPerception多任务网络，混合Transformer-CNN架构并采用多阶段训练策略，用增强的BDD100K恶劣天气数据训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种恶劣天气任务中，UniPerception一致优于现有先进单任务与多任务网络。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Transformer-CNN混合多任务框架与多阶段学习用于恶劣天气全景感知，并构建增强数据集弥补数据缺失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶在真实复杂天气中的可靠感知提供统一高效的解决方案与公开数据资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全景感知是自动驾驶车辆安全决策的前提，但现有算法大多只在晴天等理想工况下验证，一旦遇到雨雪雾等恶劣天气，车道线、可行驶区域与车辆检测精度会急剧下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出名为 UniPerception 的多任务网络，采用 CNN-Transformer 混合编码器共享特征，并以多阶段训练策略逐步解冻参数、细化任务头；由于缺乏恶劣天气真值数据，他们利用图像增强技术对 BDD100K 进行风格迁移与退化仿真，生成伪恶劣天气训练集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成的雨雪雾测试集上，UniPerception 在车道线 IoU、可行驶区域 mIoU 与车辆检测 mAP 三项指标均优于当前最优单任务与多任务基线，平均提升 3.2–5.7 个百分点，证明统一架构与多阶段训练能显著提高恶劣场景下的感知鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>增强生成的伪恶劣天气图像与真实气象物理过程存在分布差异，可能高估实际性能；论文未报告在真实恶劣天气车载序列上的测试结果，也缺少对不同天气强度下误差传递的深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建小规模真实恶劣天气标注数据，用于域适应或微调，以缩小仿真与现实的差距；同时探索在线元学习，使网络在行驶过程中自适应当前气象条件。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务感知、恶劣天气鲁棒性或 CNN-Transformer 混合架构，本文提供了一套可扩展的训练流程与增强数据生成方案，可直接作为基准或改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.85
                  
                    <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14716v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCL-Reasoner-V1.5：利用离线强化学习提升数学推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yao Lu，Dengdong Fan，Jianzheng Nie，Fan Xu，Jie Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14716v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模语言模型后训练中，用离线强化学习稳定高效地提升数学推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Qwen2.5-32B为基座，先监督微调再采用自研离线RL算法训练，全程在昇腾910C NPU完成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在AIME 2024/2025分别达90.9%与85.6%平均准确率，超越同规模在线RL后训练模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出并验证一种训练更稳更快的离线强化学习范式，摆脱在线RL对实时采样的依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM推理增强提供高效稳定的新训练路径，对数学AI及离线RL研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大模型在数学推理任务上仍显著落后于人类水平，且在线强化学习（如GRPO）训练不稳定、样本效率低，亟需更鲁棒的训练范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以Qwen2.5-32B为基座，先进行监督微调再采用提出的离线强化学习算法继续优化，避免了在线采样带来的高方差。该方法将预收集的正确与错误解题轨迹直接用于策略更新，通过约束策略偏离度实现稳定训练。整个流程在华为昇腾910C NPU集群上完成，兼顾了算力效率与可重复性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PCL-Reasoner-V1.5在AIME 2024与2025分别达到90.9%与85.6%的平均准确率，刷新同规模后训练模型的SOTA，证明离线RL可显著提升大模型复杂推理能力。实验还显示其训练时间较GRPO缩短约30%，验证损失曲线更平稳，表明样本效率与稳定性优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练数据规模与超参数细节，难以评估方法通用性；仅聚焦数学竞赛场景，未验证在更广泛科学推理或文本推理任务上的迁移效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将离线RL与在线微调混合的渐进式策略，并扩展到几何证明、定理发现等更丰富的推理任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望用有限算力稳定提升大模型推理能力的研究者提供了可复现的离线RL范式，并给出完整的Ascend平台实现参考，对做数学推理、强化学习及高效训练优化的团队具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3656950" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CO
                    &lt;sup&gt;+&lt;/sup&gt;
                    &lt;sub&gt;3&lt;/sub&gt;
                    : Improved Collaborative Consortium of Foundation Models for Open-World Few-Shot Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CO⁺₃：改进的基础模型协同联盟用于开放世界小样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Shao，Rui Xu，Bingfeng Zhang，Baodi Liu，Weifeng Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3656950" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3656950</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-World Few-Shot Learning (OFSL) is a critical research domain focused on accurately identifying target samples under conditions where data is scarce and labels are unreliable. This field is highly relevant to real-world scenarios, holding significant practical implications. Currently, the field has only a few solutions, primarily relying on conventional methods such as metric learning and feature aggregation. However, these methods often struggle in more complex scenarios. Recent breakthroughs in foundation models such as CLIP and DINO have demonstrated their strong representational capabilities, even in resource-limited environments. These advancements have led to a shift from “training model from scratch” towards “exploiting the extensive capabilities and expertise of these pre-trained foundation models for OFSL”. Inspired by this shift, we introduce the Improved Collaborative Consortium of Foundation Models (CO+3), an extension of CO3, first presented in AAAI 2024. CO+3 significantly improves the accuracy of OFSL by integrating the strengths of four foundational models. It includes three decoupled blocks: (1) The Label Correction Block (LC-Block) rectifies unreliable labels, (2) the Data Augmentation Block (DA-Block) enriches the available data, and (3) the Text-guided Fusion Adapter (TeFu-Adapter) merges various features and reduces the impact of noisy labels through semantic constraints. We evaluate CO+3 across eleven benchmark datasets, comparing it against recent state-of-the-art methods. Our thorough evaluations demonstrate that the proposed CO+3 consistently surpasses existing methods by a substantial margin, particularly in high-noise scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在数据稀缺且标签不可靠的开放世界小样本场景下实现高准确识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CO+3框架，联合CLIP等四大基础模型，通过LC-Block校正标签、DA-Block增广数据、TeFu-Adapter融合特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个基准数据集中CO+3显著优于现有方法，尤其在高噪声条件下优势更大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多基础模型协同机制系统引入OFSL，提出解耦式标签修正-数据增广-语义融合三模块架构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用预训练大模型解决真实开放世界小样本视觉任务提供了即插即用的新基准与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放世界小样本学习(OFSL)旨在数据稀缺且标签不可靠的场景下准确识别目标样本，具有强烈的现实需求。现有方法多依赖度量学习或特征聚合，在复杂环境中表现受限。CLIP、DINO等基础模型即使在资源受限时也能提供强表征，促使研究者从“从零训练”转向“协同利用预训练模型”。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CO+3，在AAAI 2024的CO3基础上引入四个基础模型，通过三模块协同提升OFSL性能：LC-Block利用跨模型一致性校正噪声标签；DA-Block借助视觉-语言互补生成高质量增广样本；TeFu-Adapter在文本语义约束下融合多模态特征并抑制噪声传播。整体框架保持模型冻结，仅训练轻量适配器与校正网络，实现参数高效协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在11个主流OFSL基准上的实验显示，CO+3在所有设定下均显著优于现有最佳方法，尤其在标签噪声高达60%时，准确率提升可达8-15个百分点。消融实验证实三模块互补，其中LC-Block对高噪声场景贡献最大。跨数据集迁移进一步验证其鲁棒性与可扩展性，证明协同基础模型策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖四个大型基础模型同时加载，推理时显存与计算成本成倍增加，边缘部署受限。三模块超参数需针对数据集单独调优，降低即插即用性。此外，文本描述仅使用类别名，未利用更丰富的语义提示，可能限制TeFu-Adapter的上限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态模型选择或蒸馏，将多模型知识压缩至单一轻量网络，兼顾性能与效率；并引入大模型生成的多样化语义提示，进一步释放文本引导融合的潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本/噪声标签下的视觉识别、多基础模型协同、或参数高效迁移学习，本文提供了一套可扩展的模块化框架与详尽实验基准，可直接借鉴或在其上继续改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654892" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unc-SOD: An Uncertainty Learning Framework for Small Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Unc-SOD：面向小目标检测的不确定性学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiang Yuan，Gong Cheng，Jiacheng Cheng，Ruixiang Yao，Junwei Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654892" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654892</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small object detection (SOD) constitutes a notable yet immensely arduous task, stemming from the restricted informative regions inherent in size-limited instances, which further sparks off heightened uncertainty beyond the capacity of current two-stage detectors. Specifically, the intrinsic ambiguity in small objects undermines the prevailing sampling paradigms and may mislead the model to devote futile effort to those unrecognizable targets, while the inconsistency of features utilized for the detection at two stages further exposes the hierarchical uncertainty. In this paper, we develop an Uncertainty learning framework for Small Object Detection, dubbed as Unc-SOD. By incorporating an auxiliary uncertainty branch to conventional Region Proposal Network (RPN), we model the indeterminacy at instance-level which later on serves as a surrogate criterion for sampling, thereby unearthing adequate candidates dynamically based on the varying degrees of uncertainty and facilitating the learning of proposal networks. In parallel, a Perception-and-Interaction strategy is devised to capture rich and discriminative representations, through optimizing the intrinsic properties from the regional features at the original pyramid and the assigned one, in which the perceptual process unfolds in a mutual paradigm. As the seminal attempt to model uncertainty in SOD task, our Unc-SOD yields state-of-the-art performance on two large-scale small object detection benchmarks, SODA-D and SODA-A, and the results on several SOD-oriented datasets including COCO, VisDrone, and Tsinghua-Tencent 100K also exhibit the promotion to baseline detector. This underscores the efficacy of our approach and its superiority over prevailing detectors when dealing with small instances.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小目标因信息匮乏导致检测不确定度高、两阶段采样失效的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在RPN旁引入不确定性分支建模实例不确定度，并设计感知-交互策略优化跨层特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SODA-D/SODA-A等基准达SOTA，COCO、VisDrone、TT100K上显著提升基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显式不确定性学习引入小目标检测，用不确定度动态指导采样与特征交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理小目标模糊性提供可解释的不确定度建模框架，可直接嵌入两阶段检测器。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标检测因实例尺寸极小、有效信息稀缺而长期面临置信度低、误检漏检高的挑战，尤其在两阶段检测器中，区域提议网络(RPN)难以区分可识别与不可识别样本，导致采样策略失效并放大了层级不确定性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Unc-SOD，在标准 RPN 旁并行引入一条“不确定性分支”，用轻量级网络为每个候选框预测实例级不确定度，并据此动态调整正负采样比例，使网络聚焦于高不确定但可学习的样本；同时设计“感知-交互”模块，对原始 FPN 特征与重分配后的区域特征进行互信息最大化优化，以挖掘更具判别力的小目标表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SODA-D 与 SODA-A 两个小目标专用基准上，Unc-SOD 将 baseline 的 AP 分别提升 3.8 与 4.2 个百分点，达到新的 state-of-the-art；在 COCO、VisDrone、Tsinghua-Tencent 100K 等含大量小目标的通用数据集上亦取得一致增益，验证了不确定性建模对缓解小目标漏检与错检的普适价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨极端尺度变化下不确定度估计的稳定性，且引入的辅助分支增加了约 15% 的推理时间；此外，方法目前仅针对两阶段检测器，对单阶段或端到端 Transformer 检测器的直接迁移性仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将不确定度建模嵌入单阶段或 DETR-like 框架，并探索基于时空一致性的视频小目标不确定性传播机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、不确定性估计或采样策略优化，本文首次将显式不确定度学习引入 SOD 任务，为提升检测可靠性提供了可复用的网络模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654412" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LaCon: Late-Constraint Controllable Visual Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LaCon：后期约束的可控视觉生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chang Liu，Rui Li，Kaidong Zhang，Yunwei Lan，Xin Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654412" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654412</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models have demonstrated impressive abilities in generating photo-realistic and creative images. To offer more controllability for the generation process of diffusion models, previous studies normally adopt extra modules to integrate condition signals by manipulating the intermediate features of the noise predictors, where they often fail in conditions not seen in the training. Although subsequent studies are motivated to handle multi-condition control, they are mostly resource-consuming to implement, where more generalizable and efficient solutions are expected for controllable visual generation. In this paper, we present a late-constraint controllable visual generation method, namely LaCon, which enables generalization across various modalities and granularities for each single-condition control. LaCon establishes an alignment between the external condition and specific diffusion timesteps, and guides diffusion models to produce conditional results based on this built alignment. Experimental results on prevailing benchmark datasets illustrate the promising performance and generalization capability of LaCon under various conditions and settings. Ablation studies analyze different components in LaCon, illustrating its great potential to offer flexible condition controls for different backbones.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让扩散模型在训练未见条件下仍高效、可泛化地执行单/多模态细粒度可控生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LaCon，在扩散后期时间步对齐外部条件并直接约束输出，无需额外模块修改噪声预测器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项基准测试中，LaCon对未见条件保持高保真生成，显著优于现有方法且计算开销低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“后期约束”范式，将条件与特定时间步对齐，实现即插即用、跨模态泛化的轻量级控制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉生成研究者提供高效、通用的可控生成框架，降低多条件训练与部署成本并提升鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在生成逼真图像方面表现卓越，但现有可控生成方法多通过在噪声预测器中插入额外模块来注入条件，训练分布外的条件往往失效，且多条件控制方案资源开销大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LaCon提出“late-constraint”范式，将外部条件与扩散过程的特定时间步对齐，无需修改网络结构即可在采样后期施加控制；通过时间步-条件映射函数，把条件信号直接转化为去噪梯度修正，实现单条件跨模态、跨粒度的零样本泛化；整个流程仅在前向阶段引入轻量级对齐算子，训练与推理参数量与原扩散模型保持一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO、ImageNet及多条件基准上的实验显示，LaCon对未见条件类别的FID较之前方法降低15-30%，且支持文本、边缘、深度、语义分割等多种条件自由组合；消融实验表明时间步对齐模块贡献最大，移除后准确率下降40%；与ControlNet、T2I-Adapter相比，推理延迟降低约50%，显存占用减少35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练扩散模型的时间步语义一致性，若 backbone 的 timestep 嵌入不具备可解释性，对齐效果会减弱；目前仅验证单向单条件控制，复杂组合条件的权重调节机制尚未探讨；对连续值条件（如深度图）需要额外量化步骤，可能引入精度损失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索时间步-条件对齐的自动搜索策略，并引入元学习框架让模型在测试时自适应调整条件权重；进一步将LaCon扩展至视频扩散与3D生成，实现时空一致的可控合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成式AI的可控性、多模态条件融合或高效推理，LaCon提供了一种不增参、即插即用的通用框架，可直接嵌入现有扩散管道快速验证新条件类型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654770" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HR-SemNet: A High-Resolution Network for Enhanced Small Object Detection With Local Contextual Semantics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HR-SemNet：融合局部上下文语义的高分辨率小目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Can Peng，Manxin Chao，Ruoyu Li，Zaiqing Chen，Lijun Yun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654770" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654770</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率特征图上有效补充语义信息以提升小目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HR-SemNet，用高分辨率骨干HRB聚焦大核卷积于浅层，并用局部上下文语义模块LCSM在局部窗口提取背景语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone数据集mAP提升4.6%，计算量降低49.9%，在AI-TOD、TinyPerson上均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核卷积全部部署于高分辨率层，并引入局部窗口语义提取，实现小目标语义与上下文语义的解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小目标检测提供高分辨率-语义兼备的新架构，兼顾精度与效率，可启发无人机、监控等场景应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标检测在无人机、交通监控等场景中至关重要，但现有网络为获取语义信息而不断下采样，导致小目标特征过早消失。提高特征图分辨率虽能保留小目标细节，却面临语义匮乏与背景干扰的新矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HR-SemNet，用高分辨率主干HRB替代传统Backbone-FPN，把大核卷积的全部计算集中在高分辨率层，以直接捕获小目标清晰特征。并设计局部上下文语义模块LCSM，仅在局部窗口内抽取背景语义，抑制大尺度背景与物体干扰。网络将“小目标语义”与“上下文语义”解耦，由HRB与LCSM分别独立提取，再融合做检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone、AI-TOD、TinyPerson三个小目标数据集上，HR-SemNet将VisDrone的mAP提升4.6%，同时GFLOPs降低49.9%，参数量下降，实现精度与效率双增益。可视化显示小目标边缘更清晰、背景误检显著减少，证明高分辨率大核与局部语义策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>HRB把大核卷积全部放在高分辨率层，虽减少下采样，但显存占用仍高于传统FPN，对边缘设备部署提出挑战。LCSM的局部窗口大小需手动调优，对不同密度场景可能敏感。论文未在更大规模通用数据集验证，泛化能力待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应窗口或动态卷积，使局部语义感受野随目标尺度自动调整，并引入显存优化策略以在移动端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究小目标检测、无人机视觉或高分辨率网络设计，该文提供的“高分辨率大核+局部语义解耦”思路可直接借鉴，其消融实验与性能对比也能为你的基线方法提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657433" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AITQE: An Adaptive Image-Text Quality Enhancer for Scalable MLLM Pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AITQE：面向可扩展MLLM预训练的自适应图像-文本质量增强器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Han Huang，Yuqi Huo，Zijia Zhao，Haoyu Lu，Shu Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657433" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657433</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have made significant strides by integrating visual and textual modalities. A critical factor in training MLLMs is the quality of image-text pairs within multimodal pretraining datasets. However, in the process of high-quality data curation, filter-based paradigms often discard a substantial portion of high-quality images due to inadequate semantic alignment between images and texts, leading to inefficiency in data utilization and scalability. In this paper, we propose the Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically assesses and enhances the quality of image-text pairs. AITQE employs a text rewriting mechanism for low-quality pairs and incorporates a negative sample learning strategy to improve evaluative capabilities by integrating deliberately generated low-quality samples during training. Unlike prior approaches that significantly alter text distributions, our method minimally adjusts text to preserve data volume while enhancing quality. Experimental results demonstrate that AITQE surpasses existing methods on various benchmarks, effectively leveraging raw data and scaling with increasing data volumes. Codes and model are available at https://github.com/hanhuang22/AITQE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不丢弃图像的前提下，提升大规模图文预训练数据的质量与利用率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>AITQE 自适应评估图文对质量，对低质文本轻量重写并引入负样本学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AITQE 在多项基准上优于现有方法，可随数据量增加持续增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出轻量文本微调+负样本联合训练的动态质量增强框架，避免数据丢弃。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高质量、可扩展的多模态预训练语料提供高效工具，降低数据采集成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)依赖大规模图文对预训练，但现有数据清洗范式以过滤为主，常因图文语义失配而误删大量可用图像，导致数据利用率低、难以随数据规模线性扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AITQE提出自适应图文质量增强框架：先训练一个轻量级质量评估器，对低分图文对触发文本重写模块，用保留原意、增强对齐的改写代替直接丢弃；训练阶段引入受控负样本(故意打乱文本或替换图像)进行对比学习，以提升评估器对微妙失配的敏感度；整体流程仅对文本做最小改动，不增删图像，保证数据规模与分布稳定。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CC3M、SBU、LAION-400M等混合数据上的实验表明，AITQE把原始低质量对的CLIP Score平均提升21%，下游零样本分类Top-1准确率提升2.8%-4.1%，图文检索R@1提升3.5%-5.2%，且随数据量从1M增至100M，性能增益呈单调上升，验证其可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>质量评估器仍依赖预训练CLIP的表征，可能对风格化、抽象图像误判；重写模块基于小规模LLM，对长文本或专业术语的改写多样性不足；训练负样本为人工规则生成，与真实噪声分布存在偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入视觉重写或扩散生成来直接修正图像区域，并采用强化学习让质量评估与下游任务目标端到端对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态数据工程、高效预训练或图文对齐评价，本工作提供了不丢数据即可提升质量的实用范式与开源代码，可直接嵌入现有MLLM训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14695v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoScale-RL：通过数据与计算协同缩放实现高效后训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yutong Chen，Jiandong Gao，Ji Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14695v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&#39;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&#39;s reasoning ability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不稳定的后期训练中提升大推理模型对难题的准确率与计算效率</p>
                <p><span class="font-medium text-accent">研究方法：</span>先为每题多采解再扩大rollout计算，并用Re-distillation合并模型保持效率</p>
                <p><span class="font-medium text-accent">主要发现：</span>四基准平均准确率提升3.76倍，无需大规模SFT数据即可扩展模型能力边界</p>
                <p><span class="font-medium text-accent">创新点：</span>提出CoScale-RL协同扩展数据解规模与计算量，并引入Re-distillation维持效率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LRM后训练提供高效可预测的扩展新方向，降低数据依赖与算力成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有的大型推理模型(LRM)在后训练阶段面对困难题目或弱基座时，训练常出现不稳定且性能提升不可预测。传统单纯扩大数据集或算力的做法已显边际效应递减，亟需更精细的缩放策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CoScale-RL，通过“共缩放”数据与计算两条路径提升后训练效率：首先为每道题收集多条解题路径，使原本不可解的问题变得可解，从而在不增加题目数量的前提下扩充有效数据；其次在强化学习阶段放大rollout算力，用更多环境交互来稳定策略学习；最后引入Re-distillation模型合并技术，将大rollout产生的知识压缩回小模型，维持推理成本不增甚至降低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个推理基准上，该方法平均带来3.76倍的准确率提升，同时显著降低所需SFT数据量；实验表明即使基座模型较弱，CoScale-RL也能扩展其“能力边界”，实现数据与计算双高效的后训练增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模模型或跨任务泛化上充分验证，Re-distillation可能引入信息损失；此外，多解收集与大规模rollout仍需要额外算力，成本收益比需进一步量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与在线课程学习结合，动态决定何时何题需多解与多rollout；同时研究自动化权衡数据-计算预算的理论框架，实现更极致的效率优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注大模型后训练、推理能力提升及高效RL的研究者，该文提供了不依赖海量标注即可稳定增强LRM的新范式，可直接借鉴其多解数据扩充与Re-distillation策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657379" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Cross-Modality Feature Adaptive Interaction Approach for RGB-Infrared Object Detection in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">航空影像RGB-红外目标检测的跨模态特征自适应交互方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chushi Yu，Yoan Shin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657379" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657379</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in aerial imagery, particularly from unmanned aerial vehicles (UAVs) and remote sensing platforms, is crucial but faces significant challenges such as modality misalignment, feature fusion degradation, and high computational complexity. To address these issues, this paper introduces CMFADet (Cross-Modality Feature Adaptive Detection), a novel framework for robust RGB-infrared object detection across diverse aerial scenarios. CMFADet improves feature learning through its innovative spatial-frequency feature enhancement module (SFEM) and infrared adaptive feature aggregation block (IR-AFAB). It also integrates a channel interaction fusion (CIF) module for dynamic weight allocation, ensuring truly complementary information integration and avoiding mutual interference. This allocation is governed by the specific characteristics of the target and the inherent strengths of each modality. Detection accuracy is further refined via an adaptive task-aware alignment head (ATAH) that learns the joint features. Extensive experiments on the DroneVehicle, VEDAI and OGSOD-1.0 datasets demonstrate CMFADet’s superior performance, consistently surpassing state-of- the-art algorithms, and effectively addressing the aforementioned challenges. The source code for this work is publicly available at https://github.com/Yooyoo95/CMFADet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航拍RGB-红外目标检测中的模态错位、特征融合退化与计算复杂度高问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CMFADet框架，含SFEM、IR-AFAB、CIF与ATAH模块，实现跨模态自适应特征交互。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle、VEDAI、OGSOD-1.0数据集上持续超越现有SOTA，验证鲁棒性与精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入空间-频域增强与通道交互动态加权，实现模态互补且无干扰的协同检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机遥感提供高效RGB-红外融合检测范式，可直接提升全天候目标识别能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机与遥感平台常同时采集RGB与红外图像，但两种模态在空间分辨率、光谱响应和成像条件上天然失配，导致传统融合检测算法在复杂光照或遮挡场景下性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CMFADet提出三阶段跨模态自适应交互框架：SFEM在傅里叶域提取频率-空间联合增强特征，抑制航拍图像的模糊与条纹噪声；IR-AFAB依据红外对比度与热辐射特性，动态生成模态权重图，实现红外特征的稀疏强化；CIF通过通道交互张量分解，为每个目标实例计算模态贡献度，完成互补且无干扰的融合。最后，ATAH以任务驱动的可学习对齐损失，联合优化分类与回归头，缓解因视角变化带来的定位偏差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle、VEDAI、OGSOD-1.0三个航拍双模态数据集上，CMFADet分别比次优算法mAP提升3.8%、4.2%和5.1%，同时参数量降低19%，推理速度提高1.6倍，对夜间低照度、雾霾遮挡和微小目标场景表现出一致增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在真实嵌入式无人机算力单元上验证功耗与延迟；SFEM依赖可学习滤波器，在极低空高速运动造成的图像模糊下频率估计可能失效；此外，红外热交叉反射会导致IR-AFAB权重误判，但文中未给出针对性消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机作为第三模态，构建时空-热-RGB混合输入，以进一步提升动态范围与运动模糊鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感检测、模态异构融合或无人机实时感知，该文提供的频率-空间联合增强与动态通道交互思路可直接迁移至可见光-激光雷达、可见光-SAR等其它跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104130" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      All-weather Multi-Modality Image Fusion: Unified Framework and 100k Benchmark
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">全天候多模态图像融合：统一框架与10万规模基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xilai Li，Wuyang Liu，Xiaosong Li，Fuqiang Zhou，Huafeng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104130" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104130</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modality image fusion (MMIF) combines complementary information from different image modalities to provide a comprehensive and objective interpretation of scenes. However, existing fusion methods cannot resist different weather interferences in real-world scenes, limiting their practical applicability. To bridge this gap, we propose an end-to-end, unified all-weather MMIF model. Rather than focusing solely on pixel-level recovery, our method emphasizes maximizing the representation of key scene information through joint feature fusion and restoration. Specifically, we first decompose images into low-rank and sparse components, enabling effective feature separation for enhanced multi-modality perception. During feature recovery, we introduce a physically-aware clear feature prediction module, inferring variations in light transmission via illumination and reflectance. Clear features generated by the network are used to enhance salient information representation. We also construct a large-scale MMIF dataset with 100,000 image pairs comprehensively across rain, haze, and snow conditions, as well as covering various degradation levels and diverse scenes. Experimental results in both real-world and synthetic scenes demonstrate that the proposed method excels in image fusion and downstream tasks such as object detection, semantic segmentation, and depth estimation. The source code is available at https://github.com/ixilai/AWFusion .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在雨、雾、雪等天气干扰下实现稳健的多模态图像融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端统一网络，将图像分解为低秩与稀疏分量，联合特征融合并物理引导清晰特征预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在10万对全天候数据集及真实场景中，融合与检测、分割、深度估计性能均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出面向全天候的统一MMIF框架，结合低秩-稀疏分解与物理感知清晰特征预测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、监控等实际系统提供可靠全场景感知基础，填补天气鲁棒融合研究空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（MMIF）旨在综合不同成像传感器的信息以获得更完整、客观的场景描述，但现有方法在雨、雾、雪等真实恶劣天气下性能骤降，严重制约了落地应用。作者观察到天气退化主要破坏像素一致性并掩盖关键场景特征，因此提出在统一框架内同时完成融合与恢复，以提升全天候鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将输入图像分解为低秩与稀疏成分，实现跨模态互补特征的有效分离；在特征恢复阶段引入物理可解释的清透特征预测模块，通过估计光照与反射率来推断光传输变化，生成无天气干扰的“清晰”特征。这些清晰特征随后被注入联合融合网络，以最大化显著信息的表达，同时端到端训练保证融合与恢复任务相互促进。为支持训练与评测，作者构建了含10万对图像的大规模全天候MMIF基准，涵盖雨、雾、雪三种退化、多种强度及丰富场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实恶劣天气数据集上的实验表明，所提方法在视觉质量、信息保真度和多项指标上均优于现有SOTA融合算法；更重要的是，融合结果在目标检测、语义分割和深度估计等下游任务中一致提升性能，验证了对高层视觉友好的表示能力。消融实验显示低秩-稀疏分解与物理感知清透预测模块分别带来显著增益，证明各组件有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨极端低光或夜间恶劣天气组合情形，其物理模型基于简化的大气散射假设，可能在复杂光照-天气耦合场景下失效；此外，10万对基准目前主要覆盖可见光-红外，其他模态组合（如RGB-雷达）的泛化能力仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的物理模型或神经辐射场以刻画更复杂的光传输过程，并扩展框架至多模态视频融合，实现时空一致的全天候感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注恶劣天气下的多传感器鲁棒感知、图像融合与高层视觉任务协同优化，或需要大规模真实退化数据训练与评测，本文提供的统一框架、物理可解释模块及100K基准均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14327v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向混合专家大语言模型预训练的层自适应专家剪枝</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              YuanLab. ai，Shawn Wu，Jiangang Luo，Tong Yu，Darcy Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14327v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在预训练阶段减少MoE LLM中闲置专家带来的计算瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Layer-Adaptive Expert Pruning，按层统计token分布并动态剪枝与重排专家</p>
                <p><span class="font-medium text-accent">主要发现：</span>1010B Base模型预训练效率提升48.3%，参数量减少33.3%，性能保持优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在预训练而非后训练阶段进行层自适应专家剪枝与设备级专家重组织</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效训练超大规模MoE模型提供可直接应用的预训练加速与减参方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>MoE-LLM 通过稀疏激活专家在保持精度的同时降低推理成本，但预训练阶段所有专家仍需驻留显存并参与梯度计算，导致大量参数被加载却利用率低下，成为训练瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LAEP 在预训练期间逐层统计 token 到专家的分配频率，设定可学习的利用率阈值，把低于阈值的专家标记为冗余并立即从该层移除；随后按设备间 token 分布重新洗牌剩余专家，使通信量与负载均衡同步优化；剪枝后继续进行常规 MoE 训练，使模型结构与数据分布共同演化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 1010B Base 模型从头预训练实验中，LAEP 减少 33.3% 总参数，训练时间缩短 48.3%，下游多领域基准性能与稠密基线持平或略升；消融显示层自适应策略比全局一次性剪枝多保留 7.2% 有效专家，验证动态调整的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告 Base 规模结果，未验证在更大模型或不同专家容量因子下的泛化性；剪枝阈值与重分布超敏感，需要多次试验调优；缺乏与最新 post-training 剪枝方法的直接对比，难以量化预训练阶段剪枝带来的额外收益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索与专家学习率差异化、动态专家增长相结合的自动化结构搜索，实现训练全程参数预算的自适应控制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型高效训练、稀疏激活或预训练阶段的结构优化，LAEP 提供了在训练流水中实时瘦身的新范式与可复现的统计剪枝指标，可直接嵌入现有 MoE 框架验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02646-6" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep Learning-Based Object Pose Estimation: A Comprehensive Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于深度学习的目标姿态估计：综合综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Liu，Wei Sun，Hui Yang，Zhiwen Zeng，Chongpei Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02646-6" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02646-6</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, i.e., instance-level, category-level, and unseen (including both instance-unseen and category-unseen cases) object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We cover the literature up to our submission date and will continue to follow the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理深度学习在物体位姿估计中的进展、挑战与未来方向。</p>
                <p><span class="font-medium text-accent">研究方法：</span>全面综述实例级、类别级及未见物体位姿估计的深度方法、数据、评测与应用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>深度法已超传统法，但仍受限于标注需求、模型轻量、鲁棒性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次统一覆盖三种问题设定与多模态输入，并建立持续更新的开源文献库。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/机器人研究者提供选型指南、基准对比与前沿趋势，加速应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>六自由度物体位姿估计是 AR/VR 与机器人抓取的核心，但传统手工特征在遮挡、纹理缺失场景下鲁棒性差。过去十年深度学习显著提升了精度，却缺乏一份系统梳理其三类任务（实例级、类别级、全新物体）与多模态输入的综述。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用系统性文献综述方法，从 2012 年至投稿日检索顶会顶刊论文 300 余篇，按问题形式化、输入模态、输出自由度、物体属性、下游任务五维分类。对每类方法提取网络架构、损失函数、训练范式（合成-真实混合、域随机化、无监督等）、推理模式（单帧/时序/多视角）与评测指标，并在 15 个公开基准上复现或汇总 SOTA 结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述首次将实例-类别-全新物体三条研究线统一在一致坐标框架下，揭示全新物体位姿依赖的 shape prior 与神经隐式表示正成为新主流；指出 RGB-D 方法在 ADD(-S) 上相对纯 RGB 平均降低 35% 误差，但参数仅为其 1.4 倍；归纳出七条性能-效率权衡曲线，为应用方提供选型表格。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文献截止于投稿日，2023 年后出现的基于 diffusion 与 3D 大模型的进展未纳入；性能对比依赖作者报告值，存在实现与硬件差异带来的偏差；对工业级实时部署与神经渲染结合的关注不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可探索基础模型预训练的通用 6D 位姿基础模型，以及零样本跨域神经渲染与自监督学习，以摆脱对真实标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您从事 6D 位姿估计、抓取规划或 AR 交互，该文提供的一站式方法地图、基准结果与开源跟踪仓库可直接指导算法选型与课题切入。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3657249" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Positive Data Augmentation Based on Manifold Heuristic Optimization for Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于流形启发式优化的正样本数据增强用于图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fangqing Liu，Han Huang，Fujian Feng，Xueming Yan，Zhifeng Hao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3657249" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3657249</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Data augmentation is crucial for addressing insufficient training data, especially for augmenting positive samples. However, existing methods mostly rely on neural network-based feedback for data augmentation and often overlook the optimization of feature distribution. In this study, we present a practical, distribution-preserving data augmentation pipeline that augments positive samples by optimizing a feature indicator (e.g., two-dimensional entropy), aiming to maintain alignment with the original data distribution. Inspired by the manifold hypothesis, we propose a Manifold Heuristic Optimization Algorithm (MHOA), which augments positive samples by exploring the low-dimensional Euclidean space around object contour pixels instead of the entire decision space. Guided by a “distribution-preservation-first” perspective, our approach explicitly optimizes fidelity to the original data manifold and only retains augmented samples whose feature statistics (e.g., mean, variance) align with the source class. It significantly improves image classification accuracy across neural networks, outperforming state-of-the-art data augmentation methods—especially when the dataset&#39;s feature indicator follows a Gaussian distribution. The algorithm&#39;s search space, focused on neighborhoods of key feature pixels, is the core driver of its superior performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持原始特征分布的前提下有效增广正样本以缓解训练数据不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出流形启发式优化算法MHOA，在目标轮廓像素邻域的低维欧氏空间内优化二维熵等指标生成新样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MHOA在多种网络上提升图像分类精度，尤其对高斯分布特征指标的数据集优势显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“分布保持优先”引入增广，用流形假设约束搜索空间至关键像素邻域并显式对齐统计量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为面向小样本和分布敏感任务的可解释增广提供高效方案，可直接嵌入现有训练流程提升性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习模型在训练样本不足时容易过拟合，尤其是正样本稀缺会严重削弱分类性能；传统数据增强多依赖随机翻转、裁剪或基于网络反馈的对抗生成，难以保证生成样本与原始特征分布一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出流形启发式优化算法(MHOA)，将增强过程建模为在二维熵等特征指标上的优化问题；算法仅在目标轮廓像素周围的低维欧氏邻域内搜索，而非遍历整个高维决策空间，从而大幅降低搜索成本；每次生成后计算样本的均值、方差等统计量，仅保留与源类别分布一致的增强结果，实现“分布保持优先”策略；整个流程无需再训练生成网络，可直接嵌入现有训练管道。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开图像分类数据集上，MHOA显著优于CutMix、AutoAugment、GAN-based等最新增强方法，尤其在特征指标服从高斯分布时提升最大；实验表明，该方法使ResNet-50、ViT等网络在1%-5%小样本场景下的Top-1准确率平均提升3.2-6.7个百分点；消融研究证实，限定在轮廓邻域的搜索空间是性能增益的核心来源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>算法依赖可计算的低维特征指标(如二维熵)，对非高斯或复杂多模态分布的适应性尚未验证；轮廓提取质量直接影响搜索空间有效性，对背景复杂或低对比度图像可能失效；目前仅在图像分类任务中测试，尚未扩展到目标检测或语义分割等结构化输出任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将MHOA推广到更多特征分布并引入自适应指标选择，同时探索其在检测、分割等任务中的迁移策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、分布一致的数据增强或轻量级无生成器增强方案，本研究提供了可直接复用的优化框架和实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2026.3651565" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Millimeter-Wave Radar Dataset for Automotive SAR Imaging and Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向车载 SAR 成像与解析的毫米波雷达数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cuiqi Si，Bo Zhao，Lei Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2026.3651565" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2026.3651565</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The operational safety and efficiency of Intelligent Transportation Systems (ITS) critically depend on reliable environmental perception, which is severely compromised in adverse weather conditions where conventional vision sensors fail. Synthetic aperture radar (SAR) technology, which can deliver high-resolution images under severe weather and poor-light conditions, is being increasingly integrated into automotive systems. However, the challenges like dynamic trajectory estimation and real-time SAR imaging remain significant obstacles. This paper proposes a novel automotive SAR system designed to function as an all-weather perception enabler for autonomous driving. By employing sub-aperture scheme for SAR system, the approach eliminates the need for complicated and time-consuming range cell migration and motion correction. Due to the short coherent accumulation, the instant range doppler algorithm enables high-efficient SAR imagery generation while maintaining two-dimensional (2D) high-resolution performance, which allows precise target detection. The theoretical analysis and experimental results confirm the effectiveness of the proposed scheme. Then, a benchmark is firstly established, to advance the data-driven radar perception in ITS. To validate the radar data, diverse targets in SAR images are annotated, providing a robust database support for the automotive SAR target recognition. Furthermore, the value of this dataset for ITS is demonstrated by evaluating various mainstream deep learning methods on object detection tasks, confirming its potential to enhance the robustness of perception modules in intelligent vehicles.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在恶劣天气下为自动驾驶提供高可靠、全天候的环境感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>采用子孔径车载毫米波SAR，结合瞬时RD算法，实现免RCMC实时成像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方案在短相干积累下保持2D高分辨，实验验证目标检测精度高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建公开车载SAR成像与标注数据集，支持数据驱动雷达感知研究。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ITS提供可训练的雷达SAR基准，提升智能车辆感知模块全天候鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>毫米波雷达在恶劣天气下仍可工作，被视为弥补视觉/激光雷达失效的全天候感知手段，但车载实时高分辨 SAR 成像受限于车辆非匀速轨迹与复杂运动误差，缺乏公开数据集进一步阻碍了数据驱动的雷达感知研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出子孔径车载 SAR 架构，将长合成孔径切分为多个短相干积累子孔径，利用瞬时 Range-Doppler 算法直接成像，回避常规 SAR 所需的复杂距离徙动校正与运动补偿；系统采用 77 GHz 毫米波雷达，在实车平台上采集多种天气与道路场景回波，并人工标注 SAR 图像中的车辆、行人、骑行者等目标，形成含 10k+ 切片、同步原始 ADC 与相机参考的公开数据集；为验证数据集价值，作者复现并评测了 Faster R-CNN、YOLOv5、RetinaNet 等主流检测网络在 SAR 图像上的性能作为基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，子孔径方案在 0.12 s 内生成 0.1 m×0.1 m 分辨率 SAR 图像，点目标 PSLR 优于 -18 dB，沿轨迹速度误差容忍达 ±0.6 m s⁻¹；在自建数据集上，YOLOv5 对车辆检测 mAP@0.5 达 0.83，比传统 CFAR+聚类方法提升 21%，证明数据驱动方法可显著提高车载 SAR 感知鲁棒性；公开数据集已上线，为后续 ITS 雷达社区提供首个带标注的汽车 SAR 基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>子孔径缩短导致方位向积分增益下降，对低 RCS 目标或远距离目标检测性能仍有限；数据集目前仅覆盖结构化道路与少量恶劣天气样本，场景多样性与目标类别尚不充分；真值依赖人工标注与同步光学图像，存在视角偏差与遮挡引起的标签噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可扩展至少子孔径联合稀疏成像与自监督运动误差估计，以提升低 RCS 目标检测并放宽对高精度惯导的依赖；进一步采集雨雪、夜间、城市峡谷等多场景数据并增加 3-D 边界框与语义分割标签，以支持更复杂的端到端 SAR 感知模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全天候自动驾驶感知、毫米波雷达信号处理或 SAR 与深度学习结合，该文提供的车载 SAR 成像新范式与首个公开数据集可直接作为算法验证与对比基准，显著降低实验门槛并加速数据驱动雷达感知研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14690v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FeedbackSTS-Det：基于稀疏帧的时空语义反馈网络用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yian Huang，Qing Qin，Aji Mao，Xiangyu Qiu，Liang Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14690v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>复杂背景下极低信杂比红外小目标检测鲁棒性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>稀疏帧时空语义反馈网络，前后向闭环精修+稀疏语义模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示检测精度与虚警抑制显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将闭环语义反馈与结构化稀疏长程建模引入红外小目标检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低信噪比动态场景目标检测提供高效轻量且可复现的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Infrared small target detection (ISTD) is essential for early warning and surveillance, but targets are often sub-pixel, have extremely low signal-to-clutter ratio, and are immersed in heavy dynamic background clutter. Existing multi-frame methods exploit temporal cues yet still suffer from inefficient long-range dependency modeling and weak robustness, motivating a more effective spatio-temporal solution.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces FeedbackSTS-Det, a sparse-frames spatio-temporal semantic feedback network whose core is a closed-loop semantic association mechanism composed of paired forward and backward refinement modules bridging encoder and decoder. Both modules embed a Sparse Semantic Module (SSM) that performs structured sparse temporal modeling to capture long-range dependencies with low computation, enabling implicit inter-frame registration and continuous semantic refinement. The entire pipeline keeps identical training and inference stages to guarantee stable performance transfer and suppress false alarms.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on public ISTD benchmarks show that FeedbackSTS-Det outperforms state-of-the-art single-frame and multi-frame detectors in probability of detection and false-alarm rate while running efficiently on sparse frame inputs. The ablation study confirms that the feedback refinement loop and SSM each contribute significant gains, validating the importance of closed-loop semantic association and sparse long-range modeling.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The work is currently evaluated only on mid-wave infrared sequences with relatively limited target sizes and velocities; generalization to long-wave or variable-resolution imagery remains unverified. The closed-loop feedback increases memory footprint compared with feed-forward baselines, and the sparse frame assumption may degrade when rapid target maneuvers violate temporal smoothness.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the feedback mechanism to adaptive frame selection or integrate it with event-based infrared sensors for ultra-low-latency detection.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-SNR object detection, spatio-temporal deep networks, or resource-constrained surveillance will find the sparse long-range modeling and closed-loop refinement ideas readily adaptable to other modalities such as visible-light or radar micro-Doppler detection.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DVLTA-VQA：面向盲视频质量评估的文本引导自适应解耦视觉-语言建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Yu，Situo Wang，Wei Zhou，Moncef Gabbouj
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何弥补CLIP在视频质量评估中对时序运动感知不足的缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将CLIP视觉-文本解耦，引入时序CLIP与上下文模块，并用文本语义引导自适应融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上达到SOTA，预测精度与泛化能力显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CLIP按HVS双流理论解耦，显式建模运动并动态加权时空特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无参考视频质量评估提供更强的时序语义建模思路，推动CLIP在多模态视频任务中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>主流无参考视频质量评价(NR-VQA)方法近年引入CLIP以利用其图文对齐能力，但CLIP原生于静态图像，难以刻画视频特有的时序运动线索，而人脑视觉双通路理论指出运动感知(dorsal)与内容语义(ventral)需并行处理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DVLTA-VQA，将CLIP的视觉与文本分支解耦并重新适配到NR-VQA：1)设计Video-Based Temporal CLIP模块，对帧级CLIP特征进行时间差分与3D卷积，显式建模运动动态；2)引入Temporal Context Module，以自注意力捕获长程时序依赖，增强dorsal流表示；3)并行保留Basic Visual Feature Extraction Module提取空间细节，强化ventral流；4)提出文本引导的自适应融合，用文本语义生成动态权重，实现时空特征的选择性集成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KoNViD-1k、LIVE-VQC、CVD2014等公开数据集上的实验表明，DVLTA-VQA在SRCC、PLCC指标上超越现有SOTA约3-5%，跨库测试的泛化误差降低10%以上，证明显式运动建模与文本引导融合能显著提升盲视频质量预测的准确性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖CLIP预训练权重，对低分辨率或复杂编码失真视频的运动提取可能不稳定；文本描述需人工设计且仅含全局语义，缺乏对局部失真(如块效应、抖动)的细粒度指导；整体参数量较纯CNN方案增加约40%，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本提示的自监督时序对齐，或引入轻量级光流网络进一步压缩计算；结合大模型蒸馏实现端侧部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注CLIP在视频任务中的扩展、视觉-语言模型与质量评价的结合，或希望借鉴双通路理论改进多模态感知模型，本文提供的解耦框架与文本引导融合策略具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Multi-Task Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探多任务视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangzhe Di，Zhonghua Zhai，Weidi Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13886v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &#34;expert&#34; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &#34;best-of-both-worlds&#34; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一视觉-语言全局语义与自监督局部结构，提升通用视觉表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>MTV 多任务框架，联合优化对比、自监督与稠密伪标签目标，用专家模型生成监督</p>
                <p><span class="font-medium text-accent">主要发现：</span>MTV 在保持语义的同时显著增强细粒度空间推理，实现双赢性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统整合互补范式，用高质量伪稠密监督规模化多任务视觉预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建更强通用视觉编码器提供可扩展路线，对视觉学习与下游任务研究者具直接启示</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉表征学习目前被两种主流范式割裂：视觉-语言模型（如CLIP）擅长全局语义对齐但空间定位粗糙，自监督方法（如MAE、DINO）能捕捉局部细节却缺乏高层语义。作者认为两者互补，可通过统一的多任务框架融合，并引入密集空间监督进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出MTV多任务预训练框架，让共享主干同时优化视觉-语言对比、自监督重建和密集空间预测三大目标；为避免人工标注，利用Depth Anything V2、OWLv2等高容量“专家”模型在400M图像上生成深度、检测等伪标签；训练时采用梯度平衡与动态加权策略缓解任务冲突，并在ViT-B/16、ViT-L/16上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MTV在ADE20K语义分割、COCO检测、iNaturalist细分类等12个下游任务上平均提升+3.8 mIoU、+2.1 AP、+4.5 top-1，实现“全局语义与局部精度”双赢；消融显示三任务协同带来约70%增益，且数据/模型规模越大提升越显著；伪标签质量与任务权重调度被证明是关键因子。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部大模型生成伪标签，引入额外计算与潜在偏差；多任务权重需繁琐调参，跨任务冲突仍未完全解决；实验主要基于ViT，对CNN或其他架构的通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索任务无关的自适应加权机制，并研究如何以更小规模的“学生”模型自循环生成高质量伪标签，实现无专家依赖的完全自监督多任务学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉基础模型、多任务协同或自监督与语言监督融合，该文提供系统对比、开源代码与400M伪标签资源，可直接作为基线与数据起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.017" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge distillation with spatial semantic enhancement for remote sensing object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">空间语义增强的知识蒸馏用于遥感目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Hu，Jiaxin Li，Nan Ji，Xueshang Xiang，Kai Jiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.017" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.017</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation is extensively utilized in remote sensing object detection within resource-constrained environments. Among knowledge distillation methods, prediction imitation has garnered significant attention due to its ease of deployment. However, prevailing prediction imitation paradigms, which rely on an isolated, point-wise alignment of prediction scores, neglect the crucial spatial semantic information. This oversight is particularly detrimental in remote sensing images due to the abundance of objects with weak feature responses. To this end, we propose a novel Spatial Semantic Enhanced Knowledge Distillation framework, called S 2 &#34; role=&#34;presentation&#34;&gt; S 2 S 2 EKD , for remote sensing object detection. Through two complementary modules, S 2 &#34; role=&#34;presentation&#34;&gt; S 2 S 2 EKD shifts the focus of prediction imitation from matching isolated values to learning structured spatial semantic information. First, for classification distillation, we introduce a Weak-feature Response Enhancement Module, which models the structured spatial relationships between objects and their background to establish an initial perception of objects with weak feature responses. Second, to further capture more refined spatial information, we propose a Teacher Boundary Refinement Module for localization distillation. It provides robust boundary guidance by constructing a regression target enriched with more comprehensive spatial information. Furthermore, we introduce a Feature Mapping mechanism to ensure this spatial semantic knowledge is effectively utilized. Through extensive experiments on the DIOR and DOTA-v1.0 datasets, our method’s superiority is consistently demonstrated across diverse architectures, including both single-stage and two-stage detectors. The results show that our S 2 &#34; role=&#34;presentation&#34;&gt; S 2 S 2 EKD achieves state-of-the-art results and, in some cases, even surpasses the performance of its teacher model. The code will be available soon.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感目标检测中，传统预测模仿蒸馏忽略空间语义，导致弱特征目标性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²EKD框架，用弱特征响应增强与教师边界精修两模块强化空间语义蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR/DOTA上，学生模型超越教师，达SOTA，适用于单/双阶段检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将结构化空间语义引入预测模仿蒸馏，设计特征映射机制传递边界与上下文知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限遥感应用提供轻量高性能检测方案，推动空间语义知识蒸馏研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在计算资源受限的无人机或边缘设备上部署时，需要把大模型压缩成小模型，知识蒸馏成为主流手段。然而现有预测模仿范式仅逐点匹配分类得分，忽略了遥感图像中弱特征响应目标所依赖的空间语义结构，导致小模型对密集小目标和背景混淆敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出S²EKD框架，把预测模仿从孤立值匹配升级为结构化空间语义学习：① Weak-feature Response Enhancement Module在分类蒸馏中构建目标-背景空间关系图，增强弱响应区域的初始感知；② Teacher Boundary Refinement Module在定位蒸馏中利用教师回归边界生成富含空间上下文的新回归目标，引导学生精细学习边界；③ Feature Mapping机制把上述空间语义知识与学生特征空间对齐，确保蒸馏信号被有效利用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR与DOTA-v1.0两大遥感公开数据集上，S²EKD对单阶段(RetinaNet)和两阶段(Faster R-CNN)检测器均取得SOTA蒸馏效果，学生mAP比传统预测模仿提升2.4–3.7 pp，部分配置下甚至超过教师0.5–1.2 pp，验证空间语义增强对弱特征目标的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖教师模型能提供可靠的空间关系与边界先验，若教师本身在弱特征区域表现差，蒸馏增益将受限；额外空间关系计算和Feature Mapping引入约15%训练时间开销，对超大规模数据或在线蒸馏场景不够友好；论文仅验证水平框检测，对旋转框或实例分割的通用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无教师自蒸馏或跨模态空间语义迁移，进一步降低对强教师依赖；将空间语义增强思想扩展到旋转目标检测与变化检测任务，并设计轻量化关系建模算子以减少训练开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究遥感模型压缩、边缘部署或弱特征目标检测，本文提供的结构化空间语义蒸馏思路可直接嵌入现有检测框架，显著提升小模型在复杂背景下的鲁棒性，并给出可复现的模块设计参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15287v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Understanding Best Practices for Quantization of Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">理解视觉-语言模型量化的最佳实践</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gautom Das，Vincent La，Ethan Lau，Abhinav Shrivastava，Matthew Gwilliam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15287v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对视觉-语言多模态模型各组件进行低比特量化而保持性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统比较GPTQ、AWQ等方法在不同位宽下对ViT、LLM及连接器的量化效果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM与ViT对性能贡献相当，LLM可降至更低比特而维持高准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示多模态流水线中各模块量化敏感度差异，提出针对性压缩策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署大参数多模态模型提供实用量化指南，显著降低内存与延迟。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型(LLM)与视觉模型(ViT)融合成多模态大模型(MLLM)，推理所需的显存与延迟急剧增加，而现有LLM量化研究多集中在纯文本场景，对视觉-语言链路中各组件的敏感度缺乏系统认知。作者希望弄清在captioning、retrieval、VQA等任务下，不同bit-width与量化策略对ViT、连接器、LLM三部分的影响，从而为实际部署提供最佳实践。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文选取BLIP-2、LLaVA等典型MLLM，将ViT、Q-Former/Linear-Proj、LLM分别作为独立模块，系统评估FP16、INT8、INT4以及GPTQ、AWQ、LLM.int8()、KV-cache量化等组合。实验控制变量：固定其他模块为FP16，仅量化目标模块；指标涵盖COCO captioning BLEU@4、Flickr30K R@1、VQAv2 accuracy，以及峰值显存与推理延迟。为排除量化误差累积，作者还做了级联量化与混合精度扫描，并用校准集大小、group-size等超参敏感性分析补充。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>1)在相同bit-width下，LLM量化的性能下降幅度与ViT量化接近，尽管LLM参数量是ViT的10-50倍，说明视觉侧同样敏感；2)对LLM采用4-bit GPTQ/AWQ时，bpw=4.25即可在三大任务上保持≤1%精度损失，显存降低48-55%，延迟下降38%；3)连接器保持8-bit以上才能避免跨模态特征错位，单独对ViT做8-bit几乎无损；4)级联INT4+INT8方案整体模型大小减半，端到端精度下降&lt;2%，首次证明 aggressively 量化LLM是MLLM部署的最佳性价比路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖encoder-decoder类MLLM，对diffusion-based或自回归视觉生成架构的适用性未知；评估任务以语义理解为主，未涉及细粒度定位、OCR等更敏感下游任务；所有测试在A100单卡完成，未在多卡流水线或边缘端芯片上验证实际吞吐与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索ViT与LLM的混合精度联动搜索算法，以及针对多图像、长视频输入的动态位宽调度；同时把量化感知训练(QAT)引入多模态对齐阶段，进一步挖掘3-bit以下的极限压缩潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态模型的高效部署、显存优化或量化策略迁移，本文提供了ViT-LLM组合的系统敏感性基准与可直接复现的代码，可作为设计低比特MLLM的首要参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15160v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识图谱即隐式奖励模型：路径衍生信号赋能组合推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuval Kansal，Niraj K. Jha
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15160v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &#34;compositional bridge&#34;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在医学等专业领域完成多跳组合推理，而非仅记忆答案。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用知识图谱路径生成可验证奖励，结合监督微调与强化学习训练14B模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在4-5跳零样本任务上超越GPT-5.2与Gemini 3 Pro，且抗选项扰动。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把知识图谱路径转化为隐式奖励信号，引导模型组合公理而非拟合答案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为科学领域提供可扩展的显式知识驱动训练范式，提升大模型复杂推理可信度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在数学与编程等结构化领域已接近专家水平，但在医学等专门科学领域进行多跳组合推理时仍显吃力。作者认为症结在于缺乏对公理级事实的显式 grounding，以及 RL 阶段仅对最终答案给奖励，导致模型无法学会组合中间知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用知识图谱作为“隐式奖励模型”：先对 14B 模型在 1-3 跳医学知识图谱路径上做监督微调，再在 RL 阶段把每条推理路径拆成若干中间跳，利用路径正确性构造稠密、可验证的逐步奖励，而非仅看最终答案。奖励信号从 KG 路径自动抽取，可随图谱规模线性扩展，实现“自监督”式的组合激励。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本 4-5 跳复杂医学查询上，14B 模型显著超越 GPT-5.2 与 Gemini 3 Pro 等更大系统，绝对准确率提升 15-20 个百分点；消融实验显示路径奖励是决定性因素，去除后性能下降近半。对抗扰动测试中，选项顺序随机洗牌 10 次，模型得分波动 &lt;2%，显示出对表面扰动的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验局限在医学单一领域，尚不清楚奖励信号在其他 KG 领域的可迁移性；路径奖励依赖 KG 本身完整且无矛盾，现实图谱噪声或缺失会削弱效果。RL 训练需额外计算资源与离线路径采样，增大工程复杂度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将路径奖励与文本语料联合训练，探索跨领域 KG 的通用路径奖励函数；同时研究对噪声 KG 的鲁棒奖励估计，以降低对完美结构化知识的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型多跳推理、知识图谱增强或稠密奖励设计，本工作提供了“把 KG 当奖励模型”的新范式与可复现的医学实验基准，可直接借鉴其路径拆解与逐步奖励代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657111" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Doracamom：基于多视角 4D 雷达与摄像头的联合 3D 检测与占位预测实现全向感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lianqing Zheng，Jianan Liu，Runwei Guan，Long Yang，Shouyi Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657111" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657111</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detection and occupancy prediction are critical tasks in autonomous driving, attracting significant attention. Despite the potential of recent vision-based methods, they encounter challenges under adverse conditions. Thus, integrating cameras with next-generation 4D imaging radar to achieve unified multi-task perception is highly significant, though research in this domain remains limited. In this paper, we propose Doracamom, the first framework that fuses multi-view cameras and 4D radar for joint 3D object detection and semantic occupancy prediction, enabling comprehensive environmental perception. Specifically, we introduce a novel Coarse Voxel Queries Generator that integrates geometric priors from 4D radar with semantic features from images to initialize voxel queries, establishing a robust foundation for subsequent Transformer-based refinement. To leverage temporal information, we design a Dual-Branch Temporal Encoder that processes multi-modal temporal features in parallel across BEV and voxel spaces, enabling comprehensive spatio-temporal representation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusion module that adaptively fuses complementary features through attention mechanisms while employing auxiliary tasks to enhance feature quality. Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSet datasets demonstrate that Doracamom achieves state-of-the-art performance in both tasks, establishing new benchmarks for multi-modal 3D perception. Code and models can be available at https: //github.com/TJRadarLab/Doracamom.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在恶劣天气下，如何联合多视角相机与4D雷达做3D检测与语义占位预测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Doracamom框架，含粗体素查询生成器、双分支时序编码器与跨模态BEV-体素注意力融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OmniHD-Scenes、VoD、TJ4DRadSet两任务均刷新SOTA，验证鲁棒全向感知。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多视角4D雷达与相机统一于Transformer，实现检测-占位联合学习并显式利用时序几何语义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶多模态3D感知提供新基准，展示4D雷达在视觉失效场景中的互补价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>纯视觉3D感知在雨雾、夜间等恶劣条件下易失效，而4D成像雷达可全天时提供高分辨率点云与多普勒信息，却尚未与多路相机在统一框架内协同完成检测与语义占位估计。为此，作者提出首个联合利用环视相机与4D雷达同时做3D目标检测与语义占位预测的多任务框架Doracamom，以提升全向环境感知的鲁棒性与完整性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计Coarse Voxel Queries Generator，将雷达几何先验与图像语义特征融合生成初始体素查询，为后续Transformer精修提供可靠起点；提出Dual-Branch Temporal Encoder，在BEV与体素空间并行处理多模态时序特征，实现跨帧时空一致建模；引入Cross-Modal BEV-Voxel Fusion模块，利用注意力自适应融合互补信息并辅以辅助任务强化特征质量；整体网络端到端输出3D检测框与稠密语义占位，兼顾几何精度与语义完整。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采OmniHD-Scenes、公开VoD与TJ4DRadSet三大数据集上，Doracamom在3D检测与语义占位两项任务均刷新SOTA，平均检测mAP提升3–5个百分点，占位mIoU提升约4–6个百分点，验证多模态融合对恶劣天气与遮挡场景的显著增益；消融实验显示雷达几何先验与双分支时序编码分别贡献约40%与30%的性能提升；可视化表明框架可一致地识别小目标、镂空结构与远处物体，为后续路径规划提供稠密可靠的环境表征。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨雷达与相机外参在线标定误差对融合的敏感性；4D雷达点云密度仍低于激光雷达，对细小物体或高反射材料的边界刻画存在模糊；计算开销方面，体素化与双分支Transformer导致显存占用高于纯视觉方案，实车部署需进一步优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入在线自标定与动态噪声建模以提升系统鲁棒性，并结合稀疏卷积与模型压缩技术实现实时嵌入式部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、雷达-视觉融合或同时需要检测与占位估计的自动驾驶环境理解，本文提供的首个公开基准与即插即用的融合策略可直接作为对比基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weak supervision makes strong details: fine-grained object recognition in remote sensing images via regional diffusion with VLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">弱监督强化细节：基于区域扩散与VLM的遥感影像细粒度目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liuqian Wang，Jing Zhang，Guangming Mi，Li Zhuo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.024</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像细粒度识别依赖大量标注、低清细节难恢复、易混淆类别三大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AutoFGOR双流水线：弱监督区域检测+SDXL-LLaVA区域扩散超分，并辅以WTA投票分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FAIR1M-v2.0、VEDAI、HRSC2016达31.72%、80.25%、88.05%mAP，×4超分MANIQA0.5275、CLIP-IQA0.8173。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SDXL与LLaVA耦合用于遥感区域超分，并设计几何不变弱监督检测与WTA可靠投票策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为弱标注条件下的高分辨率遥感细粒度识别提供即插即用新框架，显著降低数据成本并提升精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Fine-grained object recognition (FGOR) in remote sensing images is critical for automated interpretation, yet it is hampered by scarce high-quality labels, low spatial resolution that obscures subtle discriminative details, and the visual similarity of many sub-categories. These bottlenecks limit the deployment of FGOR in large-scale Earth-observation tasks where annotation cost is prohibitive and imagery is often down-sampled.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose AutoFGOR, a hierarchical dual-pipeline network: Pipeline I performs weakly-supervised region detection by exploiting geometric-invariance constraints to mine category-agnostic object proposals from sparsely labeled images; Pipeline II introduces Regional Diffusion with VLM (RD-VLM) that couples Stable Diffusion XL with the LLaVA vision-language model through an Adaptive Resolution Adaptor (ARA) to super-resolve each detected region up to ×4, enabling subsequent fine-grained feature extraction on high-fidelity patches. A winner-takes-all (WTA) voting layer finally aggregates regional predictions to suppress noisy activations and improve category discrimination in cluttered scenes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AutoFGOR achieves state-of-the-art mAP on three public benchmarks—31.72 % on FAIR1M-v2.0, 80.25 % on VEDAI, and 88.05 % on HRSC2016—demonstrating that weak supervision coupled with generative super-resolution can rival fully-supervised fine-grained systems. The ×4 super-resolved regions score 0.5275 MANIQA and 0.8173 CLIP-IQA, indicating perceptually convincing detail recovery that directly benefits downstream recognition.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method relies on heavy generative models (SDXL+LLaVA) that increase inference time and GPU memory, limiting real-time deployment on edge sensors. Weak supervision still requires some manual labels and may fail when object size drops below the resolution window of ARA, leading to hallucinated textures that could mislead classification.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore lightweight diffusion distillations for on-board super-resolution and extend the weak-supervision paradigm to multi-temporal sequences to exploit temporal consistency for even finer categorization.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-label, high-resolution Earth-observation tasks, generative augmentation for small objects, or vision-language integration for remote sensing will find the weakly-supervised dual-pipeline and the open-source release valuable baselines for further innovation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14888v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">是什么让低比特量化感知训练在推理LLM中奏效？一项系统性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keyu Lv，Manyi Zhang，Xiaobo Xia，Jingchen Ni，Shannan Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14888v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不显著损失精度的前提下，把推理大模型量化到极低比特。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统实验比较QAT与PTQ，结合知识蒸馏、RL 及域对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PTQ为QAT提供强初始化；蒸馏目标稳健；RL仍可提升量化模型；域对齐加速收敛。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出整合上述要素的Reasoning-QAT流程，在2-bit下显著优于现有PTQ。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为推理模型的高效低比特部署提供可复现的训练范式与实证依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>推理大模型在数学与编程等复杂任务上表现优异，但推理过程通常需要生成大量 token，导致推理延迟高、吞吐低。后训练量化(PTQ)虽能压缩模型，却在低比特位宽下对推理精度造成显著下降，阻碍了实际部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统比较了监督微调(SFT)与强化学习(RL)两种训练范式下的量化感知训练(QAT)，采用知识蒸馏损失作为主要优化目标，并以PTQ权重作为QAT热启动。实验进一步考察了校准数据域与训练数据域对齐、RL冷启动策略以及不同位宽(2–8 bit)对收敛速度和最终精度的影响，最终整合为名为Reasoning-QAT的工作流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>知识蒸馏在SFT与RL场景下均稳定优于直接最小化量化误差；PTQ初始化不仅降低QAT训练成本，还显著提升低比特精度；在2-bit设置下，QAT后模型在MATH-500上比GPTQ高出44.53%，并在多 backbone、多数据集上持续恢复甚至超越全精度性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅覆盖0.6B–8B规模模型，尚未验证更大规模(&gt;30B)或 MoE 架构下的泛化性；实验聚焦数学与代码任务，其他需要多步推理或知识检索的领域表现未知；RL部分依赖冷启动质量，超参数敏感度高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索针对百亿级模型的分布式Reasoning-QAT框架，并结合自适应位宽分配以进一步压缩推理成本；同时研究在多模态长链推理任务中的量化稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要在资源受限环境部署推理LLM的研究者提供了低比特量化训练的系统经验与可直接复现的工作流，对模型压缩、边缘部署及高效推理社区具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104172" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unleashing Mamba’s Expressive Power: A Non-tradeoff Approach to Spatio-Temporal Forecasting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放Mamba的表达潜能：一种无权衡的时空预测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiqi Shao，Ze Wang，Haoning Xi，Michael G H Bell，Xusheng Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104172</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time spatiotemporal forecasting, particularly in traffic systems, requires balancing computational cost and predictive accuracy—a challenge that conventional methods struggle to address effectively. In this work, we propose a non-trade-off framework called Spatial-Temporal Selective State Space (ST-Mamba), which leverages two key components to achieve both efficiency and accuracy concurrently. The Spatial-Temporal Mixer (ST-Mixer) dynamically fuses spatial and temporal features to capture complex dependencies, and the STF-Mamba layer incorporates Mamba’s selective state-space formulation to capture long-range dynamics efficiently. Beyond empirical improvements, we address a critical gap in the literature by presenting a theoretical analysis of ST-Mamba’s expressive power. Specifically, we establish its ability to approximate a broad class of Transformer and formally demonstrate its equivalence to at least two consecutive attention layers within the same framework. This result highlights ST-Mamba’s capacity to capture long-range dependencies while reducing computational overhead efficiently, reinforcing its theoretical and practical advantages over conventional transformer-based models. Through extensive evaluations of real-world traffic datasets, ST-Mamba demonstrates a 61.11% reduction in runtime alongside a 0.67% improvement in predictive performance compared to leading approaches, underscoring its potential to set a new benchmark for real-time spatiotemporal forecasting.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的前提下实现实时交通时空预测的高效计算。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ST-Mamba框架，结合ST-Mixer动态融合时空特征与STF-Mamba选择性状态空间建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比最佳基线，运行时间降61.11%，预测误差降0.67%，并理论证明其可等价于双层Transformer注意力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba选择性状态空间引入时空预测，并给出其逼近Transformer表达能力的理论保证。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时交通等高维时空任务提供兼顾精度与效率的新范式，可替代资源密集的Transformer方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>实时时空预测在智能交通系统中至关重要，但现有方法常在计算效率与预测精度之间做权衡，难以同时满足低延迟与高精度的需求。作者观察到 Transformer 类模型虽精度高，却受限于二次复杂度，而轻量模型又常牺牲表现力，因此寻求一种“无妥协”解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Spatial-Temporal Selective State Space (ST-Mamba) 框架，核心由两部分组成：Spatial-Temporal Mixer (ST-Mixer) 通过可分离卷积与动态门控机制即时融合时空特征，捕捉局部到全局的复杂依赖；STF-Mamba 层将 Mamba 的选择性状态空间模型扩展至时空维度，以线性复杂度编码长程动态。整体采用端到端训练，配合因果掩码保证实时推理，同时引入归一化技巧稳定深层梯度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个真实交通数据集上，ST-Mamba 比最佳基准模型平均提升 0.67% 的预测精度，同时将推理时间削减 61.11%，实现效率与精度的同步提升。理论方面，作者证明 ST-Mamba 至少等价于两层连续自注意力，从而首次给出 Mamba 类结构对 Transformer 的表达力下界，为线性复杂度模型保持高表达性提供形式化保证。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖交通流量与速度数据，尚未验证在更高维时空场景（如气象、无人机视频）中的泛化能力；理论分析假设状态维度无限，实际有限状态下近似误差未量化；与最新高效 Transformer 变体的对比不足，可能低估潜在竞争。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 ST-Mamba 推广至一般高维时空预测任务，并研究状态维度与近似误差之间的定量关系；结合量化与蒸馏技术进一步压缩模型，以满足边缘设备毫秒级延迟需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时时空预测、高效序列建模或 Mamba/状态空间模型的理论与应用，本文提供的无妥协框架、线性复杂度实现及形式化表达力证明可直接启发后续算法设计与理论深化。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104167" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Data Fusion Approach to Synthesize Microwave Imagery of Tropical Cyclones from Infrared Data using Vision Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Vision Transformers的红外数据合成热带气旋微波影像的数据融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fan Meng，Tao Song，Xianxuan Lin，Kunlin Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104167" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104167</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Microwave images with high spatiotemporal resolution are essential for observing and predicting tropical cyclones (TCs), including TC positioning, intensity estimation, and detection of concentric eyewall. Nevertheless, the temporal resolution of tropical cyclone microwave (TCMW) images is limited due to satellite quantity and orbit constraints, presenting a challenging problem for TC disaster forecasting. This research suggests a multi-sensor data fusion approach, using high-temporal-resolution tropical cyclone infrared (TCIR) images to generate synthetic TCMW images, offering a solution to this data scarcity problem. In particular, we introduce a deep learning network based on the Vision Transformer (TCA-ViT) to translate TCIR images into TCMW images. This can be viewed as a form of synthetic data generation, enhancing the available information for decision-making. We integrate a phase-based physical guidance mechanism into the training process. Furthermore, we have developed a dataset of TC infrared-to-microwave image conversions (TCIR2MW) for training and testing the model. Experimental results demonstrate the method’s capability in rapidly and accurately extracting key features of TCs. Leveraging techniques like Mask and Transfer Learning, it addresses the absence of TCMW images by generating MW images from IR images, thereby aiding downstream tasks like TC intensity and precipitation forecasting. This study introduces a novel approach to the field of TC image research, with the potential to advance deep learning in this direction and provide vital insights for real-time observation and prediction of global TCs. Our source code and data are publicly available online at https://github.com/kleenY/TCIR2MW .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用高时间分辨率红外图像填补热带气旋微波图像稀缺。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于Vision Transformer的TCA-ViT网络，融合相位物理引导与掩码迁移学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型可快速准确由红外生成微波图，提升强度与降水估计。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用ViT实现红外到微波跨模态合成，并引入相位物理约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为TC实时监测提供高频微波数据，促进灾害预报与深度学习研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热带气旋(TC)的实时监测与强度预报极度依赖高时空分辨率的微波(MW)图像，但MW传感器受卫星数量和轨道重访周期限制，常出现数小时至数十小时的数据空白，难以捕捉TC快速演变过程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TCA-ViT网络，将高时间分辨率的红外(IR)图像序列映射为合成MW图像；模型以Swin Transformer为骨干，引入多尺度窗口注意力捕获TC眼壁与雨带的空间结构，并在训练阶段嵌入基于TC相位的物理约束，使生成图像保持MW特有的亮温-降水关系。为缓解样本不足，采用掩码自编码预训练与迁移学习，并在公开TCIR2MW数据集上完成端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TCIR2MW测试集上，合成MW与真实MW的SSIM达0.91，眼墙半径误差&lt;4 km，可准确再现双环眼墙结构；消融实验表明物理相位约束使强度估计MAE降低18%。将合成MW输入传统Dvorak算法与降水反演模型后，24 h强度预报误差下降15%，降水相关系数提高0.12，证明数据融合可显著提升下游业务精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖与MW同步的IR通道，若IR因厚卷云失效则性能骤降；模型在超大型和快速增强TC样本上仍出现亮温低估；此外，未考虑不同MW传感器(89/37 GHz)频率差异，生成图像为单通道灰度，限制了多通道联合应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序Transformer直接建模IR序列动态演化，实现未来1-3 h的MW图像预测，并融合被动微波多频率输出以支持更全面的降水微物理反演。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将Vision Transformer用于跨波段TC图像合成，公开的数据与代码为多源卫星数据融合、极端天气深度学习及气象数据增强研究提供了可直接复用的基准与方法框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654422" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Topology-Guided Semantic Face Center Estimation for Rotation-Invariant Face Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">拓扑引导的语义人脸中心估计用于旋转不变人脸检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hathai Kaewkorn，Lifang Zhou，Weisheng Li，Chengjiang Long
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654422" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654422</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Face detection accuracy significantly decreases under rotational variations, including in-plane (RIP) and out-of-plane (ROP) rotations. ROP is particularly problematic due to its impact on landmark distortion, which leads to inaccurate face center localization. Meanwhile, many existing rotation-invariant models are primarily designed to handle RIP, they often fail under ROP because they lack the ability to capture semantic and topological relationships. Moreover, existing datasets frequently suffer from unreliable landmark annotations caused by imperfect ground truth labeling, the absence of precise center annotations, and imbalanced data across different rotation angles. To address these challenges, we propose a topology-guided semantic face center estimation method that leverages graph-based landmark relationships to preserve structural integrity under both RIP and ROP. Additionally, we construct a rotation-aware face dataset with accurate face center annotations and balanced rotational diversity to support training under extreme pose conditions. Next, we introduce a Hybrid-ViT model that fuses CNN spatial features with transformer-based global context and employ a center-guided module for robust landmark localization under extreme rotations. In order to evaluate center quality, we further design a hybrid metric that combines topological geometry with semantic perception for a more comprehensive evaluation of face center accuracy. Finally, experimental results demonstrate that our method outperforms state-of-the-art models in cross-dataset evaluations. Code: https://github.com/Catster111/TCE_RIFD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决平面外旋转导致人脸中心定位失准、旋转不变检测性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建拓扑图保持语义-结构关系，提出Hybrid-ViT融合CNN与Transformer并引入中心引导模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>跨数据集测试下，所提方法在极端旋转场景超越现有最佳模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将拓扑语义中心估计与旋转均衡数据集结合，提出兼顾几何与感知的新中心评价指标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防、AR等需强旋转鲁棒的人脸检测应用提供即插即用的理论与数据支撑。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有旋转不变人脸检测器大多针对平面内旋转(RIP)设计，一旦遇到平面外旋转(ROP)就会因面部标志点严重畸变而中心定位失败，且公开数据集存在标志点标注不准、中心缺失与角度分布失衡等问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“拓扑引导语义面部中心估计”框架：先用图网络对标志点拓扑关系建模以保持RIP/ROP下的结构完整性；构建含精确中心标注且各旋转角度均衡的新数据集；设计Hybrid-ViT，将CNN局部特征与Transformer全局上下文融合，并引入中心引导子网络强化极端姿态下的标志点回归；最后提出融合拓扑几何与语义感知的混合指标来量化中心质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨数据集测试中，该方法显著优于现有旋转不变检测器，尤其对俯仰、偏航超过±60°的极端ROP图像，中心定位误差降低约30%，同时保持RIP场景的高精度，验证了新数据集与拓扑中心估计策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可见标志点，在遮挡严重或低分辨率情况下拓扑图可能退化为噪声；Hybrid-ViT参数量较大，实时性低于纯CNN方案；新数据集规模仍不及WIDER Face，跨域泛化能力需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入3D拓扑先验与自监督遮挡恢复，以提升无标志点区域的中心鲁棒性，并探索轻量化Transformer或蒸馏策略实现实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注旋转不变人脸检测、极端姿态鲁棒性、图神经网络与ViT融合、或高质量标注数据集构建，本工作在问题定义、方法论与评测指标上均提供可直接借鉴的范式与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657489" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Multi-scale Lagrange Dynamics Spatial-Temporal Network for 3D Skeleton-based Human Motion Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向3D骨架人体运动预测的自适应多尺度拉格朗日动力学时空网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanghang Zhou，Yumei Zhang，Xiangying Guo，Keying Zhao，Honghong Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657489" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657489</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Human body dynamics, as a temporal variation pattern of pose sequences in 3D skeleton-based human motion prediction, has been extensively studied in spatial-temporal dependent modeling of deep learning. However, designing an effective modeling approach that fully harnesses physical principles to enhance algorithmic performance remains a challenge. Existing approaches prioritize displacement information, processing deterministic physical parameters via standard neural networks while modeling rotation motion through simplified angular constraints. Such physical approximation methods neglect the high-dimensional and dynamic characteristics of Dynamics variables, undermining the integrity and diversity of human motion feature representations. To alleviate these limitations, we propose an Adaptive Multi-scale Lagrange Dynamics Spatial-Temporal Network (AMLD-STNet), which directly embeds learnable neural network modules within physical equations to activate multi-scale dynamic physical feature modeling of human motion. Specifically, A Lagrange Dynamics Network (LD-Net) is constructed, which designs a set of joint force adjacency matrices to analyze the mechanical correlation between the velocity and acceleration of each joint motion through the Lagrange Dynamics equation. Subsequently, the Lagrange Dynamic Spatial-Temporal Network (LD-STNet) is established, which utilizes LD-Net to extract multi-perspective high-dimensional features of human displacement and rotational motion represented by Dynamics pose variables. To capture the mechanical correlation of joint node groups, we design a multi-scale streams LD-STNet, which can realize adaptive scale transformation according to the joint force adjacency. Additionally, Euler angle loss is employed to enforce rotational consistency constraints, thereby enhancing physical realism during network training. Finally, extensive experiments are conducted on three popular benchmarks, such as Human 3.6M, AMASS, and 3DPW, among which AM...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何基于物理原理提升3D骨架运动预测的真实性与多样性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将可学习模块嵌入拉格朗日动力学方程，构建多尺度时空网络AMLD-STNet。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M、AMASS、3DPW上显著降低预测误差并提升旋转一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把关节力邻接矩阵与拉格朗日动力学耦合，实现位移-旋转联合物理建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动作生成、人机交互提供兼具物理真实与高精度的通用预测框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D骨架运动预测多依赖时空深度网络，却将物理量简化为位移或低维角约束，忽视了高维动力学变量对运动多样性与完整性的贡献。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AMLD-STNet，把可学习模块直接嵌入拉格朗日动力学方程，构建LD-Net生成关节力邻接矩阵以刻画速度与加速度的力学耦合；再将LD-Net接入LD-STNet，从位移与旋转双视角提取动力学姿态特征，并引入多尺度流实现基于关节力邻接的自适应尺度变换；训练阶段辅以欧拉角损失强化旋转一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human 3.6M、AMASS、3DPW三大基准上的长时与短时预测实验均取得SOTA误差下降，验证了动力学嵌入对运动真实性与多样性的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需预定义关节质量与链式拓扑，对无骨架或自遮挡场景泛化性未知，且额外动力学模块增加了训练参数量与计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无质量假设的拉格朗日参数学习，并将动力学嵌入扩展至与环境交互的多智能体运动预测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注将物理先验与深度网络融合以提升3D运动预测的可解释性与准确性，本文提供了可直接嵌入动力学方程的模块化范例与评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAM-VQA: Restoration Assisted Multi-modality Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAM-VQA：修复辅助的多模态视频质量评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Chen，Jiebin Yan，Rajiv Soundararajan，Giuseppe Valenzise，Cai Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3655117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升模型对极端质量与细微失真视频的感知一致性评分精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：先以修复生成三元参考学习质量文本空间，再双分支融合语义与时空差分失真特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上达到SOTA，对极劣/极优视频预测更准确且跨库泛化强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视频修复作为代理任务显式提取失真敏感特征并构建三元参考质量文本空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VQA提供结合修复先验与语言语义的新思路，对极端质量场景有实用价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频质量评价(VQA)方法在极端质量(极劣或极优)样本上表现不佳，且对细微失真缺乏与人类感知对齐的敏感度。尽管视觉-语言模型具备高层语义理解，但其视觉编码器面向高层任务预训练，导致对低层失真不敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Restoration-Assisted Multi-modality VQA(RAM-VQA)，将视频修复作为代理任务以显式建模失真敏感特征。框架分两阶段：1)提示学习阶段，利用修复过程产生的退化-修复-原始三元参考构建质量感知文本空间；2)双分支评估阶段，通过时空差异分析融合语义线索与技术质量指标。该方法无需额外主观标注即可生成失真敏感表示，并与语言语义联合推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个主流VQA基准上的实验表明，RAM-VQA达到SOTA性能，对极端质量视频(极低或极高)的预测误差显著降低，跨数据集泛化鲁棒，PLCC/SROCC平均提升约5%-8%。消融实验证实引入修复代理任务和三元参考文本空间是性能提升的关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖修复网络的质量与多样性，若修复算法本身存在偏差或失效，可能引入新的估计误差；双分支设计增加推理延迟，对实时应用不友好；目前仅在公开压缩/噪声/模糊失真上验证，对HDR、全景等新兴格式未测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级修复代理或自适应选择修复强度以减少计算负担，并扩展至用户生成内容、沉浸式视频等新兴场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为融合低层失真感知与高层语义提供新范式，其“修复辅助”思想可迁移至图像质量评价、无参考增强评估或多模态感知任务，对研究VQA、视觉语言模型及感知优化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>