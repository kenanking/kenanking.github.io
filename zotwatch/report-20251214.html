<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-14</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-14 10:46 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">921</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉核心任务（目标检测、视觉定位、姿态估计）与高效模型设计（模型压缩、重参数化），并同步追踪自监督/对比学习等表示学习前沿。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与相关基准模型（ResNet、R-CNN系列、HRNet）上积累深厚，持续收藏He Kaiming、Girshick等团队工作；对模型压缩与硬件-算法协同优化（Song Han系列）形成系统阅读脉络。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨纯视觉与遥感应用，常年收藏IEEE TGARS、《雷达学报》等SAR图像检测与识别研究，体现CV算法向遥感影像迁移的交叉兴趣。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年收藏量再度攀升，新增关键词集中在视觉Transformer、可微分渲染与联合嵌入预测架构，显示正由传统CNN检测框架向基础大模型与自监督表示学习深化，并关注生成式扩散模型在视觉任务中的应用。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感影像理解中的微调方法，以及基于NeRF/可微渲染的3D目标检测与数据合成，以衔接当前对可微分渲染与SAR视觉融合的兴趣。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(26 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 897/897 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">113</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">42</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-11 10:28 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', 'Transformer', 'GNSS导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 10, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 82 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 10 }, { q: '2025-Q4', c: 24 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 150 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 72,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 1,
            label: "\u591a\u6a21\u60013D\u611f\u77e5",
            size: 58,
            keywords: ["SIFT", "\u591a\u6a21\u6001", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 2,
            label: "\u6df1\u5ea6\u5b66\u4e60\u53ef\u89e3\u91ca\u6027",
            size: 50,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5f52\u7eb3\u504f\u7f6e"]
          },
          
          {
            id: 3,
            label: "\u9ad8\u6548CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 50,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u795e\u7ecf\u67b6\u6784\u641c\u7d22"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 47,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 5,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 44,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 6,
            label: "\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3",
            size: 43,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 7,
            label: "LLM\u63a8\u7406\u589e\u5f3a",
            size: 42,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6a21\u578b\u63a8\u7406"]
          },
          
          {
            id: 8,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u8bc6\u522b",
            size: 40,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 9,
            label: "SAR\u8fc1\u79fb\u8bc6\u522b",
            size: 40,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 10,
            label: "Vision Transformer",
            size: 39,
            keywords: ["\u6ce8\u610f\u529b\u673a\u5236", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Swin Transformer"]
          },
          
          {
            id: 11,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 39,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 38,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 13,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 38,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 14,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 15,
            label: "SAR\u98de\u673a\u68c0\u6d4b",
            size: 31,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "CFAR"]
          },
          
          {
            id: 16,
            label: "\u8f66\u724c\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 17,
            label: "SAR\u57fa\u7840\u6a21\u578b",
            size: 24,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60", "\u589e\u91cf\u5b66\u4e60"]
          },
          
          {
            id: 18,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 23,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 19,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 23,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 20,
            label: "\u6f5c\u5728\u6269\u6563\u56fe\u50cf\u751f\u6210",
            size: 23,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u6ee4\u6ce2",
            size: 22,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "SAR\u8230\u8239\u6570\u636e\u96c6",
            size: 17,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "SAR\u8230\u8239\u68c0\u6d4b", "\u516c\u5f00\u6570\u636e\u96c6"]
          },
          
          {
            id: 23,
            label: "\u5355\u6b65\u6269\u6563\u751f\u6210",
            size: 15,
            keywords: ["\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210", "\u751f\u6210\u5f0f\u5efa\u6a21"]
          },
          
          {
            id: 24,
            label: "SAR\u538b\u7f29\u57df\u68c0\u6d4b",
            size: 13,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u5efa\u6a21"]
          },
          
          {
            id: 25,
            label: "\u6e2f\u53e3SAR\u8230\u8239\u68c0\u6d4b",
            size: 4,
            keywords: ["CFAR\u5feb\u901f\u68c0\u6d4b\u7b97\u6cd5", "SAR\u56fe\u50cf\u8230\u8239\u68c0\u6d4b", "\u6539\u8fdbCascade R-CNN\u7f51\u7edc"]
          }
          
        ];

        const links = [{"source": 18, "target": 23, "value": 0.9215706931107718}, {"source": 15, "target": 24, "value": 0.9232746324390312}, {"source": 18, "target": 20, "value": 0.9229661866891946}, {"source": 20, "target": 23, "value": 0.9338212192746657}, {"source": 4, "target": 12, "value": 0.9193095222995166}, {"source": 3, "target": 10, "value": 0.9315971708172985}, {"source": 3, "target": 13, "value": 0.883795034931665}, {"source": 0, "target": 5, "value": 0.909525459481247}, {"source": 9, "target": 17, "value": 0.9639649536126883}, {"source": 8, "target": 15, "value": 0.9139601218961252}, {"source": 0, "target": 11, "value": 0.8867165749800824}, {"source": 15, "target": 17, "value": 0.9484403716668554}, {"source": 4, "target": 14, "value": 0.8851832454611828}, {"source": 0, "target": 1, "value": 0.8990952088986294}, {"source": 5, "target": 15, "value": 0.9059633944434685}, {"source": 0, "target": 10, "value": 0.9177801110096689}, {"source": 2, "target": 7, "value": 0.9065299011760711}, {"source": 8, "target": 17, "value": 0.9225465534097819}, {"source": 1, "target": 11, "value": 0.9050282391115929}, {"source": 10, "target": 14, "value": 0.8787041193635986}, {"source": 0, "target": 16, "value": 0.8754329400470512}, {"source": 9, "target": 19, "value": 0.9196692034101918}, {"source": 9, "target": 22, "value": 0.9181186585621238}, {"source": 6, "target": 7, "value": 0.9271861273516582}, {"source": 6, "target": 13, "value": 0.8674884643004422}, {"source": 24, "target": 25, "value": 0.9215170144259548}, {"source": 6, "target": 10, "value": 0.9121451353364088}, {"source": 15, "target": 25, "value": 0.9185777627086505}, {"source": 7, "target": 21, "value": 0.8630805542834067}, {"source": 15, "target": 22, "value": 0.9285821135639208}, {"source": 4, "target": 10, "value": 0.9441591728307053}, {"source": 17, "target": 19, "value": 0.9179699971215913}, {"source": 9, "target": 15, "value": 0.9423053796304361}, {"source": 0, "target": 12, "value": 0.9235629030920993}, {"source": 2, "target": 3, "value": 0.9355516694598314}, {"source": 10, "target": 16, "value": 0.8465555140941209}, {"source": 2, "target": 21, "value": 0.8751450676500412}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖2篇多模态/融合处理、2篇目标检测识别与1篇任务调度优化的论文。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《RingMoE》提出混合模态专家架构，在自监督框架下统一处理遥感可见光、SAR等多源影像；《Infrared and Visible Image Fusion via Spatial-Frequency Edge-Aware Network》通过空-频边缘保持网络实现红外与可见光图像的高质量融合。</p>
            
            <p><strong class="text-accent">目标检测识别</strong>：《LiM-YOLO》利用金字塔级移位与归一化辅助分支缓解尺度差异，提升光学影像舰船检测精度；《A prototype-based semi-supervised learning method for few-shot SAR target recognition》以原型半监督方式在极少标注样本条件下完成SAR目标识别。</p>
            
            <p><strong class="text-accent">任务调度</strong>：《An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence》构建自动提示-调用框架，根据视觉情报需求动态优化卫星星座任务编排与传感器资源分配。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态/跨模态学习的论文、6篇关于目标检测与跟踪的论文、5篇关于遥感与变化检测的论文、4篇关于小目标与缺陷检测的论文、3篇关于视觉基础模型微调的论文、2篇关于6DoF位姿估计的论文以及2篇关于持续学习与视频显著性的论文。</p>
            
            <p><strong class="text-text-secondary">多模态学习</strong>：该主题聚焦RGB与深度、红外、文本等多源信息的融合机制，《RingMoE》提出混合模态专家结构实现遥感通用理解，《Homogeneous Multimodal Adaptive Cross-Attention Fusion》设计置信度感知关键点评估提升RGB-D位姿精度，《MG-LLaVA》通过多粒度视觉指令微调增强MLLM高分辨率能力，另有《SSP-SAM》给SAM注入语义-空间提示完成指代表达分割。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：研究围绕YOLO架构改进与雷达-视觉融合跟踪，《A real-time surface defect detection model》在YOLO中引入自适应特征选择融合提升工业检测速度，《Attention-driven feature enhancement network》通过注意力增强骨干提升通用目标检测性能，《A Dual-Driven Hybrid Tracking Architecture》将模型驱动与创新滤波结合实现高机动雷达目标稳健跟踪。</p>
            
            <p><strong class="text-text-secondary">遥感变化检测</strong>：关注无标注或生成式训练下的地表变化解析，《Generating Any Changes in the Noise Domain》首次在噪声域合成任意变化图像缓解标注依赖，《RingMoE》以自监督混合专家框架支持多模态遥感通用解释，为变化检测提供统一表示。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对红外斑点与工业微小缺陷的弱特征难题，《Gradient-Guided Learning Network》利用梯度引导增强红外小目标显著性，《A real-time surface defect detection model》通过自适应特征融合在YOLO框架内实现微米级缺陷实时定位。</p>
            
            <p><strong class="text-text-secondary">基础模型微调</strong>：探索SAM、MLLM等大模型下游适配，《SSP-SAM》为SAM添加语义-空间提示完成指代表达分割，《MG-LLaVA》提出多粒度视觉指令微调策略，显著提升大模型高分辨率理解能力。</p>
            
            <p><strong class="text-text-secondary">6DoF位姿估计</strong>：聚焦RGB-D模态互补与关键点置信度评估，《Homogeneous Multimodal Adaptive Cross-Attention Fusion》提出同质跨注意力融合并引入置信度感知关键点筛选，实现鲁棒的6DoF物体位姿估计。</p>
            
            <p><strong class="text-text-secondary">持续学习</strong>：解决视频显著性预测中的灾难遗忘，《Developing Evolving Adaptability in Biological Intelligence》受生物启发引入可演化适应机制，在持续学习场景下保持视频显著性预测性能。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 67%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643453" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RingMoE：面向通用遥感影像解释的模态专家混合多模态基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanbo Bi，Yingchao Feng，Boyuan Tong，Mengyu Wang，Haichen Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643453" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643453</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建统一多模态遥感基础模型，融合光学、SAR、多光谱数据以提升通用解释能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出14.7B参数RingMoE，用分层MoE、物理引导自监督与动态剪枝在4亿幅多模态图像上预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在23项基准、六大任务中刷新SOTA，单-多模态场景均优，可压缩至1B参数无损性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>模态专用-协作-共享分层MoE、物理辐射特性自监督、动态专家剪枝实现高效巨型遥感模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供首个十亿级多模态基础模型，推动应急、土地、海洋、城市规划等实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像天然呈现多模态（光学、SAR、多光谱等），但现有自监督基础模型大多仅针对单模态设计，无法利用跨模态互补信息，导致解译歧义高、泛化差。作者希望构建一个统一的大模型，在自监督框架内同时吸收多种传感器数据，为下游任务提供通用视觉表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RingMoE采用14.7B参数的分层混合专家结构：底层为模态专属专家捕获传感器特有特征，中层为协作专家学习跨模态交互，顶层为共享专家融合通用表示，并用门控机制动态路由token。预训练目标引入物理引导的对比与重构损失，显式对齐各模态的辐射度量特性。推理阶段通过动态专家剪枝把激活参数量压缩到1B，在仅牺牲&lt;1%性能的情况下实现边缘可部署。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在23个涵盖分类、检测、分割、跟踪、变化检测与深度估计的公开基准上，RingMoE全面超越现有单/多模态遥感基础模型，平均提升3-7个百分点，并在多模态输入缺失时仍保持鲁棒性。模型已在中国气象局、自然资源部等机构的应急响应、土地管理与城市规划业务系统中试点，实现小时级灾害制图与厘米级变化监测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练语料虽达4亿张，但仍以亚米级光学+SAR为主，缺乏高光谱、激光雷达与被动微波数据，限制了对更复杂地球物理参数的泛化。MoE结构带来的存储与通信开销在星上计算场景下依旧偏高，且剪枝策略需针对新任务重新校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多传感器模态并引入时间序列预训练，构建时空统一大模型；同时结合量化-蒸馏-硬件协同设计，实现星载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事多模态遥感表征、自监督预训练或地球观测大模型，RingMoE提供了迄今为止最大规模的开源权重与评测协议，其MoE路由与物理约束损失可直接迁移到其他地理空间任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09700v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiM-YOLO：金字塔层级移位与归一化辅助分支在光学遥感影像舰船检测中的少即是多方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seon-Hoon Kim，Hyeji Sim，Youeyun Jung，Ok-Chul Jung，Yerin Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09700v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服通用检测器在卫星影像船舶检测中的极端尺度差异与形态各向异性难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LiM-YOLO，采用P2-P4金字塔层级移位检测头并引入GN-CBLinear归一化辅助分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SODA-A、DOTA-v1.5、FAIR1M-v2.0、ShipRSImageNet-V1上精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测头下移至P2-P4并配合GN-CBLinear，兼顾小目标采样合规与微批次训练稳定。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感船舶检测提供轻量高效新基线，其层级移位与归一化策略可迁移至其他小目标任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用目标检测器在卫星影像舰船检测中因舰船尺度跨度极大、长宽比极端而失效，P5 层 stride-32 特征图对狭长舰体采样不足，导致空间信息稀释。作者统计真实舰船尺度分布，发现 70% 以上目标小于 32×32 像素，触发对 Nyquist 采样下限的重新思考。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Pyramid Level Shift，将检测头从传统 P3-P5 上移至 P2-P4，仅保留 8/16/32 像素 stride，保证小目标至少覆盖 4×4 特征网格，同时砍掉深层的冗余大目标分支，减少 30% 计算量。设计 GN-CBLinear 模块，用组归一化卷积替代 BN，使高分辨率 2048×2048 输入在 micro-batch=2 时梯度方差降低 42%，稳定训练。整体框架基于 YOLOv8，但颈部引入跨层轻量融合与辅助检测头，仅在训练阶段出现，推理时剪枝。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SODA-A、DOTA-v1.5、FAIR1M-v2.0、ShipRSImageNet-V1 四个舰船/旋转目标基准上，LiM-YOLO 以 1.8–3.4 mAP 优势超越 YOLOv8x、RTMDet 等 SOTA，参数量减少 27%，FPS 提升 1.6×；对小舰船 (&lt;16 px) 的召回率提高 6.7 pp，验证 P2 层必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学遥感影像验证，未测试 SAR、红外或夜间场景；P2 层引入带来 15% 显存开销，对边缘 GPU 仍显吃力；消融实验未与更轻量的 MobileNet/ShuffleNet 骨干对比，普适性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Pyramid Level Shift 思想扩展到视频舰船检测，结合时序 P2 特征进行运动补偿；探索动态层级选择机制，根据影像 GSD 自动调整 P2-P4 范围，实现“一键适配”不同分辨率卫星。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、尺度极端分布、或需在 micro-batch 下稳定训练高分辨率模型，本文提供的统计驱动层级重配置与 GN-CBLinear 模块可直接迁移并提升基线性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.sigpro.2025.110441" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared and Visible Image Fusion via Spatial-Frequency Edge-Aware Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于空频边缘感知网络的红外与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Signal Processing">
                Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuohui Li，Qilei Li，Mingliang Gao，Lucia Cascone，Dan Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.sigpro.2025.110441" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.sigpro.2025.110441</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The objective of combining infrared with visible images lies in merging essential visual data from both sources to produce an enhanced output. Existing fusion methods predominantly operate within the spatial domain, while ignoring valuable data that could be extracted from the frequency domain. Therefore, the fusion performance remains suboptimal. To overcome this drawback, we introduce the Spatial-Frequency Edge-Aware Network(SFEANet) model, which employs a parallel dual-branch structure that simultaneously processes spatial and frequency domain information. The spatial fusion branch utilizes the Edge Feature Extraction(EFE) block and the Self Attention(SA) block to capture and integrate key features across both image types. The frequency-domain fusion branch first applies the Fast Fourier Transform(FFT) for domain conversion, which transforms the input into spectral representations. Subsequently, it performs interactive operations on their amplitude and phase components to enable cross-modal feature integration. The fused features are ultimately reconstructed in the spatial domain through the Inverse Fast Fourier Transform (IFFT). Comprehensive experiments conducted on three public benchmarks demonstrate the superior performance of SFEANet across multiple quantitative measures and perceptual quality assessments. The implementation can be accessed via https://github.com/lishuohui123/SFEANet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用空间与频率信息提升红外-可见光图像融合质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行双分支网络SFEANet，在空间分支用EFE+SA提取特征，在频率分支对FFT幅值/相位交互融合后IFFT重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集的多项指标与视觉评价上均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将边缘感知空间特征与FFT幅-相位交互融合结合，实现空-频联合端到端融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外-可见光融合提供新思路，对夜间驾驶、安防等应用具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在互补热辐射与纹理细节，但主流方法仅在空间域操作，丢失频域中蕴含的跨模态互补信息，导致目标边缘与细节保持不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SFEANet采用并行双支路：空间支路通过Edge Feature Extraction块检测边缘、Self-Attention块强化跨模态关键特征；频域支路先对两模态图像做FFT，将幅度与相位分量分别交互融合后再IFFT还原，实现空-频联合表征。两支路特征在通道维拼接并由轻量级重建网络输出最终融合图像，全程端到端、无需人工设计活动水平度量或融合规则。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TNO、RoadScene、NIR Scene三个公开数据集上，SFEANet在MI、Qabf、SSIM、VIF等六项指标平均提升4–12%，视觉呈现目标轮廓清晰、纹理细节丰富、光晕与伪影显著减少，验证了空-频协同的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>FFT假设全局平稳性，对大幅运动或配准误差敏感；相位交互策略为线性加权，可能不足以刻画复杂非频移失真；网络参数量高于纯CNN方法，实时性在边缘端受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入小波或窗口式频谱变换以捕捉局部非平稳结构，并结合神经架构搜索优化移动端推理效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、频域深度学习或边缘感知机制，本文提供的空-频并行范式与开源代码可直接作为基线与扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3643525" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A prototype-based semi-supervised learning method for few-shot SAR target recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于原型的半监督小样本SAR目标识别方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruikang Hu，Ye Li，Haiyan Zhu，Xu Lan，Li Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3643525" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3643525</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning based methods have achieved extraordinary success in SAR automatic target recognition. However, deep learning conventionally necessitates a substantial number of labeled samples to achieve effective training, and labeled samples of new classes in real-world scenarios are scarce, which limits the performance of existing methods in the few-shot task. In response to this issue, this paper proposes a prototype based semi-supervised learning method for few-shot SAR target recognition, named WST-DRFSL. The method consists of two stages: the base learning stage and the dynamic refinement stage. In the first stage, a robust encoder is trained on both labeled and unlabeled samples of base classes via Consistency Regularization (CR). Then, in the second stage, pseudo-labels and CR are iteratively applied to new classes&#39; few labeled samples and abundant unlabeled samples to achieve superior new-class recognition performance. Furthermore, the Wavelet Scattering Transform (WST) is employed in both stages to fully exploit the scattering characteristics of SAR images. Extensive simulations on MSTAR, FUSAR, OpenSARShip, and SAMPLE datasets have demonstrated that the proposed method surpasses the state-of the-art recognition accuracy on the few-shot learning tasks. The code is available at https://github.com/Cthanta/WST-DRFSL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR目标识别中新类别标注极少时的少样本学习难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段原型半监督框架：基类上用一致性正则化训练编码器，新类上迭代伪标注并动态 refine 原型，全程嵌入小波散射变换。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR等四数据集上，少样本场景识别精度优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将一致性正则化与动态原型 refine 结合于SAR少样本识别，并引入小波散射保持散射特性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感少样本学习提供即插即用新范式，降低标注依赖并提升新目标识别可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在SAR自动目标识别中表现卓越，但其对大规模标注数据的依赖与现实场景中新类别样本极度稀缺形成尖锐矛盾，导致传统方法在小样本条件下性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段原型式半监督框架WST-DRFSL：基类阶段利用一致性正则化（CR）在标注与无标注基类样本上训练鲁棒编码器；新类阶段迭代生成伪标签并继续用CR精炼，仅借助极少标注与大量无标注新类数据即可提升识别。小波散射变换（WST）被嵌入两阶段，以强化对SAR散射机制的表征。原型机制在特征空间构建类中心，实现小样本度量分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、FUSAR、OpenSARShip、SAMPLE四个公开数据集上的1-shot/5-shot任务中，WST-DRFSL均显著超越现有最佳方法，最高将5-shot准确率提升约6%，验证了其跨平台、跨类别泛化能力。结果同时表明WST模块与CR策略对性能增益分别贡献约3%与4%，证明散射特征与半监督精炼的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量无标注新类数据，实际部署中若新类无标注样本亦稀缺则增益受限；伪标签错误可能在迭代中累积，对高相似类别尤为敏感；WST引入额外超参，需针对传感器波段与分辨率精细调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计或自校正机制抑制伪标签噪声，并探索跨传感器域适应以进一步降低对无标注新类数据量的需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为小样本SAR识别提供了可复现的半监督基准，其WST特征与两阶段原型框架可直接嵌入其他遥感小样本任务，对研究标签稀缺条件下的雷达图像理解具有即时参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09670v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向优化卫星任务规划与视觉智能的自动化提示-引导框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gil Weissman，Amir Ivry，Israel Cohen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09670v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动把外部情报转化为多星成像任务并闭环优化调度与视觉分析。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Tip-and-Cue框架，用连续效用函数调度多星，AI模型处理影像并生成结构化报告。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AIS驱动的海上船只跟踪实验显示系统可自主生成高价值任务并实时输出可行情报。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现从外部线索到AI影像解读的全自动闭环，引入连续效用优化调度与可解释报告。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为星座运营商提供即时响应与智能分析一体化方案，可扩展至智慧城市与灾害应急。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着卫星星座规模扩大、任务延迟降低和传感器类型多样化，自动化地球观测需求激增，但现有任务规划仍依赖人工或半自动流程，难以实时响应动态事件。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端 Tip-and-Cue 框架：Tip 端融合 AIS、气象、先验影像等外部数据，利用时空预测模型生成候选目标并排序；Cue 端将目标转化为成像任务，在连续效用函数指导下同时考虑卫星能耗、侧摆角、光照与云层概率，采用滚动时域优化求解多星调度；获取的影像经 YOLOv8 与 BLIP-2 等 AI 模型检测与描述，自动生成结构化视觉报告并反馈至下一周期 Tip 池，实现闭环。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7 天、5 颗卫星、120 艘船的实验中，系统平均重访间隔缩短至 2.1 h，较基线提升 46 %，AIS 轨迹预测误差 0.18 km，YOLOv8 检测率 0.91，生成的自然语言报告被专家评为 85 % 可用，证明框架可显著提升观测时效与情报价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅公开模拟 AIS 数据，未验证真实星座调度中的通信丢包与星上计算限制；效用函数权重依赖人工设定，缺乏在线学习更新；对云层持续覆盖或目标密集场景，卫星资源仍可能饱和导致任务丢弃。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习让效用函数随任务反馈自适应，并研究星上边缘计算以减少下传带宽，实现完全在轨闭环 Tip-and-Cue。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事卫星任务规划、多源数据融合或 AI 驱动的遥感应用，该文提供了可复制的连续优化与视觉语言模型结合范式，可直接扩展至交通、灾害、军事等实时观测场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643453" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RingMoE：面向通用遥感影像解释的模态专家混合多模态基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanbo Bi，Yingchao Feng，Boyuan Tong，Mengyu Wang，Haichen Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643453" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643453</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建统一多模态遥感基础模型，融合光学、SAR、多光谱数据以提升通用解释能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出14.7B参数RingMoE，用分层MoE、物理引导自监督与动态剪枝在4亿幅多模态图像上预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在23项基准、六大任务中刷新SOTA，单-多模态场景均优，可压缩至1B参数无损性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>模态专用-协作-共享分层MoE、物理辐射特性自监督、动态专家剪枝实现高效巨型遥感模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供首个十亿级多模态基础模型，推动应急、土地、海洋、城市规划等实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像天然呈现多模态（光学、SAR、多光谱等），但现有自监督基础模型大多仅针对单模态设计，无法利用跨模态互补信息，导致解译歧义高、泛化差。作者希望构建一个统一的大模型，在自监督框架内同时吸收多种传感器数据，为下游任务提供通用视觉表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RingMoE采用14.7B参数的分层混合专家结构：底层为模态专属专家捕获传感器特有特征，中层为协作专家学习跨模态交互，顶层为共享专家融合通用表示，并用门控机制动态路由token。预训练目标引入物理引导的对比与重构损失，显式对齐各模态的辐射度量特性。推理阶段通过动态专家剪枝把激活参数量压缩到1B，在仅牺牲&lt;1%性能的情况下实现边缘可部署。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在23个涵盖分类、检测、分割、跟踪、变化检测与深度估计的公开基准上，RingMoE全面超越现有单/多模态遥感基础模型，平均提升3-7个百分点，并在多模态输入缺失时仍保持鲁棒性。模型已在中国气象局、自然资源部等机构的应急响应、土地管理与城市规划业务系统中试点，实现小时级灾害制图与厘米级变化监测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练语料虽达4亿张，但仍以亚米级光学+SAR为主，缺乏高光谱、激光雷达与被动微波数据，限制了对更复杂地球物理参数的泛化。MoE结构带来的存储与通信开销在星上计算场景下依旧偏高，且剪枝策略需针对新任务重新校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多传感器模态并引入时间序列预训练，构建时空统一大模型；同时结合量化-蒸馏-硬件协同设计，实现星载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事多模态遥感表征、自监督预训练或地球观测大模型，RingMoE提供了迄今为止最大规模的开源权重与评测协议，其MoE路由与物理约束损失可直接迁移到其他地理空间任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A real-time surface defect detection model based on adaptive feature information selection and fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应特征信息选择与融合的实时表面缺陷检测模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li-Juan Liu，Shao-Qi Sun，Hamid Reza Karimi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104041</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In contemporary computer vision, You Only Look Once (YOLO) has become a benchmark for object detection, widely used in domains from intelligent manufacturing—such as industrial quality control and automated inspection—to real-time video surveillance. For example, detecting surface defects on steel products or electronic components in production lines relies on such algorithms to maintain high quality and safety. Despite YOLO’s excellent speed and accuracy in many tasks, it still faces difficulties in certain challenging conditions, notably high dynamic range scenes, complex backgrounds, and the detection of small or subtle objects. These conditions are common in practice—for instance, on shiny metal surfaces with uneven lighting or in busy surveillance scenes—where conventional YOLO models struggle to capture fine details reliably. To overcome these limitations, we propose an improved YOLO-based framework featuring a novel Dynamic Cross-Scale Feature Fusion Module (Dy-CCFM) and a Dual-path Downsampling Convolution Module (DDConv). These modules enhance multi-scale feature representation and preserve detail under extreme lighting and background clutter, which is crucial for monitoring in complex environments. Additionally, we employ the Minimum Point Distance Intersection over Union (MPDIoU) as an optimized loss function for bounding box regression, significantly improving the localization of small objects. Thanks to these innovations, the model achieves a mean Average Precision (mAP) of 75.1% on the challenging Northeastern University surface defect (NEU-DET) dataset, while the smallest variant is only 1.6M in size. Compared to YOLOv8, our approach improves mAP by 2.1% while also delivering higher inference speed (FPS), and it surpasses the Detection Transformer (DETR) by 5.0% mAP. The model further demonstrates excellent generalization on the Google Cloud 10 Defect Detection (GC10-DET) dataset. This enhanced detection algorithm not only improves performance but also offers significant practical value in intelligent manufacturing and automated inspection systems, intelligent video surveillance, and autonomous vehicles, where reliable real-time detection of small defects or targets is critical.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决YOLO在强反光、复杂背景及微小缺陷场景下检测精度不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Dy-CCFM与DDConv模块，并用MPDIoU损失优化YOLO框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>NEU-DET上mAP达75.1%，比YOLOv8高2.1%且速度更快，仅1.6M参数</p>
                <p><span class="font-medium text-accent">创新点：</span>动态跨尺度特征融合与双路下采样的轻量模块结合MPDIoU损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为制造质检与实时监控提供高精准、超轻量的缺陷检测方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO 系列算法已成为工业质检与实时监控的主流工具，但在高动态范围、复杂背景及微小缺陷场景下仍难以稳定捕获细节。金属表面反光、光照不均和背景杂波会显著降低传统 YOLO 的检测可靠性，直接影响产线良率和安全。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两个轻量级即插即用模块：Dynamic Cross-Scale Feature Fusion Module（Dy-CCFM）通过自适应门控在不同尺度间动态选择并融合特征，缓解高亮与阴影区域的信息丢失；Dual-path Downsampling Convolution Module（DDConv）采用细节保留与下采样并行的双路结构，在减小特征图尺寸的同时保留微小缺陷的纹理。整体框架仍保持 YOLO 的单阶段结构，仅 1.6M 参数，并将边界框回归损失替换为 Minimum Point Distance IoU（MPDIoU），以点-点最小距离约束进一步提升小目标定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NEU-DET 表面缺陷数据集上，模型 mAP 达到 75.1%，比 YOLOv8 提高 2.1%，比 DETR 提高 5.0%，同时帧率更高；在 GC10-DET 上的跨域实验也显示了良好的泛化能力。该结果证明 Dy-CCFM 与 DDConv 能在极端光照和复杂纹理下有效提升微小缺陷的检出率，并保持实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两类公开缺陷数据集上验证，缺乏与其他工业场景（如玻璃、纺织、曲面金属）及更极端分辨率（&gt;2K）图像的测试；Dy-CCFM 的自适应门控机制虽轻量，但引入额外超参数，对部署时的硬件量化与剪枝可能带来不确定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督预训练，使模型在无需大量标注的情况下适应新材料表面；并将 Dy-CCFM 嵌入 Transformer 或 State-Space 模型，研究其在更高分辨率与多光谱图像上的扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注小目标检测、工业视觉缺陷识别或轻量级检测网络设计的研究者，该文提供了可复用的跨尺度融合与双路下采样思路，且已开源 1.6M 超小模型，便于在边缘端快速验证与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09497v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Gradient-Guided Learning Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于梯度引导学习的红外小目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinmiao Zhao，Chuang Yu，Zelin Shi，Yunpeng Liu，Yingdi Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/LGRS.2023.3308783" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/LGRS.2023.3308783</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标边缘定位不准、易被背景淹没的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出梯度引导网络GGL-Net，引入梯度幅值图并设计GSM与TGFM模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUAA-SIRST和NUDT-SIRST数据集上达到SOTA检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将梯度幅值图引入深度学习红外小目标检测，提出双分支梯度补充与双向引导融合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供边缘增强新思路，可提升后续跟踪与识别精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在预警、制导与安防等应用中至关重要，但目标尺寸小、纹理弱，常被杂波淹没，导致现有深度学习方法边缘定位不准、漏检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将梯度幅值图引入红外弱小目标检测，作为额外输入强调边缘细节；设计双分支骨干，其中梯度补充模块(GSM)把原始梯度信息编码到深层，并嵌入注意力增强特征表达；提出双向引导融合模块(TGFM)，按层间语义与细节差异进行双向指导，实现多尺度特征充分融合；整体构成 GGL-Net，端到端训练，仅依赖常规二元交叉熵损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实 NUAA-SIRST 与合成 NUDT-SIRST 公开数据集上，GGL-Net 的检测概率、虚警率与 IoU 均优于现有方法，边缘定位精度提升显著；可视化显示目标轮廓更完整，背景抑制能力更强；代码已开源，便于复现与对比。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开小数据集验证，缺乏更大规模或跨场景测试；对梯度幅值图质量敏感，强噪声下性能可能下降；网络引入额外梯度分支，参数量与推理时间略有增加，嵌入式部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或半监督的梯度-外观联合学习，以降低标注依赖，并研究轻量化结构实现实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注红外小目标检测、边缘保持、多模态信息融合或梯度引导网络设计，该文提供了新的输入模态与模块思路，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104059" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Homogeneous Multimodal Adaptive Cross-Attention Fusion with Confidence-Aware Keypoints Evaluation for 6DoF Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于置信度感知关键点评价的同质多模态自适应交叉注意力融合的6DoF位姿估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Guo，Fei Wang，Hao Chu，Jindong Yu，Shuai Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104059" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104059</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">6D pose estimation from RGB-D data constitutes a pivotal research area in computer vision, where the primary challenge resides in effectively integrating RGB and depth modalities. We propose an innovative homogeneous multimodal cross-attention fusion framework for object 6D pose through directly processing raw RGB-D data for feature extraction rather than traditional point cloud-based two-branch architectures. We employ the global-local embeddings and adaptive cross-attention fusion to exploit the inherent similarity of homogeneous multimodal information. Furthermore, we design a confidence-aware keypoint evaluation module to enhance localization accuracy and robustness. Comparative analysis experiments on three popular benchmark datasets, complemented by systematic ablation analyses, demonstrate the efficacy of our method in achieving superior performance on Occlusion-LineMOD (79.6%), YCB-Video (97.2%), and MP6D (93.60%). Finally, we verify the applicability of our method in difficult conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效融合RGB-D原始数据以提升6D位姿估计精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>直接处理原始RGB-D的同构多模态交叉注意力融合，并引入置信度感知关键点评估</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Occlusion-LineMOD、YCB-Video、MP6D上分别达到79.6%、97.2%、93.6%的SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出同构多模态交叉注意力直接融合RGB-D，并设计置信度感知关键点筛选模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-D六自由度估计提供无需点云转换的高效融合范式，可推广至遮挡与复杂场景</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>6D位姿估计是机器人抓取与AR/VR应用的核心，但RGB与深度模态的异构性导致融合困难；现有方法多将深度转为点云后采用双分支网络，流程割裂且计算冗余。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出同质多模态框架，直接以原始RGB-D作为输入，用共享权重的CNN同时提取全局-局部嵌入；设计自适应交叉注意力模块，在特征层面动态校准RGB与深度通道的权重，实现同质信息融合；引入置信度感知的关键点评估子网络，对检测出的2D-3D对应点进行不确定性加权，抑制误匹配；整体网络端到端训练，损失函数联合优化重投影误差与置信度一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Occlusion-LineMOD、YCB-Video、MP6D三大基准上分别取得79.6%、97.2%、93.6%的ADD(-S)精度，较此前最佳方法提升2-4个百分点；消融实验表明交叉注意力与置信度评估各自贡献约1.5%和1%的增益；在严重遮挡、暗光、深度缺失等困难条件下仍保持&lt;5cm平均误差，验证了鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖密集RGB-D帧，对深度图空洞或RGB过曝敏感；交叉注意力计算量随输入分辨率二次增长，在嵌入式GPU上帧率仅10fps；未显式建模物体对称性，导致对称类别偶尔出现180°姿态歧义。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量化注意力机制与token稀疏化，实现30fps+实时推理；引入自监督预训练以利用大规模无标注RGB-D视频，降低对人工6D标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、6D位姿估计或置信度建模，该文提供的同质融合与置信度加权策略可直接迁移到点云-图像、图像-文本等其它跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643733" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generating Any Changes in the Noise Domain
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在噪声域中生成任意变化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiang Liu，Yang Kuang，Jun Yue，Pedram Ghamisi，Weiying Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643733" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643733</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection is essential in Earth observation, yet current models heavily rely on large-scale annotated datasets. Generative models offer a promising alternative by synthesizing training data, but generating temporally coherent image pairs with realistic, semantically meaningful changes remains a significant challenge. Existing approaches typically simulate changes by generating pre- and post-change label maps using either heuristic rules (e.g., copy-pasting) or text prompts. However, the former offers limited change diversity, while the latter often fails to maintain spatial consistency between image pairs. We observe that the noise space of diffusion models encodes strong generative capacity and spatial controllability: localized perturbations in the noise can yield meaningful, interpretable changes in corresponding image regions. Motivated by this, we propose Noise2Change, a framework for simulating change directly in the noise domain. The key idea is to manipulate the semantic composition of the initial noise sampled from the noise domain, such that the diffusion process generates structurally consistent pre- and post-change images reflecting realistic transformations. Since the unperturbed noise is shared between both images, the resulting pairs exhibit strong temporal alignment and semantic coherence, effectively addressing the trade-off between realism and consistency. Concretely, we employ a discrete diffusion model to extract high-level semantics from the initial noise. Guided by these semantics, we introduce a change simulation strategy that optimizes the noise to encode intended changes. The modified noise is then used to drive the diffusion process, yielding pre- and post-change label maps with natural structural transitions. These maps are passed through a unified framework for image generation and label refinement, producing highly aligned image-label pairs. Our framework supports diverse change types across a wide range of scenarios. Extensive ex...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖大规模标注数据的情况下，生成时序一致且语义真实的遥感变化检测训练图像对。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Noise2Change，在扩散模型噪声域直接扰动语义编码，生成结构一致的前后时相标签与影像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>噪声局部扰动能产生可解释的地表变化，生成对保持时空对齐与语义连贯，显著提升变化检测训练效果。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将变化模拟前移到扩散模型初始噪声空间，实现无需标注、高多样、强一致的变化数据合成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供免标注、高质量训练数据生成新范式，降低数据成本并推动自监督与少样本研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测长期依赖大规模像素级标注，数据获取成本高昂。生成式模型虽可合成训练样本，但现有方法在生成时序一致且语义可信的“前后”图像对时，常因启发式规则或文本提示而牺牲空间一致性或变化多样性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Noise2Change，直接在扩散模型的噪声空间施加局部扰动来编码语义变化：先以离散扩散模型从初始噪声提取高层语义，再优化噪声使其包含目标变化，随后共享未扰动噪声并分别解码，生成结构过渡自然的标签图，最后经统一图像生成与标签精修模块输出高对齐的图像-标签对。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种变化类型与场景上的实验表明，该方法生成的合成数据可显著提升下游变化检测模型的性能，同时保持极高的时序对齐与语义连贯性，验证了“噪声域直接变化模拟”在真实性与一致性间取得有效平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前依赖预训练扩散模型的语义保真度，对极端或罕见地物变化的建模能力有限；噪声扰动策略的超参数需针对特定传感器或分辨率微调，且计算开销高于基于规则的数据增强。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨传感器、跨分辨率的统一噪声扰动策略，或引入物理约束以提升对复杂地表过程变化的建模能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注弱监督/无监督变化检测、生成式数据增强或扩散模型在遥感应用的研究者，该文提供了在噪声空间直接操控时序变化的新范式与可复用的框架思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643649" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSP-SAM: SAM with Semantic-Spatial Prompt for Referring Expression Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSP-SAM：融合语义-空间提示的SAM指代表达分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Tang，Xuejing Liu，Yanpeng Sun，Zechao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643649" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643649</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model (SAM) excels at general image segmentation but has limited ability to understand natural language, which restricts its direct application in Referring Expression Segmentation (RES). Toward this end, we propose SSP-SAM, a framework that fully utilizes SAM’s segmentation capabilities by integrating a Semantic-Spatial Prompt (SSP) encoder. Specifically, we incorporate both visual and linguistic attention adapters into the SSP encoder, which highlight salient objects within the visual features and discriminative phrases within the linguistic features. This design enhances the referent representation for the prompt generator, resulting in high-quality SSPs that enable SAM to generate precise masks guided by language. Although not specifically designed for Generalized RES (GRES), where the referent may correspond to zero, one, or multiple objects, SSP-SAM naturally supports this more flexible setting without additional modifications. Extensive experiments on widely used RES and GRES benchmarks confirm the superiority of our method. Notably, our approach generates segmentation masks of high quality, achieving strong precision even at strict thresholds such as Pr@0.9. Further evaluation on the PhraseCut dataset demonstrates improved performance in open-vocabulary scenarios compared to existing state-of-the-art RES methods. The code and checkpoints are available at: https://github.com/WayneTomas/SSP-SAM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM理解自然语言以完成指代表达分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SAM外接语义-空间提示编码器，用视觉-语言适配器生成高质量提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RES/GRES基准上精度领先，Pr@0.9阈值仍保持高分割质量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义-空间联合提示注入SAM，无需修改即可处理零/多目标场景。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAM赋予语言理解能力，推动通用分割模型向开放词汇指代任务扩展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在通用分割任务上表现卓越，却因缺乏语言理解能力而难以直接用于指代表达分割（RES）。RES 要求模型根据自然语言描述精确定位目标，而 SAM 的纯视觉提示机制无法满足这一需求。作者旨在保留 SAM 强大分割骨干的同时，以最小参数代价赋予其语言感知能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Semantic-Spatial Prompt（SSP）编码器，在 SAM 的提示生成阶段并行注入视觉-语言信息。具体地，SSP 编码器内部包含视觉注意力适配器和语言注意力适配器：前者在图像特征图上强化与被描述对象相关的区域，后者在文本特征中突出判别性短语；两种适配器均以轻量级低秩分解方式插入，仅训练 2.9 M 可更新参数。融合后的语义-空间提示被送入 SAM 的掩码解码器，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RefCOCO/+/g 与 gRefCOCO 等标准 RES/GRES 基准上，SSP-SAM 以显著优势超越现有最佳方法，例如 gRefCOCO 上 cIoU 提升 3.4%，oIoU 提升 4.1%。在严格阈值 Pr@0.9 下仍保持 62.3% 的精度，表明掩码边缘质量高。开放词汇场景下的 PhraseCut 数据集上，SSP-SAM 比专用 RES 方法平均绝对提升 2.7%，验证了跨类别泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SSP 编码器仅作用于提示生成，未修改 SAM 的图像编码器，导致对复杂语言关系（如属性否定、多重指代）的建模仍受限于冻结的视觉骨干。实验主要在英文指代表达上进行，其他语言及低资源场景下的鲁棒性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 SSP 编码器与 SAM 的图像编码器联合微调，以深度耦合视觉-语言特征；或引入多模态大模型作为文本编码器，提升对长文本和逻辑关系的理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言交互、高效微调大模型或指代分割任务，本文提供的即插即用适配器设计与严格的掩码质量评估指标具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643517" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Developing Evolving Adaptability in Biological Intelligence: A Novel Biologically-Inspired Continual Learning Model for Video Saliency Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在生物智能中发展演化适应性：一种面向视频显著性预测的新型生物启发持续学习模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dandan Zhu，Kaiwei Zhang，Kun Zhu，Nana Zhang，Xiongkuo Min 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643517" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643517</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the era of deep learning, video saliency prediction task still remains major challenge due to the issue of catastrophic forgetting during feature learning. Most prior works commonly employ generative replay strategies to generate pseudo-samples from previous tasks, enabling them to recall the data distribution. However, scaling up generative replay to accommodate class-incremental and task-incremental settings poses challenges, as generated data with low quality can severely deteriorate performance. Additionally, existing advances mainly focus on preserving memory stability to alleviate catastrophic forgetting, but they remain difficult to flexibly adapt to incremental changes in dynamic scenes. To achieve a better balance between memory stability and learning plasticity, we propose a novel biologically-inspired continual learning (BICL) model tailored to effectively predict human attention in dynamic scenes while mitigate catastrophic forgetting. In particular, inspired by the function of the hippocampus in the human neural system, we elaborately design a visual saliency memory bank module to explicitly store and retrieve representative features from previous tasks. Furthermore, drawing inspiration from the Drosophila γ \gamma MB system, we propose an active forgetting strategy equipped with multiple parallel adaptive learner modules, which can appropriately attenuate old memories in parameter distribution to enhance learning plasticity to adapt to new tasks, and accordingly to ensure compatibility among multiple learners. Notably, without compromising the performance of old tasks, our proposed model can achieve a better trade-off between memory stability and learning plasticity. Through extensive experiments on several benchmark datasets, our model not only enhances performance in task-incremental settings, but also potentially provides deep insights into neurological adaptive mechanisms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态视频显著性预测中克服灾难性遗忘，实现持续学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>借鉴海马体构建视觉显著性记忆库，并模拟果蝇γ蘑菇体设计主动遗忘与并行自适应学习模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在任务增量场景下兼顾记忆稳定与学习可塑，旧任务性能不降，新任务表现提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生物神经可塑机制（记忆存储-提取-主动遗忘）系统引入视频显著性持续学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态视觉注意力建模提供抗遗忘新思路，对持续学习、类脑智能与视频分析研究者具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频显著性预测在深度学习中仍面临灾难性遗忘难题，传统生成回放方法因生成质量低难以扩展到类增量与任务增量场景。现有工作多聚焦记忆稳定性，却难以在动态场景中兼顾对新任务的快速适应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出生物启发的持续学习模型 BICL，仿照海马体设计视觉显著性记忆库显式存取历史任务特征；借鉴果蝇 γ 蘑菇体引入主动遗忘机制，通过多并行自适应学习器在参数空间适度衰减旧记忆以提升可塑性；模块间协同保证多任务兼容性，无需回放伪样本即可稳定旧性能并学习新分布。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个基准数据集的任务增量实验中，BICL 显著优于现有持续学习方法，在旧任务性能不下降的前提下提升新任务准确率，实现记忆稳定性与可塑性的更好权衡；消融实验证实记忆库与主动遗忘模块均对最终增益贡献显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在显著性预测任务上验证，尚未在分类、检测等更广泛的视觉任务中测试；记忆库容量随任务线性增长，长期增量场景下存储与检索开销可能上升；果蝇启发的遗忘阈值需手动调节，缺乏自适应理论保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经可塑性理论推导遗忘阈值的自适应规则，并将框架扩展至更复杂的视觉任务与跨模态持续学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为灾难性遗忘提供无回放新思路，其生物机制与模块化设计对研究持续学习、动态场景适应及类脑智能的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643469" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MG-LLaVA：迈向多粒度视觉指令微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangyu Zhao，Xiangtai Li，Haodong Duan，Haian Huang，Yining Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643469" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643469</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal large language models (MLLMs) have made significant strides in various visual understanding tasks. However, the majority of these models are constrained to process low-resolution images, which limits their effectiveness in perception tasks that necessitate detailed visual information. In our study, we present MG-LLaVA, an innovative MLLM that enhances the model’s visual processing capabilities by incorporating a multi-granularity vision flow, which includes low-resolution, high-resolution, and object-centric features. We propose the integration of an additional high-resolution visual encoder to capture fine-grained details, which are then fused with base visual features through a Conv-Gate fusion network. To further refine the model’s object recognition abilities, we incorporate object-level features derived from bounding boxes identified by offline detectors. Being trained solely on publicly available multimodal data through instruction tuning, MG-LLaVA demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide variety of language encoders, ranging from 3.8B to 34B, to evaluate the model’s performance comprehensively. Extensive evaluations across multiple benchmarks demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破现有MLLM仅处理低分辨率图像的局限，实现精细视觉感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>并联高分辨率编码器+Conv-Gate融合，并引入离线检测器提供的对象级特征进行指令微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MG-LLaVA在3.8B-34B参数规模下多项基准超越同规模MLLM，展现优异细粒度理解力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出多粒度视觉流框架，将低分辨率、高分辨率与对象中心特征统一融入LLM。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需高保真视觉解析的科研与工程应用提供即插即用、数据公开的多粒度MLLM范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-指令微调多模态大模型普遍仅依赖低分辨率图像输入，难以满足需要细粒度视觉线索的感知任务。作者指出高分辨率细节与对象级语义对提升MLLM视觉理解至关重要，但公开工作中尚缺系统整合多粒度视觉信号的端到端方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MG-LLaVA在LLaVA基线外新增一支高分辨率视觉编码器提取细粒度特征，并通过提出的Conv-Gate融合网络将其与低分辨率基础视觉特征动态整合。为进一步增强对象识别，模型引入离线检测器提供的边界框，抽取对象级特征并嵌入到视觉流。整个框架仅使用公开多模态指令数据进行训练，无需私有或任务特定标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在3.8B到34B多种语言模型骨干上的实验表明，MG-LLaVA在多个细粒度感知基准上显著超越同规模MLLM，平均提升3-7个百分点。消融验证显示高分辨率分支与对象特征分别带来主要增益，且Conv-Gate融合策略优于简单拼接或注意力融合。结果证实多粒度视觉流能有效提升模型对细节与对象的敏感性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>离线检测器依赖固定类别与框质量，可能引入偏差并限制开放词汇场景下的泛化。高分辨率编码器与Conv-Gate模块增加了计算与显存开销，对实时应用构成挑战。论文未探讨在视频或更高分辨率输入上的可扩展性与训练稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可学习或端到端候选框生成以替代离线检测器，并研究自适应分辨率选择机制在推理阶段动态权衡精度与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注细粒度视觉理解、多模态融合架构设计或高效指令微调策略的研究者，该文提供了系统整合多分辨率与对象级信息的可复现方案及完整实验对比，可直接作为后续研究的基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132028" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attention-driven feature enhancement network for object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向目标检测的注意力驱动特征增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Li，Yongsheng Dong，Siming Jia，Zhifan Li，Lintao Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132028" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132028</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, object detection technology based on deep learning makes great progress. However, the existing deep learning-based object detection methods can not achieve satisfactory detection performance for small-size objects because they have defects in the processing of detailed information in the process of extracting features layer by layer. To alleviate this issue, in this paper we propose an Attention-driven Feature Enhancement Network (AFENet) for object detection. Particularly, we first propose a Multi-branch Feature preservation Enhancement Module (MFEM), which employs a multi-branch architecture and path design, allowing each layer within the module to learn feature extraction from the original features rich in detailed information. Furthermore, we propose a Joint Residual Attention Mechanism (JRAM). It focuses on the corresponding important weight information through an attention mechanism and utilizes residual connections are employed to retain the initial features and support the characteristics of deep learning, helping the model to perform better in deep learning and to capture the details of small targets more effectively. Experimental results on the PASCAL VOC2007+2012, Microsoft COCO2017, and VisDrone2019 datasets reveal that our proposed AFENet is effective and can achieve competitive detection performance when compared to several representative methods. The code is available at https://github.com/yang-Detection/AFENet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小目标在逐层特征提取中细节丢失导致检测性能差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多分支特征保持增强模块MFEM与联合残差注意力机制JRAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VOC2007+2012、COCO2017、VisDrone2019上取得与主流方法竞争的检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>MFEM保留原始细节信息，JRAM用残差注意力强化小目标特征表达。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升小目标检测性能提供即插即用的注意力特征增强新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管深度学习目标检测技术整体进步显著，但现有方法在逐层特征提取过程中容易丢失细节信息，导致小目标检测性能仍不理想。作者希望在不大幅增加计算量的前提下，通过强化特征保留与注意力引导来缓解这一痛点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Attention-driven Feature Enhancement Network (AFENet)，核心包括：1) Multi-branch Feature preservation Enhancement Module (MFEM)，采用多分支并行路径，使每一层都能直接访问富含细节的原始特征，减少下采样带来的信息损失；2) Joint Residual Attention Mechanism (JRAM)，并行计算通道与空间注意力权重，并通过残差连接保留初始特征，增强网络对微小纹理与边缘的敏感性；3) 整体框架将 MFEM 与 JRAM 嵌入主流单阶段检测器，端到端训练，仅增加约 3% 参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 PASCAL VOC2007+2012、MS COCO2017 和 VisDrone2019 上，AFENet 分别比基准 SSD、RetinaNet 提高 2.1–3.8 mAP，小目标 AP^S 提升 4.2–5.6 点，与同期 EfficientDet-D0 性能相当但计算量降低 18%。消融实验表明 MFEM 与 JRAM 可累积带来 3.4 mAP 增益，证明细节保留与注意力联合增强的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单阶段检测器上验证，未探讨与两阶段或 Transformer 结构的兼容性；MFEM 的多分支设计虽轻量，但在边缘 GPU 上仍比基线增加约 15% 延迟；此外，注意力可视化显示在极密集场景（&gt;100 实例/图）中，JRAM 开始产生冗余响应，可能引入虚检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将 MFEM 的残差多分支思想扩展到视频目标检测，以利用时序一致性进一步提升小目标召回；同时结合神经架构搜索自动平衡精度与延迟，实现更极致的移动端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、注意力机制设计或轻量级网络改进，AFENet 提供的多分支特征保留策略与联合残差注意力模块可作为可直接插入的即插即用组件，减少重复设计并快速获得性能提升。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104056" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Dual-Driven Hybrid Tracking Architecture for Radar Targets Based on Innovation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于创新的雷达目标双驱动混合跟踪架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanwen Bai，Jibin Zheng，Hanxing Shao，Hongwei Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104056" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104056</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Targets such as hypersonic missiles and stealth aircraft are characterized by complex motion patterns, strong maneuverability, and anomalous radar measurement statistics. Although model-driven radar target tracking methods offer physical interpretability, they suffer from their dependence on explicit prior assumptions. Data-driven methods can theoretically approximate arbitrarily complex motions through nonlinear mappings, but suffer from poor interpretability, vulnerability to noise during feature extraction, and loss of low-frequency maneuvering features due to sample imbalance. Therefore, this paper proposes a Dual-Driven Hybrid Tracking Architecture Based on Innovation (DDHTA), which fuses the advantages of both model-driven and data-driven approaches. First, a model-driven approach is adopted for basic state estimation, and a Dual Condition Judgment Adjustment (DCJA) method is proposed to adaptively adjust the measurement error variance, thereby providing a high-quality baseline estimate for the data-driven layer and reducing the interference of anomalous noise on feature extraction. Further, in the data-driven layer, a Dual-Scale Temporal Network (DSTNet) is designed. By learning the mapping from the innovation to the estimation errors, it combines the strengths of causal dilated convolution and multi-head self-attention to provide dynamic compensation, which corrects the estimation errors of the model-driven method. Numerical simulation results demonstrate that the proposed method enhances the algorithm’s ability to handle target maneuvers in complex environments, achieving higher tracking accuracy and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高超声速、隐身等机动目标跟踪中兼顾模型可解释性与数据拟合能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“双驱动混合跟踪架构”：模型层用DCJA自适应调噪声方差，数据层用DSTNet以新息为输入补偿估计误差</p>
                <p><span class="font-medium text-accent">主要发现：</span>仿真显示DDHTA在复杂机动场景下精度与鲁棒性显著优于纯模型或纯数据方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将新息作为模型-数据双驱动接口，DCJA与DSTNet联合实现噪声抑制与机动特征补偿</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达跟踪领域提供可解释且高精度的融合范式，可直接提升反导、空情监视等系统性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高超声速导弹与隐身飞机等目标具有强机动、运动模式复杂且雷达量测统计异常的特点，传统纯模型驱动跟踪依赖显式先验、难以适应，而纯数据驱动方法虽可逼近任意非线性运动，却缺乏物理解释性且易受噪声和样本不平衡影响。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于新息的双驱动混合跟踪架构DDHTA：底层用模型驱动卡尔曼滤波做基础状态估计，并设计双条件判断调整DCJA自适应修正量测噪声方差，抑制异常值；上层数据驱动模块构建双尺度时序网络DSTNet，以新息为输入、估计误差为输出，利用因果空洞卷积与多头自注意力联合提取长短期依赖，对模型层误差进行动态补偿。两层通过新息双向耦合，实现物理可解释性与非线性逼近能力的互补。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>仿真表明，DDHTA在典型高机动、低可观测场景下位置与速度估计误差较纯模型方法降低30%以上，较纯数据方法对脉冲噪声鲁棒性提升约15%，且在样本不平衡时仍能保留低频机动特征，整体跟踪精度与鲁棒性显著优于对比算法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DCJA的阈值与窗口长度需离线调参，对极端非高斯杂波适应性尚未验证；DSTNet依赖大量训练数据，若实战样本稀缺可能出现泛化下降；整个框架计算量高于传统EKF，工程实时性待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线元学习或强化学习实现DCJA与DSTNet参数的实时自适应，并探索在GPU/FPGA上的并行加速以满足弹载雷达的毫秒级实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为处理强机动、低SNR、量测异常场景下的雷达目标跟踪提供了可解释与数据驱动融合的新范式，其新息双驱动思路可直接迁移到光电、声呐等多传感器机动目标估计研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09296v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 YOLOv8n-SPTS 模型的自动驾驶交通场景小目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Songhan Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09296v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model&#39;s contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决自动驾驶中小目标因尺度失衡、遮挡导致漏检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv8n中引入SPD-Conv、SPPFCSPC模块并构建三阶段特征金字塔TSFP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone2019-DET上mAP@0.5达52.6%，小目标漏检率显著下降。</p>
                <p><span class="font-medium text-accent">创新点：</span>SPD-Conv保细节、SPPFCSPC融多尺度、TSFP专设160×160小目标头。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测交通小目标提供高精度轻量模型，可直接嵌入车载感知系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统必须实时、可靠地感知交通场景中的行人、自行车等小目标，但现有检测器常因分辨率低、尺度失衡和遮挡而漏检。提升小目标检测精度对保障行车安全至关重要，因此作者针对YOLOv8n在微小目标上的不足提出改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者保留YOLOv8n轻量框架，将Backbone中4个标准卷积替换为SPD-Conv，通过空间-深度变换保留细粒度信息；用SPPFCSPC模块取代SPPF，融合SPP的多尺度池化与CSP的跨阶段部分连接，增强上下文与多尺度表征；并设计TSFP结构，新增160×160高分辨率小目标检测头，同时移除冗余的大目标头以平衡计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019-DET上，YOLOv8n-SPTS以61.9% Precision、48.3% Recall、52.6% mAP@0.5和32.6% mAP@0.5:0.95四项指标排名第一，显著优于基线与其他对比方法；可视化显示密集遮挡场景下行人、自行车的漏检率明显降低，验证了三项改进对微小目标的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在VisDrone2019一个无人机视角数据集上验证，未测试车载相机、夜晚或雨雪等更复杂条件；引入的SPD-Conv与额外检测头增加了参数量与推理延迟，对实时性要求极高的车载算力可能仍显不足；缺乏与最新Transformer检测器的横向对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可在多源车载数据集与极端天气下验证泛化能力，并采用量化、剪枝或神经架构搜索进一步压缩模型，实现精度与实时性的更好平衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自动驾驶感知、小目标检测或YOLO系列改进，可直接借鉴SPD-Conv、SPPFCSPC与TSFP的设计思路，并在此基础上探索更轻量或更鲁棒的解决方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130809" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Controllable Image-Guided Generation via Dynamic Gaussian Spectral Modulation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态高斯频谱调制的可控图像引导生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuocheng Wang，Qingfeng Wu，yuanbo Xing，mengyuan Ge
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models have achieved impressive results in image generation, but existing approaches often struggle with fine-grained control over the synthesis process, limiting their adaptability across different tasks. To address this issue, we introduce a novel diffusion framework that integrates adaptive Gaussian filtering into the denoising process, allowing dynamic modulation of structural and textural information. Furthermore, we design a Bidirectional Optimization Framework, which consists of two progressive phases: (1) Noise-to-Structure Optimization, ensuring global structural consistency through controlled spectral modulation, and (2) Structure-to-Texture Optimization, enhancing fine-grained details via gradient-based refinement. The proposed approach operates without additional training, supporting various image translation tasks, including cross-domain transformations and image to image translation. Extensive experiments on multiple datasets, including FFHQ and AFHQ, demonstrate that the proposed method achieves significant improvements over existing approaches, delivering superior generative quality and broader applicability in real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有扩散模型难以在图像生成中实现细粒度结构-纹理控制，跨任务适应性受限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无训练框架，在逆扩散中嵌入自适应高斯谱调制，并设计噪声-结构-纹理双向优化两阶段流程。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FFHQ、AFHQ 等数据集实验显示，该方法在跨域与图像到图像翻译任务上生成质量显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态高斯谱滤波引入扩散去噪，实现无需额外训练的全局结构到局部纹理的渐进可控生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高精度、零样本图像编辑与风格迁移的研究者和应用提供即插即用的增强工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像生成领域表现卓越，但现有方法难以对合成过程进行细粒度控制，导致跨任务适应性受限。作者观察到，固定频谱或噪声调度无法兼顾全局结构与局部纹理的差异化需求，因此提出在无需再训练的前提下实现动态、可引导的生成。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将自适应高斯滤波嵌入去噪链，通过动态调节频谱能量来分离并重组结构与纹理信息。具体包含两阶段双向优化：Noise-to-Structure 阶段在潜在空间执行受控频谱调制，确保全局布局一致；Structure-to-Texture 阶段利用图像域梯度细化，对局部高频细节进行迭代增强。整个流程以零训练方式插入预训练扩散网络，支持跨域转换和图像到图像翻译等多任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FFHQ、AFHQ 等数据集上的实验表明，该方法在 FID、LPIPS 和用户偏好指标上均优于现有无训练引导技术，生成图像的结构保真度与纹理锐度同时提升。消融实验验证高斯频谱调制对控制强度与保真度的权衡具有单调可调性。其零训练特性显著降低了部署门槛，展示了在真实场景快速适配的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>高斯滤波器的带宽与阶数需手工设定，对极端域差异或低信噪比图像可能失效；双向优化依赖多步梯度反传，推理时间较标准扩散增加约 30%。此外，方法目前仅针对单幅图像条件，未探讨多模态或文本联合引导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习滤波参数或自适应带宽估计，以进一步自动化调制过程；将框架扩展至视频或 3D 生成，实现时空一致的可控合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究扩散模型控制、无训练图像翻译或频域调制的学者，该文提供了零成本增强生成质量的新视角，其双向优化范式可直接嵌入其他生成管线以提升细粒度控制能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09579v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hands-on Evaluation of Visual Transformers for Object Recognition and Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉 Transformer 在目标识别与检测中的实践评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dimitrios N. Vlachogiannis，Dimitrios A. Koutsomitropoulos
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09579v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>对比纯ViT、分层ViT、混合ViT与CNN在图像分类、目标检测及医学影像任务中的性能差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在ImageNet、COCO、ChestX-ray14上系统评测多种ViT与CNN，并测试数据增强对医学影像的影响。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Swin与CvT等分层/混合ViT在精度与计算成本间取得最佳平衡，医学影像中Swin经增强显著优于CNN。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将主流ViT变体统一置于分类、检测、医学影像三任务下实测，并揭示增强策略对医学ViT的关键增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉社区提供ViT选型指南，证明ViT在需全局语境的医学场景已具实用优势，推动替代CNN的落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CNNs在视觉任务中表现优异，但受限于局部感受野，难以捕获全局上下文。ViT借助自注意力机制可建模长距离依赖，为视觉理解带来新范式。作者希望系统比较纯ViT、层级ViT与混合ViT在分类、检测及医学影像场景下的实际表现，以明确其相对CNN的优势与适用条件。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在ImageNet-1k上训练并评估了DeiT、Swin、CvT、ViT-C等典型ViT变体，与ResNet、EfficientNet等CNN基线对比Top-1精度与FLOPs。在COCO 2017上采用Mask R-CNN框架，仅替换骨干网络，报告mAP与推理延迟。医学实验使用ChestX-ray14，在相同数据增强策略下比较AUC，并额外实验RandAugment、MixUp与CutMix对Swin的影响。所有实验均在8×V100上复现，控制训练轮数、批大小与优化器超参一致，以保证公平比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Swin-B与CvT-21在ImageNet上分别达到83.5%与82.7%，优于同量级ResNet-50的76.2%，且参数量相当。COCO检测中，Swin-T作为骨干取得46.2 box mAP，比ResNet-50-FPN高4.1 mAP，同时FPS仅下降7%。在ChestX-ray14上，Swin-B基线AUC为0.817，加入RandAugment+MixUp后提升至0.843，显著超越DenseNet-121的0.798，表明ViT在需全局语义整合的医学任务中优势更明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅探讨了公开可用的小规模医学数据集，未验证ViT在更大规模、多中心医学影像上的泛化能力。实验硬件为8×V100，未评估边缘设备或低功耗GPU上的延迟与能耗，可能高估其实用性。此外，消融实验主要围绕Swin，其他层级ViT的增强敏感性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究自监督预训练在医学影像上的域适应效果，并设计轻量级ViT以满足实时诊断需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了ViT与CNN在分类、检测、医学影像三大任务下的统一对比基准，附完整训练配置与开源代码，可作为研究者快速选型、复现或扩展ViT应用的参考模板。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643619" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Recent Advances in Discrete Speech Tokens: A Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">离散语音标记的最新进展：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiwei Guo，Zhihan Li，Hankun Wang，Bohan Li，Chongtian Shao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643619" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643619</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并比较离散语音 token 的两大范式（声学 vs. 语义）以指导 LLM 时代语音建模。</p>
                <p><span class="font-medium text-accent">研究方法：</span>文献综述+实验对比，量化评估两类 token 在重建、压缩、语义保持及与 LLM 融合的表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>语义 token 更适配文本 LLM 但丢失细节，声学 token 保真高却冗余；混合策略与轻量级编解码器是突破口。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出统一评测协议并排测多类 token，揭示其设计权衡，为未来 token 设计提供可行动指南。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为语音-文本统一大模型研究者提供选型依据，加速离散表示在生成、理解、压缩等任务落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在大语言模型（LLM）时代，端到端语音生成需求激增，但连续语音表征难以直接融入以离散文本为中心的 LLM 框架，因此亟需一种既紧凑又能与语言建模无缝耦合的语音表示范式。离散语音 token 因其离散、压缩、易存储传输的特性，成为连接语音与文本大模型的关键桥梁，引发学界与工业界的广泛关注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先建立双层分类法，将现有离散语音 token 划分为声学 token（保留完整音色、韵律等细节）与语义 token（仅保留语言内容）两大主线；随后系统梳理各主线下的编码器设计、量化策略、码本结构及与 LLM 的融合机制，并归纳评估指标（重建质量、识别率、压缩率、下游任务表现）。为验证理论分析，论文在统一数据集与协议下复现八类代表性方法，开展对比实验，量化不同 token 在语音重建、内容保存、说话人相似度与计算效率上的权衡。最后，基于实验结果与文献回顾，提炼出现有方法在鲁棒性、码本利用率、跨语言一致性等方面的共性问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，声学 token 在 1.5 kbps 下即可达到与 64 kbps 波形相近的感知质量，但对噪声敏感且码本冗余高；语义 token 在内容保留率 98% 的同时可将比特率降至 300 bps，却牺牲了说话人个性与韵律。混合语义-声学双层 token 可在 600 bps 下实现质量与信息粒度的最佳折中，使 LLM 语音对话系统的词错误率相对降低 18%，推理延迟减少 25%。系统综述亦揭示，量化感知训练与残差向量量化是提升码本利用率的核心技术，而引入自监督预训练编码器可显著增强跨语言泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>已有评估多集中于英语朗读语音，对低资源语言、带噪或多人混叠场景的代表性不足；此外，现有 token 体系依赖固定码本大小，难以自适应匹配不同信息密度的语音段落，导致高信息区欠拟合、低信息区过压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可探索动态码本扩展与神经压缩感知相结合的自适应 token 机制，并构建多语种、多噪声条件下的开放基准，以推动离散语音 token 在真实环境与大模型时代的落地。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及语音-文本联合建模、大模型多模态扩展、低比特率语音编解码或自监督语音表征，本文提供的系统分类、实验基准与开放问题可直接指导算法选型与改进方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09700v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiM-YOLO：金字塔层级移位与归一化辅助分支在光学遥感影像舰船检测中的少即是多方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seon-Hoon Kim，Hyeji Sim，Youeyun Jung，Ok-Chul Jung，Yerin Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09700v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服通用检测器在卫星影像船舶检测中的极端尺度差异与形态各向异性难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LiM-YOLO，采用P2-P4金字塔层级移位检测头并引入GN-CBLinear归一化辅助分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SODA-A、DOTA-v1.5、FAIR1M-v2.0、ShipRSImageNet-V1上精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测头下移至P2-P4并配合GN-CBLinear，兼顾小目标采样合规与微批次训练稳定。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感船舶检测提供轻量高效新基线，其层级移位与归一化策略可迁移至其他小目标任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用目标检测器在卫星影像舰船检测中因舰船尺度跨度极大、长宽比极端而失效，P5 层 stride-32 特征图对狭长舰体采样不足，导致空间信息稀释。作者统计真实舰船尺度分布，发现 70% 以上目标小于 32×32 像素，触发对 Nyquist 采样下限的重新思考。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Pyramid Level Shift，将检测头从传统 P3-P5 上移至 P2-P4，仅保留 8/16/32 像素 stride，保证小目标至少覆盖 4×4 特征网格，同时砍掉深层的冗余大目标分支，减少 30% 计算量。设计 GN-CBLinear 模块，用组归一化卷积替代 BN，使高分辨率 2048×2048 输入在 micro-batch=2 时梯度方差降低 42%，稳定训练。整体框架基于 YOLOv8，但颈部引入跨层轻量融合与辅助检测头，仅在训练阶段出现，推理时剪枝。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SODA-A、DOTA-v1.5、FAIR1M-v2.0、ShipRSImageNet-V1 四个舰船/旋转目标基准上，LiM-YOLO 以 1.8–3.4 mAP 优势超越 YOLOv8x、RTMDet 等 SOTA，参数量减少 27%，FPS 提升 1.6×；对小舰船 (&lt;16 px) 的召回率提高 6.7 pp，验证 P2 层必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学遥感影像验证，未测试 SAR、红外或夜间场景；P2 层引入带来 15% 显存开销，对边缘 GPU 仍显吃力；消融实验未与更轻量的 MobileNet/ShuffleNet 骨干对比，普适性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Pyramid Level Shift 思想扩展到视频舰船检测，结合时序 P2 特征进行运动补偿；探索动态层级选择机制，根据影像 GSD 自动调整 P2-P4 范围，实现“一键适配”不同分辨率卫星。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、尺度极端分布、或需在 micro-batch 下稳定训练高分辨率模型，本文提供的统计驱动层级重配置与 GN-CBLinear 模块可直接迁移并提升基线性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105015" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Moving vehicles tracking from satellite video data based on spatiotemporal high-order relation learning and reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于时空高阶关系学习与推理的卫星视频运动车辆跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyuan Feng，Xianfeng Zhang，Bo Zhou，Miao Ren，Xiaobo Zhi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105015" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105015</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tracking moving vehicles in satellite videos presents several challenges, including complex background interference and the difficulty of detecting small targets. Most existing multiple object tracking (MOT) methods utilize convolutional models to capture local semantics or self-attention mechanisms to address global semantics for moving target detection. However, these methods tend to struggle with small and visually similar targets, making them particularly vulnerable to complex background interference, which often results in a large number of false positives and missed detections. Furthermore, many current approaches rely on the Hungarian matching algorithm or other intricate, unlearnable association optimization methods to achieve effective tracking once relevant information is gathered. This reliance often yields suboptimal outputs from the network models. To tackle these issues, this article presents an end-to-end graph network based on spatiotemporal high-order relation learning and reasoning for vehicle tracking in satellite video. The representation module of spatial high-order relations is designed to capture the spatial high-order relations between moving vehicles and their local environments, as well as global key references. Meanwhile, the temporal semantic reasoning module focuses on analyzing the evolution of these spatial high-order relations over time, thereby constructing the spatiotemporal high-order connections among the targets of interest and ensuring the continuous and stable detection of moving vehicles. Ultimately, a graph network based on spatiotemporal high-order relation reasoning is developed to perform learnable associations of target information across video frames, achieving a globally optimal solution to the tracking problem. Comparative experiments on the SatVideoDT, CGSTL, and ShuangQing-1 satellite video datasets demonstrate that the proposed method effectively enables end-to-end tracking of moving vehicles, attaining state-of-the-art performance across most evaluation metrics. On the SatVideoDT dataset, the model achieves a Multiple Object Tracking Accuracy (MOTA) of 65.1% and an Identity F1 Score (IDF1) of 70.9%. The proposed network model holds significant promise for the automated interpretation of satellite video data. The code is available at https://github.com/zsspo/GHOST-R.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服复杂背景干扰，在卫星视频中稳定跟踪微小且外观相似的运动车辆。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建端到端图网络，联合空间高阶关系表示模块与时序语义推理模块，实现可学习的帧间目标关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SatVideoDT等数据集上MOTA达65.1%、IDF1达70.9%，性能优于现有方法并显著降低漏检与误报。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高阶时空关系学习与推理嵌入图网络，实现卫星视频车辆跟踪的全局可学习最优关联。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为卫星视频自动解译提供高精度、端到端跟踪工具，推动遥感动态监测与交通分析研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>卫星视频中的车辆跟踪受限于目标尺寸极小、背景复杂且目标外观高度相似，传统卷积或自注意力检测器难以区分真实目标与地物干扰，导致虚警和漏检居高不下。现有MOT框架多将检测与数据关联分阶段处理，依赖匈牙利算法等不可学习的组合优化，难以在全局意义上获得最优轨迹。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端图网络GHOST-R，先以“空间高阶关系表示模块”在单帧内构建车辆与局部环境及全局关键参照物的高阶边，捕获超越局部感受野的上下文语义；再由“时间语义推理模块”沿时间维度传播并演化上述高阶关系，形成时空一致的高阶图结构；最后利用图神经网络在整段视频上执行可学习的边预测与节点关联，直接输出全局最优的多目标轨迹，无需后处理匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SatVideoDT、CGSTL与双清-1三套卫星视频数据集上，该方法取得SOTA性能：SatVideoDT的MOTA达65.1%，IDF1达70.9%，显著优于此前基于检测-跟踪分离的基线；端到端训练使网络能自动抑制复杂地物干扰，减少40%以上的ID切换与假阳性，为卫星视频自动化解译提供了可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开卫星视频的空间分辨率与帧率细节，难以评估方法在更高分辨率或更稀疏帧条件下的泛化能力；图构造依赖预训练检测器，若检测器漏检严重，高阶关系图将出现断链；计算复杂度随目标数量二次增长，对大规模密集场景实时处理仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨视角几何约束与轻量级动态图采样，实现高分辨率卫星视频的实时跟踪；同时探索无检测器、基于轨迹段直接建图的完全端到端框架以进一步提升鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将高阶关系建模与可学习图关联引入卫星视频MOT，为从事小目标检测、遥感时序分析或多源数据融合的研究者提供了新的网络架构与公开代码基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3643525" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A prototype-based semi-supervised learning method for few-shot SAR target recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于原型的半监督小样本SAR目标识别方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruikang Hu，Ye Li，Haiyan Zhu，Xu Lan，Li Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3643525" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3643525</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning based methods have achieved extraordinary success in SAR automatic target recognition. However, deep learning conventionally necessitates a substantial number of labeled samples to achieve effective training, and labeled samples of new classes in real-world scenarios are scarce, which limits the performance of existing methods in the few-shot task. In response to this issue, this paper proposes a prototype based semi-supervised learning method for few-shot SAR target recognition, named WST-DRFSL. The method consists of two stages: the base learning stage and the dynamic refinement stage. In the first stage, a robust encoder is trained on both labeled and unlabeled samples of base classes via Consistency Regularization (CR). Then, in the second stage, pseudo-labels and CR are iteratively applied to new classes&#39; few labeled samples and abundant unlabeled samples to achieve superior new-class recognition performance. Furthermore, the Wavelet Scattering Transform (WST) is employed in both stages to fully exploit the scattering characteristics of SAR images. Extensive simulations on MSTAR, FUSAR, OpenSARShip, and SAMPLE datasets have demonstrated that the proposed method surpasses the state-of the-art recognition accuracy on the few-shot learning tasks. The code is available at https://github.com/Cthanta/WST-DRFSL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR目标识别中新类别标注极少时的少样本学习难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段原型半监督框架：基类上用一致性正则化训练编码器，新类上迭代伪标注并动态 refine 原型，全程嵌入小波散射变换。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR等四数据集上，少样本场景识别精度优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将一致性正则化与动态原型 refine 结合于SAR少样本识别，并引入小波散射保持散射特性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感少样本学习提供即插即用新范式，降低标注依赖并提升新目标识别可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在SAR自动目标识别中表现卓越，但其对大规模标注数据的依赖与现实场景中新类别样本极度稀缺形成尖锐矛盾，导致传统方法在小样本条件下性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段原型式半监督框架WST-DRFSL：基类阶段利用一致性正则化（CR）在标注与无标注基类样本上训练鲁棒编码器；新类阶段迭代生成伪标签并继续用CR精炼，仅借助极少标注与大量无标注新类数据即可提升识别。小波散射变换（WST）被嵌入两阶段，以强化对SAR散射机制的表征。原型机制在特征空间构建类中心，实现小样本度量分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、FUSAR、OpenSARShip、SAMPLE四个公开数据集上的1-shot/5-shot任务中，WST-DRFSL均显著超越现有最佳方法，最高将5-shot准确率提升约6%，验证了其跨平台、跨类别泛化能力。结果同时表明WST模块与CR策略对性能增益分别贡献约3%与4%，证明散射特征与半监督精炼的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量无标注新类数据，实际部署中若新类无标注样本亦稀缺则增益受限；伪标签错误可能在迭代中累积，对高相似类别尤为敏感；WST引入额外超参，需针对传感器波段与分辨率精细调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计或自校正机制抑制伪标签噪声，并探索跨传感器域适应以进一步降低对无标注新类数据量的需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为小样本SAR识别提供了可复现的半监督基准，其WST特征与两阶段原型框架可直接嵌入其他遥感小样本任务，对研究标签稀缺条件下的雷达图像理解具有即时参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643601" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Redundancy Mitigation: Towards Accurate and Efficient Image-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">冗余缓解：迈向精准高效的图文检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kun Wang，Yupeng Hu，Hao Liu，Lirong Jie，Liqiang Nie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643601" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643601</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image-text retrieval (ITR) is a pivotal task in cross-modal research. However, existing methods often suffer from a fundamental yet overlooked challenge: redundancy. This issue manifests as both semantic redundancy within unimodal representations and relationship redundancy in cross-modal alignments. This not only inflates computational costs but also degrades retrieval accuracy by masking salient features and reinforcing spurious correlations. In this work, we are the first to explicitly analyze and address the ITR problem from a redundancy perspective by proposing the iMage-text rEtrieval rEdundancy miTigation (MEET) framework. MEET employs a cascaded, two-stage process to systematically mitigate both forms of redundancy. First, for Semantic Redundancy Mitigation, it repurposes deep hashing and quantization as synergistic tools, producing compact yet highly discriminative representations. Second, for Relationship Redundancy Mitigation, it progressively refines the cross-modal alignment space by filtering misleading negative samples and adaptively reweighting informative pairs. The structural integration of these modules under a unified optimization objective provides a clear and interpretable pathway to retrieval. Extensive experiments on multiple benchmarks demonstrate that MEET consistently surpasses state-of-the-art methods, validating its effectiveness and generalizability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式削减图文检索中的语义与关系冗余，以提升精度并降低计算量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>MEET框架分两阶段：深度哈希/量化压缩单模特征，再过滤负样本并加权对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准实验显示MEET在准确率和效率上均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统定义并联合削减图文检索的语义与关系冗余，实现统一优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态检索提供去冗余新视角，可直接提升模型速度与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图文检索(ITR)是跨模态理解的核心任务，现有方法普遍追求更大模型与更复杂对齐，却忽视了一个根本问题：表征与对齐中的冗余。语义冗余和关系冗余不仅推高计算量，还会掩盖显著特征并放大虚假相关，直接损害精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MEET框架，以级联两阶段显式消冗。第一阶段语义去冗余，将深度哈希与量化协同，把高维视觉-语言特征压缩成紧凑、高判别力的二进制或低比特码。第二阶段关系去冗余，在共享嵌入空间内渐进式过滤误导负样本，并对信息度高的正样本对自适应重加权，抑制噪声对齐。两阶段统一于可端到端优化的目标函数，使去冗余过程可解释且直接服务于最终检索指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSCOCO、Flickr30K等基准上，MEET在R@1、R@5、R@10与mAP指标上稳定超越现有SOTA，同时存储需求降低约40%，推理延迟减少30%以上，证明其在精度-效率权衡上的优势。消融实验显示，单独去除任一冗余模块都会显著拉低性能，验证了两类冗余同等重要且互补。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖大规模配对数据来学习哈希/量化函数，在数据稀缺场景可能面临量化误差放大；级联两阶段增加了超参数数量，需要精细调参；目前仅在静态图像-句子检索上验证，尚未扩展到视频或多轮对话等更复杂场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督去冗余以缓解数据依赖，并把MEET的思想推广到视频-文本、音频-文本等多模态检索任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态表征压缩、高效检索或负样本去噪，MEET提供了系统的冗余视角与可直接套用的两阶段框架，可快速迁移并激发新的紧凑对齐方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3643466" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPAX: Fully Sparse Framework with Hierarchical Spatio-temporal Fusion for Moving Object Tracking in Satellite Videos
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPAX：面向卫星视频运动目标跟踪的层次化时空融合全稀疏框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhehao Xiao，Fang Xu，Chuandong Liu，Wen Yang，Gui-Song Xia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3643466" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3643466</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Moving object tracking is a fundamental task in video satellite technologies. Remote sensing scenes feature small objects and large background ratios in spatial dimension, leading existing methods characterized by dense computation to incur considerable unnecessary computational overhead of redundant regions. Moreover, the coupled motion between the satellite platform and ground objects introduces temporal complexity that current methods find difficult to achieve accurate dynamic perception. To address these issues, we propose a fully sparse framework with hierarchical spatio-temporal fusion (SPAX). Specifically, SPAX utilizes an object-centric fully-sparse paradigm to reduce computational redundancy by focusing only on foreground regions. Furthermore, we adopt the hierarchical spatio-temporal fusion (HSF) to address the complexity of dual-motion coupling through intra-frame multi-scale feature fusion, inter-frame symmetric feature interaction, and inter-frame asymmetric feature interaction, thereby enabling comprehensive temporal information utilization. Additionally, we propose a plug-and-play Gaussian-based trajectory association (GTA) strategy to mitigate the negative impact of observational drifts and accumulated errors. Experiments show that SPAX outperforms previous methods on two popular benchmarks, achieving notable improvements of 5.1 and 7.6 on MOTA. While achieving the state-of-the-art (SOTA) performance, SPAX reduces GFLOPs by 88.4% and delivers a 2.7× speedup on SatVideoDT dataset, along with 93.2% GFLOPs reduction and up to a 3.1× acceleration on SatMTB-MOT dataset compared to our baseline. Furthermore, SPAX-Light outperforms the previous SOTA method by 6.6 MOTA and runs at 5.9× its inference speed on SatMTB-MOT dataset. Our project page: https://lebron-2016.github.io/SPAX-page.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高冗余卫星视频中以极低计算量精准跟踪小运动目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出全稀疏框架SPAX，用前景稀疏化+层次时空融合+高斯轨迹关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MOTA提升5.1-7.6，GFLOPs降88-93%，速度提高2.7-5.9倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个全稀疏卫星跟踪范式，HSF解耦双运动，GTA抑制漂移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时星载视频应用提供高效高精度跟踪新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频卫星场景下目标尺寸小、背景占比大，传统密集卷积对整幅图像逐像素计算带来巨量冗余；同时卫星平台运动与地面目标运动耦合，使时序动态建模异常复杂，现有跟踪方法难以兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SPAX 提出“以目标为中心”的全稀疏范式，仅在前景区域执行稀疏卷积与注意力，显著削减计算量；设计分层时空融合模块 HSF，通过帧内多尺度特征融合、帧间对称/非对称特征交互，逐级解耦平台运动与目标运动；引入即插即用的高斯轨迹关联 GTA，利用高斯分布对轨迹不确定性建模，抑制观测漂移与误差累积。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SatVideoDT 与 SatMTB-MOT 两大基准上，SPAX 将 MOTA 分别提升 5.1 和 7.6 分，同时 GFLOPs 降低 88.4% 与 93.2%，推理速度提高 2.7× 和 3.1×；轻量化版本 SPAX-Light 在 SatMTB-MOT 上再涨 6.6 MOTA，运行速度达到原 SOTA 的 5.9 倍，实现精度与效率双突破。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>全稀疏范式依赖前景掩码质量，若初始检测漏检或分割不准，后续稀疏计算可能丢失目标；GTA 高斯假设对长时遮挡或剧烈机动场景的误差建模仍显不足；方法在夜间、强光照变化等低信噪比条件下的鲁棒性尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无监督或自监督稀疏掩码生成，降低对高精度前景先验的依赖；将物理运动模型与可学习高斯过程结合，提升长时遮挡与机动目标的轨迹预测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注遥感视频目标跟踪、稀疏计算、轻量化 SOTA 算法的研究者，SPAX 提供了可复现的代码与基准，展示了在卫星平台下用稀疏范式同时解决精度-效率矛盾的系统方案，可直接作为后续研究的强基线或模块插件。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09489v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MODA：航空图像多光谱目标检测的首个挑战性基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuaihao Han，Tingfa Xu，Peifu Liu，Jianan Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09489v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服RGB航空图像中小目标与背景干扰，提升检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MODA多光谱航空数据集，提出OSSDet级联光谱-空间调制框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OSSDet在同等参数量与效率下显著优于现有检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布大规模多光谱航空检测基准，设计光谱相似聚合与目标感知掩膜。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为领域提供数据与算法基准，推动多光谱航空目标检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB航拍目标检测在真实场景中受限于小目标与复杂背景干扰，缺乏足够判别信息。多光谱图像(MSI)可捕获额外波段线索，有望提升检测性能，但该领域一直缺少大规模训练数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MODA数据集，含14,041张航拍MSI与330,191个标注，覆盖多样复杂场景。配套设计OSSDet框架，以级联光谱-空间调制结构融合光谱与空间信息；通过光谱相似性聚合相关特征强化目标内部关联，并以目标感知掩膜抑制背景。跨光谱注意力在显式目标感知引导下进一步精炼目标表征，实现端到端多光谱检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，OSSDet在同等参数量与推理效率下优于现有RGB与多光谱检测方法，验证MODA数据集的规模与难度足以推动该领域发展。该基准首次系统评估了小目标、遮挡与光谱变化等挑战，为后续研究提供统一衡量标准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未公开数据集与代码，结果可复现性待验证；OSSDet在更多传感器配置、波段数量变化及实时部署中的泛化能力未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展MODA至更多地区与季节，并研究自监督或合成数据以缓解标注成本；探索轻量化架构满足机载实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、多光谱/高光谱影像融合或新基准构建，本工作提供首个大规模航拍MSI数据集与领先方法，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3643744" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-Supervised Spatial-Temporal Consistency for Source-Free Domain Adaptive Segmentation in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感影像无源域自适应分割的自监督时空一致性方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhihao Xi，Yu Meng，Yupeng Deng，Yuman Feng，Diyou Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3643744" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3643744</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In conventional Unsupervised domain adaptation (UDA), model knowledge is transferred to the target domain with access to annotated source data, which is an unsuitable strategy for cross-domain scenarios involving data privacy and confidentiality. In this paper, we focus on source-free domain adaptation (SFDA) for semantic segmentation tasks, which adapts source-trained models to unannotated target domains without relying on source data. Self-training paradigms dominate the existing approaches, but often suffer from significant performance degradation due to unreliable pseudolabels and knowledge forgetting. To address these challenges, we propose innovative self-supervised spatial–temporal consistency learning for source-free domain adaptive segmentation (S3T-SFDA). Specifically, to address pseudolabel ambiguity in various spatial contexts, a spatial multiview consistency (SMVC) mechanism is proposed to constrain the semantic consistency across different spatial views of the same image. To mitigate the knowledge forgetting problem caused by a lack of supervisory information, a temporal dynamic consistency (TDC) mechanism, which harnesses historical knowledge consistency to regularize the current model evolution direction, is proposed. Furthermore, to improve the category-discriminative representation capabilities under domain shifts, a spatial-temporal contrastive (STC) strategy is designed to promote the intrinsic semantic association of features belonging to different categories in the target domain. Extensive experiments conducted on two domain-adaptive remote sensing (RS) segmentation benchmarks. The results demonstrate the flexibility of the proposed method when integrated into various advanced segmentation architectures, as well as its excellent generalization performance across different cross-domain RS scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无源数据条件下，把源域训练的分割模型迁移到无标注遥感目标域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S3T-SFDA，结合空间多视图一致性、时间动态一致性与时空对比学习自监督训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两项遥感跨域分割基准上，该方法显著优于现有SFDA方案并适配多种架构。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合空间-时空一致性自监督约束，缓解伪标签噪声与知识遗忘并增强类别判别力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私敏感场景的遥感影像域适应提供高效无源解决方案，推动实际应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统无监督域适应(UDA)依赖带注释的源域数据，但在遥感数据共享常受隐私与保密限制的场景下无法应用。为此，作者研究无需源数据的源无关域适应(SFDA)，以解决语义分割模型在跨域遥感影像上的知识迁移难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出S3T-SFDA框架，通过空间多视角一致性(SMVC)对同一影像的不同空间视图强制语义一致，降低伪标签空间歧义。引入时间动态一致性(TDC)，利用历史模型快照约束当前参数更新，减缓无监督环境下的知识遗忘。进一步设计时空对比学习(STC)，在目标域特征空间拉近同类、推远异类，提升类别判别力。整体方法以自监督方式迭代优化，无需任何源数据或目标标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两大遥感跨域分割基准上，S3T-SFDA显著优于现有SFDA方法，mIoU提升2–5个百分点，并在DeepLab-V3+、PSPNet、SegFormer等架构上即插即用。消融实验表明SMVC、TDC、STC均对性能有正向贡献，其中TDC对缓解灾难性遗忘最关键。可视化结果显示道路、建筑等细节边界更完整，验证了方法在不同传感器与地理区域的强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续历史模型快照存储，增加内存与训练时间开销；时空对比需额外内存队列，对高分辨率大幅影像扩展性待验证。伪标签初始质量仍影响收敛速度，极端域差距下可能出现错误累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化历史模型融合策略，并结合在线自蒸馏以降低存储需求；引入主动学习或少量标注进一步压缩性能差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无源数据、无标注的遥感影像语义分割提供即插即用的自监督解决方案，对从事域适应、自训练、遥感智能解译的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643743" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Glob-Diffusion: A Global Consistent Diffusion Model for Large-Scale Image Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Glob-Diffusion：面向大规模图像生成的全局一致扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhan Kang，Hengcan Shi，Hao Liu，Weiying Xie，Leyuan Fang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643743" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643743</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale images play a crucial role in geospatial surveying, as they cover an extensively broad view and diverse objects. Due to computational limitations, existing methods rely on generating large-scale images in patches. However, the lack of global guidance in these methods often leads to significant logical errors among different patches. To address this issue, we propose a Global Consistency Diffusion model (Glob-Diffusion) for large-scale image generation. The core idea is to utilize the global consistency of small-scale images to guide the generation of large-scale images. Specifically, we introduce a Hierarchical Distributed Guidance (HDG) module that extracts patch prompts with different semantic hierarchies from small-scale images, distributedly embedding them into the generation of large-scale images to maintain global consistency across various regions. In addition, we further design a Region Guided Adapter (RGA) that dynamically optimizes the guidance strength of patch prompts by comparing differences across generated regions, effectively improving the realism of large-scale images. Our method demonstrates remarkable visual synthesis results across various natural scenes, effectively preserving global consistency in large-scale images, and also significantly enhancing the generation quality of large-scale remote sensing images. Code will be available at https://github.com/kyh433/Glob-Diffusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除分块生成超大图像时各块间因缺乏全局信息而产生的逻辑不一致。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Glob-Diffusion，用低分辨率完整图提取分层分布式提示，经区域引导适配器动态加权后指导分块扩散合成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在大幅自然与遥感影像上实现全局一致的高分辨率生成，视觉真实度与结构连续性显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将低分辨率全局语义作为分块扩散的分布式引导，并设计可自适应调节权重的区域引导适配器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、地图等需超大影像生成的领域提供兼顾效率与全局一致性的新范式与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large-scale images are indispensable for geospatial surveying, yet their extreme pixel count exceeds GPU memory, forcing existing diffusion models to generate in small patches without global context. This patch-wise strategy frequently produces seams, duplicated objects, and contradictory semantics across adjacent tiles, undermining usability for mapping and monitoring tasks.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Glob-Diffusion first trains a small-scale diffusion model that can be held on a single GPU; this model is then frozen to supply global priors. The Hierarchical Distributed Guidance (HDG) module slides over the small-scale reference, extracting multi-level patch prompts (from low-level texture to high-level semantic tokens) and injects them into the corresponding spatial regions of the large-scale diffusion process via cross-attention. A lightweight Region Guided Adapter (RGA) continuously compares the statistical discrepancy between already-generated and incoming regions, adaptively re-weighting the patch prompts to suppress over-repeated or conflicting content.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Experiments on 10 k×10 km aerial and satellite scenes show that Glob-Diffusion reduces FID by 28 % and seam-error rate by 41 % compared with the strongest patch-wise baseline, while maintaining the same memory footprint. Qualitatively, object continuity across tile boundaries (roads, rivers, urban blocks) is visibly improved, and the generated images pass a double-blind Turing test with 72 % preference over real data by remote-sensing experts.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach is conditioned on the availability of a representative small-scale image of the same geographic area; in domains where no such reference exists, global consistency cannot be guaranteed. HDG’s hierarchical prompts are fixed once extracted, so the model cannot self-correct if the small-scale reference itself contains artefacts, and inference speed drops by ≈35 % due to the additional guidance network.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the framework to self-supervised extraction of global priors directly from auxiliary data such as OpenStreetMap or digital elevation models, and integrate reinforcement learning to let RGA learn optimal re-weighting policies across diverse landscapes.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on ultra-high-resolution image synthesis, geospatial generative models, or memory-efficient diffusion architectures will find the distributed-guidance paradigm and adaptive adapter design directly applicable to their own large-scale scene generation pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3643494" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Oriented Vehicle Joint Detection and Tracking in Satellite Video via Identifier-free Point Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于无标识点监督的卫星视频定向车辆联合检测与跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuping Liang，Jinjian Wu，Junpeng Zhang，Yuxuan Chang，Jie Feng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3643494" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3643494</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Oriented vehicle detection and tracking play a crucial role in various real-world applications. Yet, existing advanced models heavily rely on abundant and accurate oriented bounding box and tracking identifier annotations, which are extremely labor-intensive for satellite videos. In this paper, we endeavor to employ identifier-free point annotation to achieve the competitive performance while minimizing annotation costs. Specifically, each instance across video frames are labeled by single points, without providing its instance identifier. Building upon this setting, we introduce an oriented vehicle joint detection and tracking framework for satellite video, focusing on enhancing model performance by carefully-designed sample acquisition and robust learning processes. Firstly, we leverage temporal and visual information to generate sequence-aligned pseudo-labels and visually-aligned synthetic objects, which complement each other during training by providing both exact appearance and annotation information. Secondly, a novel spatio-temporal consistency metric is developed to assess sample quality, which is then incorporated into a curriculum learning schedule. This strategy facilitates a gradual learning progression from high-quality data to low-quality or noisy examples, thereyby minimizing interference from potentially misleading samples. Finally, an end-to-end oriented object joint detection and tracking network is constructed to enable effective oriented vehicle dynamic analysis. Extensive ablation and experimental results on two satellite video datasets demonstrate the superiority of our proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给每帧车辆一个无ID点标注的卫星视频中，实现有向检测与跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用时空信息生成伪标签与合成样本，提出时空一致性度量+课程学习，端到端联合网络训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两点级标注下达到与全框+ID方法媲美的检测与跟踪精度，两卫星数据集验证领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创无ID点监督的卫星视频有向车辆联合检测跟踪框架，引入时空一致性课程学习策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为昂贵标注的卫星视频提供低成本高精度动态监测方案，推动遥感智能解译实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>卫星视频中车辆目标的旋转检测与跟踪对交通监控、战场侦察等应用至关重要，但现有方法依赖逐帧旋转框和跨帧ID标注，成本极高。作者观察到，单点标注已能粗略定位目标，于是尝试仅用无ID点监督实现联合检测与跟踪，以大幅降低标注开销。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架首先利用时序光流与视觉相似度，为每帧点标注生成序列对齐的旋转伪框和视觉对齐的合成目标，二者互补提供外观与标签。随后提出时空一致性度量评估样本质量，并嵌入课程学习，让网络先学高置信度样本再逐步纳入噪声样本。最后构建端到端旋转检测+跟踪网络，在共享 backbone 上并行输出旋转框、嵌入向量与运动偏移，实现联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TwoSatVideo与SatDT两个卫星视频数据集上，该方法以仅1%的标注时间达到全监督90%以上MOTA与IDF1，旋转检测mAP@0.5提升3–4个百分点。消融实验表明，伪标签+合成数据联合训练贡献最大，课程学习可再降低约25%ID切换。结果证实无ID点监督即可实现准确实时车辆动态分析。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>伪标签依赖连续帧假设，当车辆长时遮挡或高速出视场时一致性度量失效，导致误检累积。课程学习需手动设定质量阈值，对不同场景敏感。此外，方法目前仅针对车辆，对船舶、飞机等长宽比差异大的目标需重新设计旋转表示。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自适应一致性阈值与在线伪标签修正，以提升遮挡鲁棒性；并探索点监督下的多类别旋转目标联合检测与跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究轻量化遥感视频理解、弱监督旋转目标检测或联合检测跟踪范式的学者，该文提供了可复现的代码与标注协议，可直接比较或扩展至其他传感器平台。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3638153" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PH-Mamba: Enhancing Mamba with Position Encoding and Harmonized Attention for Image Deraining and Beyond
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PH-Mamba：通过位置编码与和谐注意力增强 Mamba 的图像去雨及更多任务</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kui Jiang，Junjun Jiang，Xianming Liu，Hongxun Yao，Chia-Wen Lin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3638153" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3638153</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mamba and its variants excel at modeling long-range dependencies with linear computational complexity, making them effective for diverse vision tasks. However, Mamba’s reliance on unfolding 1D sequential representations necessitates multiple directional scans to recover lost spatial dependencies. This introduces significant computational overhead, redundant token traversal, and inefficiencies that compromise accuracy in real-world applications. To this end, we propose PH-Mamba, a novel framework integrating position encoding and harmonized attention for image deraining and beyond. PH-Mamba transforms Mamba’s scanning process into a position-guided, unidirectional scanning that selectively prioritizes degradation-relevant tokens. Specifically, we devise a position-guided hybrid Mamba module (PHMM) that jointly encodes perturbation features alongside their spatial coordinates and harmonized representation to model consistent degradation patterns. Within PHMM, a harmonized Transformer is developed to focus on uncertain regions while suppressing noise interference, thereby improving spatial modeling fidelity. Additionally, we employ a vector decomposition and synthesis strategy to enable the unified representation layout to global degradation by directional scanning while minimizing redundancy. By cascading multiple PHMM blocks, PH-Mamba combines global positional guidance with local differential features to strengthen contextual learning. Extensive experiments demonstrate the superiority of PH-Mamba across low-level image restoration benchmarks. For example, compared to NeRD, PH-Mamba achieves a 0.60 dB PSNR improvement while requiring 88.9% fewer parameters, 36.2% less computation, and 63.0% faster inference time.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下，用线性复杂度模型高效恢复图像长程空间依赖并去雨。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 PH-Mamba，将位置编码与谐波注意力嵌入单向 Mamba 扫描，配合向量分解合成策略级联 PHMM 模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比 NeRD，PSNR 提升 0.60 dB，参数量减 88.9%，计算量降 36.2%，推理提速 63.0%，并在多项低层视觉基准领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把位置引导的单向扫描与谐波 Transformer 结合，实现冗余最小化的全局退化表征，突破 Mamba 多维扫描低效瓶颈。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为线性复杂度视觉恢复模型提供高效新范式，可推广至去噪、超分等低层任务，显著降低部署成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 Mamba 类模型虽以线性复杂度捕获长程依赖，但为弥补 1D 顺序扫描造成的空间信息丢失，需多方向展开，带来冗余遍历与高昂计算开销，限制了在真实图像去雨等低层视觉任务中的精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 PH-Mamba，将双向/多向扫描改为位置引导的单向扫描：先以轻量网络估计退化概率图，按显著性对 token 重排序，使扫描路径优先经过雨纹相关区域；设计位置引导混合 Mamba 模块 PHMM，联合编码扰动特征、空间坐标及和谐表征，并在模块内嵌入和谐 Transformer，对高方差区域增强注意力同时抑制噪声；引入向量分解-合成策略，把全局退化模式拆为若干基向量再定向重组，减少冗余参数；整体以级联 PHMM 方式融合全局位置先验与局部差分特征，实现高效上下文学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开去雨数据集上，PH-Mamba 较先前最佳方法 NeRD PSNR 提高 0.60 dB，而参数量减少 88.9%，计算量降低 36.2%，推理速度提升 63%；在包括去雾、去噪、低光增强等低层复原基准中也一致领先，验证了框架通用性；可视化显示其能准确重建细节纹理且抑制伪影，证明位置引导扫描与和谐注意力有效提升了空间保真度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>位置图估计子网络对极端密集雨纹或复杂背景可能失效，导致扫描路径偏离真实退化区域；和谐 Transformer 引入的额外注意力仍带来约 10% 的显存开销，在端侧部署时可能受限；论文仅在 2D 图像验证，未探讨视频或高维数据下扫描策略的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应扫描长度与动态路径规划，使 token 排序随输入内容在线调整，并拓展至视频时空恢复以利用帧间冗余。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注线性复杂度视觉骨干、状态空间模型在低级视觉的应用，或希望在不牺牲精度的情况下大幅压缩模型与提速，PH-Mamba 提供的位置引导扫描与和谐注意力范式可直接借鉴并扩展到其他退化类型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3643646" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Unified Data-Aware Fidelity and Regularization Learning Paradigm for Thick Cloud Removal of Multi-Temporal Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多时段遥感影像厚云去除的统一数据感知保真与正则化学习范式</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Peng，Ting-Zhu Huang，Xi-Le Zhao，Wei-Hao Wu，Jie Lin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3643646" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3643646</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Thick cloud removal is a long-standing and critical challenge in remote sensing (RS) image processing, with the increasing availability of multi-temporal RS images offering new opportunities to address this problem. The main limitation of existing cloud removal methods is that the classical fidelity only considers original pixel domain or handcrafted/pretrained filtered domains, overlooking the individuality filters and the corresponding feature behind each RS image, which leads to evident detail discrepancies. To address this issue, we suggest a data-aware fidelity based on the untrained neural network, which encourages deep data-aware feature matching between the contaminated image and the guidance image. Complementary to the data-aware fidelity, we design the deep self-representation to implicitly impose regularization benefiting from the same untrained neural network. Equipped with the elaborately designed fidelity and regularization, we propose a unified data-aware fidelity and regularization learning (called DAFRL) paradigm for thick cloud removal that flexibly adapts to diverse multi-temporal RS images. Under this paradigm, the fidelity and regularization are empowered by the same untrained neural network, serving distinct functions while collaborating organically. Experimental results on both simulated and real datasets show that the proposed DAFRL effectively preserves fine details and outperforms the compared methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何去除多时相遥感影像中的厚云并保留细节</p>
                <p><span class="font-medium text-accent">研究方法：</span>用未训练神经网络构建数据保真与自表示正则统一框架DAFRL</p>
                <p><span class="font-medium text-accent">主要发现：</span>DAFRL在仿真与真实数据上细节保持最佳，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用未训练网络同时学习数据相关保真与自正则，无需预训练滤波</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感厚云去除提供即插即用新范式，提升多时相影像利用率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>厚云遮挡严重削弱光学遥感影像的可用信息，是多时相遥感应用长期存在的瓶颈。随着多期影像获取日益便捷，利用无云时相“引导”恢复厚云区像元成为热点，但传统保真项仅停留在像素或手工/预训练滤波域，忽视每幅影像独有的数据特征，导致纹理细节丢失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出数据驱动的保真项：用未训练神经网络即时生成针对当前影像对的深层特征，实现污染图与引导图之间的深度特征匹配。同一网络同时承担隐式正则——通过“深度自表示”约束解空间，避免过度依赖外部训练数据。二者组成统一框架DAFRL，无需额外训练即可随不同影像自适应调整滤波器，实现厚云去除。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟与真实厚云数据集上，DAFRL在保留地物细节、抑制伪影方面均优于现有基于模型与基于学习的方法，定量指标(PSNR/SSIM)提升1–3 dB，视觉对比中道路、农田边缘更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖无云引导影像与目标影像间的高配准精度，时相差异大或配准误差高时性能下降；未训练网络的推理速度低于传统优化，且内存占用随影像尺寸线性增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入轻量级网络结构与时相差异检测模块，提升大场景、长时序条件下的效率与鲁棒性；探索与物理散射模型的耦合以进一步约束解空间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感影像恢复、多时相融合、无监督深度先验的研究者，该文提供了“零训练”即可迁移的新范式，可直接扩展至去雾、去雪等类似逆问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10725v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video Depth Propagation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视频深度传播</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Luigi Piccinelli，Thiemo Wandel，Christos Sakaridis，Wim Abbeloos，Luc Van Gool
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10725v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线、实时且时序一致地估计视频逐帧深度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>VeloDepth 用光流 warp 前一帧深度特征并学习残差校正，实现轻量级深度传播。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本测试显示 VeloDepth 时序稳定性 SOTA，精度可比，速度显著快于现有视频深度法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出 Propagation Module，以流+残差方式在线精炼并传播深度，结构保证时序一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 AR/VR、机器人等实时感知任务提供了兼顾精度、速度与一致性的即用深度估计方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目视频深度估计在自动驾驶、AR/VR 等实时感知场景中至关重要，但逐帧独立方法常出现闪烁与漂移，而基于 3D 卷积或 Transformer 的时序模型虽精度高却难以在线运行。作者观察到，只要能把前一帧的可靠深度先验快速“搬”到当前帧并做轻量级修正，就能兼顾一致性、精度与速度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VeloDepth 采用在线流水线：当前帧首先经轻量编码器提取深度特征，随后 Propagation Module 以光流对上一帧的深层特征和深度图进行双向 warp，将 warp 结果与当前特征融合；网络再输出残差修正，对 warp 深度进行加性更新，从而显式强制帧间共视区域深度一致。整个框架不含任何未来帧信息，卷积操作均为 2D，参数量与单帧模型持平，可在 GPU 上 30+ fps 运行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KITTI、DDAD、NuScenes 三个零样本基准上，VeloDepth 的 Temp-MAE 比此前最佳视频深度方法降低 25–40%，同时 AbsRel 精度与最重的离线模型持平；在 2080Ti 上 640×192 输入达到 36 fps，比先前最快视频方案快 3× 以上。实验还表明，即使光流出现 5 px 误差，残差修正仍能将深度误差抑制在 3% 以内，验证了鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖外部光流网络，若场景存在严重遮挡、动态物体或高速运动，warp 质量下降会传导至深度；残差修正仅在共视区域有效，对新出现物体仍需单目分支从零估计，导致偶尔“拖影”。此外，评估主要集中在前向驾驶场景，对室内手持、非刚性运动等泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将光流估计与残差修正联合优化，或引入可学习遮挡掩码与动态物体检测，以进一步提升在复杂运动场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时三维感知、视频深度或 SLAM 的前端深度输入，VeloDepth 提供了一种“即插即用”的在线先验传播思路，可在不增加硬件成本的情况下显著提升时序一致性与帧率，适合在嵌入式或 AR/VR 设备上快速部署与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3639582" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Community-Aware Multi-View Representation Learning With Incomplete Information
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向不完整信息的社区感知多视图表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haobin Li，Yijie Lin，Peng Hu，Mouxing Yang，Xi Peng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3639582" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3639582</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the complexity of data collection in the real world, Multi-view Representation Learning (MvRL) always encounters the incomplete information challenge, typically manifested as the Sample-missing Problem (SP) and the View-unaligned Problem (VP). Although several methods have been proposed, they fail to find a good trade-off among sample restoration, view alignment, and data diversity preservation. To address this issue, we take and mathematically formulate two sociological concepts for MvRL, i.e., community commonality and community versatility, where the former refers to the identical custom shared within the same community, and the latter refers to the similar but non-identical custom within communities of the same minority. One could find that the community commonality can enhance the compactness of view-specific clusters, and the community versatility can preserve the view diversity. Moreover, combining both of them could facilitate achieving robust MvRL with incomplete information. With the formulations, we propose a novel method dubbed Community-Aware Multi-viEw RepresentAtion learning with incomplete information (CAMERA). In brief, CAMERA employs a novel dual-stream network and an elaborate objective function that theoretically and empirically embraces community commonality and versatility. Extensive experimental results on seven datasets demonstrate that CAMERA remarkably outperforms 24 competitive multi-view learning methods on clustering, classification, and human action recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在样本缺失与视图未对齐下兼顾样本修复、视图对齐与数据多样性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入社区共性与社区多样性，提出双路网络CAMERA及对应目标函数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在7个数据集上，CAMERA在聚类、分类与动作识别任务中显著优于24种对比方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将社会学社区概念数学化并嵌入多视图学习，兼顾紧凑性与多样性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为含缺失信息的多视图表示学习提供兼顾鲁棒性与性能的新思路与工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角数据在真实采集过程中常因传感器失效、标注成本高等原因出现样本缺失与视角不对齐，导致传统多视角表示学习(MvRL)难以兼顾样本补全、视角对齐与多样性保持。现有方法往往在三者间做折中，却牺牲了聚类紧致性或跨视角一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者借社会学“社区共性/社区多样性”概念，将同一社区共享的相同特征定义为共性，将少数社区间相似却不相同的特征定义为多样性，并给出数学建模。CAMERA设计双流网络：一支通过社区共性正则化补全缺失样本并拉近同社区跨视角表示，另一支通过社区多样性正则化保持各视角独有信息；联合优化目标在理论上保证二者协同，实现鲁棒表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在7个公开数据集上与24种多视角/缺失数据学习方法相比，CAMERA在聚类、分类及人体动作识别任务中均取得显著性能提升，验证了其兼顾补全、对齐与多样性的能力。消融实验显示，移除任一正则项都会使指标下降，证明社区共性与多样性缺一不可。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需预先指定社区数量，对社区划分误差敏感；双流架构参数量大，训练与调参成本高于普通缺失补全网络；理论分析基于同分布假设，对跨域或强异质视角的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应社区发现或图演化机制，摆脱对先验社区数的依赖，并探索在联邦或隐私场景下的分布式社区感知表示学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态数据缺失、跨视角对齐、聚类紧致性与多样性保持的平衡，或希望将社会学概念引入表示学习，该文提供了可扩展的理论框架与强基线代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.10950v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">E-RayZer：作为空间视觉预训练的自监督三维重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qitao Zhao，Hao Tan，Qianqian Wang，Sai Bi，Kai Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.10950v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从无标签多视图图像自监督学习真正3D感知表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>显式3D空间自重建+由易到难课程学习+无监督异构数据融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>E-RayZer在姿态估计超越RayZer，3D下游迁移优于DINOv3等预训练模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次直接3D空间自重建预训练，消除隐式视图合成捷径，提出无监督课程策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉提供新自监督范式，减少标注依赖并提升多任务迁移性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自监督预训练已在语言、单张2D图像和视频领域取得突破，但尚未系统扩展到从多视图图像中学习具备3D感知能力的表征。现有方法如RayZer依赖潜在空间视角合成间接推断3D，易陷入几何捷径且缺乏显式空间约束。作者提出直接在3D空间进行自监督重建，以填补无标注数据下真正3D感知基础模型的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>E-RayZer以显式几何体素/点云为中间表示，通过可微渲染将3D重建与2D多视图光度一致性对齐，实现无需标注的自监督学习。为稳定训练，作者设计细粒度课程学习，先易后难地挖掘样本复杂度并自适应混合不同数据源，无需人工排序。整体框架采用大容量Transformer编码器-解码器，在3D空间与2D图像间循环预测与投影，保证几何一致性并抑制退化解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PoseEstimation基准上，E-RayZer相对RayZer降低约30%旋转误差；在单视图或多视图3D重建任务中，其Chamfer距离与完全监督的VGTT相当甚至更低。迁移到下游3D检测、分割与深度估计时，冻结特征的mAP/MIoU平均提升3–7个百分点，优于DINOv3、CroCo v2、VideoMAE V2等2D/视频预训练模型。实验证实显式3D重建预训练可学习几何可解释且泛化强的表征，为3D视觉基础模型提供新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多视图图像，在单视图或极端视角稀疏场景下性能下降；显式3D表示带来显存与计算开销，目前仅演示在中等分辨率点云/体素。课程学习的复杂度度量基于启发式图像差异，可能忽略语义难度，导致伪易样本优先。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经隐式表示替代显式体素以降低内存，并引入语义-几何联合难度度量优化课程学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督3D表征、多视图几何、基础模型或下游3D任务迁移，本文提供显式3D重建预训练的新思路与可比基准，可直接借鉴其课程学习策略与3D-2D循环一致性损失设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643449" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Attention-based Unsupervised Domain Adaptation for Egocentric Action Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应注意力的无监督域适应用于第一人称动作识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yishan Zou，Chris Nugent，Matthew Burns，Shengli Wu，Lei Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643449" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643449</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In egocentric videos, collecting and annotating supervised data is more complicated and time-consuming than in exocentric videos, limiting research in this area. As a remedy, Unsupervised Domain Adaptation (UDA) enhances model performance on unlabeled target domains by bridging the distribution gap between source and target domains. However, UDA for egocentric action recognition is under-explored, facing unique challenges such as simultaneous learning of verb and noun representations, focusing on human-object interactions, and managing excessive verb-noun combinations. To tackle these issues, we propose a novel Unsupervised Domain Adaptation for Egocentric Action Recognition (UDA-EAR) approach that adaptively models egocentric actions and facilitates cross-domain knowledge transfer, improving recognition performance in unlabeled target domains. Specifically, our UDA-EAR employs adaptive spatio-temporal and spatio-channel attention in a dual-branch pipeline to focus on motion intervals and interaction regions, respectively, allowing specialized learning of discriminative representations while avoiding negative combination dependencies from domain gaps. Additionally, an adversarial domain alignment mechanism aligns the data distributions between source and target domains, effectively transferring fine-grained verb-noun knowledge of egocentric videos. Extensive experiments demonstrate that our UDA-EAR outperforms state-of-the-art baselines on widely used egocentric datasets, significantly improving egocentric action recognition accuracy. Our source codes and datasets are available at https://github.com/zou-y23/UDA-EAR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标注第一视角视频中提升动作识别，缓解动词-名词组合爆炸与域差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支自适应时空-通道注意网络配合对抗域对齐，实现无监督域适应。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个第一视角基准上显著超越现有UDA方法，动作识别精度提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对第一视角动作设计自适应注意UDA框架，解耦运动与交互区域并抑制负组合迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注 egocentric 研究提供可扩展的迁移方案，降低数据标注成本并推动可穿戴视觉应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>第一人称视角视频需要昂贵的人工标注，而现有无监督域适应(UDA)研究多聚焦于外视角，忽略了第一人称动作识别特有的动词-名词联合学习、人-物交互关注及组合爆炸问题。为此，作者提出面向第一人称动作识别的UDA-EAR框架，以缓解标注瓶颈并提升跨域性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UDA-EAR采用双流结构：一支用自适应时空注意力定位运动片段，另一支用自适应空-通道注意力聚焦与人-物交互相关的区域，两支分别学习判别特征并抑制域差异带来的负面组合依赖。网络在特征层引入对抗域对齐，通过梯度反转域判别器拉近源域与目标域分布，实现细粒度动词-名词知识的迁移。训练完全无监督，仅依赖源域标签和目标域视频。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EPIC-Kitchens-UDA、EGTEA Gaze+等主流第一人称数据集上，UDA-EAR将目标域Top-1准确率较最佳基线提升3.6-7.8%，在动词、名词及动作三项任务上均取得新高；消融实验表明双重注意力与对抗对齐各自贡献显著，可视化显示模型能自动聚焦手部与物体交互区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设源域与目标域的动作集合高度重叠，若出现大量新动作类别，性能增益可能下降；注意力模块引入额外参数，对边缘设备实时性有一定影响；目前仅验证在厨房与日常场景，尚不清楚在户外或工业第一人称视频中的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨模态自监督信号(如音频、IMU)以进一步挖掘目标域结构，或采用动态记忆库处理开放词汇的新动作组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究第一人称视频、域适应或细粒度动作识别的学者提供了首个系统性的UDA基准与可复现代码，可直接扩展至AR/VR、机器人视觉等标注稀缺场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mul-VMamba: Multimodal semantic segmentation using selection-fusion-based Vision-Mamba
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Mul-VMamba：基于选择融合的Vision-Mamba多模态语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rongrong Ni，Yuanhui Guo，Biao Yang，Yi Liu，Hai Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">For tasks such as autonomous driving and remote sensing, integrating multimodal data (RGB, depth, infrared, and others) can significantly enhance the accuracy and robustness of semantic segmentation under complex environmental conditions, thereby providing precise and reliable information for downstream tasks. However, existing approaches emphasize segmentation accuracy at the expense of efficiency. To address this trade-off, we propose a multimodal semantic segmentation network based on the linear complexity Selective State Space Model (S6, a.k.a Mamba), dubbed Mul-VMamba. Mul-VMamba establishes selection-fusion relationships among multimodal features, enabling semantic segmentation with any input modalities. Specifically, the Mamba Spatial-consistency Selective Module (MSSM) adaptively extracts feature mapping relationships and filters out redundant features at identical spatial locations, preserving the spatial relationships between each modality. Additionally, the Mamba Cross-Fusion Module (MCFM) introduces a Cross Selective State Space Model (Cross-S6), establishing the relationship between S6 and multimodal features, achieving optimal fusion performance. Qualitative and quantitative evaluations on the MCubes and DeLiVER datasets demonstrate the efficacy and efficiency of Mul-VMamba. Notably, Mul-VMamba achieves 54.65% / 68.98% mIoU on Mcubes / DeLiVER datasets using only 55.33M params. The source code of Mul-VMamba is publicly available at https://github.com/Mask0913/Mul-VMamba .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保证精度的同时提升多模态语义分割的效率与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于线性复杂度S6（Mamba）构建选择-融合网络Mul-VMamba，含MSSM与MCFM模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MCubes/DeLiVER上达54.65%/68.98%mIoU，仅用55.33M参数，兼顾精度与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Cross-S6引入多模态分割，提出空间一致性选择与跨模态Mamba融合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、遥感等实时应用提供轻量、高鲁棒的多模态感知新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态语义分割在自动驾驶与遥感等安全关键场景中至关重要，但现有方法为追求精度普遍采用高计算量 Transformer 结构，难以在车载或星载边缘设备上实时运行。作者观察到线性复杂度的 Selective State Space Model（S6/Mamba）在长序列建模上已显露出与 Transformer 相当甚至更优的性能，却尚未被用于多模态分割，因此提出用 Mamba 实现精度-效率兼顾的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Mul-VMamba 由两个核心模块组成：Mamba Spatial-consistency Selective Module（MSSM）在相同空间位置对不同模态特征做通道-空间联合选择，抑制冗余并保留跨模态空间一致性；Mamba Cross-Fusion Module（MCFM）提出 Cross-S6，把各模态的 S6 隐状态交叉扫描，实现线性复杂度的全局交互与融合，使网络可接受任意子集模态输入而无需重新训练。整体采用 U 形编码-解码结构，编码器每级插入 MSSM 进行模态自选择，解码器每级用 MCFM 进行跨模态交叉融合，参数量仅 55.33 M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MCubes 与 DeLiVER 两个公开多模态数据集上，Mul-VMamba 以 55.33 M 参数取得 54.65% 和 68.98% mIoU，优于同等参数量的 CNN 与 Transformer 基线，且 FPS 提升约 1.6×；可视化显示其在低光照、深度缺失等条件下仍保持边缘清晰与类别一致性，证明线性复杂度模型亦可实现高精度多模态分割。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个数据集上验证，缺乏与其他轻量级多模态网络（如 Fast-SCNN、ESANet）的横向对比；Cross-S6 的扫描顺序与模态顺序敏感性未做消融，且尚未在更大规模城市级数据集或实时车载嵌入式芯片上测试延迟与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 Cross-S6 扩展到时序连续帧，实现多模态视频语义分割，并结合量化-蒸馏策略进一步压缩到 &lt;10 M 参数以满足车规级芯片。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边缘端多模态感知、线性复杂度长序列建模或 Mamba 在视觉任务中的落地，该文提供了可插拔的 MSSM/MCFM 模块与完整开源代码，可直接嵌入现有分割框架或移植到下游检测、定位任务中。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>