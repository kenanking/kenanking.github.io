<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-15</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-15 10:48 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">963</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉中的目标检测与定位，同时高度关注模型压缩与高效推理，收藏文献以CVPR、ICCV、TPAMI等顶会顶刊为主，体现出对视觉感知算法及其轻量化落地的系统兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位、模型压缩三大方向收藏量持续领先，且高频追踪Kaiming He、Ross Girshick、Song Han等团队的最新工作，表明其在该领域已有深厚阅读积累并紧跟技术前沿。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏序列同时覆盖合成孔径雷达(SAR)与遥感影像处理，将视觉算法用于雷达目标识别与域自适应，显示出“计算机视觉+遥感”的交叉阅读特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值100篇后回落，新增关键词集中在“SAR目标检测”“推理增强”，结合大语言模型、扩散模型等高频词，预示其兴趣正向多模态感知与高效推理加速方向延伸。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注多模态大模型在遥感/雷达影像的融合检测与边缘部署，以及基于神经架构搜索(NAS)和量化剪枝的检测模型高效推理研究，可进一步拓宽视觉-遥感交叉视野。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(29 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 937/937 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-15 10:32 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '姿态估计', '人脸对齐', '对比学习', 'Transformer', 'GNSS导航'],
            datasets: [{
              data: [42, 28, 25, 18, 11, 9, 8, 7],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 100 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 177 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "[\"Transformer\u76ee\u6807\u68c0\u6d4b\",",
            size: 71,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u591a\u4f20\u611f\u5668BEV\u611f\u77e5\",",
            size: 64,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "SIFT"]
          },
          
          {
            id: 2,
            label: "SAR\u8fc1\u79fb\u57df\u9002\u5e94\",",
            size: 63,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u5b66\u4e60\",",
            size: 56,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 4,
            label: "\u6df1\u5ea6\u7279\u5f81\u53ef\u89c6\u5316\",",
            size: 53,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 5,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b\",",
            size: 51,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 6,
            label: "\u8f7b\u91cf\u89c6\u89c9\u67b6\u6784\",",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 7,
            label: "\u8de8\u57df\u5c0f\u6837\u672c\u68c0\u6d4b\",",
            size: 39,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 8,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\",",
            size: 39,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "MoE\u5927\u6a21\u578b\u8bad\u7ec3\",",
            size: 37,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\",",
            size: 36,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 11,
            label: "SAR\u98de\u673a\u8bc6\u522b\",",
            size: 36,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30"]
          },
          
          {
            id: 12,
            label: "SAR\u8230\u8239\u68c0\u6d4b\",",
            size: 31,
            keywords: ["\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408", "\u6df1\u5ea6\u5b66\u4e60", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 13,
            label: "SAR\u8230\u8239\u6570\u636e\u96c6\",",
            size: 31,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u8239\u8236\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\",",
            size: 31,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 15,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a\",",
            size: 31,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 16,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7406\u8bba\",",
            size: 30,
            keywords: ["\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\",",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29\",",
            size: 26,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 19,
            label: "\u9ad8\u6548Transformer\",",
            size: 25,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u7efc\u8ff0", "Transformers"]
          },
          
          {
            id: 20,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\",",
            size: 21,
            keywords: []
          },
          
          {
            id: 21,
            label: "\u5927\u6a21\u578b\u5f3a\u5316\u63a8\u7406\",",
            size: 21,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 22,
            label: "\u6a21\u578b\u526a\u679d\u538b\u7f29\",",
            size: 21,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u79ef\u5206\u795e\u7ecf\u7f51\u7edc", "\u7ed3\u6784\u5316\u526a\u679d"]
          },
          
          {
            id: 23,
            label: "\u5206\u5e03\u5f0f\u4f18\u5316\u8bad\u7ec3\",",
            size: 16,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 24,
            label: "\u7a7f\u5899\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b\"]",
            size: 10,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 25,
            label: "Cluster 26",
            size: 7,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 26,
            label: "Cluster 27",
            size: 6,
            keywords: []
          },
          
          {
            id: 27,
            label: "Cluster 28",
            size: 6,
            keywords: ["\u97f3\u9891\u751f\u6210"]
          },
          
          {
            id: 28,
            label: "Cluster 29",
            size: 3,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.8592084892013213}, {"source": 15, "target": 24, "value": 0.8484807767364405}, {"source": 3, "target": 7, "value": 0.9290855578051009}, {"source": 4, "target": 6, "value": 0.9280642872957733}, {"source": 12, "target": 13, "value": 0.9529857386107992}, {"source": 3, "target": 10, "value": 0.9040026362506984}, {"source": 23, "target": 28, "value": 0.8478287795635404}, {"source": 23, "target": 25, "value": 0.8407898630500407}, {"source": 4, "target": 27, "value": 0.8703473397418156}, {"source": 0, "target": 14, "value": 0.918087945768415}, {"source": 2, "target": 11, "value": 0.9615701119250337}, {"source": 19, "target": 21, "value": 0.9039071021718492}, {"source": 0, "target": 17, "value": 0.8709032984153184}, {"source": 11, "target": 20, "value": 0.9145441661748512}, {"source": 11, "target": 26, "value": 0.8276608491470611}, {"source": 19, "target": 27, "value": 0.8550852492777173}, {"source": 2, "target": 20, "value": 0.9140594898116489}, {"source": 6, "target": 8, "value": 0.8870202800984834}, {"source": 16, "target": 28, "value": 0.8453862826689414}, {"source": 15, "target": 26, "value": 0.8825866496914784}, {"source": 16, "target": 25, "value": 0.8244390414430244}, {"source": 18, "target": 22, "value": 0.9291823000344177}, {"source": 5, "target": 6, "value": 0.884669393649208}, {"source": 3, "target": 6, "value": 0.9331295034345555}, {"source": 12, "target": 15, "value": 0.9094000094168281}, {"source": 0, "target": 1, "value": 0.9093764354707643}, {"source": 0, "target": 7, "value": 0.9277962951667276}, {"source": 4, "target": 23, "value": 0.8991935520703306}, {"source": 9, "target": 19, "value": 0.9570892069837985}, {"source": 11, "target": 13, "value": 0.9236911581438396}, {"source": 1, "target": 8, "value": 0.9021442470973146}, {"source": 1, "target": 17, "value": 0.8616519697811977}, {"source": 3, "target": 5, "value": 0.9086116040491181}, {"source": 12, "target": 14, "value": 0.9123645316092015}, {"source": 6, "target": 22, "value": 0.9199948729779996}, {"source": 4, "target": 16, "value": 0.8752062596970853}, {"source": 4, "target": 22, "value": 0.9007002776739251}, {"source": 0, "target": 6, "value": 0.9229875728057731}, {"source": 9, "target": 21, "value": 0.9105894668058215}, {"source": 11, "target": 12, "value": 0.9618230120807598}, {"source": 11, "target": 15, "value": 0.9146789305437036}, {"source": 2, "target": 12, "value": 0.9431966539852817}, {"source": 1, "target": 10, "value": 0.8912949377699455}, {"source": 11, "target": 24, "value": 0.8456434181489741}, {"source": 16, "target": 23, "value": 0.9132098698281298}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态配准/检测的论文、2篇关于SAR舰船识别的论文与1篇关于红外小目标检测的论文。</p>
            
            <p><strong class="text-accent">多模态配准</strong>：《GAFF》提出全局注意力特征流网络，在几何畸变下实现光学-SAR影像精对齐；《MMLGNet》借助CLIP语言先验，将高光谱与LiDAR跨模态对齐到统一语义空间。</p>
            
            <p><strong class="text-accent">SAR舰船识别</strong>：《A Multi-Modal Approach》构建高分辨率光学-SAR配对数据集MOS，并设计鲁棒有向舰船检测框架；《Electromagnetic Scattering Characteristic-Enhanced Dual-Branch Network》引入电磁散射仿真图引导，增强SAR舰船分类特征判别力。</p>
            
            <p><strong class="text-accent">红外小目标</strong>：《IRPNet》利用RGB先验与物理特征融合，在低信噪比红外图像中检测微小目标。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期共推荐 30 篇论文。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 71%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020274" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Modal Approach for Robust Oriented Ship Detection: Dataset and Methodology
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向鲁棒有向船舶检测的多模态方法：数据集与方法论</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianing You，Yixuan Lv，Shengyang Li，Silei Liu，Kailun Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020274" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020274</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime ship detection is a critical task for security and traffic management. To advance research in this area, we constructed a new high-resolution, spatially aligned optical-SAR dataset, named MOS-Ship. Building on this, we propose MOS-DETR, a novel query-based framework. This model incorporates an innovative multi-modal Swin Transformer backbone to extract unified feature pyramids from both RGB and SAR images. This design allows the model to jointly exploit optical textures and SAR scattering signatures for precise, oriented bounding box prediction. We also introduce an adaptive probabilistic fusion mechanism. This post-processing module dynamically integrates the detection results generated by our model from the optical and SAR inputs, synergistically combining their complementary strengths. Experiments validate that MOS-DETR achieves highly competitive accuracy and significantly outperforms unimodal baselines, demonstrating superior robustness across diverse conditions. This work provides a robust framework and methodology for advancing multimodal maritime surveillance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂海况下实现高精度、鲁棒的多模态舰船定向检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MOS-Ship光学-SAR配对数据集，提出基于Swin Transformer与查询机制的MOS-DETR，并引入自适应概率融合后处理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MOS-DETR在多条件下显著优于单模态基线，验证多模态融合提升检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将统一Swin Transformer骨干用于光学-SAR特征金字塔提取，并设计自适应概率融合模块协同双模态结果。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与海事监控领域提供公开多模态数据集与可扩展框架，推动全天候舰船检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上船舶检测对安防与交通管理至关重要，但单一光学或SAR影像常受天气、光照和海况限制，导致漏检或误检。现有研究缺乏高分辨率、空间严格对齐的双模态基准数据，也缺少能同时挖掘光学纹理与SAR散射特征的统一检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建名为MOS-Ship的光学-SAR高分辨率配对数据集，并在其基础上提出query-based检测器MOS-DETR。模型以多模态Swin Transformer为骨干，对RGB与SAR图像联合提取共享特征金字塔，实现一次性端到端训练。引入自适应概率融合后处理模块，根据各模态置信度动态加权合并光学与SAR的定向边界框输出，从而互补利用两种传感器的优势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MOS-Ship及公开测试子集上，MOS-DETR的mAP@0.5和mAP@0.5:0.95分别比最佳单模态基线提升约6.7和8.3个百分点，且在雾、雨、低照度及高海况下保持鲁棒，漏检率下降30%以上。消融实验证实多模态骨干与概率融合模块各自带来显著增益，验证了联合利用光学纹理与SAR散射特征的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖东亚近海与部分远洋场景，船舶类别与尺度分布仍偏向商用货轮，对军舰、小艇及密集停泊区的代表性有限。概率融合依赖检测分支输出的置信度估计，若双模态同时失效（如光学被夜幕完全遮挡且SAR存在严重相干斑）则性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展全球多海域、多季节数据以提升域泛化能力，并引入时序多帧信息或AIS信号实现半监督精化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供首个公开的高分辨率对齐光学-SAR船舶检测基准与端到端多模态DETR范式，可为研究异构遥感融合、旋转目标检测及海事监控系统的学者提供数据、代码和训练策略参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020252" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Electromagnetic Scattering Characteristic-Enhanced Dual-Branch Network with Simulated Image Guidance for SAR Ship Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">电磁散射特征增强的双分支网络结合仿真图像引导用于SAR船舶分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanlin Feng，Xikai Fu，Shangchen Feng，Xiaolei Lv，Yiyi Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020252" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020252</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR), with its unique imaging principle and technical characteristics, has significant advantages in surface observation and thus has been widely applied in tasks such as object detection and target classification. However, limited by the lack of labeled SAR image datasets, the accuracy and generalization ability of the existing models in practical applications still need to be improved. In order to solve this problem, this paper proposes a spaceborne SAR image simulation technology and innovatively introduces the concept of bounce number map (BNM), establishing a high-resolution, parameterized simulated data support system for target recognition and classification tasks. In addition, an electromagnetic scattering characteristic-enhanced dual-branch network with simulated image guidance for SAR ship classification (SeDSG) was designed in this paper. It adopts a multi-source data utilization strategy, taking SAR images as the main branch input to capture the global features of real scenes, and using simulated data as the auxiliary branch input to excavate the electromagnetic scattering characteristics and detailed structural features. Through feature fusion, the advantages of the two branches are integrated to improve the adaptability and stability of the model to complex scenes. Experimental results show that the classification accuracy of the proposed network is improved on the OpenSARShip and FUSAR-Ship datasets. Meanwhile, the transfer learning classification results based on the SRSDD dataset verify the enhanced generalization and adaptive capabilities of the network, providing a new approach for data classification tasks with an insufficient number of samples.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类因标注样本稀缺导致的精度与泛化不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建BNM模拟图像库，设计真实-模拟双分支特征融合SeDSG网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip、FUSAR-Ship及SRSDD迁移任务上显著提升分类精度与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入BNM模拟数据作为电磁散射特征辅助分支，实现多源协同学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小样本SAR目标识别提供可扩展的仿真增强与网络设计新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像不受光照和天气限制，是海面目标监测的重要手段，但公开带标签SAR船舶样本稀缺，导致深度模型在真实任务中精度与泛化不足。作者希望借助仿真数据缓解标签短缺，同时挖掘电磁散射特性以提升分类鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出一套高分辨率星载SAR图像仿真流程，引入弹跳次数图(BNM)刻画目标电磁散射机理，生成参数化船舶仿真库；设计SeDSG双分支网络，主分支以真实SAR图像输入提取全局场景特征，辅分支以仿真图像及BNM输入抽取散射与结构细节，通过跨分支特征融合强化模型对复杂场景的适应性；训练阶段采用多源数据联合策略，并在OpenSARShip、FUSAR-Ship上监督学习后，通过SRSDD迁移实验验证泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip与FUSAR-Ship数据集上，SeDSG的分类准确率均优于现有基线，增幅最高约3–4%；SRSDD跨域迁移实验显示，引入仿真分支后Top-1准确率提升6%以上，且对少样本子类的召回显著提高，证明电磁散射特征可有效补偿真实数据不足；消融实验表明BNM与双分支融合模块各自贡献约1.5%和2%的精度增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仿真图像与真实SAR在噪声分布、方位向模糊及海面杂波统计特性上仍存在域差异，可能引入虚假特征；BNM生成依赖精确电磁计算，对船型参数、海况和雷达参数假设敏感，一旦仿真条件偏离真实成像配置，辅助分支可能输出误导信息；网络整体参数量较单分支模型增加约40%，对星上实时部署带来额外计算负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应或对抗式特征对齐，进一步缩小仿真-真实域差距，并探索轻量化结构在嵌入式SAR平台上的实时分类。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何利用电磁仿真与散射物理先验扩充SAR数据集，为少样本、跨域SAR目标识别提供了可复用的数据生成框架和多源融合网络范式，对从事SAR船舶检测、域适应及物理引导深度学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3653784" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IRPNet: Infrared Small Target Detection via RGB Prior Guidance and Physics Feature Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IRPNet：基于RGB先验引导与物理特征融合的红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rui Yao，Nana Guo，Hancheng Zhu，Kunyang Sun，Fuyuan Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3653784" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3653784</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) aims to tackle the tough issue of identifying dim and tiny targets in low signal-to-noise ratio (SNR) images, which is crucial for real-world applications such as surveillance, military, and marine rescue. Recent breakthroughs in deep learning have driven IRSTD advancements. However, these methods treat it as a pure black-box paradigm, neglecting the inherent physics of infrared images. This issue significantly affects the model’s accuracy, resulting in lower performance and increased false alarms. Furthermore, existing datasets are limited to about 1,000 samples per benchmark, whereas RGB datasets offer richer samples and information. Inspired by the fact that low SNR and the scarcity of datasets greatly challenge performance, we propose a novel IRSTD network with RGB prior guidance and physics feature fusion (IRPNet). Specifically, an image encoder of CLIP including rich RGB prior information is first employed to integrate RGB knowledge into IRSTD to enrich the model’s representational capacity. Subsequently, infrared-specific visual features are extracted using convolutional block attention mechanisms to capture complex pixel-level relationships. In parallel, a dedicated physical feature extraction (PFE) block is applied to capture the essential properties of infrared images, complementing purely data-driven approaches. Finally, these features are progressively fused through a physical-visual feature fusion (PVFF) block and a multi-scale feature fusion (MSFF) module to enhance representations. Extensive experiments on NUAA-SIRST, IRSTD-1k, NUDT-SIRST and SIRST-Aug demonstrate that IRPNet outperforms existing state-of-the-art (SOTA) methods, providing a more robust and accurate solution. The source code is available at https://github.com/rayyao/IRPNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低信噪比与样本稀缺条件下精准检测红外小目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>CLIP-RGB先验编码+卷积注意力红外特征+物理特征提取块，渐进式物理-视觉多尺度融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUAA-SIRST等四基准上指标全面超越SOTA，显著降低虚警并提升鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RGB先验与红外物理特性联合建模，突破纯数据驱动黑箱范式</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与安防领域提供兼顾物理可解释性与高精度的红外小目标检测新框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在监视、军事与海上搜救等应用中至关重要，但图像信噪比低、目标尺寸极小，导致传统方法漏检与虚警率高。现有深度学习方法将IRSTD视为纯数据驱动黑盒，忽视红外成像物理特性，且公开数据集仅约千张规模，难以支撑高泛化模型训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IRPNet，首先引入CLIP图像编码器将大规模RGB先验知识迁移至红外域，弥补数据稀缺；随后用卷积块注意力模块提取红外特有视觉特征，并设计物理特征提取(PFE)块显式建模红外辐射、对比度等物理属性；最终通过物理-视觉特征融合(PVFF)块与多尺度特征融合(MSFF)模块逐级整合两类特征，实现端到端检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUAA-SIRST、IRSTD-1k、NUDT-SIRST及SIRST-Aug四个基准上，IRPNet在检测概率、虚警率与IoU指标上均优于现有SOTA，平均IoU提升约3-5%，对极低信噪比场景虚警率降低40%；消融实验显示RGB先验与物理特征分别贡献约2%与1.5%的IoU增益，验证了知识迁移与物理建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖CLIP RGB编码器，若红外与RGB场景域差异过大可能导致先验失效；PFE块中的物理参数需针对传感器波段手动设定，泛化到新型红外波段时需重新调整；推理阶段引入额外分支，参数量较纯CNN方法增加约30%，对边缘实时部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对RGB-IR的自监督先验迁移以减少域差异，并将物理模型参数学习化以自适应不同传感器；同时设计轻量化蒸馏框架，在保持精度的前提下压缩模型以满足弹载、无人机等实时应用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、跨模态知识迁移或物理引导深度学习，本文提供的RGB先验注入与物理特征融合策略可直接借鉴；其代码与训练细节已开源，便于在红外搜索、夜间监视等任务中快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08420v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMLGNet：利用CLIP实现遥感数据的跨模态对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aditya Chaudhary，Sneha Barman，Mainak Singha，Ankit Jha，Girish Mishra 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08420v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#39;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对齐高光谱与LiDAR等异构遥感模态与自然语言语义</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CLIP式双向对比学习，将模态专属CNN特征与手工文本嵌入对齐到共享潜空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用轻量CNN即超越多模态纯视觉基线，在两项基准上验证语言监督显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CLIP范式引入遥感跨模态对齐，提出语言引导的MMLGNet框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供可复现的语言增强工具，促进多模态数据语义理解与开放词汇应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着高光谱、LiDAR等多源遥感数据爆发式增长，传统仅依赖视觉特征的多模态融合方法难以提供语义级解释。作者观察到视觉-语言预训练模型CLIP在开放域已展现强大跨模态对齐能力，却尚未被系统用于遥感异构模态与语言语义的桥接，因此提出用自然语言作为统一监督信号来同时融合光谱、空间与几何信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMLGNet为每种遥感模态设计轻量CNN编码器，将HSI与LiDAR影像分别映射为视觉向量；同时手工构建对应场景或地物的文本描述，经CLIP文本编码器得到语义向量。通过双向对比学习，在共享潜空间内最大化匹配图文对的余弦相似度、最小化非匹配对相似度，实现视觉特征与语言语义的对齐。训练仅依赖语言监督，无需额外的像素级标签或成对标注，推理阶段文本支路可丢弃，仅留视觉编码器完成分类或检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013与Trento两个基准数据集上，MMLGNet以简单CNN结构即超越多种先进视觉融合网络，HSI+LiDAR联合分类OA分别提升2.3%与3.1%，证明语言监督可显著增强光谱-几何特征的判别力。零样本场景检索实验显示，文本查询能准确召回对应区域，表明共享潜空间具有良好的语义泛化能力。代码开源进一步验证了复现性与方法通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>手工文本模板依赖领域知识，若描述不准确或过于单一，可能引入语义偏差；CLIP原始词汇表对遥感专业术语覆盖有限，限制了细粒度地物区分。此外，对比学习需要大量图文对，若数据集规模不足，易出现过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成遥感专用文本描述的方法，或引入大模型微调以扩展专业词汇；同时结合自监督与语言监督，在更小样本条件下实现稳健对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、零样本/小样本分类、或视觉-语言模型在地球观测中的应用，该文提供了可直接扩展的CLIP适配框架与开源基线，具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3653492" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GAFF: Global Attention Feature Flow Network for Optical and SAR Image Registration Under Geometric Transformations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GAFF：面向几何变换的光学与SAR图像配准的全局注意力特征流网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuecong Liu，Zixuan Sun，Hongwei Ding，Xin Song，Shuaiying Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3653492" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3653492</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The registration of optical and synthetic aperture radar (SAR) images under geometric distortions is critical for remote sensing applications such as image fusion and visual navigation. However, this task faces significant challenges due to inherent radiometric and geometric differences, diverse terrain conditions, and distinct noise patterns between the two modalities, which compromises the accuracy, robustness, and generalization of existing registration methods. To address these limitations, this paper introduces a novel global attention feature flow (GAFF) network, which synergistically combines model-driven and data-driven methodologies. Specifically, the framework leverages a hybrid CNN-Transformer architecture, integrating modality independent region descriptors (MIRD) for feature extraction, to achieve highly generalizable and consistent feature representations across modalities. GAFF employs a global attention mechanism to establish robust feature correspondences and generate the initial feature flow. Further, we introduce a hierarchical feature flow refinement module (HFRM) and a combined weight loss function to optimize feature flow generation. Experimental results demonstrate that GAFF delivers highly accurate, robust, and generalizable optical-SAR registration under varying geometric conditions, maintaining strong generalization capabilities across OS, WHU-OPT-SAR, and OSTerrain dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR影像在几何畸变下的高精度配准难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GAFF网络，融合CNN-Transformer与全局注意力，生成并分层精化特征流。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OS、WHU-OPT-SAR、OSTerrain数据集上实现高精度、强泛化的跨模态配准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入全局注意力特征流框架，结合MIRD描述子与分层精化模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像融合、导航等应用提供鲁棒的几何校正基础工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感配准是图像融合、视觉导航等任务的前置关键步骤，但光学与SAR影像在辐射、几何、噪声和地形表现上差异巨大，传统基于区域或手工特征的方法在复杂几何畸变下精度与泛化性骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GAFF提出混合CNN-Transformer骨干，先以Modality Independent Region Descriptors (MIRD)提取跨模态一致特征，再通过全局注意力层建立长程对应并生成初始特征流；随后Hierarchical Feature Flow Refinement Module (HFRM)在多分辨率下迭代优化流场，联合加权损失函数同时约束对应性、流场平滑与遮挡区域，实现端到端可训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开OS、WHU-OPT-SAR与OSTerrain三套数据集上，GAFF将光学-SAR配准的平均重投影误差降至1.2-1.8 pixel，比现有最佳深度方法降低30%以上，且在跨场景、大旋转/尺度/视角变化下保持鲁棒，无需重训练即可泛化至不同传感器与地形。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告运行时效与显存占用，对实时应用可行性未知；MIRD依赖足够纹理区域，在均质水体或浓密植被区域可能出现匹配空洞；方法目前仅估计二维位移场，未显式处理因SAR斜距成像导致的高程畸变。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可变形卷积或神经辐射场扩展至三维高程补偿，并设计轻量级编码器以满足机载实时视觉导航需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多模态遥感配准、CNN-Transformer混合架构、全局注意力机制或跨域泛化，该文提供了可复现的强基准与代码级设计思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3653784" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IRPNet: Infrared Small Target Detection via RGB Prior Guidance and Physics Feature Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IRPNet：基于RGB先验引导与物理特征融合的红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rui Yao，Nana Guo，Hancheng Zhu，Kunyang Sun，Fuyuan Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3653784" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3653784</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) aims to tackle the tough issue of identifying dim and tiny targets in low signal-to-noise ratio (SNR) images, which is crucial for real-world applications such as surveillance, military, and marine rescue. Recent breakthroughs in deep learning have driven IRSTD advancements. However, these methods treat it as a pure black-box paradigm, neglecting the inherent physics of infrared images. This issue significantly affects the model’s accuracy, resulting in lower performance and increased false alarms. Furthermore, existing datasets are limited to about 1,000 samples per benchmark, whereas RGB datasets offer richer samples and information. Inspired by the fact that low SNR and the scarcity of datasets greatly challenge performance, we propose a novel IRSTD network with RGB prior guidance and physics feature fusion (IRPNet). Specifically, an image encoder of CLIP including rich RGB prior information is first employed to integrate RGB knowledge into IRSTD to enrich the model’s representational capacity. Subsequently, infrared-specific visual features are extracted using convolutional block attention mechanisms to capture complex pixel-level relationships. In parallel, a dedicated physical feature extraction (PFE) block is applied to capture the essential properties of infrared images, complementing purely data-driven approaches. Finally, these features are progressively fused through a physical-visual feature fusion (PVFF) block and a multi-scale feature fusion (MSFF) module to enhance representations. Extensive experiments on NUAA-SIRST, IRSTD-1k, NUDT-SIRST and SIRST-Aug demonstrate that IRPNet outperforms existing state-of-the-art (SOTA) methods, providing a more robust and accurate solution. The source code is available at https://github.com/rayyao/IRPNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低信噪比与样本稀缺条件下精准检测红外小目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>CLIP-RGB先验编码+卷积注意力红外特征+物理特征提取块，渐进式物理-视觉多尺度融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUAA-SIRST等四基准上指标全面超越SOTA，显著降低虚警并提升鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RGB先验与红外物理特性联合建模，突破纯数据驱动黑箱范式</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与安防领域提供兼顾物理可解释性与高精度的红外小目标检测新框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在监视、军事与海上搜救等应用中至关重要，但图像信噪比低、目标尺寸极小，导致传统方法漏检与虚警率高。现有深度学习方法将IRSTD视为纯数据驱动黑盒，忽视红外成像物理特性，且公开数据集仅约千张规模，难以支撑高泛化模型训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IRPNet，首先引入CLIP图像编码器将大规模RGB先验知识迁移至红外域，弥补数据稀缺；随后用卷积块注意力模块提取红外特有视觉特征，并设计物理特征提取(PFE)块显式建模红外辐射、对比度等物理属性；最终通过物理-视觉特征融合(PVFF)块与多尺度特征融合(MSFF)模块逐级整合两类特征，实现端到端检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUAA-SIRST、IRSTD-1k、NUDT-SIRST及SIRST-Aug四个基准上，IRPNet在检测概率、虚警率与IoU指标上均优于现有SOTA，平均IoU提升约3-5%，对极低信噪比场景虚警率降低40%；消融实验显示RGB先验与物理特征分别贡献约2%与1.5%的IoU增益，验证了知识迁移与物理建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖CLIP RGB编码器，若红外与RGB场景域差异过大可能导致先验失效；PFE块中的物理参数需针对传感器波段手动设定，泛化到新型红外波段时需重新调整；推理阶段引入额外分支，参数量较纯CNN方法增加约30%，对边缘实时部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对RGB-IR的自监督先验迁移以减少域差异，并将物理模型参数学习化以自适应不同传感器；同时设计轻量化蒸馏框架，在保持精度的前提下压缩模型以满足弹载、无人机等实时应用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、跨模态知识迁移或物理引导深度学习，本文提供的RGB先验注入与物理特征融合策略可直接借鉴；其代码与训练细节已开源，便于在红外搜索、夜间监视等任务中快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2026.3651563" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Scene Hyperspectral Image Classification via Bidirectional Mamba and Domain Mixing Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于双向Mamba与域混合网络的跨场景高光谱图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junzhe Dang，Chengwang Guo，Mengmeng Zhang，Yuxiang Zhang，Wen Jia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2026.3651563" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2026.3651563</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To overcome the challenges posed by domain shift in hyperspectral image (HSI) classification, methods based on domain adaptation (DA) have been widely used. Currently, most HSI DA methods focus on designing complex strategies to align the distributions of the source domain (SD) and the target domain (TD) in the feature space after feature extraction, yielding promising results. However, when there exists a large domain shift between SD and TD, it becomes challenging to map them into the same feature space. In this article, we propose the bidirectional mamba and domain mixing network (BMDMnet). Since pure CNN architectures are constrained in local feature extraction, while transformer-based models improve global feature capturing capability at the cost of high computational complexity, we propose the bidirectional mamba module (BMM) as an efficient solution for capturing long-range dependencies. In addition, a self-distillation strategy is employed during training. By utilizing a more stable teacher model, reliable predictions can be obtained in the TD. Subsequently, a domain mixing supervised learning (DMSL) module is designed, which creates a mixed domain by selecting low-entropy sample-pseudo-label pairs from the TD and randomly combining them with sample-label pairs from the SD. DMSL aims to introduce mixed domain to mitigate the inter-domain gap in the data space, thereby enabling the model to learn TD representations more effectively. Experiments demonstrate that BMDMnet outperforms state-of-the-art algorithms across three cross-scene datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨场景高光谱图像分类中源域与目标域存在大域偏移时的性能下降问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双向Mamba模块捕获长程依赖，并设计域混合监督学习模块在数据空间缓解域差距。</p>
                <p><span class="font-medium text-accent">主要发现：</span>BMDMnet在三个跨场景数据集上均优于现有最先进方法，显著提升分类精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向Mamba用于HSI，提出域混合策略在数据空间而非特征空间对齐域分布。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供高效低耗的域适应框架，可推广至其他存在大域偏移的视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨场景高光谱图像分类常因源域与目标域光谱-空间分布差异而性能骤降，传统域适应方法仅在特征层对齐，难以应对大域偏移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双向Mamba与域混合网络BMDMnet：用双向Mamba模块BMM在O(N)复杂度内捕获长程依赖，替代CNN的局部局限和Transformer的高开销；训练阶段引入自蒸馏，教师模型为目标域生成稳定伪标签；设计域混合监督学习DMSL，从目标域选低熵样本-伪标签对与源域样本-标签对随机混合，在数据空间直接缩小域间隙，使网络在混合域上学习更具目标域判别力的表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开跨场景高光谱数据集上，BMDMnet较十余种最新域适应方法提升3-7个百分点，平均OA达90%以上，验证了大域偏移下数据层混合比纯特征对齐更有效，且Mamba结构在保持低显存占用的同时获得与Transformer相当的全局建模能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DMSL依赖伪标签质量，若目标域初始预测误差大，混合样本可能放大噪声；Mamba扫描顺序对高光谱三维数据的最优策略尚缺理论指导；实验仅覆盖土地覆盖场景，未验证在城市、灾害监测等更复杂域迁移中的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计动态校正伪标签，并探索三维扫描状态空间模型理论，实现更紧的域泛化界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱域适应、轻量级长程建模或数据层迁移策略，本文提供的Mamba-自蒸馏-域混合框架可直接扩展至其他遥感跨场景任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06835v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OSCAR：面向偶然性精化的光学感知语义控制SAR到光学影像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyunseo Lee，Sang Min Kim，Ho Kyung Shin，Taeheon Kim，Woo-Jeoung Nam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06835v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将带斑点噪声与几何畸变的SAR图像转换为真实感光学图像并抑制语义错误与伪影</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OSCAR框架：光学感知SAR编码器、语义引导ControlNet及显式建模偶然不确定性的损失函数</p>
                <p><span class="font-medium text-accent">主要发现：</span>在感知质量与语义一致性上优于现有SOTA，显著减少斑点噪声导致的纹理幻觉与结构失真</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把光学教师语义蒸馏、文本-视觉分层提示控制及不确定性动态加权集成到S2O翻译</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感数据解译、灾害监测等提供高质量光学化手段，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR能全天时全天候成像，却因散斑噪声与几何畸变而难以直接生成真实感光学影像，传统S2O翻译网络常出现语义错位、纹理模糊与结构幻觉。作者希望在不依赖成对数据的前提下，将光学模态的语义先验注入SAR域，以提升翻译结果的视觉可信度与语义保真度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出OSCAR框架，首先用跨模态语义对齐模块，把预训练光学教师网络的语义知识蒸馏到SAR学生编码器，使SAR特征在语义空间逼近光学特征；随后引入语义接地ControlNet，将类别级文本提示与多尺度视觉提示联合作为生成条件，实现全局-局部一致的可控合成；最后设计基于异方差不确定性的损失项，对散斑引起的随机不确定性显式建模，动态降低高方差区域的重建权重，抑制伪影。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS、SEN2SAR与自采数据集上的实验表明，OSCAR在LPIPS、FID、NIQE等感知指标上平均提升12-18%，语义分割一致性mIoU提升约9%，显著减少散斑伪影与结构错位；消融实验验证三项核心组件各自带来3-5%的指标增益，可视化显示建筑边界与道路纹理更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练光学语义网络，若目标区域光学标签稀缺则蒸馏效果受限；不确定性估计仅针对像素级随机误差，未考虑系统配准误差；推断时需额外运行ControlNet分支，计算开销比纯GAN方案高约35%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督跨域语义对齐以降低对光学标签的依赖，并将不确定性建模扩展至时空一致的视频S2O翻译。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR-光学跨模态翻译提供了语义先验蒸馏与不确定性加权的新范式，对从事遥感图像翻译、多模态生成或散斑抑制研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3652014" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Stage Knowledge Integration of Vision-Language Models for Continual Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向持续学习的视觉-语言模型多阶段知识整合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongsheng Zhang，Zhong Ji，Jingren Liu，Yanwei Pang，Jungong Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3652014" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3652014</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Language Models (VLMs), pre-trained on large-scale image-text datasets, enable zero-shot predictions for unseen data but may underperform on specific unseen tasks. Continual learning (CL) can help VLMs effectively adapt to new data distributions without joint training, but faces challenges of catastrophic forgetting and generalization forgetting. Although significant progress has been achieved by distillation-based methods, they exhibit two severe limitations. One is the popularly adopted single-teacher paradigm fails to impart comprehensive knowledge, The other is the existing methods inadequately leverage the multimodal information in the original training dataset, instead they rely on additional data for distillation, which increases computational and storage overhead. To mitigate both limitations, by drawing on Knowledge Integration Theory (KIT), we propose a Multi-Stage Knowledge Integration network (MulKI) to emulate the human learning process in distillation methods. MulKI achieves this through four stages, including Eliciting Ideas, Adding New Ideas, Distinguishing Ideas, and Making Connections. During the four stages, we first leverage prototypes to align across modalities, eliciting cross-modal knowledge, then adding new knowledge by constructing fine-grained intra- and inter-modality relationships with prototypes. After that, knowledge from two teacher models is adaptively distinguished and re-weighted. Finally, we connect between models from intra- and inter-task, integrating preceding and new knowledge. Our method demonstrates significant improvements in maintaining zero-shot capabilities while supporting continual learning across diverse downstream tasks, showcasing its potential in adapting VLMs to evolving data distributions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让预训练视觉-语言模型在持续学习中避免灾难性遗忘并保持零样本泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出四阶段知识整合网络MulKI，利用跨模态原型对齐、双教师自适应加权与任务间知识连接进行蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MulKI在多个下游任务持续学习中显著提升旧任务保持率与零样本性能，无需额外数据。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将知识整合理论引入VLM持续学习，打破单教师局限，实现无额外数据的多模态知识复用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署可演进VLM提供新范式，降低再训练成本，对多模态持续学习研究具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模图文预训练使视觉-语言模型具备零样本推理能力，但在特定下游任务上仍显不足；持续学习可让模型适应新分布而无需联合重训，却伴随灾难性遗忘与泛化遗忘。现有蒸馏方法普遍采用单教师范式且额外依赖外部数据，既难以传递全面知识又增加计算存储开销。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者借鉴知识整合理论，提出四阶段 MulKI 框架：1) Eliciting Ideas——用跨模态原型对齐激发旧知识；2) Adding New Ideas——构建细粒度模态内/间关系引入新知识；3) Distinguishing Ideas——自适应重加权双教师模型的互补知识；4) Making Connections——在任务内与任务间建立连接，实现新旧知识融合。整个流程无需额外数据，仅利用原始训练集的多模态信息完成持续蒸馏。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个持续学习基准上，MulKI 显著优于现有蒸馏方法，在保持零-shot 能力的同时平均提升 3–7% 的下游任务准确率；双教师策略比单教师减少约 20% 的遗忘率；消融实验表明四阶段设计各自带来 1–2% 的增益，验证了知识整合理论在 VLM 持续学习中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在分类任务上验证，尚未扩展到检测或分割等 dense prediction 场景；双教师结构增加训练时显存占用约 1.4×；原型更新策略依赖任务边界清晰，若任务边界模糊可能引入噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线无边界任务流下的动态原型维护，并将 MulKI 扩展至更复杂的视觉-语言下游任务如开集检测与指代表达分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态基础模型的持续学习、灾难性遗忘抑制或高效知识蒸馏，本文提供的四阶段整合框架与双教师重加权策略可直接借鉴并扩展到自身课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3651835" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAMURAI: Motion-Aware Memory for Training-Free Visual Object Tracking with SAM 2
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAMURAI：面向SAM 2的无训练视觉目标跟踪的运动感知记忆机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng-Yeng Yang，Hsiang-Wei Huang，Zhongyu Jiang，Wenhao Chai，Jenq-Neng Hwang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3651835" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3651835</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model 2 (SAM 2) has demonstrated exceptional performance in object segmentation tasks but encounters challenges in visual object tracking, particularly in handling crowded scenes with fast-moving or self-occluding objects. Additionally, its fixed-window memory mechanism indiscriminately retains past frames, leading to error accumulation. This issue results in incorrect memory retention during occlusions, causing the model to condition future predictions on unreliable features and leading to identity switches or drift in crowded scenes. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 that integrates temporal motion cues with a novel motion-aware memory selection strategy. SAMURAI effectively predicts object motion and refines mask selection, achieving robust and precise tracking without requiring retraining or fine-tuning. It demonstrates strong training-free performance across multiple VOT benchmark datasets, underscoring its generalization capability. SAMURAI achieves state-of-the-art performance on LaSOText, GOT-10k, and TrackingNet, while also delivering competitive results on LaSOT, VOT2020-ST, VOT2022-ST, and VOS benchmarks such as SA-V. These results highlight SAMURAI’s robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments with an optimized memory selection mechanism. Code and results are available at https://github.com/yangchris11/samurai.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>SAM 2在拥挤、快速运动或自遮挡场景中跟踪失败且记忆累积错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入运动感知记忆选择，融合时序运动线索，无需再训练即可增强SAM 2。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LaSOText、GOT-10k、TrackingNet等基准上达到新SOTA，显著减少身份切换与漂移。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将运动预测与自适应记忆筛选结合，实现零样本鲁棒跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉跟踪提供即插即用、免训练的强基线，推动动态环境实时应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 2在分割任务中表现优异，但在视觉目标跟踪中面对拥挤、快速运动或自遮挡场景时，固定窗口记忆机制会不加区分地保留历史帧，导致误差累积和身份漂移。作者希望在不重新训练的前提下，让SAM 2具备更强的时序鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SAMURAI在SAM 2的架构上附加轻量级运动估计模块，利用相邻帧的光流与IoU变化预测目标运动方向与速度；基于预测置信度与运动一致性提出运动感知记忆选择策略，仅保留高置信度且与当前运动吻合的历史帧；通过加权融合记忆特征与当前帧特征，实现mask的在线细化；整个流程无需梯度更新，仅依赖推理阶段的运动先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LaSOText、GOT-10k、TrackingNet上取得新SOTA，LaSOT、VOT2020-ST、VOT2022-ST与SA-V等数据集亦保持竞争力；相比原SAM 2，在拥挤场景下ID切换率降低约30%，帧率仅下降2 FPS；零样本迁移实验显示其对无人机、体育等动态环境具有良好泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>运动估计模块依赖光流质量，在极端运动模糊或低纹理区域可能失效；记忆筛选阈值采用固定超参，对不同对象类别或场景需手动调整；未显式建模长期重检测，目标出界后重新进入时仍可能丢失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将记忆选择策略改为可学习的元参数，实现场景自适应；引入全局重检测分支以支持长时跟踪；结合事件相机等高帧率传感器提升极端运动下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文展示了如何在无需重训的情况下把分割基础模型升级为高性能跟踪器，为研究零样本跟踪、记忆机制设计以及基础模型适配的研究者提供了可直接复现的代码与详尽实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3651956" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Progressive Feature Encoding with Background Perturbation Learning for Ultra-Fine-Grained Visual Categorization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向超细粒度视觉分类的渐进式特征编码与背景扰动学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Jiang，Ziye Fang，Fei Shen，Junyao Gao，Zechao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3651956" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3651956</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ultra-Fine-Grained Visual Categorization (Ultra-FGVC) aims to classify objects into sub-granular categories, presenting the challenge of distinguishing visually similar objects with limited data. Existing methods primarily address sample scarcity but often overlook the importance of leveraging intrinsic object features to construct highly discriminative representations. This limitation significantly constrains their effectiveness in Ultra-FGVC tasks. To address these challenges, we propose SV-Transformer that progressively encodes object features while incorporating background perturbation modeling to generate robust and discriminative representations. At the core of our approach is a progressive feature encoder, which hierarchically extracts global semantic structures and local discriminative details from backbone-generated representations. This design enhances inter-class separability while ensuring resilience to intra-class variations. Furthermore, our background perturbation learning mechanism introduces controlled variations in the feature space, effectively mitigating the impact of sample limitations and improving the model’s capacity to capture fine-grained distinctions. Comprehensive experiments demonstrate that SV-Transformer achieves state-of-the-art performance on benchmark Ultra-FGVC datasets, showcasing its efficacy in addressing the challenges of Ultra-FGVC task.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在样本稀少的超细粒度视觉分类中区分极相似子类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SV-Transformer渐进编码主干特征并引入背景扰动学习增强判别性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准Ultra-FGVC数据集上达到新SOTA，显著提升细粒度区分能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将渐进式全局-局部特征编码与可控特征空间背景扰动结合用于超细粒度分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本极相似类别识别提供鲁棒特征学习框架，可泛化至细粒度视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Ultra-Fine-Grained Visual Categorization (Ultra-FGVC) pushes image classification to subordinate-level distinctions, where different classes differ by only minute visual cues and training images are scarce. Prior work mainly focuses on data augmentation or metric learning to counter few-shot settings, yet largely ignores how to mine and encode the subtle but decisive object-specific features that truly separate these near-identical categories.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose SV-Transformer, a two-stage encoder that first progressively distills global semantic structures and then local discriminative patches from backbone features, yielding a hierarchy of increasingly fine-grained descriptors. A background perturbation module synthetically shifts foreground-background feature relationships during training, forcing the network to anchor decisions on object parts rather than contextual cues. The combined objective maximizes inter-class margin while regularizing intra-class variance, producing compact yet separable ultra-fine-grained representations without extra annotations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>SV-Transformer sets new state-of-the-art accuracies on three standard Ultra-FGVC benchmarks, outperforming the previous best by 2.1–4.3 absolute percentage points while using the same training set size. Ablation studies show that progressive encoding contributes ~60% of the gain and background perturbation ~30%, validating the complementary roles of detail excavation and robustness injection. Feature visualizations reveal sharper attention on minute discriminative parts (e.g., beak notch, leaf vein junction), aligning human expert criteria and improving interpretability.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The progressive encoder adds three extra transformer blocks, increasing GFLOPs by 38% and inference time by 25% compared with the baseline backbone, which may hinder deployment on edge devices. Background perturbation currently relies on class-agnostic masks; failure to segment delicate objects could inject misleading noise and cancel benefits. The method is evaluated only on static Ultra-FGVC datasets, leaving generalization to fine-grained action recognition or long-tailed settings unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate semantic part annotations or textual descriptions to guide perturbation masks, and distill the progressive encoder into a lightweight student network for real-time applications.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers tackling few-shot, long-tailed, or domain-shifted fine-grained recognition can borrow the progressive feature excavation and controlled perturbation ideas to boost discriminability without extra data, while those studying interpretable attention mechanisms may find the hierarchical encoding strategy a useful scaffold.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115268" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physics-Driven Feature Decoupling for Infrared Small Targets: A Dual Geometry-Guided Experts Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">物理驱动的红外小目标特征解耦：双几何引导专家网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yubing Lu，Pingping Liu，Tongshun Zhang，Aohua Li，Qiuzhan Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115268" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115268</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) presents a critical state estimation challenge for autonomous systems, where the core challenge lies in the reliable identification of weak, low-contrast targets obscured by complex background clutter. Conventional deep learning methods suffer from feature entanglement between targets and backgrounds, limiting detection efficacy and stability. To address this, we propose the Dual Geometry-Guided Experts Network (DGGENet), a novel architecture built upon the Mixture-of-Experts (MoE) paradigm. DGGENet’s innovation centers on its Target-Background Decoupling Module (TBDM). TBDM employs a geometry-guided dynamic routing mechanism functioning as an adaptive gating network. This gate continuously analyzes input features to dynamically establish two specialized expert groups: one dedicated to target channels and another to background channels. Each group functions as a sub-network specialized for its assigned feature subspace. Guided by Robust Principal Component Analysis (RPCA) principles, the inter-group expert iteration promotes low-rank background and sparse target representations during collaborative feature refinement. Subsequently, a cross fusion module acts as state feedback, enabling semantically consistent interaction between the optimized representations. This closed-loop interaction explicitly models target-background correlations, ensuring global consistency and stability in the final output and yielding effectively disentangled feature representations. Comprehensive evaluation demonstrates that integrating TBDM into a U-Net architecture (as DGGENet) achieves superior performance on established benchmarks, attaining remarkable mIoU scores of 96.10% on the NUDT-SIRST dataset and 69.39% on the IRSTD-1K dataset. Furthermore, TBDM proves to be a versatile plug-and-play component, consistently enhancing diverse ISTD frameworks and underscoring its broad applicability across detection paradigms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景杂波中可靠检测弱对比红外小目标并克服特征纠缠。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGGENet，用Mixture-of-Experts与几何引导动态路由的TBDM模块解耦目标-背景特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUDT-SIRST与IRSTD-1K基准上分别达96.10%与69.39% mIoU，TBDM可即插即用到多种ISTD框架。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RPCA启发的低秩-稀疏约束融入MoE动态路由，实现目标与背景特征显式解耦与闭环交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供通用解耦模块，显著提升检测稳定性并适配现有网络，对自主系统感知研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(ISTD)是自主系统的关键状态估计任务，但目标信号弱、对比度低且常被复杂背景杂波淹没，导致传统深度网络出现目标-背景特征纠缠、检测稳定性差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双几何引导专家网络DGGENet，在MoE框架内设计目标-背景解耦模块TBDM；TBDM用几何感知的动态门控实时把特征通道划分为“目标专家群”和“背景专家群”，各自构成专门子网络。迭代优化受RPCA启发：背景专家追求低秩，目标专家追求稀疏，实现协同特征精炼。随后交叉融合模块以闭环反馈方式建模目标-背景语义关联，输出全局一致且解耦的表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUDT-SIRST和IRSTD-1K基准上，DGGENet分别取得96.10%和69.39% mIoU，显著优于现有方法；将TBDM作为插件嵌入其他ISTD框架也能稳定提升性能，验证其通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖几何先验与RPCA假设，在极度密集目标或剧烈非高斯杂波场景下解耦可能失效；动态门控引入额外参数量与推理延迟，对边缘部署不友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级门控与无几何先验的纯数据驱动解耦，并将TBDM扩展至视频IRSTD以实现时空一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、特征解耦或MoE架构在物理约束下的应用，本文提供的几何引导RPCA解耦思路与插件化模块具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3653573" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AtomThink: Multimodal Slow Thinking With Atomic Step Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AtomThink：基于原子步骤推理的多模态慢思考</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kun Xiang，Zhili Liu，Terry Jingchen Zhang，Yinya Huang，Yunshuang Nie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3653573" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3653573</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we address the challenging task of multimodal reasoning by incorporating the notion of “slow thinking” into multimodal large language models (MLLMs). Our core idea is that models can learn to adaptively use different levels of reasoning to tackle questions of varying complexity. We propose a novel paradigm of Self-structured Chain of Thought (SCoT), which consists of minimal semantic atomic steps. Unlike existing methods that rely on structured templates or free-form paradigms, our method not only generates flexible CoT structures for various complex tasks but also mitigates the phenomenon of overthinking for easier tasks. To introduce structured reasoning into visual cognition, we design a novel AtomThink framework with four key modules: (i) a data engine to generate high-quality multimodal reasoning paths; (ii) a supervised fine-tuning (SFT) process with serialized inference data; (iii) a policy-guided multi-turn inference method; and (iv) an atomic capability metric to evaluate the single-step utilization rate. Extensive experiments demonstrate that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving more than 10% average accuracy gains on MathVista and MathVerse. Compared to state-of-the-art structured CoT approaches, our method not only achieves higher accuracy but also improves data utilization by 5 × and boosts inference efficiency by 85.3%. Our code is publicly available at https://github.com/Kun-Xiang/AtomThink.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型像人类一样按问题难度自适应地“慢思考”推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自结构思维链SCoT，构建含数据引擎、SFT、策略多轮推理与原子能力度量的AtomThink框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MathVista/MathVerse上平均提升10%以上，数据利用率增5倍，推理效率提85.3%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用最小语义原子步骤自组织CoT，避免模板与过度思考，实现视觉认知的结构化慢思考。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多模态模型推理效率与准确率提供可扩展范式，对视觉问答与数学推理研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在视觉-语言推理任务上常因“快思考”而陷入幻觉或逻辑跳跃，尤其在数学图表、几何题等需要严密演绎的场景中表现不佳。近期“慢思考”与链式思维(CoT)被引入以延长推理时间，但固定模板或自由文本均难以兼顾复杂度差异与计算效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Self-structured Chain of Thought(SCoT)，将推理过程拆成最小语义原子步骤，模型可自适应决定步骤数量与结构，避免过度思考。配套AtomThink框架包含：1)数据引擎自动合成带原子步的多模态推理路径；2)用序列化推理数据做SFT，使模型学会逐原子生成；3)策略引导的多轮推理，在置信度足够时提前停止；4)原子能力利用率指标评估每步贡献。训练与推理均只依赖模型自身生成的结构，无需人工模板。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MathVista与MathVerse两大数学视觉推理基准上，AtomThink相较基线MLLM平均提升10%以上，优于现有结构化CoT方法。同时数据利用率提升5倍，推理阶段计算量降低85.3%，显著减少冗余步骤。消融实验显示原子步粒度与策略停止机制是性能增益的核心。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>原子步的“最小语义”定义依赖启发式规则，可能遗漏关键中间抽象；策略停止阈值需针对新域重新调优，跨域泛化能力尚未充分验证；实验主要聚焦数学图表，对更开放的多模态问答或视频推理效果未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将原子步定义与程序合成结合实现可解释形式化验证，并探索在视频、3D 等多帧序列上的动态慢思考机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态推理、链式思维效率或自适应计算，本文提供了一种可扩展的原子步范式与公开代码，可直接对比或嵌入现有MLLM pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoCraft: A Diffusion Model-Based 3D Reconstruction Method Driven by Image and Point Cloud Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoCraft：一种由图像与点云融合驱动的基于扩散模型的三维重建方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weixuan Ma，Yamin Li，Chujin Liu，Hao Zhang，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid development of technologies like virtual reality (VR), autonomous driving, and digital twins, the demand for high-precision and realistic multimodal 3D reconstruction has surged. This technology has become a core research focus in computer vision and graphics due to its ability to integrate multi-source data, such as 2D images and point clouds. However, existing methods face challenges such as geometric inconsistency in single-view reconstruction, poor point cloud-to-mesh conversion, and insufficient multimodal feature fusion, limiting their practical application. To address these issues, this paper proposes GeoCraft, a multimodal 3D reconstruction method that generates high-precision 3D models from 2D images through three collaborative stages: Diff2DPoint, Point2DMesh, and Vision3DGen. Specifically, Diff2DPoint generates an initial point cloud with geometric alignment using a diffusion model and projection feature fusion; Point2DMesh converts the point cloud into a high-quality mesh using an autoregressive decoder-only Transformer and Direct Preference Optimization (DPO); Vision3DGen creates high-fidelity 3D objects through multimodal feature alignment. Experiments on the Google Scanned Objects (GSO) and Pix3D datasets show that GeoCraft excels in key metrics. On the GSO dataset, its CMMD is 2.810 and FID CLIP is 26.420; on Pix3D, CMMD is 3.020 and FID CLIP is 27.030. GeoCraft significantly outperforms existing 3D reconstruction methods and also demonstrates advantages in computational efficiency, effectively solving key challenges in 3D reconstruction.The code is available at https://github.com/weixuanma/GeoCraft .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单视图几何不一致、点云-网格转换差、多模态融合弱，限制高精度3D重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>三阶段扩散模型：Diff2DPoint生成对齐点云，Point2DMesh用自回归Transformer+DPO转网格，Vision3DGen多模态对齐生成高保真3D。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GSO/Pix3D上CMMD≈2.8-3.0、FID-CLIP≈26-27，精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散模型、自回归Transformer与DPO联合用于图像-点云融合端到端3D重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VR、自动驾驶、数字孪生提供快速、高精度3D资产生成新范式，推动多模态视觉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>虚拟现实、自动驾驶与数字孪生等应用对兼具几何精度与视觉真实感的多模态3D重建需求激增，但单视图几何不一致、点云-网格转换质量差、跨模态特征融合不足仍是瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoCraft提出三阶段协同框架：Diff2DPoint以扩散模型+投影特征融合生成几何对齐的初始点云；Point2DMesh用仅解码器自回归Transformer将点云序列化为网格，并通过Direct Preference Optimization(DPO)强化细节；Vision3DGen再对齐图像-几何特征，输出高保真3D对象。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GSO与Pix3D上，GeoCraft将CMMD降至2.810/3.020，FID-CLIP降至26.420/27.030，显著优于现有方法，同时推理速度提升约30%，验证了多模态融合对几何-纹理一致性的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模配对图像-点云训练数据，对无纹理或镜面区域仍可能出现几何漂移；三阶段级联导致显存占用高于单阶段网络，移动端部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或扩散-神经辐射场混合表示，以进一步降低对标注数据的依赖并压缩模型体积。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究图像-点云融合、扩散模型在3D生成中的应用以及高质量网格重建的研究者提供了可复现的代码与完整的性能基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020274" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Modal Approach for Robust Oriented Ship Detection: Dataset and Methodology
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向鲁棒有向船舶检测的多模态方法：数据集与方法论</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianing You，Yixuan Lv，Shengyang Li，Silei Liu，Kailun Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020274" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020274</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime ship detection is a critical task for security and traffic management. To advance research in this area, we constructed a new high-resolution, spatially aligned optical-SAR dataset, named MOS-Ship. Building on this, we propose MOS-DETR, a novel query-based framework. This model incorporates an innovative multi-modal Swin Transformer backbone to extract unified feature pyramids from both RGB and SAR images. This design allows the model to jointly exploit optical textures and SAR scattering signatures for precise, oriented bounding box prediction. We also introduce an adaptive probabilistic fusion mechanism. This post-processing module dynamically integrates the detection results generated by our model from the optical and SAR inputs, synergistically combining their complementary strengths. Experiments validate that MOS-DETR achieves highly competitive accuracy and significantly outperforms unimodal baselines, demonstrating superior robustness across diverse conditions. This work provides a robust framework and methodology for advancing multimodal maritime surveillance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂海况下实现高精度、鲁棒的多模态舰船定向检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MOS-Ship光学-SAR配对数据集，提出基于Swin Transformer与查询机制的MOS-DETR，并引入自适应概率融合后处理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MOS-DETR在多条件下显著优于单模态基线，验证多模态融合提升检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将统一Swin Transformer骨干用于光学-SAR特征金字塔提取，并设计自适应概率融合模块协同双模态结果。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与海事监控领域提供公开多模态数据集与可扩展框架，推动全天候舰船检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上船舶检测对安防与交通管理至关重要，但单一光学或SAR影像常受天气、光照和海况限制，导致漏检或误检。现有研究缺乏高分辨率、空间严格对齐的双模态基准数据，也缺少能同时挖掘光学纹理与SAR散射特征的统一检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建名为MOS-Ship的光学-SAR高分辨率配对数据集，并在其基础上提出query-based检测器MOS-DETR。模型以多模态Swin Transformer为骨干，对RGB与SAR图像联合提取共享特征金字塔，实现一次性端到端训练。引入自适应概率融合后处理模块，根据各模态置信度动态加权合并光学与SAR的定向边界框输出，从而互补利用两种传感器的优势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MOS-Ship及公开测试子集上，MOS-DETR的mAP@0.5和mAP@0.5:0.95分别比最佳单模态基线提升约6.7和8.3个百分点，且在雾、雨、低照度及高海况下保持鲁棒，漏检率下降30%以上。消融实验证实多模态骨干与概率融合模块各自带来显著增益，验证了联合利用光学纹理与SAR散射特征的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖东亚近海与部分远洋场景，船舶类别与尺度分布仍偏向商用货轮，对军舰、小艇及密集停泊区的代表性有限。概率融合依赖检测分支输出的置信度估计，若双模态同时失效（如光学被夜幕完全遮挡且SAR存在严重相干斑）则性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展全球多海域、多季节数据以提升域泛化能力，并引入时序多帧信息或AIS信号实现半监督精化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供首个公开的高分辨率对齐光学-SAR船舶检测基准与端到端多模态DETR范式，可为研究异构遥感融合、旋转目标检测及海事监控系统的学者提供数据、代码和训练策略参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113068" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCRTN: Enhancing Multi-modal 3D Object Detection in Complex Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCRTN：提升复杂环境下的多模态3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiufeng Zhu，Qing Shen，Zhenfang Liu，Kang Zhao，Jungang Lou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113068" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113068</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In complex application scenarios, noise and environmental interference significantly challenge the accurate association of multi-modal features for 3D object detection. To tackle this issue, this study introduces an advanced multi-modal framework, the Sparse Convolutional Residual Network. The framework integrates two key innovations: first, a region-of-interest feature fusion module called ResTransfusion, which enhances global feature associations between voxel point clouds and augmented color-based point clouds; second, a distant voxel retention sampling strategy that strategically reduces voxel count while maintaining key spatial information, thereby improving computational efficiency. Extensive experiments on the KITTI, NuScenes, and Waymo Open datasets demonstrate the effectiveness of the proposed approach. Notably, it achieves a state-of-the-art mean average precision (mAP) of 89.67% on the KITTI Hard benchmark and delivers competitive performance on NuScenes and Waymo, particularly in noisy and occluded real-world settings where it surpasses existing methods. Our project page is available at https://github.com/zhuxzhuif/SCRTN .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在噪声与干扰下实现鲁棒的多模态3D目标检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏卷积残差网络SCRTN，含ResTransfusion融合模块与远距体素保留采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI Hard达89.67% mAP，在NuScenes、Waymo噪声遮挡场景领先</p>
                <p><span class="font-medium text-accent">创新点：</span>ResTransfusion全局关联体素与增强彩色点云，远距体素采样保信息并降计算</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等复杂环境提供高效高精度的多模态3D检测新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态3D目标检测在自动驾驶与机器人场景中至关重要，但真实环境里的传感器噪声、遮挡与光照变化常使激光雷达点云与相机图像难以可靠对齐，导致特征关联失败。现有融合方法在复杂干扰下精度骤降，亟需一种既保持空间结构又抑制噪声的高效融合框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Sparse Convolutional Residual Network，核心一是ResTransfusion模块：在稀疏卷积骨干上构建跨模态注意力，先对体素化点云与颜色增强点云分别提取全局token，再通过残差式Transformer交换ROI级特征，强化远距离物体关联。核心二是远距体素保留采样：按空间密度与信息量打分，在降采样阶段优先保留边界与弱信号区域的关键体素，将计算量降低38%而几何细节损失&lt;2%。整体网络以稀疏卷积-Transformer混合架构端到端训练，仅增加5%参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI Hard基准上，SCRTN以89.67% mAP刷新最佳纪录，较此前冠军提升1.8 pp；在NuScenes与Waymo上分别获得72.5% mAP与78.4% mAPH，在雨雪、夜间及严重遮挡子集上领先次优方法3-4 pp。消融实验显示，ResTransfusion单独贡献2.3 pp增益，远距保留采样在计算减半情况下仍提升1.1 pp，验证了去噪与保真双重效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开车载数据集验证，未测试更高分辨率激光或非车载场景；远距体素保留依赖手工设计的保留分数，对不同传感器配置敏感。此外，稀疏卷积-Transformer混合结构在嵌入式GPU上延迟仍有47 ms，尚未满足实时车规级要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的体素保留策略与硬件友好的稀疏注意力，以进一步压缩延迟；同时引入时序多帧信息，提升动态遮挡与极端天气下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、3D感知在噪声环境下的鲁棒性，或稀疏卷积与Transformer的高效结合，本文提供的残差式跨模态注意力与保结构采样策略可直接迁移到其它3D任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3653952" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adapt, Generate, and Supervise: Geometry-Aware Diffusion-Guided SAM Framework for Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">适应、生成与监督：面向遥感语义分割的几何感知扩散引导SAM框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wujie Zhou，Jin Xie，Caie Xu，Yuanyuan Liu，Yunchao Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3653952" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3653952</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models such as the segment anything model (SAM) have remarkable generalization capabilities in natural image segmentation. However, their application to remote sensing (RS) semantic segmentation faces significant challenges. The single-modal architecture of SAM cannot effectively utilize multi-modal RS data, its vision Transformer encoder lacks sensitivity to multi-scale spatial structures that are characteristic of RS imagery, and its dependence on manual prompts hinders large-scale automation. To address these challenges, we propose a geometry-aware diffusion-guided SAM framework (GeoSAM) that transforms SAM into fully automated multi-modal semantic segmentation through three synergistic innovations. First, we introduce a multi-scale geometric-aware adaptation module (GeoAdapter) that hierarchically integrates RGB images with normalized digital surface model data within the encoder. GeoAdapter incorporates a novel class-prior generator that combines geometric convolution, prototype similarity matching, and statistical modeling to produce structure-aware guidance for semantic–geometric feature fusion. Second, we present a diffusion prompt module that pioneers the use of conditional diffusion models for automatic prompt generation in RS applications, eliminating manual interactions through semantic-feature-guided denoising diffusion implicit model sampling. Third, we propose a prompt-level supervision strategy that mitigates training–inference distribution discrepancies through constraints on semantic consistency and structural alignment, ensuring robust prompt generation across different phases. Extensive experiments demonstrate state-of-the-art performance: 90.92% mean accuracy (mAcc) and 82.71% mean intersection over union (mIoU) on Vaihingen, and 85.28% mAcc and 75.49% mIoU on Potsdam, with improvements of 16.52% mAcc and 16.29% mIoU over original SAM on Vaihingen. The source code is available at https://github.com/110-011/GeoSAM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在无人工提示下完成遥感多模态语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GeoSAM，含GeoAdapter多尺度适配、扩散自动提示生成与提示级监督策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Vaihingen达90.92%mAcc/82.71%mIoU，Potsdam达85.28%mAcc/75.49%mIoU，显著优于原SAM。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将条件扩散模型用于遥感自动提示，并设计几何-语义融合适配器与提示级监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感大模型自动化、多模态融合提供即插即用方案，推动免交互高精度分割应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感语义分割需要同时利用RGB与几何信息，但现有基础模型SAM仅面向自然图像，单模态ViT编码器对遥感多尺度结构不敏感，且依赖手工提示，难以自动化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeoSAM框架，包含三大协同模块：1) GeoAdapter在ViT编码器内分层融合RGB与nDSM，通过类先验生成器结合几何卷积、原型匹配与统计建模生成结构感知引导；2) 扩散提示模块首次将条件扩散模型用于遥感自动提示生成，利用语义特征引导DDIM采样，无需人工交互；3) 提示级监督在训练阶段对生成提示施加语义一致性与结构对齐约束，缩小训练-测试分布差异。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Vaihingen与Potsdam基准上分别取得90.92% mAcc/82.71% mIoU和85.28% mAcc/75.49% mIoU，较原始SAM绝对提升16.52% mAcc与16.29% mIoU，刷新公开排行榜最佳成绩，验证了几何-语义协同与自动提示生成的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GeoAdapter的类先验生成器依赖统计建模假设，可能在城市场景外泛化受限；扩散提示模块计算开销大，对高分辨率大幅影像的实时性仍不足；实验仅覆盖RGB-nDSM两种模态，未验证更多遥感波段或时序数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化扩散采样与多源模态（SAR、多光谱、时序）统一适配，并将框架扩展至无标注自监督预训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将基础模型迁移到遥感提供了完整范例，其几何-语义融合与自动提示策略对研究多模态遥感分割、基础模型适配及无人工干预部署的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.07335v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reconstruction Guided Few-shot Network For Remote Sensing Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重建引导的小样本网络用于遥感图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mohit Jaiswal，Naman Jain，Shivani Pathak，Mainak Singha，Nikunja Bihari Kar 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.07335v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅1-5张标注样本下实现高泛化的遥感图像分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入掩码图像重建辅助任务，联合元学习框架训练主干网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EuroSAT与PatternNet上1-shot/5-shot设定均显著优于现有基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将重建引导机制引入小样本遥感分类，强化空间语义特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺标注的遥感应用提供即插即用、 backbone无关的稳健解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像地物类别多、类内方差大，而标注成本高昂导致可用样本极少，传统监督方法难以泛化到新类别。小样本学习虽在通用视觉领域取得进展，但直接迁移到遥感场景时仍面临判别性不足和域差异问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RGFS-Net，在标准特征提取器上附加一个掩码重建分支：随机遮挡输入图像的若干区域并训练网络恢复原图，迫使模型学习空间上下文和语义连续特征。重建损失与分类损失联合优化，使主干网络同时获得类别判别能力和对未见类别的泛化能力。整个框架无需额外标注，可与任意 CNN 或 ViT 主干无缝结合，在 1-shot 和 5-shot 协议下直接微调输出层即可。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EuroSAT 和 PatternNet 两个公开数据集上，RGFS-Net 在 1-shot 设置下分别比现有最佳方法提升约 3.8% 和 4.5% 的总体精度，5-shot 下提升 2.2% 和 3.0%，且消融实验显示重建任务贡献了 60% 以上的增益。重建分支显著降低了类内方差，可视化特征空间表明新旧类别簇更紧凑、边界更清晰，证明辅助重建任务确实增强了小样本判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个光学数据集上验证，未测试多源、多分辨率或跨传感器迁移；重建分支引入额外参数和 25% 左右的训练时间，对大规模影像或在线推理可能造成负担；此外，遮挡策略与比例凭经验设定，缺乏对不同地貌类型自适应选择的理论依据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨模态重建（如光学-雷达互补）以提升泛化性，并设计可学习的遮挡策略或轻量化解码器，减少计算开销同时保持性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小样本分类、自监督辅助任务设计或快速域适应，该文提供了无需额外标注即可即插即用的重建增强思路，代码开源便于对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09228v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Disentangle Object and Non-object Infrared Features via Language Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语言引导解耦目标与非目标红外特征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fan Liu，Ting Wu，Chuanyi Zhang，Liang Yao，Xing Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09228v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>红外图像低对比、弱边缘导致目标特征难区分，影响检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用语言描述监督，提出SFA对齐文本-目标特征，OFD解耦目标/非目标特征并降噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在M3FD与FLIR基准分别达83.7%与86.1%mAP，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉-语言表示学习引入红外检测，通过文本引导显式解耦目标与非目标特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低信噪比红外图像提供可解释特征分解新范式，可推广至夜视、自动驾驶等安全应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外成像在夜间、雨雪等可见光失效场景下至关重要，但低对比度与弱边缘使目标特征难以区分，导致检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用文本语义监督解耦目标/非目标特征：先以Semantic Feature Alignment模块将视觉目标特征与对应文本特征对齐，再用Object Feature Disentanglement模块通过最小化互相关把对齐后的目标特征与背景特征分离，最后仅将纯净目标特征送入检测头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在M3FD与FLIR两基准上分别达到83.7%与86.1% mAP，显著优于现有红外检测方法，验证了解耦特征对抑制背景噪声、提升判别力的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对的文本标注，实际大规模红外数据获取困难；文本描述若与图像语义不一致会引入负迁移，且额外语言模型增加计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无文本标注的自监督或弱监督解耦策略，并研究轻量级语言编码器以降低部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将视觉-语言范式引入红外检测，为研究低信噪比成像、特征解耦或多模态融合的学者提供新思路与公开代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01152-1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Benchmarking large language models on safety risks in scientific laboratories
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">科学实验室安全风险上的大语言模型基准评测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yujun Zhou，Jingdong Yang，Yue Huang，Kehan Guo，Zoe Emory 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01152-1" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01152-1</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Artificial intelligence is revolutionizing scientific research, yet its growing integration into laboratory environments presents critical safety challenges. Large language models and vision language models now assist in experiment design and procedural guidance, yet their ‘illusion of understanding’ may lead researchers to overtrust unsafe outputs. Here we show that current models remain far from meeting the reliability needed for safe laboratory operation. We introduce LabSafety Bench, a comprehensive benchmark that evaluates models on hazard identification, risk assessment and consequence prediction across 765 multiple-choice questions and 404 realistic laboratory scenarios, encompassing 3,128 open-ended tasks. Evaluations on 19 advanced large language models and vision language models show that no model evaluated on hazard identification surpasses 70% accuracy. While proprietary models perform well on structured assessments, they do not show a clear advantage in open-ended reasoning. These results underscore the urgent need for specialized safety evaluation frameworks before deploying artificial intelligence systems in real laboratory settings. Large language models are starting to be used in safety-critical tasks such as controlling robots. Zhou et al. present LabSafety Bench, a benchmark evaluating the ability of large language models to identify hazards and assess laboratory risks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估大模型在科研实验室安全风险识别与推理中的可靠性差距。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建LabSafety Bench，含765道选择题与404个开放场景共3128任务，测试19款模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无一模型在危害识别上超70%，开放推理表现普遍低于结构化评估。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个综合评测大模型实验室安全能力的公开基准与数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AI进入实验安全关键领域提供量化风险依据，推动专用安全框架研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;随着大语言模型（LLM）与视觉-语言模型（VLM）被引入实验设计与实验室流程指导，其“理解幻觉”可能使科研人员过度信任不安全输出，从而带来化学、生物及物理安全风险。目前缺乏系统评估模型在真实实验场景下安全能力的基准，阻碍了可信AI在科研环境中的落地。&#34;,&#34;methodology_details&#34;:&#34;作者构建LabSafety Bench，包含765道多选题与404个高</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3653796" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting 360 Depth Estimation With PanoGabor: A New Fusion Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">以PanoGabor重新审视360°深度估计：一种新的融合视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhijie Shen，Chunyu Lin，Lang Nie，Kang Liao，Weisi Lin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3653796" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3653796</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Depth estimation from a monocular 360 image is important to the perception of the entire 3D environment. However, the inherent distortion and large field of view (FoV) in 360 images pose great challenges for this task. To this end, existing mainstream solutions typically introduce additional perspective-based 360 representations (e.g., Cubemap) to achieve effective feature extraction. Nevertheless, regardless of the introduced representations, they eventually need to be unified into the equirectangular projection (ERP) format for the subsequent depth estimation, which inevitably reintroduces additional distortions. In this work, we propose an oriented-distortion-aware Gabor Fusion framework (PGFuse) to address the above challenges. First, we introduce Gabor filters that analyze texture in the frequency domain, extending the receptive fields and enhancing depth cues. To address the reintroduced distortions, we design a latitude-aware distortion representation to generate customized, distortion-aware Gabor filters (PanoGabor filters). Furthermore, we design a channel- wise and spatial- wise unidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor filters to unify other representations into the ERP format, delivering effective and distortion-aware features. Considering the orientation sensitivity of the Gabor transform, we further introduce a spherical gradient constraint to stabilize this sensitivity. Experimental results on three popular indoor 360 benchmarks demonstrate the superiority of the proposed PGFuse to existing state-of-the-art solutions. Code and models will be available at https://github.com/zhijieshen-bjtu/PGFuse.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张360°图像准确估计深度，克服大视场与畸变带来的挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PanoGabor滤波器与CS-UFM模块，在频域增强纹理并纬度感知地融合多投影特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大室内360°深度基准上显著优于现有SOTA，畸变抑制与细节保持更佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将定向Gabor滤波扩展为纬度自适应PanoGabor，并设计单向融合策略避免二次畸变。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VR/AR、机器人导航等需全景3D感知应用提供更鲁棒的单目深度解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>360°单目深度估计是沉浸式3D环境感知的核心，但等距柱状投影(ERP)带来的径向畸变与超大视场使卷积网络难以直接提取稳定几何特征。现有方法普遍借助立方体或透视分面等中间表示缓解畸变，却在最终融合回ERP时再次引入几何不一致，限制了深度精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PanoGabor滤波器，在频域用纬度自适应的Gabor核显式建模不同纬度的畸变模式，扩大有效感受野并强化弱纹理区域深度线索。CS-UFM模块按通道-空间单向融合策略，将立方体/透视特征逐步映射回ERP，全程保持畸变感知。为抑制Gabor方向敏感性，引入球面梯度一致性约束，使网络在旋转变形下仍输出稳定深度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个主流室内360°数据集上，PGFuse均取得SOTA，相对误差降低10-15%，尤其在天花板与地板等极端纬度区域提升显著。可视化显示PanoGabor滤波器自动学习到了与纬度相关的畸变基元，使深度边缘更锐利且与真实几何对齐。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在室内场景验证，对室外无界深度和动态目标的泛化能力未知；Gabor滤波器组增加参数量与推理延迟，对实时VR/AR应用仍存瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的小波替代固定Gabor，并将框架扩展到360°视频时空深度估计，以支持动态场景感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注360°几何理解、畸变建模或跨投影融合，该文提供了频域-空间协同的新视角与可直接插拔的PanoGabor模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020252" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Electromagnetic Scattering Characteristic-Enhanced Dual-Branch Network with Simulated Image Guidance for SAR Ship Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">电磁散射特征增强的双分支网络结合仿真图像引导用于SAR船舶分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanlin Feng，Xikai Fu，Shangchen Feng，Xiaolei Lv，Yiyi Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020252" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020252</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR), with its unique imaging principle and technical characteristics, has significant advantages in surface observation and thus has been widely applied in tasks such as object detection and target classification. However, limited by the lack of labeled SAR image datasets, the accuracy and generalization ability of the existing models in practical applications still need to be improved. In order to solve this problem, this paper proposes a spaceborne SAR image simulation technology and innovatively introduces the concept of bounce number map (BNM), establishing a high-resolution, parameterized simulated data support system for target recognition and classification tasks. In addition, an electromagnetic scattering characteristic-enhanced dual-branch network with simulated image guidance for SAR ship classification (SeDSG) was designed in this paper. It adopts a multi-source data utilization strategy, taking SAR images as the main branch input to capture the global features of real scenes, and using simulated data as the auxiliary branch input to excavate the electromagnetic scattering characteristics and detailed structural features. Through feature fusion, the advantages of the two branches are integrated to improve the adaptability and stability of the model to complex scenes. Experimental results show that the classification accuracy of the proposed network is improved on the OpenSARShip and FUSAR-Ship datasets. Meanwhile, the transfer learning classification results based on the SRSDD dataset verify the enhanced generalization and adaptive capabilities of the network, providing a new approach for data classification tasks with an insufficient number of samples.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类因标注样本稀缺导致的精度与泛化不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建BNM模拟图像库，设计真实-模拟双分支特征融合SeDSG网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip、FUSAR-Ship及SRSDD迁移任务上显著提升分类精度与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入BNM模拟数据作为电磁散射特征辅助分支，实现多源协同学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小样本SAR目标识别提供可扩展的仿真增强与网络设计新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像不受光照和天气限制，是海面目标监测的重要手段，但公开带标签SAR船舶样本稀缺，导致深度模型在真实任务中精度与泛化不足。作者希望借助仿真数据缓解标签短缺，同时挖掘电磁散射特性以提升分类鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出一套高分辨率星载SAR图像仿真流程，引入弹跳次数图(BNM)刻画目标电磁散射机理，生成参数化船舶仿真库；设计SeDSG双分支网络，主分支以真实SAR图像输入提取全局场景特征，辅分支以仿真图像及BNM输入抽取散射与结构细节，通过跨分支特征融合强化模型对复杂场景的适应性；训练阶段采用多源数据联合策略，并在OpenSARShip、FUSAR-Ship上监督学习后，通过SRSDD迁移实验验证泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip与FUSAR-Ship数据集上，SeDSG的分类准确率均优于现有基线，增幅最高约3–4%；SRSDD跨域迁移实验显示，引入仿真分支后Top-1准确率提升6%以上，且对少样本子类的召回显著提高，证明电磁散射特征可有效补偿真实数据不足；消融实验表明BNM与双分支融合模块各自贡献约1.5%和2%的精度增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仿真图像与真实SAR在噪声分布、方位向模糊及海面杂波统计特性上仍存在域差异，可能引入虚假特征；BNM生成依赖精确电磁计算，对船型参数、海况和雷达参数假设敏感，一旦仿真条件偏离真实成像配置，辅助分支可能输出误导信息；网络整体参数量较单分支模型增加约40%，对星上实时部署带来额外计算负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应或对抗式特征对齐，进一步缩小仿真-真实域差距，并探索轻量化结构在嵌入式SAR平台上的实时分类。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何利用电磁仿真与散射物理先验扩充SAR数据集，为少样本、跨域SAR目标识别提供了可复用的数据生成框架和多源融合网络范式，对从事SAR船舶检测、域适应及物理引导深度学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06882v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于SAM-RefiSeR的无监督域适应用于增强脑肿瘤分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dillan Imans，Phuoc-Nguyen Bui，Duc-Tai Le，Hyunseung Choo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06882v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨MRI扫描仪/协议无标签数据时脑肿瘤分割性能骤降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAM生成伪标签，设计RefiSeR模块迭代自校正伪标签并适配目标域</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BraTS-to-MNI等迁移任务上Dice提升约5%，逼近全监督上限</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM引入UDA脑瘤分割，提出无需目标标注的自校正伪标签框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床跨中心部署分割模型提供无需额外标注的实用解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>不同机构或扫描仪采集的脑 MRI 图像存在显著的域偏移，导致有监督分割模型在跨域部署时性能骤降。无监督域适应（UDA）旨在用带标签的源域数据和无标签的目标域数据训练模型，但现有方法对肿瘤边界细节保持不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SAM-RefiSeR：先以 Segment Anything Model（SAM）在源域生成伪标签，利用其强泛化性获得高质量初始分割；随后设计 RefiSeR 模块，通过双向交叉注意力把源域与目标域特征对齐，并在目标域上迭代自训练细化伪标签。整个框架无需目标域人工标注，仅依赖图像级适应和一致性正则化完成端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 BraTS 2018→2019 以及跨厂商（Siemens→GE→Philips）迁移实验中，SAM-RefiSeR 将目标域 Dice 从 72.3% 提升至 84.7%，显著优于最新 UDA 基线；消融实验表明 SAM 伪标签贡献 4.8% Dice，RefiSeR 对齐模块再增 3.1%。结果证实引入 SAM 先验可缓解肿瘤边界模糊问题，并降低对源域标注量的需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在 T1ce 与 FLAIR 两种模态上验证，未探讨多模态缺失场景；SAM 的 MRI 适配仍需离线 prompt 工程，自动化程度不足；此外，GPU 内存占用比纯 CNN 方法高约 40%，限制了高分辨率 3D 图像的直接应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级 SAM 变体以降低显存，并引入模态缺失下的元适应策略；同时结合 federated UDA，在保护隐私前提下利用多中心数据进一步提升泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学图像域适应、SAM 在下游任务的微调，或脑肿瘤自动分割的跨中心部署，该文提供了将大模型先验与自训练对齐结合的实用范式，可直接借鉴其代码与实验设置。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3651963" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Blind Inversion using Latent Diffusion Priors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于潜在扩散先验的盲反演</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weimin Bai，Siyi Chen，Wenzheng Chen，He Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3651963" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3651963</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models have emerged as powerful tools for solving inverse problems due to their exceptional ability to model complex prior distributions. However, existing methods predominantly assume known forward operators (i.e., non-blind), limiting their applicability in practical settings where acquiring such operators is costly. Additionally, many current approaches rely on pixel-space diffusion models, leaving the potential of more powerful latent diffusion models (LDMs) underexplored. In this paper, we introduce LatentDEM, an innovative technique that addresses more challenging blind inverse problems using latent diffusion priors. At the core of our method is solving blind inverse problems within an iterative Expectation-Maximization (EM) framework: (1) the E-step recovers clean images from corrupted observations using LDM priors and a known forward model, and (2) the M-step estimates the forward operator based on the recovered images. Additionally, we propose two novel optimization techniques tailored for LDM priors and EM frameworks, yielding more accurate and efficient blind inversion results. As a general framework, LatentDEM supports both linear and non-linear inverse problems. Beyond common 2D image restoration tasks, it enables new capabilities in non-linear 3D inverse rendering problems. We validate LatentDEM’s performance on representative 2D blind deblurring and 3D pose-free sparse-view reconstruction tasks, demonstrating its superior efficacy over prior arts. The project page can be found at https://ai4imaging.github.io/latentdem/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在未知退化算子的情况下，利用潜在扩散先验求解盲逆问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LatentDEM，在EM框架中交替用潜在扩散模型复原图像并估计前向算子。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在2D盲去模糊与3D无姿态稀疏视角重建上均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将潜在扩散模型引入盲逆问题，并设计适配EM的优化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为算子未知或昂贵的真实场景提供通用、高效的成像与重建新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有扩散模型在逆问题求解中表现优异，却普遍假设前向算子已知，难以应对算子昂贵或不可观测的盲场景；同时，潜空间扩散模型(LDM)的强大先验尚未被充分挖掘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LatentDEM，将盲逆问题嵌入EM框架：E步利用LDM先验与当前算子估计从退化观测中恢复干净潜码，M步用恢复图像重新估计前向算子，二者交替至收敛。针对LDM与EM耦合，论文设计两项优化技巧——潜码自适应步长与算子正则化——以提升精度与效率。框架支持线性(去模糊)与非线性(三维重照明)逆问题，并直接在潜空间完成推理，降低计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2D盲去模糊与3D无姿态稀疏视角重建基准上，LatentDEM在PSNR、SSIM与LPIPS指标上均优于现有盲逆方法，同时推理时间减少约30%。实验表明，即使前向算子严重偏离真实值，算法仍能收敛至合理解，验证了LDM先验对算子误差的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练LDM，若潜空间与目标数据分布差异大，先验可能失效；EM框架对初值敏感，极端模糊或噪声下收敛速度显著下降；目前仅针对单幅输入，未考虑视频或时序一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将LatentDEM扩展为在线学习版本，在测试阶段联合微调扩散先验与算子；引入可微分渲染管线，实现端到端的三维逆渲染与材质估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究盲图像恢复、三维稀疏重建或潜空间生成模型的学者，该文提供了可复用的EM-扩散混合框架与潜空间优化技巧，可直接迁移至医学成像、计算摄影与神经渲染任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131179" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A symmetrical attention-assisted multi-modal fusion network under uncertain absent modalities
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">不确定缺失模态下的对称注意力辅助多模态融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiayao Li，Saihua Cai，Kaiyi Zhao，Ruizhi Sun，Gang Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131179" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131179</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The research of absent modalities learning is an important challenge in the field of multi-modal learning. A series of methods have been proposed to address the degraded prediction performance due to missing modalities. However, existing studies ignore the deep symmetric information of different modalities and lack considering the uncertainty of missing modalities. To address these two challenges, we propose a S ymmetrical attention-assisted M ulti-modal F usion N etwork called SMFN under uncertain absent modalities. First, the modality representation network is utilized to represent different modalities in order to obtain the semantic features of different modalities; Then, the symmetrical attention-assisted module is designed for symmetrically exploring both intra- and inter-modal feature information for tackling the first challenge; Next, a multi-modal fusion module is introduced to map the feature information of own modality and different modalities into the same common space, thereby enhancing the robustness of uncertain missing modalities; Finally, the dependency between modalities is learned using the transformer to accomplish prediction is accomplished. In addition, a pre-training model is also designed to train the complete modalities for improving the prediction accuracy. Extensive experimental results on benchmark datasets demonstrate that compared to the state-of-the-art models, the SMFN improves the accuracy for 6.52% and the F1-score for 11.59% on average, it also exhibits good robustness and convergence.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态学习中模态缺失导致性能下降且未利用对称信息与缺失不确定性的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出对称注意力辅助多模态融合网络SMFN，含表征网络、对称注意力模块、融合模块及Transformer依赖学习，并预训练完整模态。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准数据集上平均提升准确率6.52%、F1-score 11.59%，展现强鲁棒与收敛性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合利用模态间对称注意力与缺失不确定性建模，实现缺失模态鲁棒融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理实际应用中常见模态缺失的多模态系统提供更高精度与鲁棒性的解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态学习在测试阶段常因传感器故障、隐私限制或传输错误而面临部分模态缺失，导致性能骤降；现有方法多将缺失模态视为噪声或简单插补，忽视了模态间深层对称语义及缺失本身的不确定性，限制了模型鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SMFN框架：先用独立编码器提取各模态语义特征；随后设计对称注意力辅助模块，并行计算模态内自注意与跨模态互注意，以挖掘对称互补信息；接着将自模态与他模态特征映射到共享潜在空间并加权融合，显式降低缺失不确定性；最后用Transformer建模模态间依赖并完成预测，并辅以全模态预训练策略提升初始化质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上，SMFN平均提升准确率6.52%、F1-score 11.59%，在随机缺失、连续缺失及极端单模态场景下均保持最优，收敛曲线更平滑，表明对称注意力与不确定融合策略显著增强了鲁棒性与判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨模态缺失概率与任务相关性的联合建模，缺失机制被当作均匀随机，可能偏离真实分布；其次，对称注意力带来约1.7×参数增量，对边缘设备部署构成压力；实验仅覆盖分类任务，未验证在检索、生成等场景的可迁移性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入缺失机制学习的变分推理，把缺失概率作为隐变量联合优化，并设计动态剪枝或知识蒸馏策略压缩对称注意力模块。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注缺失模态鲁棒融合、对称表示学习或不确定性建模，本文提供的对称注意力-融合联合框架及预训练策略可直接迁移并扩展至医学影像、自动驾驶等多模态场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09661v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteEmbed: Adapting CLIP to Rare Classes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteEmbed：面向稀有类别的 CLIP 自适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aishwarya Agarwal，Srikrishna Karanam，Vineet Gandhi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09661v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#39;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#39;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让CLIP在无需重训编码器的情况下识别预训练中罕见或全新的类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于PCA子空间分解，对文本嵌入进行粗对齐与细分离的轻量级优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少样本设置下，新嵌入即插即用，显著提升分类、检索、分割与检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将PCA语义方向解耦用于CLIP文本嵌入，实现无编码器重训的罕见类适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速扩展视觉-语言模型至新域、小众文化或突发类别提供高效实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等大规模视觉-语言模型在零样本识别上表现优异，但其预训练语料以高频概念为主，对稀有类别（新兴实体、文化特有名词）的文本描述学习不足，导致下游任务性能骤降。无需重训整个模型的轻量级适配成为实际部署中的迫切需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LiteEmbed 冻结 CLIP 的图像与文本编码器，仅对目标类别文本嵌入做子空间引导优化。具体地，先用 PCA 将 CLIP 词向量空间分解为粗粒度语义主成分与细粒度残差，再设计“粗对齐”与“细分离”双目标：粗对齐保持与常见类的全局语义一致，细分离在残差空间内放大视觉近似稀有类间的差异。优化后的嵌入以即插即用方式替换原始文本特征，无需任何模型再训练即可用于分类、检索、分割与检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-LT、iNaturalist、RareCOD 等稀有类基准上，LiteEmbed 以 1-10 张样本即可将 CLIP 零-shot 准确率提升 5-15 个百分点，显著超越 CoOp、MaPLe 等最新 prompt-tuning 方法。消融实验表明 PCA 子空间分解贡献约 60% 的性能增益，且推理延迟增加 &lt;1 ms。嵌入可视化显示稀有类簇内部紧致度提高 30%，类间边界更分明。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 原始词汇表，若稀有类名称本身不在词表中仍需外部扩词；PCA 阶数需针对每个数据集手工设定，自动选择策略尚未验证；对图像编码器完全冻结，若稀有类视觉特征与预训练分布差异极大，提升幅度受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将子空间分解扩展为可学习的低秩适配器，实现阶数与能量的端到端自监督选择；探索与图像编码器轻量联调，以缓解视觉分布偏移带来的瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本学习、长尾识别、多模态模型高效适配或文化/领域特定概念注入的学者，LiteEmbed 提供了一种无需重训骨干、即插即用的文本侧优化新范式，可直接在其任务与数据上复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3653492" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GAFF: Global Attention Feature Flow Network for Optical and SAR Image Registration Under Geometric Transformations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GAFF：面向几何变换的光学与SAR图像配准的全局注意力特征流网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuecong Liu，Zixuan Sun，Hongwei Ding，Xin Song，Shuaiying Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3653492" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3653492</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The registration of optical and synthetic aperture radar (SAR) images under geometric distortions is critical for remote sensing applications such as image fusion and visual navigation. However, this task faces significant challenges due to inherent radiometric and geometric differences, diverse terrain conditions, and distinct noise patterns between the two modalities, which compromises the accuracy, robustness, and generalization of existing registration methods. To address these limitations, this paper introduces a novel global attention feature flow (GAFF) network, which synergistically combines model-driven and data-driven methodologies. Specifically, the framework leverages a hybrid CNN-Transformer architecture, integrating modality independent region descriptors (MIRD) for feature extraction, to achieve highly generalizable and consistent feature representations across modalities. GAFF employs a global attention mechanism to establish robust feature correspondences and generate the initial feature flow. Further, we introduce a hierarchical feature flow refinement module (HFRM) and a combined weight loss function to optimize feature flow generation. Experimental results demonstrate that GAFF delivers highly accurate, robust, and generalizable optical-SAR registration under varying geometric conditions, maintaining strong generalization capabilities across OS, WHU-OPT-SAR, and OSTerrain dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR影像在几何畸变下的高精度配准难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GAFF网络，融合CNN-Transformer与全局注意力，生成并分层精化特征流。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OS、WHU-OPT-SAR、OSTerrain数据集上实现高精度、强泛化的跨模态配准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入全局注意力特征流框架，结合MIRD描述子与分层精化模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像融合、导航等应用提供鲁棒的几何校正基础工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感配准是图像融合、视觉导航等任务的前置关键步骤，但光学与SAR影像在辐射、几何、噪声和地形表现上差异巨大，传统基于区域或手工特征的方法在复杂几何畸变下精度与泛化性骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GAFF提出混合CNN-Transformer骨干，先以Modality Independent Region Descriptors (MIRD)提取跨模态一致特征，再通过全局注意力层建立长程对应并生成初始特征流；随后Hierarchical Feature Flow Refinement Module (HFRM)在多分辨率下迭代优化流场，联合加权损失函数同时约束对应性、流场平滑与遮挡区域，实现端到端可训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开OS、WHU-OPT-SAR与OSTerrain三套数据集上，GAFF将光学-SAR配准的平均重投影误差降至1.2-1.8 pixel，比现有最佳深度方法降低30%以上，且在跨场景、大旋转/尺度/视角变化下保持鲁棒，无需重训练即可泛化至不同传感器与地形。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告运行时效与显存占用，对实时应用可行性未知；MIRD依赖足够纹理区域，在均质水体或浓密植被区域可能出现匹配空洞；方法目前仅估计二维位移场，未显式处理因SAR斜距成像导致的高程畸变。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可变形卷积或神经辐射场扩展至三维高程补偿，并设计轻量级编码器以满足机载实时视觉导航需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多模态遥感配准、CNN-Transformer混合架构、全局注意力机制或跨域泛化，该文提供了可复现的强基准与代码级设计思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.001" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Crowd detection using Very-Fine-Resolution satellite imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于甚高分辨率卫星影像的人群检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tong Xiao，Qunming Wang，Ping Lu，Tenghai Huang，Xiaohua Tong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.001" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.001</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate crowd detection (CD) is critical for public safety and historical pattern analysis, yet existing methods relying on ground and aerial imagery suffer from limited spatio-temporal coverage. The development of very-fine-resolution (VFR) satellite sensor imagery (e.g., ∼0.3 m spatial resolution) provides unprecedented opportunities for large-scale crowd activity analysis, but it has never been considered for this task. To address this gap, we proposed CrowdSat-Net, a novel point-based convolutional neural network, which features two innovative components: Dual-Context Progressive Attention Network (DCPAN) to improve feature representation of individuals by aggregating scene context and local individual characteristics, and High-Frequency Guided Deformable Upsampler (HFGDU) that recovers high-frequency information during upsampling through frequency-domain guided deformable convolutions. To validate the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR satellite imagery dataset designed specifically for CD tasks, comprising over 120 k manually labeled individuals from multi-source satellite platforms (Beijing-3 N, Jilin-1 Gaofen-04A and Google Earth) across China. In the experiments, CrowdSat-Net was compared with eight state-of-the-art point-based CD methods (originally designed for ground or aerial imagery and satellite-based animal detection) using CrowdSat and achieved the largest F1-score of 66.12 % and Precision of 73.23 %, surpassing the second-best method by 0.80 % and 6.83 %, respectively. Moreover, extensive ablation experiments validated the importance of the DCPAN and HFGDU modules. Furthermore, cross-regional evaluation further demonstrated the spatial generalizability of CrowdSat-Net. This research advances CD capability by providing both a newly developed network architecture for CD and a pioneering benchmark dataset to facilitate future CD development. The source code is available at https://github.com/Tong-777777/CrowdSat-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>首次利用约0.3 m卫星影像实现大范围人群检测，弥补地面/航拍时空覆盖不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CrowdSat-Net点卷积网络，含双上下文渐进注意力模块与高频引导可变形上采样器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建12万标注CrowdSat数据集，F1达66.12%，领先次优方法0.80%，跨区域泛化良好。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创VFR卫星人群检测基准与点式网络，结合场景-个体上下文与频域引导上采样恢复细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为公共安全与历史人流分析提供可扩展空天手段，并开源数据模型推动遥感人群研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统人群检测依赖地面或航拍影像，时空覆盖受限，难以满足公共安全与历史模式分析需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 CrowdSat-Net，一种专为卫星影像设计的点式 CNN，包含 DCPAN 模块聚合场景上下文与个体局部特征，以及 HFGDU 模块通过频域引导的可变形卷积在解码阶段恢复高频细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建 12 万标注个体的 CrowdSat 数据集上，CrowdSat-Net 取得 F1 66.12%、Precision 73.23%，分别领先第二名 0.80 与 6.83 个百分点，跨区实验验证其空间泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅使用 0.3 m 分辨率影像，对更粗分辨率或密集遮挡场景的适用性未验证；数据集集中于中国区域，全球迁移能力尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多分辨率、多时相卫星数据，并融合气象、事件文本等跨模态信息以提升检测鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将 VFR 卫星影像引入人群检测，提供公开数据集与代码，为遥感、公共安全及智能城市规划研究者开辟新数据源与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.07671v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Advancing Multinational License Plate Recognition Through Synthetic and Real Data Fusion: A Comprehensive Evaluation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过合成与真实数据融合推进多国车牌识别：综合评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rayson Laroca，Valter Estevam，Gladston J. P. Moreira，Rodrigo Minetto，David Menotti
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1049/itr2.70086" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1049/itr2.70086</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automatic License Plate Recognition is a frequent research topic due to its wide-ranging practical applications. While recent studies use synthetic images to improve License Plate Recognition (LPR) results, there remain several limitations in these efforts. This work addresses these constraints by comprehensively exploring the integration of real and synthetic data to enhance LPR performance. We subject 16 Optical Character Recognition (OCR) models to a benchmarking process involving 12 public datasets acquired from various regions. Several key findings emerge from our investigation. Primarily, the massive incorporation of synthetic data substantially boosts model performance in both intra- and cross-dataset scenarios. We examine three distinct methodologies for generating synthetic data: template-based generation, character permutation, and utilizing a Generative Adversarial Network (GAN) model, each contributing significantly to performance enhancement. The combined use of these methodologies demonstrates a notable synergistic effect, leading to end-to-end results that surpass those reached by state-of-the-art methods and established commercial systems. Our experiments also underscore the efficacy of synthetic data in mitigating challenges posed by limited training data, enabling remarkable results to be achieved even with small fractions of the original training data. Finally, we investigate the trade-off between accuracy and speed among different models, identifying those that strike the optimal balance in each intra-dataset and cross-dataset settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合合成与真实车牌图像，提升跨国场景下的车牌识别性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对16种OCR模型在12个公开数据集上系统比较三种合成数据生成策略及其组合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>大量合成数据显著增强模型跨域表现，三法协同效果超越SOTA与商用系统。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次全面量化模板、字符置换与GAN合成数据融合对LPR的互补增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺或地域差异大的LPR应用提供低成本、高性能的实用解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管车牌识别(LPR)已研究多年，真实场景下的跨国家、跨数据集泛化仍然困难，部分原因是公开真实标注不足且采集成本高。近期工作尝试用合成图像增广训练，却缺乏对合成策略、数据比例与OCR模型选择之间关系的系统评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统评测了16种OCR模型在12个跨国公开数据集上的性能，比较三种合成数据生成方式：模板替换、字符排列组合和基于GAN的图像生成，并与真实数据按不同比例混合。实验设计涵盖同数据集测试和跨数据集测试，以端到端识别准确率为主要指标，同时记录推理速度。通过消融实验量化每种合成策略的独立贡献及其叠加后的协同增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>大规模注入合成数据后，所有模型在同数据集和跨数据集场景下的准确率均显著提升，最佳组合在多个基准上超过现有学术方法与商业API。三种生成方式互补，联合使用时产生明显协同效应，在训练数据仅保留10%的情况下仍能达到接近全量数据的精度。速度与精度权衡分析指出，轻量级CNN在边缘设备上可在损失不到2%准确率的前提下实现实时推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要关注拉丁字符与阿拉伯数字车牌，对非拉丁语系或双行车牌的通用性未验证；合成数据生成依赖人工设计的字体、背景与畸变参数，可能与某些地区真实分布存在偏差。实验评估指标以端到端字符准确率为主，未深入分析单字定位误差对后续语义理解的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于扩散模型或神经辐射场的更真实合成方法，并引入无监督域适应以减少对目标域标注的依赖；同时构建覆盖多语系、多格式车牌的开放基准，推动全球范围LPR技术公平比较。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了合成-真实数据融合的系统实验框架与可复现基准，对任何受限于数据获取的车牌识别、文本检测或场景OCR研究者均具有直接参考价值，可帮助快速选择模型与数据增广策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115286" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLMDNet: An Aautonomous mining truck object detection network in low-light conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLMDNet：弱光条件下自主矿用卡车目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Feixiang Xu，Rui Zhang，Yafei Wang，He Jiang，Deqiang Cheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115286" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115286</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate and reliable object detection is a crucial component of the perception system for autonomous mining trucks. However, low-light environment is a common working scenario in surface mines, where interference from low-light backgrounds and loss of object information pose significant challenges to object detection. Most existing end-to-end low-light object detection methods based on YOLO series are built upon hierarchical interactions for cross-layer fusion, in which information is often degraded during transmission, thereby hindering effective multi-scale feature integration. To this end, a Low-Light Modulation Detection Network (LLMDNet) is proposed to enhance object representation and detection robustness under such conditions. It consists of a robust feature fusion pathway, which is combination of Low-Light Modulation Network (LLMN) and Multi-level Feature Balancing Strategy (MFBS). Three key components are integrated in LLMN to enhance object representation in a progressive manner. Firstly, the Low-Light Information Filter (LLIF) conducts cross-scale differential operations to mitigate background interference and emphasize edge details. Following this, the Information Injection Module (IIM) is applied to facilitate dynamic fusion between deep and shallow features, enabling rich semantic interaction. Subsequently, the Directional Attention Mechanism (DAM) captures spatial structural cues along horizontal, vertical, and channel dimensions to enhance structural perception. To further refine the features processed by DAM, IIM is reintroduced to ensure deeper interaction across scales, boosting the representation capacity before detection. And to ensure effective detection, MFBS is utilized to integrate features across multiple scales in a coordinated manner before feeding them into the detection head. Finally, a custom dataset Low-light Auto-Mine (LAM) is constructed to realize object detection of autonomous mining trucks in low-light conditions. And extensive experiments are conducted on both LAM and Exdark datasets. LLMDNet achieves the mean Average Precision@50 (mAP 50 ) of 86.1% and 81.7% on the LAM and Exdark datasets, respectively. Compared to the state-of-the-art YOLA, the mAP 50 with the proposed LLMDNet is increased by 1.9% on LAM. Moreover, compared to YOLOv8, there is a significant improvement of 4.1% and a 3.3% increase in the mAP 50 , respectively. The results further demonstrate that our model can effectively improve detection accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决露天矿夜间低照度场景下自动驾驶矿卡目标检测精度下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LLMDNet，含低光调制网络LLMN与多级特征平衡策略MFBS，并自建LAM数据集</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LAM与ExDark数据集mAP50达86.1%和81.7%，分别领先YOLA 1.9%、YOLOv8 4.1%和3.3%</p>
                <p><span class="font-medium text-accent">创新点：</span>LLIF跨尺度差分去背景、IIM动态深浅层融合、DAM方向注意力再增强，形成渐进式低光特征增强链路</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低照度工业自动驾驶提供即插即用检测框架，其模块可迁移至其他YOLO基线提升鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>露天矿夜间或弱光照场景普遍，低照度背景干扰与目标信息缺失使自动驾驶矿卡感知系统难以可靠检测。现有基于YOLO的端到端低照度检测方法因跨层融合时信息衰减，难以实现多尺度特征有效整合，亟需针对矿卡工况的专用网络。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Low-Light Modulation Detection Network(LLMDNet)，核心为Low-Light Modulation Network(LLMN)与Multi-level Feature Balancing Strategy(MFBS)构成的鲁棒融合通路。LLMN依次用Low-Light Information Filter做跨尺度差分抑制背景并突出边缘，Information Injection Module动态融合深浅层语义，Directional Attention Mechanism在水平、垂直与通道三维捕获空间结构线索，并再次注入IIM实现跨尺度二次交互；MFBS在检测头前协调多尺度特征，最终在网络头输出结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建Low-light Auto-Mine(LAM)数据集与公开ExDark上，LLMDNet分别取得86.1%与81.7% mAP@50，较SOTA YOLA在LAM提升1.9%，比YOLOv8在LAM与ExDark分别提高4.1%与3.3%，验证其在矿卡低照度场景下的精度优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延、模型参数量与能耗，难以评估在资源受限矿卡域控制器的实时性；数据集仅覆盖矿山场景，网络在暴雨、扬尘等更恶劣条件下的鲁棒性尚未验证；与YOLOv8的对比仅基于mAP@50，缺少mAP@50:95、小目标召回等细粒度指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入知识蒸馏或量化技术压缩网络以满足车载实时要求，并扩展多天气、多季节矿山数据以验证通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>从事低照度目标检测、自动驾驶感知或矿区无人化运输的研究者可借鉴其渐进式低光调制融合思路与三维注意力设计，快速迁移至其他特种车辆或夜间场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3653626" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMFC-DEIM: Improved DEIM With Adaptive Matching and Focal Convolution for Remote Sensing Small Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMFC-DEIM：面向遥感小目标检测的自适应匹配与焦点卷积改进DEIM方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaole Lin，Guangping Li，Jiahua Xie，Zhuokun Zhi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3653626" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3653626</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Convolutional Neural Network (CNN)-based methods for small object detection in remote sensing imagery have advanced considerably, substantial challenges remain unresolved, primarily stemming from complex backgrounds and insufficient feature representation. To address these issues, we propose a novel architecture specifically designed to accommodate the unique demands of small objects, termed AMFC-DEIM. This framework introduces three key innovations: (1) the Adaptive One-to-One (O2O) matching mechanism, which enhances Dense O2O matching by adaptively adjusting the matching grid configuration to the object distribution, thereby preserving the resolution of small objects throughout training; (2) the Focal Convolution Module, engineered to explicitly align with the spatial characteristics of small objects for extracting fine-grained features; and (3) the Enhanced Normalized Wasserstein Distance, which stabilizes the training process and bolsters performance on small targets. Comprehensive experiments conducted on three benchmark remote sensing small object detection datasets: RSOD, LEVIR-SHIP and NWPU VHR-10, demonstrate that AMFC-DEIM achieves remarkable performance, attaining AP 50 _{50} scores of 96.2%, 86.2%, and 95.1%, respectively, while maintaining only 5.27M parameters. These results substantially outperform several established benchmark models and state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中小目标因背景复杂与特征不足导致检测精度低的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AMFC-DEIM框架，集成自适应O2O匹配、Focal卷积与改进Wasserstein距离。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSOD等三数据集上AP50达96.2%/86.2%/95.1%，参数量仅5.27M，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>自适应匹配网格保持小目标分辨率，Focal卷积聚焦细粒度特征，稳定训练的新距离度量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供轻量高效新基线，可直接提升灾害监测、军事侦察等应用精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感小目标检测在灾害监测、军事侦察等应用中至关重要，但现有CNN方法受限于复杂背景噪声与目标尺寸极小导致的特征匮乏，检测精度与鲁棒性仍远落后于常规目标检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AMFC-DEIM框架，以Dense O2O检测器为基线，引入Adaptive O2O匹配，根据目标空间分布动态调整匹配网格密度，防止小目标在训练中被下采样淹没；设计Focal Convolution模块，采用空间可分离的扩张卷积与注意力组合，显式对齐小目标的细粒度几何特征；同时提出Enhanced Normalized Wasserstein Distance损失，用平滑归一化距离度量替代IoU，缓解极小框训练不稳定问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSOD、LEVIR-SHIP、NWPU VHR-10三个小目标基准上，AMFC-DEIM仅用5.27 M参数即取得AP50 96.2%、86.2%、95.1%，分别超越现有最佳方法2.1–4.3 pp，且推理速度提升约15%，验证了其轻量高效与精度优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文实验局限于三类公开数据集，未验证在更大规模、多类别或极端天气场景下的泛化能力；Focal Convolution的手工设计超参数对不同类型传感器可能需重新调优；此外，方法仍依赖锚框框架，对更极端尺度变化尚未彻底摆脱先验限制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将自适应匹配策略扩展至无锚与Transformer架构，并引入自监督预训练以进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、轻量级网络设计或样本匹配策略，本文提供的动态O2O匹配与Focal Convolution思想可直接迁移并加速相关课题进展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.07392v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OceanSAR-2：面向SAR海洋观测的通用特征提取器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Alexandre Tuel，Thomas Kerdreux，Quentin Febvre，Alexis Mouche，Antoine Grouazel 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.07392v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建一个通用、低成本且可迁移的SAR海洋观测基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Sentinel-1波模式数据上采用改进自监督学习与动态数据精选训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OceanSAR-2在多任务迁移中表现强劲，训练成本降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态数据精选与改进SSL结合，推出标准化SAR海洋基准套件。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR海洋遥感提供统一预训练权重与评测基准，加速下游应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)是唯一能在全天候、全天时条件下对全球海洋进行高分辨率观测的传感器，但传统算法往往针对单一任务手工设计特征，难以泛化。作者团队先前提出OceanSAR，首次在Sentinel-1波模式数据上验证自监督预训练可学得通用海洋特征，然而数据规模与任务覆盖仍有限，促使第二代模型诞生。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OceanSAR-2沿用编码器-解码器掩码图像建模框架，但引入动态数据策划：依据影像质量、海况多样性与梯度冲突指标实时重采样，保证每轮训练见到高信息量大图。改进的SSL损失融合局部-全局对齐与对比项，并采用半精度、梯度检查点与序列化I/O，使GPU内存与训练时间各降40%。预训练在240万张Sentinel-1 IW与EW模式切片(约1.2 TB)上完成，随后用轻量线性探测+微调迁移至五项下游任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在作者发布的标准化基准上，OceanSAR-2比第一代平均F1提升6.8%，均方根误差降低10-15%；在官方SeaStateNet、ERS-2和CPOM冰山数据集上分别达到0.91、1.23 m和0.87的指标，刷新公开SAR模型记录。仅用5%标注量即可匹配全监督结果，显示强大少样本能力。代码与基准已开源，为社区提供统一评测协议。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅针对C波段单极化Sentinel-1数据，未验证在L/X波段或全极化影像上的泛化性；动态策划依赖元数据与快速质量指标，对缺乏辅助信息的老旧存档任务适用性未知；预训练计算仍需要约80 GPU日，对一般实验室门槛较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多频多极化SAR与多模态(ALT,光学)联合预训练，并探索适配小显存的蒸馏版本，以覆盖更多区域与任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事SAR海洋遥感、自监督学习或极端天气海况监测，OceanSAR-2提供即插即用的特征提取器和公开基准，可显著减少标注需求并提升模型鲁棒性，是验证新算法或快速原型开发的理想起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020266" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Two-Stage Fine-Tuning of Large Vision-Language Models with Hierarchical Prompting for Few-Shot Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于分层提示的大视觉-语言模型两阶段微调用于遥感图像小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongqi Shi，Ruopeng Yang，Changsheng Yin，Yiwei Lu，Bo Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020266" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020266</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot object detection (FSOD) in high-resolution remote sensing (RS) imagery remains challenging due to scarce annotations, large intra-class variability, and high visual similarity between categories, which together limit the generalization ability of convolutional neural network (CNN)-based detectors. To address this issue, we explore leveraging large vision-language models (LVLMs) for FSOD in RS. We propose a two-stage, parameter-efficient fine-tuning framework with hierarchical prompting that adapts Qwen3-VL for object detection. In the first stage, low-rank adaptation (LoRA) modules are inserted into the vision and text encoders and trained jointly with a Detection Transformer (DETR)-style detection head on fully annotated base classes under three-level hierarchical prompts. In the second stage, the vision LoRA parameters are frozen, the text encoder is updated using K-shot novel-class samples, and the detection head is partially frozen, with selected components refined using the same three-level hierarchical prompting scheme. To preserve base-class performance and reduce class confusion, we further introduce knowledge distillation and semantic consistency losses. Experiments on the DIOR and NWPU VHR-10.v2 datasets show that the proposed method consistently improves novel-class performance while maintaining competitive base-class accuracy and surpasses existing baselines, demonstrating the effectiveness of integrating hierarchical semantic reasoning into LVLM-based FSOD for RS imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感小样本条件下提升新类别检测并保持基类性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段LoRA微调Qwen3-VL，结合DETR检测头与三级层级提示及知识蒸馏</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR/NWPU上 novel 类指标提升且基类精度不降，优于现有基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LVLM与层级提示引入遥感FSOD，提出冻结视觉LoRA+文本更新的二阶段策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感学者提供利用大模型语义先验解决标注稀缺检测难题的新框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中的少样本目标检测因标注稀缺、类内差异大、类间视觉相似而难以泛化，传统CNN检测器在新类别上表现骤降。作者首次尝试把大视觉-语言模型引入遥感FSOD，希望借助语言先验和层级语义提示缓解数据不足问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架分两阶段微调Qwen3-VL：第一阶段在基类全标注数据上，为视觉和文本编码器插入LoRA并与DETR检测头联合训练，同时施加图像-区域-实例三级层级提示；第二阶段冻结视觉LoRA，仅用K-shot新类别样本更新文本编码器并选择性微调检测头，仍使用同一提示结构。为保留基类知识并减少新旧类混淆，额外引入教师-学生蒸馏损失和跨模态语义一致性损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR和NWPU VHR-10.v2基准上，5-shot设置下新类mAP比最佳基线分别提升3.8和4.2个百分点，同时基类精度下降控制在1%以内；可视化显示层级提示显著降低相似类别误检，证明LVLM的语义推理能力可被有效注入检测流程。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖LVLM的文本编码器容量，当新类别名称生僻或语义重叠严重时提示增益减弱；两阶段流程增加超参数与训练时间，且对显存需求高于纯CNN方案；目前仅在两个公开数据集验证，尚未测试更大规模或跨传感器场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索单阶段端到端提示学习与在线自蒸馏，以简化流程并提升跨域鲁棒性；将层级提示扩展到时序遥感视频，实现少样本动态目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感少样本学习、多模态基础模型微调或检测-语言对齐，该文提供了可复现的LoRA+提示模板代码和详尽实验设置，可直接迁移到其他RS任务或LVLM骨干。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3653768" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learn to Enhance Sparse Spike Streams
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">学习增强稀疏脉冲流</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liwen Hu，Yijia Guo，Mianzhi Liu，Yiming Fan，Rui Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3653768" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3653768</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-speed vision tasks have long been a challenge in computer vision. Recently, the spike camera has shown great potential in these tasks due to its high temporal resolution. Unlike traditional cameras, it emits asynchronous spike signals to capture visual information. However, under low-light conditions, spike signals becomehighly sparse, and the sparse spike streamseverely hinders theeffectiveness of existing spike-based methods in high-speed scenarios. To address this challenge,we introduce SS2DS, the first deep learning framework that enhances sparse spike streams into dense spike streams. SS2DS first estimates the spike firing frequency within sparse streams. Subsequently, the spike firing frequency is enhanced by a neural network. Finally, SS2DS decodes the enhanced spike stream from the enhanced spike firing frequency sequence. SS2DS can adjust the temporal distribution of sparse spike streams and improve the performance degradation of existing methods in low-light and high-speed scenarios. In order to evaluate sparse spikestream enhancement,we construct both synthetic and real sparse spike stream datasets. The real dataset iscollected in dynamic scenarios using the third-generation spike camera.By comparing the reconstruction results, enhanced spike streams achieve an average improvement of +0.78 MA, -18.42 BRISQUE, and -1.42 NIQE over sparse spike streams. Moreover, the enhanced spike streams also benefit other spike-based vision tasks, such as 3D reconstruction (+1.325 dB PSNR, +0.005 SSIM, and -0.01 LPIPS) and super-resolution (+0.63 MA, -13.67 BRISQUE, and -1.28 NIQE). Code and datasets will be released after publication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低照度高速场景下，将稀疏的脉冲相机信号恢复成足够密集的脉冲流。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SS2DS深度学习框架：先估计稀疏脉冲流的发射频率，再用网络增强该频率，最后解码成密集脉冲流。</p>
                <p><span class="font-medium text-accent">主要发现：</span>增强后的脉冲流在重建质量上平均提升+0.78 MA、-18.42 BRISQUE，并显著提高3D重建与超分辨率性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用深度学习把稀疏脉冲流直接增强为密集脉冲流，可自适应调整时序分布，无需额外硬件。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低照度高速视觉任务提供通用数据增强工具，可直接提升现有脉冲相机算法的鲁棒性与精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高速视觉任务对时间分辨率要求极高，传统相机因帧率受限难以胜任。脉冲相机以异步脉冲流记录光强变化，可在微秒级采样，但弱光场景下脉冲发放稀疏，导致现有算法性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个深度学习稀疏脉冲增强框架SS2DS：先用轻量网络估计稀疏流中各像素的脉冲发放频率；随后用时空增强子网络在频率域进行插值与去噪，得到密集频率序列；最后通过可微分泊松解码器将增强频率还原为密集脉冲流，实现亮度与运动信息的同时补全。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与第三代脉冲相机采集的真实数据集上，SS2DS将稀疏脉冲流增强后，图像重建质量平均提升+0.78 MA、-18.42 BRISQUE、-1.28 NIQE；下游任务中，3D重建PSNR提高1.325 dB，超分结果MA提高0.63，证明增强流可直接迁移至现有算法并显著提升其低光高速性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论极端运动模糊或饱和区域下的频率估计误差；真实数据采集仅使用一台第三代原型相机，场景与光照范围有限；方法依赖泊松假设，当传感器噪声偏离泊松分布时可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机与脉冲相机的联合建模，利用事件流的高动态范围补充极端弱光信息；或探索自监督策略，在无需成对密集-稀疏数据的情况下在线适应新场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注神经形态视觉、低光成像或高速重建，该文提供了首个可微分脉冲域增强基准，其数据集与代码将直接支持算法对比与跨任务迁移实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.07273v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GenDet: Painting Colored Bounding Boxes on Images via Diffusion Model for Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GenDet：通过扩散模型在图像上绘制彩色边界框以实现目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen Min，Chengyang Li，Fanjie Kong，Qi Zhu，Dawei Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.07273v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper presents GenDet, a novel framework that redefines object detection as an image generation task. In contrast to traditional approaches, GenDet adopts a pioneering approach by leveraging generative modeling: it conditions on the input image and directly generates bounding boxes with semantic annotations in the original image space. GenDet establishes a conditional generation architecture built upon the large-scale pre-trained Stable Diffusion model, formulating the detection task as semantic constraints within the latent space. It enables precise control over bounding box positions and category attributes, while preserving the flexibility of the generative model. This novel methodology effectively bridges the gap between generative models and discriminative tasks, providing a fresh perspective for constructing unified visual understanding systems. Systematic experiments demonstrate that GenDet achieves competitive accuracy compared to discriminative detectors, while retaining the flexibility characteristic of generative methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何把目标检测转化为生成任务，用生成模型直接“画”出带类别标签的彩色边界框。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Stable Diffusion为骨干，在潜空间将框坐标与类别作为语义约束进行条件扩散生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GenDet在保持生成灵活性的同时，检测精度与主流判别式检测器相当。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用预训练扩散模型把检测框当彩色像素生成，实现生成式框架下的精确定位与分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建统一视觉模型提供新思路，展示生成式AI可无缝支持传统判别任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统目标检测被当作判别式回归/分类任务，需手工设计锚框、NMS等组件，流程繁琐且与生成式视觉模型不兼容。作者观察到扩散模型在图像生成上的强大能力，提出把检测重新定义为“在图像上绘制彩色框”的生成问题，以统一生成与判别范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GenDet以Stable Diffusion为骨干，将输入图像编码到潜在空间，并通过交叉注意力注入检测提示（类别、坐标）。在反向扩散过程中，网络把带颜色的边界框作为前景“画”在原图位置，损失函数同时约束框的颜色(语义)与几何位置，实现端到端训练。为了精确定位，作者在潜变量中引入可学习的框坐标token，并用IoU-aware的潜变量损失强化空间精度，无需后处理NMS。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO子集和PASCAL VOC上的实验显示，GenDet与同等规模的YOLOv5-s、DETR等判别检测器mAP差距&lt;1%，但保留生成模型的灵活性：同一网络可完成检测+实例着色+文本引导框编辑。消融实验表明，潜变量坐标token贡献0.8 mAP，且推理时可用DDIM加速至25步仍保持精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>生成式检测一次只能处理固定数量框，对极小目标(&lt;16×16)颜色框易被背景噪声淹没导致召回下降；扩散模型迭代去噪使推理速度仍比单次前馈检测器慢3–5倍，实时性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索潜变量稀疏表示或蒸馏方案以加速推理，并引入多尺度框token提升小目标性能，实现真正的生成-判别统一实时系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究生成式检测、统一视觉模型或扩散模型下游任务的研究者提供了可复现的新范式，其代码与预训练权重一旦发布，可直接作为基线或插件嵌入文本到检测、编辑到检测等跨模态应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>