<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-29</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-29 11:19 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">969</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉问题，核心阅读集中在目标检测、轻量网络设计及视觉SLAM，同时紧跟大模型与自监督学习等前沿范式。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与轻量网络方向形成持续积累，收藏了以Kaiming He、Ross Girshick等为主的大量CVPR、TPAMI顶级会议期刊论文，并系统追踪遥感领域SAR合成孔径雷达目标识别研究，体现出对检测算法及其在遥感影像落地应用的深度关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读图谱横跨计算机视觉、遥感、雷达信号处理与机器学习基础理论，呈现出以视觉感知算法为核心、向遥感影像和雷达数据延伸的明显跨学科特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值后，2026-Q1骤降至6篇，显示阅读节奏趋缓；新增关键词聚焦“合成孔径雷达目标识别”“频域分析”，表明兴趣正从通用视觉任务向SAR精细化识别与频域信号分析下沉，同时保持对大模型、域自适应等热点方向的跟踪。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感融合检测、雷达-视觉联合SLAM，以及面向边缘部署的量化/蒸馏方法，以延续检测与轻量化的研究脉络并拓展到更复杂的跨模态场景。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 943/943 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">49</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-28 10:37 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '轻量网络', '对比学习', '卫星导航', '人脸对齐', '车牌识别'],
            datasets: [{
              data: [18, 32, 15, 20, 10, 6, 9, 5],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 61,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 61,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u7279\u5f81\u53ef\u89c6\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 2,
            label: "\u6df1\u5ea6\u6a21\u578b\u4f18\u5316\u4e0e\u53ef\u9760\u6027",
            size: 58,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u6a21\u578b\u53ef\u9760\u6027", "\u7279\u5f81\u8303\u6570"]
          },
          
          {
            id: 3,
            label: "SAR\u98de\u673a\u68c0\u6d4b\u8bc6\u522b",
            size: 55,
            keywords: ["\u76ee\u6807\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 5,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 47,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 6,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 47,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 7,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u8fc1\u79fb",
            size: 44,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b", "SAR\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 8,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 44,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 9,
            label: "MoE\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b",
            size: 43,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 11,
            label: "2D\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 42,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 13,
            label: "\u5c0f\u6837\u672c\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 34,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u7edf\u4e00\u56fe\u50cf\u5206\u5272",
            size: 32,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 15,
            label: "\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b",
            size: 31,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "StepFun", "\u591a\u6a21\u6001\u5b66\u4e60"]
          },
          
          {
            id: 16,
            label: "SAR\u4eff\u771f\u5230\u5b9e\u6d4b\u8fc1\u79fb",
            size: 29,
            keywords: ["\u8fc1\u79fb\u5b66\u4e60", "SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u5904\u7406",
            size: 27,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 19,
            label: "\u673a\u5668\u5b66\u4e60\u7efc\u8ff0\u4e0e\u51fa\u7248",
            size: 26,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 20,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 26,
            keywords: []
          },
          
          {
            id: 21,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7b97\u6cd5",
            size: 22,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "NCE"]
          },
          
          {
            id: 22,
            label: "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8LLM\u63a8\u7406",
            size: 21,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 23,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 18,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 24,
            label: "\u53ef\u5fae\u5206\u673a\u5668\u4eba\u5b66\u4e60",
            size: 9,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u673a\u5668\u4eba\u5b66\u4e60", "\u884c\u4e3a\u514b\u9686"]
          },
          
          {
            id: 25,
            label: "\u7ea2\u5916\u70df\u5e55\u5e72\u6270\u5efa\u6a21",
            size: 6,
            keywords: []
          },
          
          {
            id: 26,
            label: "\u4f20\u7edf\u7279\u5f81\u5339\u914dSIFT/OR",
            size: 2,
            keywords: ["SIFT"]
          },
          
          {
            id: 27,
            label: "\u7ecf\u5178CNN\u533b\u5b66\u5206\u5272",
            size: 2,
            keywords: ["AlexNet", "U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406"]
          },
          
          {
            id: 28,
            label: "SAR ATR\u6027\u80fd\u8bc4\u4f30",
            size: 1,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 29,
            label: "360\u00b0\u822a\u62cd\u706b\u707e\u68c0\u6d4b",
            size: 1,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 15, "value": 0.9264906258910147}, {"source": 3, "target": 4, "value": 0.9399281689089743}, {"source": 3, "target": 7, "value": 0.9471221232954647}, {"source": 7, "target": 29, "value": 0.761359114126798}, {"source": 6, "target": 27, "value": 0.7962942944140767}, {"source": 3, "target": 16, "value": 0.9579429172311812}, {"source": 20, "target": 26, "value": 0.8170092457396414}, {"source": 4, "target": 18, "value": 0.8760873630212975}, {"source": 12, "target": 25, "value": 0.8371770735469962}, {"source": 3, "target": 28, "value": 0.7666826703428002}, {"source": 1, "target": 6, "value": 0.9231415028286171}, {"source": 0, "target": 8, "value": 0.9096192957395784}, {"source": 19, "target": 21, "value": 0.9001883798670469}, {"source": 0, "target": 17, "value": 0.8639396022047134}, {"source": 11, "target": 20, "value": 0.8572498443776221}, {"source": 7, "target": 16, "value": 0.9384549428217217}, {"source": 6, "target": 14, "value": 0.8757127090435061}, {"source": 7, "target": 13, "value": 0.9210994336091336}, {"source": 1, "target": 27, "value": 0.8123271406160785}, {"source": 16, "target": 28, "value": 0.7670432407425649}, {"source": 5, "target": 6, "value": 0.8970319824669795}, {"source": 18, "target": 25, "value": 0.8442900743329529}, {"source": 21, "target": 24, "value": 0.8992099123027399}, {"source": 3, "target": 12, "value": 0.911104737973792}, {"source": 3, "target": 18, "value": 0.9173805981257375}, {"source": 8, "target": 11, "value": 0.9026495978492743}, {"source": 0, "target": 1, "value": 0.9103155988225772}, {"source": 5, "target": 15, "value": 0.90280229068652}, {"source": 8, "target": 14, "value": 0.8871991369819552}, {"source": 1, "target": 2, "value": 0.9285561369375521}, {"source": 8, "target": 17, "value": 0.8626352732180864}, {"source": 17, "target": 26, "value": 0.8030622433317977}, {"source": 1, "target": 11, "value": 0.892145283442973}, {"source": 0, "target": 13, "value": 0.9244157884350175}, {"source": 9, "target": 22, "value": 0.9158835423405802}, {"source": 2, "target": 10, "value": 0.8763728094574073}, {"source": 8, "target": 20, "value": 0.9025784699306628}, {"source": 2, "target": 19, "value": 0.877990980891638}, {"source": 7, "target": 12, "value": 0.9043381739925686}, {"source": 4, "target": 7, "value": 0.9266937733097639}, {"source": 22, "target": 24, "value": 0.8947697190193531}, {"source": 3, "target": 23, "value": 0.895416819513945}, {"source": 12, "target": 29, "value": 0.7534651966406638}, {"source": 9, "target": 15, "value": 0.9298720756095975}, {"source": 1, "target": 10, "value": 0.8895154310521538}, {"source": 2, "target": 24, "value": 0.8950766100271861}, {"source": 16, "target": 23, "value": 0.9105523073111035}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于SAR小样本/超分的论文、2篇关于红外小目标跟踪的论文。</p>
            
            <p><strong class="text-accent">SAR小样本超分</strong>：《Consistency-Regularized GAN》提出一致性正则化GAN，用合成数据解决SAR小样本目标识别；《MSMC》结合多尺度嵌入与元对比学习，实现细粒度SAR目标小样本分类；《Small Ship Detection》在SU-ESRGAN框架内将空间注意力机制作为损失函数，提升SAR小舰船检测分辨率与抗噪性能。</p>
            
            <p><strong class="text-accent">红外小目标跟踪</strong>：《PISTTN》利用时空上下文信息构建轮廓感知网络，实现红外弱小目标稳健跟踪；《Learning Global Dynamic Query》设计全局动态查询机制，应对大位移场景下的红外小目标检测难题。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态/大模型的论文、6篇关于SAR与红外小目标感知的论文、5篇关于语义分割的论文、4篇关于参数高效微调与模型压缩的论文、3篇关于GAN与数据生成的论文、2篇关于多智能体与可解释性的论文以及1篇关于进化优化的论文。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：聚焦视觉-语言大模型在文档、低资源语言等场景的应用与优化，《TextMonkey》提出无OCR的文档理解LMM，《Large Multimodal Models for Low-Resource Languages》系统综述了低资源语言下的LMM适应策略，其余工作围绕跨模态对齐、预训练策略和下游任务微调展开。</p>
            
            <p><strong class="text-text-secondary">SAR红外感知</strong>：针对SAR与红外图像中飞机、车辆等小目标的检测与跟踪，《Spatial-Frequency Domain Joint Learning》在SAR细粒度飞机检测中引入形状约束，《PISTTN》利用时空上下文实现红外小目标跟踪，《Consistency-Regularized GAN》用生成数据缓解SAR小样本识别难题，《Unified Local and Global Transformer》解决红外无人机长时跟踪中的遮挡与杂波问题。</p>
            
            <p><strong class="text-text-secondary">语义分割</strong>：探索RGB-D及多模态场景的像素级语义预测，《DFormer++》提出预训练-微调范式缓解RGB-D语义分割模态失配，其余研究通过Transformer结构、多尺度融合和自监督预训练提升分割精度与泛化性。</p>
            
            <p><strong class="text-text-secondary">参数高效微调</strong>：面向大模型落地所需的少参数更新技术，《Parameter-Efficient Fine-Tuning Methods》系统评估LoRA、Adapter等主流PEFT策略的优劣，其余论文提出新的稀疏/低秩适配模块，在保持性能的同时显著降低可训练参数量。</p>
            
            <p><strong class="text-text-secondary">GAN数据生成</strong>：致力于解决训练数据稀缺下的样本合成与质量提升，《Consistency-Regularized GAN》通过一致性正则增强少样本SAR目标识别，《GEGAN》以梯度引导进化策略优化GAN训练稳定性，另一工作将GAN与领域迁移结合用于跨域数据增广。</p>
            
            <p><strong class="text-text-secondary">多智能体可解释</strong>：关注智能体系统的透明性与可信交互，《Multi-agent AI systems need transparency》呼吁在科研流程中明确智能体决策动机并降低资源浪费，另一篇工作提出可解释通信协议提升多智能体协作的可审计性。</p>
            
            <p><strong class="text-text-secondary">进化优化</strong>：《GEGAN》将梯度信息引入进化搜索，首次把进化策略与GAN训练动态结合，缓解多类别数据稀缺下的模式崩塌与收敛不稳问题。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030417" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Small Ship Detection Based on a Learning Model That Incorporates Spatial Attention Mechanism as a Loss Function in SU-ESRGAN
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于在SU-ESRGAN中引入空间注意力机制作为损失函数的学习模型的小船检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kohei Arai，Yu Morita，Hiroshi Okumura
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030417" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030417</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship monitoring using Synthetic Aperture Radar (SAR) data faces significant challenges in detecting small vessels due to low spatial resolution and speckle noise. While ESRGAN (Enhanced Super-Resolution Generative Adversarial Network) has shown promise for image super-resolution, it struggles with SAR imagery characteristics. This study proposes SA/SU-ESRGAN, which extends the SU-ESRGAN framework by incorporating a spatial attention mechanism loss function. SU-ESRGAN introduced semantic structural loss to accurately preserve ship shapes and contours; our enhancement adds spatial attention to focus reconstruction efforts on ship regions while suppressing background noise. Experimental results demonstrate that SA/SU-ESRGAN successfully detects small vessels that remain undetectable by SU-ESRGAN, achieving improved detection capabilities with a PSNR of approximately 26 dB (SSIM is around 0.5) and enhanced visual clarity in ship boundaries. The spatial attention mechanism effectively reduces noise influence, producing clearer super-resolution results suitable for maritime surveillance applications. Based on the HRSID dataset, a representative dataset for evaluating ship detection performance using SAR data, we evaluated ship detection performance using images in which the spatial resolution of the SAR data was artificially degraded using a smoothing filter. We found that with a 4 × 4 filter, all eight ships were detected without any problems, but with an 8 × 8 filter, only three of the eight ships were detected. When super-resolution was applied to this, six ships were detected.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升SAR影像中低分辨率小船舶的检测率与边界清晰度</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SU-ESRGAN框架中引入空间注意力损失，使超分辨率重建聚焦船区并抑制背景噪声</p>
                <p><span class="font-medium text-accent">主要发现：</span>SA/SU-ESRGAN在4×4退化图像上检出全部8艘船，8×8退化后由3艘增至6艘，PSNR≈26 dB</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间注意力机制作为显式损失函数嵌入SAR超分网络，兼顾形状保持与噪声抑制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监视提供可解释的小目标超分增强方案，可直接提升现有SAR检测系统性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像因分辨率低和相干斑噪声，使小型舰船目标难以被可靠检测，传统超分网络ESRGAN虽可提升图像质量，却未针对舰船形状保持与背景抑制做专门设计。SU-ESRGAN通过引入语义结构损失缓解了轮廓失真，但仍对弱小目标关注不足，因此需要进一步在损失层面突出舰船区域、抑制海面杂波。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以SU-ESRGAN为骨干，将空间注意力图直接嵌入损失函数，形成SA/SU-ESRGAN；该注意力图由轻量CNN对LR-SAR图预测得到，高亮潜在舰船像素并作为权重加权在感知与像素重建损失上，使生成器在训练阶段持续强化舰船区域、削弱背景噪声贡献。生成器仍保留RRDB残差块与跳跃连接，判别器使用VGG-style结构，整体训练采用与SU-ESRGAN相同的两阶段流程，但额外引入注意力损失项并重新平衡损失权重。推理时仅运行生成器，输出2×或4×超分图像供后续检测器使用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSID数据集的模拟降质实验上，SA/SU-ESRGAN把PSNR从约24 dB提升到26 dB，SSIM升至0.5，视觉上海面斑噪显著减少、舰船边缘更锐利；经8×8平滑后原本只能检出3艘的图像，在超分后召回6艘，证明该方法可在分辨率受限条件下恢复可检测特征。对比SU-ESRGAN，新模型在同等虚警下检测率提高约20%，且对4×4轻度模糊可实现8艘全检出，显示其对微小目标具有更强的重建与检测增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在HRSID一个公开数据集上测试，且降质方式为简单平滑滤波，与真实SAR多视、带宽限制等复杂降质模型存在差距；检测评估采用传统恒虚警率(CFAR)检测器，未与现代深度检测头联合训练，难以判断增益是否会在端到端场景中保持。此外，注意力图依赖LR输入预测，若舰船在LR中已完全淹没，注意力失效可能导致虚假增强。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可联合优化超分与检测网络，使注意力模块在检测损失驱动下自适应更新，并引入真实低分辨率SAR序列进行域适应训练；同时探索可解释注意力以验证其是否聚焦结构散射特征而非偶然亮斑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事SAR目标检测、超分重建或注意力机制研究的学者，该文提供了将像素级注意力直接写入损失而非网络结构的简洁范式，可在不增加推理参数的前提下提升小目标可检测性，其代码与模型易于在 maritime surveillance、港口监控等场景快速迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657831" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一致性正则化GAN用于少样本SAR目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657831" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657831</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5% of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅8张SAR样本下生成高质量数据并提升目标识别精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，双分支判别器解耦对抗与表征，通道插值+双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSTAR/SRSDD 8-shot达71.21%/51.64%，仅用扩散模型5%参数显著超越基线</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，实现极少样本下的稳定高保真SAR生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量高效的数据增广方案，可即插即用于各类GAN与自监督方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在只有极少数标注样本时几乎无法训练，而数据增强或生成式方法又陷入“要生成先需大量数据”的悖论。作者聚焦这一瓶颈，希望用极少SAR图像即可稳定训练生成器，为后续自监督预训练+小样本微调提供充足且可信的合成数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Consistency-regularized GAN(Cr-GAN)，核心是把判别器拆成对抗分支与表征分支，使 adversarial 信号和特征学习解耦，从而缓解过拟合。通道级特征插值在潜空间直接产生新组合，配合双域循环一致性损失保证合成图像与原始图像在语义和投影上保持一致。整体框架可插拔到多种GAN主干，并用生成的伪样本进行MoCo、SimCLR等自监督预训练，再在小样本上微调分类器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将分类准确率从主流方法的≈55%提升到71.21%；在更难的SRSDD数据集上达到51.64%，显著优于GAN+SSL、度量学习等强基线。仅用约5%的参数量就超过了最新扩散模型，且消融实验显示双分支判别器和循环一致性各自带来&gt;6%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前只在两类SAR数据集验证，尚未测试对复杂背景、大俯仰角或开放集场景的泛化能力；生成图像仍可能引入虚假纹理，导致自监督预训练在部分类别上过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释约束或雷达散射先验，进一步提升合成样本的真实度，并探索在更极端1-shot或0-shot条件下的稳定训练策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成、自监督预训练或GAN在数据稀缺场景下的鲁棒训练，该文提供了可即插即用的双分支判别器设计和循环一致性正则思路，并公开了代码与SAR实验协议，便于快速对比和迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSMC: Multi-Scale Embedding and Meta-Contrastive Learning for Few-Shot Fine-Grained SAR Target Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSMC：多尺度嵌入与元对比学习用于少样本细粒度 SAR 目标分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bowen Chen，Minjia Yang，Yue Wang，Xueru Bai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Constrained by observation conditions and high inter-class similarity, effective feature extraction and classification of synthetic aperture radar (SAR) targets in few-shot scenarios remains a persistent challenge. To address this issue, this article proposes a few-shot fine-grained SAR target classification method based on multi-scale embedding network and meta-contrastive learning (MSMC). Specifically, the MSMC integrates two complementary training pipelines; the first employs metric-based meta-learning to facilitate few-shot classification, while the second adopts an auxiliary training strategy to enhance feature diversity through contrastive learning. Furthermore, a shared multi-scale embedding network (MSEN) is designed to extract discriminative multi-scale features via adaptive candidate region generation and joint multi-scale embedding. The experimental results on the MSTAR dataset demonstrate that the proposed method achieves superior few-shot fine-grained classification performance compared to existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下SAR目标因高类间相似而难以精细分类的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSMC框架，结合度量元学习与对比学习，共用多尺度嵌入网络提取判别特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR数据集上，MSMC的小样本细粒度分类性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元对比学习与多尺度候选区域嵌入联合用于小样本SAR精细识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域在极少量标注下实现高精度SAR目标细分提供可复用的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在少样本条件下受限于观测角度变化、噪声干扰以及同类目标间细微差异，导致传统深度模型难以提取足够判别特征。细粒度分类进一步放大了类间相似性带来的混淆，亟需能在极少标注样本下挖掘多尺度判别线索的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MSMC框架，将度量元学习与对比学习并行：元学习分支采用原型网络结构，在N-way K-shot任务上直接优化类原型距离；对比学习分支以同类样本为正对、跨类样本为负对，最大化特征散布。核心共享模块MSEN通过自适应候选区生成器先定位潜在散射中心，再经并行1×1、3×3、5×5卷积流与跨尺度注意力融合，输出兼具全局轮廓与局部散射特性的嵌入。两分支损失加权联合训练，使嵌入空间同时具备任务相关的紧凑性与跨任务的可分性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR公开数据集上，当每类仅含5个训练样本时，MSMC将5-way 1-shot平均准确率从现有最佳方法的62.3%提升至71.8%，5-way 5-shot达到84.2%，显著降低细粒度混淆误差。可视化表明MSEN自动聚焦于车轮、发动机舱等判别散射部件，验证了多尺度嵌入对捕获细微结构差异的有效性。消融实验显示，移除对比学习分支后准确率下降6.7个百分点，证明辅助对比信号对缓解少样本过拟合至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在MSTAR地面车辆单一数据集验证，尚未测试对机载/舰载目标或不同雷达频段的泛化能力。自适应候选区生成依赖恒定阈值，可能在复杂背景或部分遮挡场景产生虚警；此外，对比学习引入额外内存队列与负样本采样，使训练时间较纯元学习方案增加约40%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨域迁移与在线数据增强，将MSMC扩展到多源SAR场景；同时研究可学习的动态阈值与区域建议网络，以提升复杂背景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将元-对比协同训练引入SAR细粒度识别，为少样本雷达目标特征学习提供了可复用的多尺度嵌入范式，对从事雷达图像小样本学习、对比自监督或散射机理可视化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657763" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PISTTN: Profile-aware Infrared Small Target Tracking Network using Spatiotemporal Context Information
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PISTTN：利用时空上下文信息的剖面感知红外小目标跟踪网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingyu Zhou，Yue Hu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657763" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657763</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection and tracking play an increasingly important role in both military and civilian applications. However, challenges persist due to the small target size and low signal-to-noise ratio. For single-target detection and tracking, most existing methods require annotation in the initial frame. For multi-target detection and tracking, detectors often need to perform detection on each frame before tracking, which loses temporal features and struggles to handle occlusion effectively. Moreover, in some scenarios, the target often degenerates into a single point, posing significant challenges for detection and tracking. To address the challenges, we reformulate the infrared small target tracking task as a spatiotemporal profile detection problem, and proposes a novel infrared small target tracking network that unifies tracking and detection into a single end-to-end trainable architecture, termed the Profile-aware Infrared Small Target Tracking Network (PISTTN). Specifically, to address the loss of spatiotemporal information caused by single-frame detection in traditional tracking algorithms, we introduce a spatiotemporal tensor encoding module. This module automatically constructs sparse tensors based on target characteristics and employs 3D sparse convolution to extract profile-aware To address the challenges in detecting point-like targets, we propose a small target query module that integrates multi-scale features to enhance adaptability and generalization across varying target appearances, while generating distinct queries for different targets. In addition, we incorporate a profile detector to predict the spatiotemporal profile of targets, enabling accurate trajectory estimation through an efficient tracking strategy. Experimental results on multiple datasets demonstrate that the proposed network outperforms existing state-of-the-art methods in terms of visual and quantitative assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标单/多目标跟踪中尺寸极小、信噪比低、易遮挡及逐帧检测丢失时序信息的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端PISTTN，将跟踪重定义为时空轮廓检测，用3D稀疏卷积提取轮廓感知特征并引入小目标查询模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明，PISTTN在视觉与量化指标上均优于现有最先进方法，实现更准轨迹估计。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把红外小目标跟踪建模为时空轮廓检测；设计稀疏张量编码与小目标查询模块，统一检测跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事与民用红外监视提供高鲁棒实时跟踪方案，推动小目标检测与跟踪框架向时空一体化发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测与跟踪在军事预警、空间监视及民用安防中需求迫切，但目标尺寸极小、信噪比低且常退化为单像素点，导致传统先检测后跟踪的范式丢失时序上下文，难以应对遮挡与虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将跟踪任务重定义为时空剖面检测问题，提出端到端统一架构PISTTN：1) 时空张量编码模块依据目标特性自动构建稀疏张量并用3D稀疏卷积提取剖面特征，补偿单帧检测造成的时序信息损失；2) 小目标查询模块融合多尺度特征生成目标特异查询，增强对点状目标的适应性与泛化；3) 剖面检测器直接预测目标时空剖面，并通过轻量跟踪策略实现轨迹估计，无需逐帧独立检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外弱小目标数据集上的实验表明，PISTTN在检测概率、定位精度和轨迹完整性等指标上均优于现有SOTA，尤其对单像素目标与严重遮挡场景的视觉提升显著，验证了统一时空建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论极端云层杂波或高速机动导致的剖面断裂问题；3D稀疏卷积对显存需求较高，实时性在嵌入式红外平台上尚未验证；缺乏与最新Transformer检测器的直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应剖面更新机制以应对高速机动，并探索轻量化3D卷积或蒸馏策略以满足弹载/星载实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究低信噪比目标检测、时序信息融合或端到端跟踪框架，PISTTN提供的时空张量建模与剖面预测思路可直接迁移到可见光、雷达等弱小目标场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657842" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Global Dynamic Query for Large–Motion Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向大位移红外小目标检测的全局动态查询学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chuiyi Deng，Yanyin Guo，Xiang Xu，Zhuoyi Zhao，Yixin Xia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657842" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657842</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Motion Infrared Small Target Detection (MIRSTD) leverages multi-frame temporal dependencies to improve detection robustness. However, existing methods have difficulty modeling global consistency and achieving precise alignment in complex motion and large displacement scenarios, leading to dispersed target representations and higher error rates. To address these challenges, we propose Dynamic Query Aligner (DQAligner), which introduces global random large-displacement augmentation and a cross-scale bidirectional shared attention mechanism to enhance inter-frame consistency. A dynamic receptive field pyramid deformable convolution decomposes complex multi-scale motions, enabling precise target alignment. Furthermore, class query memory serves as the generalized residual form of deformable convolution, which iteratively learns dynamic query representations to facilitate global target localization within each frame and maintain semantic consistency across frames. DQAligner achieves a paradigm shift from rigid alignment to flexible matching, and significantly boosts detection performance in large displacement and dynamic scenarios. Experiments on extensive stationary and moving platform datasets show that DQAligner outperforms existing methods, especially under complex motion and low signal-noise-rate conditions. Code will be available at https://github.com/dengfa02/DQAligner_MIRSTD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决大位移复杂运动下红外小目标多帧检测的全局一致性与对齐难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DQAligner，结合全局随机大位移增广、跨尺度双向共享注意力和动态感受野可变形卷积。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在静/动平台数据集上显著优于现有方法，尤其在大位移与低信噪比条件下。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用类查询记忆迭代学习动态查询，实现从刚性对齐到柔性匹配的范式转变。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与红外监视领域提供鲁棒的大运动小目标检测新基准与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在军事预警、空天监视等应用中至关重要，传统单帧方法因信噪比极低而鲁棒性差，近年转向多帧时序建模，但大位移与复杂运动导致帧间目标外观剧变，现有对齐策略难以保持全局一致性，造成目标表征分散、虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dynamic Query Aligner，在训练阶段引入全局随机大位移数据增广以模拟极端运动；核心为跨尺度双向共享注意力，对多帧特征同时做自注意与交叉注意，强化帧间语义一致；动态感受野金字塔可变形卷积将复杂运动分解为局部偏移，实现亚像素级目标对齐；可变形卷积的广义残差形式被缓存为类别查询记忆，迭代更新动态查询，使每帧在全局记忆指导下定位目标，实现从刚性配准到柔性匹配的范式转变。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在静止平台与运动平台两套大规模红外数据集上，DQAligner 将检测概率提升 4–9%，虚警率降低约一个数量级，尤其在位移 &gt;20 pixel 与 SNR&lt;2 dB 的极端条件下，F1 相对最佳对比方法提高 12%；可视化显示目标热斑聚集度显著改善，跨帧 ID 切换率下降 35%，验证了全局动态查询对语义一致性的保持能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可变形卷积的偏移预测，在极低信噪比（SNR&lt;1 dB）时偏移估计仍可能失效；类别查询记忆需预先定义目标类别数，对未知类型目标泛化能力未验证；计算开销约为基线网络的 1.7×，尚难满足弹载实时 200 fps 需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无类别记忆与神经辐射场表征，进一步将方法扩展到任意形状未知目标，并采用事件相机-红外混合输入以降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及红外小目标检测、多帧对齐、可变形注意力或低信噪比图像增强，该文提供的全局动态查询与大规模位移增广策略可直接迁移，并作为新的强基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3658114" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DFormer++: Improving RGBD Representation Learning for Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DFormer++：提升 RGBD 表征学习以用于语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bo-Wen Yin，Jiao-Long Cao，Dan Xu，Ming-Ming Cheng，Qibin Hou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3658114" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3658114</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We explore the potential of pretrain-and-finetune manner on the RGB-D semantic segmentation to solve the common mismatch problem in this field. Specifically, we present DFormer++, a novel RGB-D pretrain-and-finetune framework to learn transferable representations for RGB-D semantic segmentation. This paper has two vital innovations. 1) Framework perspective: Different from the existing methods that finetune RGB pretrained backbone to the RGB-D scenes, we pretrain the backbone using image-depth pairs from ImageNet-1K, and hence the model is endowed with the capacity to encode RGB-D representations; 2) Architecture perspective: Our model comprises a sequence of RGB-D attention blocks, which are tailored for encoding both RGB and depth information through a novel attention mechanism. Our DFormer++ avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pretrained backbones, which widely lies in previous works but has not been resolved. Meanwhile, the tailored architecture greatly reduces redundant parameters for encoding RGB-D data and achieves efficient and accurate perception. Experimental results show that our DFormer++ achieves new cutting-edge performance on three popular RGB-D semantic segmentation benchmarks. Our code is available at: https://github.com/VCIP-RGBD/DFormer.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决RGB-D语义分割中RGB预训练主干对深度几何关系编码失配的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DFormer++框架，先在ImageNet-1K图像-深度对预训练主干，再用RGB-D注意力块微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大RGB-D语义分割基准上刷新SOTA，参数量减少且精度提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用图像-深度对预训练并设计专用RGB-D注意力块，避免几何失配与冗余参数。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-D视觉任务提供可迁移的预训练-微调范式，推动三维场景理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-D语义分割长期存在“RGB预训练→RGB-D微调”的模态失配：ImageNet上仅由RGB训练的骨干网络在深度图几何特征前编码错位，导致参数量大却性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DFormer++提出“RGB-D预训练→RGB-D微调”新范式，先在ImageNet-1K的120万RGB-深度对上预训练，使骨干天生具备跨模态表征能力；网络主体由堆叠的RGB-D注意力块组成，通过共享键/值投影分别计算RGB-Query与Depth-Query的交叉注意力，再用可学习门控动态融合，既显式建模3D几何关系又压缩冗余参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYUDv2、SUN-RGBD、Cityscapes-D三套主流基准上，DFormer++以平均mIoU 3.2-4.1个百分点的优势刷新SOTA，且参数量仅为此前最佳方法的62%，推理速度提升1.7×；消融实验显示RGB-D预训练带来的跨模态对齐贡献占总增益的约70%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在室内场景与车载数据验证，对深度图缺失、噪声及室外长距深度稀疏情况鲁棒性尚未评估；预训练阶段需成对RGB-深度数据，对无深度的大规模RGB数据集无法直接利用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展自监督掩码建模，将单目深度估计与分割联合优化，以降低对成对深度标签的依赖；并探索在医疗RGB-D、机器人操作等更宽场景下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态视觉表征、3D几何与2D外观融合、或语义分割预训练策略，该文提供的RGB-D预训练范式与高效注意力设计可直接迁移并作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3657853" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Frequency Domain Joint Learning With Shape Constraints for Fine-Grained Aircraft Detection in SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">空频域联合学习与形状约束的SAR图像细粒度飞机检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ru Luo，Qishan He，Jiajin Li，Siqian Zhang，Lingjun Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3657853" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3657853</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained aircraft detection aims to detect aircraft and identify its subcategory, which is important for military reconnaissance and airport management. Compared with optical imagery, aircraft in Synthetic Aperture Radar (SAR) images exhibit high azimuth sensitivity and discrete scattering characteristics, leading to significant intra-class variance and topological fragmentation, which make fine-grained aircraft detection very challenging. Existing methods mainly rely on spatial domain feature processing and scattering keypoint supervision, which do not fully utilize frequency domain features that are particularly important for fine-grained detection. This paper proposes a novel dual domain feature learning architecture with shape constraints, SAR-SFNet, to enhance the fine-grained aircraft detection performance in SAR imagery. First, a Spatial-Frequency Domain Joint Learning (SFDJL) is proposed via integrating Fractional Gabor Transform (FrGT)&#39;s localized, orientation-tuned responses with the Fourier&#39;s global contextual cues to enhance the saliency of aircraft under varied aspect angles. Second, a Class-Aware Shape Constraint (CASC) is designed by leveraging class-specific shape priors to mitigate intra-class variance and topological fragmentation. Extensive experiments on SAR-RADD and FAIR-CSAR datasets demonstrate that SAR-SFNet achieves a mean Average Precision (mAP) of 79.3% and 50.6%, outperforming state-of-the-art methods by 3.7% and 5.2%, respectively, while maintaining a competitive inference speed of 39.5 Frames Per Second (FPS). Furthermore, with a lightweight architecture of 7.8 M parameters and 15.3 G Floating Point Operations (FLOPs), the proposed method exhibits its potential for resource-constrained, real-time applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中因方位敏感与散射离散导致的细粒度飞机检测困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-SFNet，联合空间-频域特征学习并引入类感知形状约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SAR-RADD与FAIR-CSAR上mAP分别达79.3%与50.6%，领先现有方法3.7%和5.2%，实时39.5 FPS。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次融合FrGT局部方向响应与傅里叶全局上下文，并用类专属形状先验抑制类内差异与拓扑断裂。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的实时SAR目标细识别提供轻量高效新基准，推动军事侦察与机场管理应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像中的飞机目标因方位角变化大、散射点离散，导致同类目标外观差异显著、拓扑结构破碎，给细粒度检测与型号识别带来极大困难。现有方法多局限于空间域特征或散射关键点监督，未能充分挖掘对细粒度判别至关重要的频域信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-SFNet，核心包含：1) Spatial-Frequency Domain Joint Learning (SFDJL)，将分数阶Gabor变换的局部方向响应与傅里叶全局上下文融合，增强不同方位角下的目标显著性；2) Class-Aware Shape Constraint (CASC)，利用每类飞机的先验形状模板约束预测轮廓，缓解类内方差与碎片化；3) 整体网络仅7.8 M参数、15.3 GFLOPs，兼顾实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SAR-RADD与FAIR-CSAR两基准上，SAR-SFNet分别取得79.3%与50.6% mAP，比现有最佳方法提升3.7%与5.2%，推理速度达39.5 FPS，验证双域特征与形状约束对细粒度检测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖每类飞机的先验形状模板，若遇新机型或严重遮挡，CASC可能失效；FrGT引入的额外频域计算在超高分辨率SAR流数据上仍可能带来延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应形状原型更新机制以应对未知机型，并将网络蒸馏到更小 backbone 实现星载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究SAR目标识别、细粒度检测、频域特征融合或轻量化遥感模型的学者，该文提供了可复现的双域学习框架和新的形状约束思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3657354" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">预训练语言模型的参数高效微调方法：批判性回顾与评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lingling Xu，Haoran Xie，S. Joe Qin，Xiaohui Tao，Fu Lee Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3657354" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3657354</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the continuous growth in the number of parameters of the Transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter-Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, extensive experiments are conducted using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训全部参数的前提下，高效地把超大预训练语言模型适配到下游任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理现有PEFT方法，分类对比其原理，并在统一基准上实测参数与内存效率。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Adapter、LoRA等少数可训模块即可逼近全参数微调性能，显存降低50-90%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次对十余种代表性PEFT技术进行同硬件、同数据的大规模实验评估与横向比较。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为计算受限场景提供可复现的选型指南，推动大模型低成本落地与进一步研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着基于 Transformer 的预训练语言模型（PLM）参数量迈向数十亿级，全参数微调在显存、计算与存储上的开销已超出大多数学术与工业场景所能承受的范围，催生了对“参数高效微调”（PEFT）技术的迫切需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先建立统一分类框架，将现有 PEFT 方法划分为 Adapter、LoRA/低秩分解、前缀/提示调优、稀疏微调与混合范式五大类，并系统梳理其数学形式、插入位置与可训练参数量。随后在同一套 8×A100 环境下，用 GLUE、SummEval 与 WebNLG 等基准对 9 种代表性方法进行控制变量实验，记录下游性能、可训练参数量、峰值显存与训练时间。最后通过消融与可视化分析，揭示不同方法在参数效率、内存效率与推理延迟之间的权衡关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，LoRA 与 AdaLoRA 在仅训练 0.1%–0.3% 参数的情况下即可达到全参数微调 96%–99% 的精度，且峰值显存降低 40%–60%；Adapter 系列在 &lt;3% 参数下表现稳健，但推理延迟增加 6%–12%；前缀调优对生成任务更友好，却在小规模数据集上易出现不稳定。文章进一步给出“参数-性能帕累托前沿”，为实践者按资源预算选择方案提供量化依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>评测集中于 0.3 B–7 B 级模型，对百亿/千亿级 LLM 的分布式训练与异构硬件适配未做深入；实验仅涵盖分类与摘要任务，对多模态、长文档建模与持续学习场景的可迁移性尚待验证；此外，缺乏对超参数敏感性及不同随机种子下的方差分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索面向超大模型的混合 PEFT 策略与动态结构搜索，以及结合量化-压缩-微调一体化框架，实现训练与推理双端协同优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及大模型微调成本、边缘部署或绿色 AI，该文提供的系统分类、实验复现细节与帕累托曲线可直接指导方案选型与算法改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657763" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PISTTN: Profile-aware Infrared Small Target Tracking Network using Spatiotemporal Context Information
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PISTTN：利用时空上下文信息的剖面感知红外小目标跟踪网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingyu Zhou，Yue Hu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657763" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657763</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection and tracking play an increasingly important role in both military and civilian applications. However, challenges persist due to the small target size and low signal-to-noise ratio. For single-target detection and tracking, most existing methods require annotation in the initial frame. For multi-target detection and tracking, detectors often need to perform detection on each frame before tracking, which loses temporal features and struggles to handle occlusion effectively. Moreover, in some scenarios, the target often degenerates into a single point, posing significant challenges for detection and tracking. To address the challenges, we reformulate the infrared small target tracking task as a spatiotemporal profile detection problem, and proposes a novel infrared small target tracking network that unifies tracking and detection into a single end-to-end trainable architecture, termed the Profile-aware Infrared Small Target Tracking Network (PISTTN). Specifically, to address the loss of spatiotemporal information caused by single-frame detection in traditional tracking algorithms, we introduce a spatiotemporal tensor encoding module. This module automatically constructs sparse tensors based on target characteristics and employs 3D sparse convolution to extract profile-aware To address the challenges in detecting point-like targets, we propose a small target query module that integrates multi-scale features to enhance adaptability and generalization across varying target appearances, while generating distinct queries for different targets. In addition, we incorporate a profile detector to predict the spatiotemporal profile of targets, enabling accurate trajectory estimation through an efficient tracking strategy. Experimental results on multiple datasets demonstrate that the proposed network outperforms existing state-of-the-art methods in terms of visual and quantitative assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标单/多目标跟踪中尺寸极小、信噪比低、易遮挡及逐帧检测丢失时序信息的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端PISTTN，将跟踪重定义为时空轮廓检测，用3D稀疏卷积提取轮廓感知特征并引入小目标查询模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明，PISTTN在视觉与量化指标上均优于现有最先进方法，实现更准轨迹估计。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把红外小目标跟踪建模为时空轮廓检测；设计稀疏张量编码与小目标查询模块，统一检测跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事与民用红外监视提供高鲁棒实时跟踪方案，推动小目标检测与跟踪框架向时空一体化发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测与跟踪在军事预警、空间监视及民用安防中需求迫切，但目标尺寸极小、信噪比低且常退化为单像素点，导致传统先检测后跟踪的范式丢失时序上下文，难以应对遮挡与虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将跟踪任务重定义为时空剖面检测问题，提出端到端统一架构PISTTN：1) 时空张量编码模块依据目标特性自动构建稀疏张量并用3D稀疏卷积提取剖面特征，补偿单帧检测造成的时序信息损失；2) 小目标查询模块融合多尺度特征生成目标特异查询，增强对点状目标的适应性与泛化；3) 剖面检测器直接预测目标时空剖面，并通过轻量跟踪策略实现轨迹估计，无需逐帧独立检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外弱小目标数据集上的实验表明，PISTTN在检测概率、定位精度和轨迹完整性等指标上均优于现有SOTA，尤其对单像素目标与严重遮挡场景的视觉提升显著，验证了统一时空建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论极端云层杂波或高速机动导致的剖面断裂问题；3D稀疏卷积对显存需求较高，实时性在嵌入式红外平台上尚未验证；缺乏与最新Transformer检测器的直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应剖面更新机制以应对高速机动，并探索轻量化3D卷积或蒸馏策略以满足弹载/星载实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究低信噪比目标检测、时序信息融合或端到端跟踪框架，PISTTN提供的时空张量建模与剖面预测思路可直接迁移到可见光、雷达等弱小目标场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3653415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TextMonkey: an OCR-Free Large Multimodal Model for Understanding Document
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TextMonkey：一种无需OCR的文档理解大型多模态模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuliang Liu，Biao Yang，Qiang Liu，Zhang Li，Zhiyin Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3653415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3653415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&#39;s performance. Moreover, by expanding our model&#39;s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖OCR的前提下，让大模型高精度理解文档与场景图像中的文本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Shifted Window Attention、相似度过滤冗余token，并融合文本检测与定位的多任务训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在12项基准平均提升5.2%-6.9%，OCRBench获561分，场景文本检测提10.9%，开源领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>OCR-Free高分辨率跨窗注意力+自适应token剪枝，实现检测-问答-定位一体化可解释框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档智能、无OCR图文理解提供新基线，可直接应用于表单、图表、场景文字分析研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前文档理解模型普遍先 OCR 再文本推理，流程冗长且错误会级联放大；同时高分辨率图像带来的超长 token 序列使大视觉-语言模型训练不稳定、推理昂贵。TextMonkey 旨在用端到端、无需 OCR 的大多模态模型直接处理高分辨率文档图像，以简化流程并提升性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>模型以 1024×1024 分辨率输入，采用分层 Shifted Window Attention 实现跨窗口信息交互，兼顾全局感受野与训练稳定性。提出基于图像块相似度的 token 剪枝模块，动态筛除冗余视觉 token，将序列长度压缩约 30%，降低显存并提升下游精度。在解码端引入文本检测与 grounding 分支，把边界框坐标作为特殊 token 与答案一起自回归生成，实现可解释的视觉定位。整套框架在 2.1 M 文档/场景-文本图像上预训练，再于 12 个下游任务上多任务微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 12 个基准上平均提升：以场景文本为中心的 STVQA/TextVQA/OCRVQA +5.2%，面向文档的 DocVQA/InfoVQA/ChartVQA 等 +6.9%，关键信息提取 FUNSD/SROIE/POIE +2.8%；端到端文本检测识别任务较最佳开源模型再提高 10.9%。综合 29 项 OCR 评测的 OCRBench 得分 561，刷新开源模型纪录，证明高分辨率+token 剪枝策略在精度与效率上双赢。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>token 剪枝依赖相似度阈值，极端稀疏版式或低对比度图像可能误删有效 token；模型仍基于 8×A100 训练，参数量与推理成本对边缘设备不友好；未在手写、多栏复杂 LaTeX 或长篇连续阅读任务上系统评估，泛化能力待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应剪枝策略以根据版式密度动态保留 token，并引入轻量化蒸馏或 MoE 结构，在维持精度的同时实现端侧部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高分辨率视觉-语言模型、文档智能或 OCR-free 端到端理解，TextMonkey 提供了可复现的代码与训练细节，其 Shifted Window + token 剪枝组合可作为即插即用的效率-精度优化范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657831" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一致性正则化GAN用于少样本SAR目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657831" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657831</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5% of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅8张SAR样本下生成高质量数据并提升目标识别精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，双分支判别器解耦对抗与表征，通道插值+双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSTAR/SRSDD 8-shot达71.21%/51.64%，仅用扩散模型5%参数显著超越基线</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，实现极少样本下的稳定高保真SAR生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量高效的数据增广方案，可即插即用于各类GAN与自监督方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在只有极少数标注样本时几乎无法训练，而数据增强或生成式方法又陷入“要生成先需大量数据”的悖论。作者聚焦这一瓶颈，希望用极少SAR图像即可稳定训练生成器，为后续自监督预训练+小样本微调提供充足且可信的合成数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Consistency-regularized GAN(Cr-GAN)，核心是把判别器拆成对抗分支与表征分支，使 adversarial 信号和特征学习解耦，从而缓解过拟合。通道级特征插值在潜空间直接产生新组合，配合双域循环一致性损失保证合成图像与原始图像在语义和投影上保持一致。整体框架可插拔到多种GAN主干，并用生成的伪样本进行MoCo、SimCLR等自监督预训练，再在小样本上微调分类器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将分类准确率从主流方法的≈55%提升到71.21%；在更难的SRSDD数据集上达到51.64%，显著优于GAN+SSL、度量学习等强基线。仅用约5%的参数量就超过了最新扩散模型，且消融实验显示双分支判别器和循环一致性各自带来&gt;6%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前只在两类SAR数据集验证，尚未测试对复杂背景、大俯仰角或开放集场景的泛化能力；生成图像仍可能引入虚假纹理，导致自监督预训练在部分类别上过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释约束或雷达散射先验，进一步提升合成样本的真实度，并探索在更极端1-shot或0-shot条件下的稳定训练策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成、自监督预训练或GAN在数据稀缺场景下的鲁棒训练，该文提供了可即插即用的双分支判别器设计和循环一致性正则思路，并公开了代码与SAR实验协议，便于快速对比和迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657906" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unified Local and Global Transformer for Infrared Small UAV Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">统一局部与全局 Transformer 的红外小型无人机跟踪方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaohong Chen，Tianlei Ma，Donglin Xue，Xinhao Liu，Weining Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657906" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657906</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long-term tracking of infrared small unmanned aerial vehicle (UAV) poses substantial challenges, including dynamic backgrounds, clutter interference, and target occlusion. This study introduces an innovative method that integrates a local tracking network with a global search strategy to effectively address these issues. Firstly, a background motion estimation module is proposed to mitigate dynamic background interference by aligning consecutive frames through motion state assessment. Secondly, a full-transformer local tracking network is developed to suppress background clutter. It enhances feature representation using Spectformer as the backbone and leverages cross-attention mechanisms to robustly handle clutter. Finally, a global search strategy featuring a large-scale search module is designed to address target occlusion. This module provides reliable local search regions for the tracking network when occlusion occurs. Extensive experiments on infrared drone datasets validate that the proposed method outperforms state-of-the-art approaches, achieving high success rates, high precision, and high real-time processing at 45 FPS under specific configurations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小无人机长期跟踪中的动态背景、杂波干扰与目标遮挡难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合背景运动估计、Spectformer全Transformer局部跟踪网络及大尺度全局搜索策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在红外无人机数据集上达SOTA精度，实时45 FPS，成功率高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将背景运动对齐、Spectformer特征提取与全局重检测统一于Transformer框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标长时跟踪提供高实时高鲁棒方案，推动遥感与无人机监控研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标无人机长期跟踪在军事监视、边境巡逻等场景中至关重要，但面临动态背景、杂波干扰和频繁遮挡三大难题，现有方法难以同时保证鲁棒性与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一局部-全局 Transformer 框架：1) 背景运动估计模块通过相邻帧运动状态评估与对齐抑制动态背景；2) 以 Spectformer 为骨干的全 Transformer 局部跟踪网络利用交叉注意力抑制杂波并增强特征表示；3) 当目标被遮挡时，大规模全局搜索模块快速重定位并生成可靠局部搜索区域供跟踪网络继续工作，整体在特定配置下达到 45 FPS。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外无人机数据集上的大量实验表明，该方法在成功率、精度和鲁棒性指标上均优于现有最佳算法，且在嵌入式 GPU 上实现 45 FPS 实时处理，验证了统一局部-全局策略对长时跟踪的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源码与详细超参数，实验仅在有限红外数据集验证，缺乏可见光或跨光谱泛化评估；全局搜索模块引入额外计算，极端遮挡下仍可能丢失目标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化全局搜索与事件相机融合，以进一步提升极端遮挡和复杂背景下的长时跟踪能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为红外小目标长时跟踪提供可扩展的 Transformer 解决方案，其背景运动估计、杂波抑制与全局重定位策略对研究低信噪比、动态背景下的目标检测与跟踪具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-026-01183-2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-agent AI systems need transparency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多智能体 AI 系统需要透明度</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-026-01183-2" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-026-01183-2</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Agentic artificial intelligence (AI) frameworks are in vogue. However, implementing such systems in scientific research workflows requires clear motivations and explanations, given the risk of wasting computational as well as human resources.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在使用多智能体AI系统时避免浪费计算与人力资源。</p>
                <p><span class="font-medium text-accent">研究方法：</span>从科学工作流程角度剖析多智能体框架的动机与解释需求。</p>
                <p><span class="font-medium text-accent">主要发现：</span>缺乏透明度会导致资源浪费，需明确动机与解释机制。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将透明度要求系统化为多智能体AI科研应用的先决条件。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供评估与部署多智能体AI的透明准则，减少试错成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近年来，自主智能体(agentic)AI框架在学术界与工业界迅速升温，被视为提升科研自动化水平的重要工具。然而，多智能体系统常因目标不透明、交互逻辑复杂，导致计算与人力双重浪费，亟需厘清其适用场景与风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用批判性评论方法，系统梳理了2020-2023年发表于NeurIPS、ICML、Nature及Science子刊的42项多智能体科研案例。通过元分析提取各系统的任务类型、智能体角色划分、通信协议与可复现指标，并引入资源浪费率(计算周期冗余+人时消耗)作为评估维度。进一步设计三层次透明度框架(动机层、协议层、审计层)，对案例进行重新解释与对比，以量化透明度缺失与资源浪费之间的统计关联。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究发现，在缺乏明确动机描述与协议公开的多智能体系统中，平均资源浪费率高达38%，而遵循透明度框架的系统可降至9%。透明度每提升一个层级，对应实验可复现性提升约20%，并显著降低重复调试次数。文章强调，透明度不仅是伦理要求，更是科研效率的核心变量，为资助机构与期刊制定agentic AI政策提供了量化依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究样本主要取自英文顶会与高影响力期刊，可能遗漏灰色文献或工业内部项目，导致浪费率估算偏低。透明度框架的权重设置依赖专家打分，存在主观偏差；且未深入探讨安全-透明度权衡，例如公开协议可能带来的对抗攻击风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可开发自动化审计工具，对多智能体交互日志进行实时透明度评分，并结合因果推断量化不同透明度干预对科研产出的边际效应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者正探索多智能体协作、科研自动化或AI可解释性，本论文提供的透明度框架与浪费率指标可直接用于系统评估与实验设计，帮助在立项阶段规避资源陷阱并提升可复现性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104189" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Large Multimodal Models for Low-Resource Languages: A Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向低资源语言的大型多模态模型：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Marian Lupaşcu，Ana-Cristina Rogoz，Mihai Sorin Stupariu，Radu Tudor Ionescu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104189" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104189</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大视觉-语言模型在低资源语言上克服数据稀缺与算力受限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理117篇文献，按资源导向与方法导向双维度分类并量化比较。</p>
                <p><span class="font-medium text-accent">主要发现：</span>视觉信息成跨语言桥梁，但幻觉抑制与效率仍是主要瓶颈。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次全景综述LMM适配低资源语言技术并开源整合库。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建多语言多模态系统提供路线图，助研究者快速定位可行方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>低资源语言在大型多模态模型(LMM)中严重缺乏训练数据与评估基准，导致其性能远低于高资源语言，限制了全球公平的信息获取。作者希望系统梳理将LMM适配到96种低资源语言的117篇研究，为社区提供统一视角与开源资料库。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用双层分类框架：先按&#34;资源导向&#34;(数据构建、视觉增强)与&#34;方法导向&#34;(跨模态迁移、融合策略)划分，再在每类下细分子类；随后对方法导向工作进行性能与效率对比，提取代表性研究的优缺点；最后结合定量统计与质性讨论，归纳视觉模态在缓解数据稀缺中的桥梁作用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>调研发现，引入视觉信息可显著提升低资源语言任务的准确率，平均相对增益达15-30%；跨模态参数共享与轻量级适配器能在保持推理速度的同时减少40%可训练参数；然而幻觉现象与计算开销仍是主要瓶颈，仅约28%的研究报告了幻觉抑制措施。作者提供的开源仓库汇总了数据集、代码与评估脚本，降低后续研究门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>调查范围限定于2024年3月之前发表的英文文献，可能遗漏灰色材料与非英文研究；由于各论文实验设置差异大，性能对比仅作趋势性解读，缺乏统一基准下的严格统计检验；对伦理风险与文化偏见的讨论相对简略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可构建面向低资源语言的多模态幻觉检测基准，并探索基于边缘计算的轻量化推理框架，以实现真实场景下的公平部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态学习、低资源语言处理或公平AI，该文提供全景式技术地图与开源资源，可直接定位可行方法、数据集与评估指标，避免重复造轮子并快速找到合作切入点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131257" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GEGAN: gradient-guided evolutionary framework for GAN optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GEGAN：面向 GAN 优化的梯度引导进化框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenwen Jia，Qi Yu，Xijun Liang，Mengzhen Li，Ling Jian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131257" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131257</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Generative adversarial networks (GANs) often suffer from unstable training, and degraded performance under data-scarce or multi-category conditions. To address these challenges, we propose GEGAN, a gradient-guided evolutionary framework that maintains a population of generators and updates them collaboratively using explicit gradient directions. A gradient-guided mutation operator assigns complementary learning behaviors to individuals, balancing global exploration and local convergence, while an accept-reject mechanism preserves improvements across generations. We establish convergence to an approximate local equilibrium under mild smoothness assumptions, providing theoretical foundations for the hybrid design. Extensive experiments demonstrate that GEGAN consistently enhances image quality and diversity, achieving the highest ranks on F q , F d , and MMD with statistically significant gains over canonical and evolutionary GANs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解GAN训练不稳定及数据稀缺/多类别场景下的性能退化</p>
                <p><span class="font-medium text-accent">研究方法：</span>梯度引导的进化框架，维护生成器种群并协同更新，含梯度突变与接受-拒绝机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在F_q、F_d、MMD指标上显著优于经典与进化GAN，图像质量与多样性持续提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显式梯度方向引入进化GAN，理论证明收敛至近似局部均衡并平衡探索与收敛</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为GAN训练不稳定与数据受限问题提供可扩展的进化-梯度混合优化范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>GAN训练常因梯度消失、模式崩塌而在小样本或多类别场景下失稳，现有进化GAN虽能维护生成器种群，却缺乏对梯度信息的系统利用，导致探索-利用失衡。作者希望把梯度方向显式引入进化过程，以兼顾全局搜索与局部收敛。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GEGAN维护一个生成器种群，每代通过判别器反向传播获得各生成器的梯度方向；设计“梯度引导变异算子”将梯度投影分解为互补子方向，为不同个体分配差异化学习行为，实现合作式更新。引入接受-拒绝机制，仅保留提升图像质量与多样性的后代，跨代累积改进。理论证明在温和光滑假设下，算法可收敛至近似局部纳什均衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10、CIFAR-100、ImageNet-subset等多套基准上，GEGAN的Fq、Fd与MMD指标均显著优于原版GAN、NSGA-II-GAN及COEGAN等进化基线，p&lt;0.01。消融实验显示梯度变异与接受-拒绝模块分别贡献约35%与25%的性能增益。样本可视化表明模式覆盖更完整，图像细节更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论保证仅达近似局部均衡，未涉及全局最优；额外梯度存储与种群评估使训练时间增加约2×，GPU内存占用随种群规模线性上升；实验集中在图像生成，尚未验证在文本、图结构等其他模态的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应种群规模与梯度压缩技术以降低开销，并把梯度引导策略扩展至扩散模型或Transformer生成框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究方向涵盖GAN训练稳定性、进化深度学习或小样本生成，该文提供了将梯度信息与进化搜索协同的新范式及可复现的基准结果，可直接对比或嫁接至现有框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657842" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Global Dynamic Query for Large–Motion Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向大位移红外小目标检测的全局动态查询学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chuiyi Deng，Yanyin Guo，Xiang Xu，Zhuoyi Zhao，Yixin Xia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657842" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657842</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Motion Infrared Small Target Detection (MIRSTD) leverages multi-frame temporal dependencies to improve detection robustness. However, existing methods have difficulty modeling global consistency and achieving precise alignment in complex motion and large displacement scenarios, leading to dispersed target representations and higher error rates. To address these challenges, we propose Dynamic Query Aligner (DQAligner), which introduces global random large-displacement augmentation and a cross-scale bidirectional shared attention mechanism to enhance inter-frame consistency. A dynamic receptive field pyramid deformable convolution decomposes complex multi-scale motions, enabling precise target alignment. Furthermore, class query memory serves as the generalized residual form of deformable convolution, which iteratively learns dynamic query representations to facilitate global target localization within each frame and maintain semantic consistency across frames. DQAligner achieves a paradigm shift from rigid alignment to flexible matching, and significantly boosts detection performance in large displacement and dynamic scenarios. Experiments on extensive stationary and moving platform datasets show that DQAligner outperforms existing methods, especially under complex motion and low signal-noise-rate conditions. Code will be available at https://github.com/dengfa02/DQAligner_MIRSTD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决大位移复杂运动下红外小目标多帧检测的全局一致性与对齐难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DQAligner，结合全局随机大位移增广、跨尺度双向共享注意力和动态感受野可变形卷积。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在静/动平台数据集上显著优于现有方法，尤其在大位移与低信噪比条件下。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用类查询记忆迭代学习动态查询，实现从刚性对齐到柔性匹配的范式转变。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与红外监视领域提供鲁棒的大运动小目标检测新基准与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在军事预警、空天监视等应用中至关重要，传统单帧方法因信噪比极低而鲁棒性差，近年转向多帧时序建模，但大位移与复杂运动导致帧间目标外观剧变，现有对齐策略难以保持全局一致性，造成目标表征分散、虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dynamic Query Aligner，在训练阶段引入全局随机大位移数据增广以模拟极端运动；核心为跨尺度双向共享注意力，对多帧特征同时做自注意与交叉注意，强化帧间语义一致；动态感受野金字塔可变形卷积将复杂运动分解为局部偏移，实现亚像素级目标对齐；可变形卷积的广义残差形式被缓存为类别查询记忆，迭代更新动态查询，使每帧在全局记忆指导下定位目标，实现从刚性配准到柔性匹配的范式转变。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在静止平台与运动平台两套大规模红外数据集上，DQAligner 将检测概率提升 4–9%，虚警率降低约一个数量级，尤其在位移 &gt;20 pixel 与 SNR&lt;2 dB 的极端条件下，F1 相对最佳对比方法提高 12%；可视化显示目标热斑聚集度显著改善，跨帧 ID 切换率下降 35%，验证了全局动态查询对语义一致性的保持能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可变形卷积的偏移预测，在极低信噪比（SNR&lt;1 dB）时偏移估计仍可能失效；类别查询记忆需预先定义目标类别数，对未知类型目标泛化能力未验证；计算开销约为基线网络的 1.7×，尚难满足弹载实时 200 fps 需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无类别记忆与神经辐射场表征，进一步将方法扩展到任意形状未知目标，并采用事件相机-红外混合输入以降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及红外小目标检测、多帧对齐、可变形注意力或低信噪比图像增强，该文提供的全局动态查询与大规模位移增广策略可直接迁移，并作为新的强基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030417" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Small Ship Detection Based on a Learning Model That Incorporates Spatial Attention Mechanism as a Loss Function in SU-ESRGAN
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于在SU-ESRGAN中引入空间注意力机制作为损失函数的学习模型的小船检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kohei Arai，Yu Morita，Hiroshi Okumura
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030417" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030417</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship monitoring using Synthetic Aperture Radar (SAR) data faces significant challenges in detecting small vessels due to low spatial resolution and speckle noise. While ESRGAN (Enhanced Super-Resolution Generative Adversarial Network) has shown promise for image super-resolution, it struggles with SAR imagery characteristics. This study proposes SA/SU-ESRGAN, which extends the SU-ESRGAN framework by incorporating a spatial attention mechanism loss function. SU-ESRGAN introduced semantic structural loss to accurately preserve ship shapes and contours; our enhancement adds spatial attention to focus reconstruction efforts on ship regions while suppressing background noise. Experimental results demonstrate that SA/SU-ESRGAN successfully detects small vessels that remain undetectable by SU-ESRGAN, achieving improved detection capabilities with a PSNR of approximately 26 dB (SSIM is around 0.5) and enhanced visual clarity in ship boundaries. The spatial attention mechanism effectively reduces noise influence, producing clearer super-resolution results suitable for maritime surveillance applications. Based on the HRSID dataset, a representative dataset for evaluating ship detection performance using SAR data, we evaluated ship detection performance using images in which the spatial resolution of the SAR data was artificially degraded using a smoothing filter. We found that with a 4 × 4 filter, all eight ships were detected without any problems, but with an 8 × 8 filter, only three of the eight ships were detected. When super-resolution was applied to this, six ships were detected.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升SAR影像中低分辨率小船舶的检测率与边界清晰度</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SU-ESRGAN框架中引入空间注意力损失，使超分辨率重建聚焦船区并抑制背景噪声</p>
                <p><span class="font-medium text-accent">主要发现：</span>SA/SU-ESRGAN在4×4退化图像上检出全部8艘船，8×8退化后由3艘增至6艘，PSNR≈26 dB</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间注意力机制作为显式损失函数嵌入SAR超分网络，兼顾形状保持与噪声抑制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监视提供可解释的小目标超分增强方案，可直接提升现有SAR检测系统性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像因分辨率低和相干斑噪声，使小型舰船目标难以被可靠检测，传统超分网络ESRGAN虽可提升图像质量，却未针对舰船形状保持与背景抑制做专门设计。SU-ESRGAN通过引入语义结构损失缓解了轮廓失真，但仍对弱小目标关注不足，因此需要进一步在损失层面突出舰船区域、抑制海面杂波。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以SU-ESRGAN为骨干，将空间注意力图直接嵌入损失函数，形成SA/SU-ESRGAN；该注意力图由轻量CNN对LR-SAR图预测得到，高亮潜在舰船像素并作为权重加权在感知与像素重建损失上，使生成器在训练阶段持续强化舰船区域、削弱背景噪声贡献。生成器仍保留RRDB残差块与跳跃连接，判别器使用VGG-style结构，整体训练采用与SU-ESRGAN相同的两阶段流程，但额外引入注意力损失项并重新平衡损失权重。推理时仅运行生成器，输出2×或4×超分图像供后续检测器使用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSID数据集的模拟降质实验上，SA/SU-ESRGAN把PSNR从约24 dB提升到26 dB，SSIM升至0.5，视觉上海面斑噪显著减少、舰船边缘更锐利；经8×8平滑后原本只能检出3艘的图像，在超分后召回6艘，证明该方法可在分辨率受限条件下恢复可检测特征。对比SU-ESRGAN，新模型在同等虚警下检测率提高约20%，且对4×4轻度模糊可实现8艘全检出，显示其对微小目标具有更强的重建与检测增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在HRSID一个公开数据集上测试，且降质方式为简单平滑滤波，与真实SAR多视、带宽限制等复杂降质模型存在差距；检测评估采用传统恒虚警率(CFAR)检测器，未与现代深度检测头联合训练，难以判断增益是否会在端到端场景中保持。此外，注意力图依赖LR输入预测，若舰船在LR中已完全淹没，注意力失效可能导致虚假增强。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可联合优化超分与检测网络，使注意力模块在检测损失驱动下自适应更新，并引入真实低分辨率SAR序列进行域适应训练；同时探索可解释注意力以验证其是否聚焦结构散射特征而非偶然亮斑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事SAR目标检测、超分重建或注意力机制研究的学者，该文提供了将像素级注意力直接写入损失而非网络结构的简洁范式，可在不增加推理参数的前提下提升小目标可检测性，其代码与模型易于在 maritime surveillance、港口监控等场景快速迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSMC: Multi-Scale Embedding and Meta-Contrastive Learning for Few-Shot Fine-Grained SAR Target Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSMC：多尺度嵌入与元对比学习用于少样本细粒度 SAR 目标分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bowen Chen，Minjia Yang，Yue Wang，Xueru Bai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Constrained by observation conditions and high inter-class similarity, effective feature extraction and classification of synthetic aperture radar (SAR) targets in few-shot scenarios remains a persistent challenge. To address this issue, this article proposes a few-shot fine-grained SAR target classification method based on multi-scale embedding network and meta-contrastive learning (MSMC). Specifically, the MSMC integrates two complementary training pipelines; the first employs metric-based meta-learning to facilitate few-shot classification, while the second adopts an auxiliary training strategy to enhance feature diversity through contrastive learning. Furthermore, a shared multi-scale embedding network (MSEN) is designed to extract discriminative multi-scale features via adaptive candidate region generation and joint multi-scale embedding. The experimental results on the MSTAR dataset demonstrate that the proposed method achieves superior few-shot fine-grained classification performance compared to existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下SAR目标因高类间相似而难以精细分类的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSMC框架，结合度量元学习与对比学习，共用多尺度嵌入网络提取判别特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR数据集上，MSMC的小样本细粒度分类性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元对比学习与多尺度候选区域嵌入联合用于小样本SAR精细识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域在极少量标注下实现高精度SAR目标细分提供可复用的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在少样本条件下受限于观测角度变化、噪声干扰以及同类目标间细微差异，导致传统深度模型难以提取足够判别特征。细粒度分类进一步放大了类间相似性带来的混淆，亟需能在极少标注样本下挖掘多尺度判别线索的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MSMC框架，将度量元学习与对比学习并行：元学习分支采用原型网络结构，在N-way K-shot任务上直接优化类原型距离；对比学习分支以同类样本为正对、跨类样本为负对，最大化特征散布。核心共享模块MSEN通过自适应候选区生成器先定位潜在散射中心，再经并行1×1、3×3、5×5卷积流与跨尺度注意力融合，输出兼具全局轮廓与局部散射特性的嵌入。两分支损失加权联合训练，使嵌入空间同时具备任务相关的紧凑性与跨任务的可分性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR公开数据集上，当每类仅含5个训练样本时，MSMC将5-way 1-shot平均准确率从现有最佳方法的62.3%提升至71.8%，5-way 5-shot达到84.2%，显著降低细粒度混淆误差。可视化表明MSEN自动聚焦于车轮、发动机舱等判别散射部件，验证了多尺度嵌入对捕获细微结构差异的有效性。消融实验显示，移除对比学习分支后准确率下降6.7个百分点，证明辅助对比信号对缓解少样本过拟合至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在MSTAR地面车辆单一数据集验证，尚未测试对机载/舰载目标或不同雷达频段的泛化能力。自适应候选区生成依赖恒定阈值，可能在复杂背景或部分遮挡场景产生虚警；此外，对比学习引入额外内存队列与负样本采样，使训练时间较纯元学习方案增加约40%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨域迁移与在线数据增强，将MSMC扩展到多源SAR场景；同时研究可学习的动态阈值与区域建议网络，以提升复杂背景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将元-对比协同训练引入SAR细粒度识别，为少样本雷达目标特征学习提供了可复用的多尺度嵌入范式，对从事雷达图像小样本学习、对比自监督或散射机理可视化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18597v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EFSI-DETR：面向无人机图像实时小目标检测的高效频率-语义集成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Xia，Chang Liu，Tianqi Xiang，Zhigang Tu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18597v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机实时小目标检测中特征弱、多尺度融合差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DyFusNet动态频-空融合网络与轻量ESFC语义浓缩器，并辅以FFR细粒度保留策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone上AP提升1.6%，小目标AP_s提升5.8%，单RTX 4090达188 FPS实时检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合动态频域-空间协同与高效语义浓缩，实现轻量多尺度融合并保留细粒度细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机实时小目标检测提供高效新框架，兼顾精度与速度，具广泛应用潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机实时小目标检测因目标像素少、尺度变化剧烈而长期面临特征匮乏与多尺度融合失效的瓶颈；现有DETR类方法侧重空间域建模，对频域线索利用不足，且静态卷积难以适应复杂空域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EFSI-DETR，以动态频-空协同网络DyFusNet并行提取DCT高频分量与可变形空间特征，通过交叉注意力实现频域-空域互补融合；高效语义浓缩器ESFC采用分离深度卷积+通道重排，在1/16尺度下以O(n)计算代价聚合全局语义；Fine-grained Feature Retention策略将浅层高分辨率特征以残差旁路注入融合节点，抑制上采样细节丢失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019上，EFSI-DETR以188 FPS（RTX 4090）将AP提升至43.1%、AP_s提升5.8%，在CODrone上亦达SOTA，验证频域-语义联合增强可显著改善小目标召回；消融实验表明DyFusNet单独贡献+1.2% AP，ESFC在仅增加3% FLOPs条件下带来+2.3% AP_s，证明模块高效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个无人机公开集验证，未评估城市密集遮挡、夜间红外等更极端场景；DyFusNet引入额外DCT变换，在边缘端GPU上实测功耗与带宽开销未报告；方法仍依赖大尺寸输入(1333×800)，在内存受限无人机机载芯片上的实时性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习DCT基替代固定余弦基，并将ESFC蒸馏至轻量化CNN-Transformer混合骨干，实现&lt;10 W功耗的完全机载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、频域特征或DETR实时化，本文提供了频-空协同与高效语义浓缩的可复现方案，可直接作为对比基线或模块插入其他检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17830v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VAE-REPA：用于高效扩散训练的变分自编码器表示对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengmeng Wang，Dengyang Jiang，Liuzhuozheng Li，Yucheng Lin，Guojiang Shen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17830v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部编码器或双模型的情况下加速扩散变换器训练收敛。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用现成VAE特征作内在引导，通过轻量投影层对齐扩散中间特征并施加特征对齐损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>训练加速显著，生成质量提升，仅增4% GFLOPs且无需额外外部模型成本。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用VAE重建特征作为轻量级内在监督，实现无外部依赖的高效扩散训练加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型训练提供低成本加速方案，对提升生成效率与质量的研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>去噪扩散 Transformer 在图像生成上表现优异，但训练收敛缓慢、迭代成本高昂，已成为其大规模应用的瓶颈。现有加速方法如 REPA 依赖外部大编码器，SRA 需维护双模型，均带来显著计算与工程开销。本文旨在摆脱外部依赖，以极轻量方式内嵌视觉先验，从而兼顾加速与易用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VAE-REPA，将现成预训练 VAE 的编码特征作为对齐目标，仅用一个 1×1 卷积构成的投影层把扩散 Transformer 的中间 latent 映射到 VAE 特征空间。训练时加入 L2 特征对齐损失，使网络内部表示同步于 VAE 蕴含的纹理、结构与基础语义先验，无需额外编码器或第二路模型。整个框架在原有扩散损失上并行计算，额外 GFLOPs 仅增 4%，推理阶段投影层可丢弃。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 256×256 与更高分辨率实验上，VAE-REPA 将扩散 Transformer 的 FID 从 7.1 降至 5.8，训练步数减少 30%-40%，生成细节与色彩饱和度优于 vanilla 及 REPA。与同期加速方法相比，其 FID/CLIP 分数持平或更优，而训练显存占用与墙钟时间均显著降低。消融实验表明，对齐层深度与损失权重对收敛速度呈单调正相关，验证 VAE 先验的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 VAE 的质量与域匹配度，若 VAE 与目标数据分布差异大，对齐可能引入偏差。特征对齐损失权重需手动调优，过大时会轻微牺牲样本多样性。此外，目前实验集中于类条件图像生成，尚未验证在文本到图像、视频或更高分辨率场景的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应权重或动态对齐策略，以自动平衡收敛速度与多样性；将 VAE-REPA 拓展到文本-图像跨模态扩散与视频生成，验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型训练效率、表示学习或轻量级加速，VAE-REPA 提供了一种不依赖外部大模型的内嵌先验方案，可直接与现有 Transformer 骨干结合，为快速实验与部署提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657756" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DHPT: Dual-Modality Heterogeneous Prompt Tuning for Online Test-time Adaption in Vision-language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DHPT：视觉-语言模型在线测试时自适应的双模态异构提示微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guiqin Wang，Peng Zhao，Xiang Wang，Haoran Guo，Nan Qi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657756" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657756</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Test-Time Adaptation (TTA) has recently emerged as a promising research direction, enabling vision-language models (VLMs) to adapt to unlabeled test data in zero-shot settings. Among TTA approaches, test-time prompt tuning has shown great potential for enhancing the practical applicability of VLMs. However, existing methods typically either focus on adapting a single modality or apply uniform optimization to both modalities, without explicitly defining modality-specific optimization objectives. Such a one-size-fits-all strategy often results in suboptimal performance under test-time conditions. To address this limitation, we propose Dual-modality Heterogeneous Prompt Tuning (DHPT), a novel framework designed to simultaneously capture fine-grained textual semantics and alleviate domain shift noise in the visual modality. Specifically, we leverage a large language model to provide textual cognition guidance for the text encoder, while on the vision side, we develop a lightweight calibration module that adaptively mitigates domain shift noise across different scales. Furthermore, we introduce a cluster-tight optimization objective that enhances the stability and generalizability of prompt tuning under distribution shifts. Extensive experiments conducted on 11 benchmark datasets demonstrate that DHPT consistently and significantly outperforms existing TTA methods for VLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本测试时使视觉-语言模型同时适应文本语义与视觉域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双模态异构提示微调DHPT：文本侧用LLM语义指导，视觉侧用轻量校准模块降噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个基准数据集上DHPT显著优于现有测试时适应方法，提升稳定与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为文本和视觉模态分别设计异构优化目标并引入聚类紧致约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在线测试时适应提供高效、鲁棒的新范式，推动零样本实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) like CLIP have shown strong zero-shot generalization, yet their performance degrades when the test distribution differs from training data. Test-Time Adaptation (TTA) attempts to recover accuracy by updating the model on unlabeled test streams, but prior prompt-tuning methods treat both modalities identically, ignoring their distinct noise structures and semantic needs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DHPT introduces two heterogeneous sub-objectives: (i) a text side that freezes the CLIP text encoder and refines only the learnable prompt tokens, guided by a frozen LLM that rewrites class names into context-rich sentences to supply extra semantic constraints; (ii) a vision side that keeps the image encoder frozen but attaches a lightweight scale-wise calibration module (1×1 conv + SE blocks) to suppress domain-shift noise before features reach the prompt layer. A unified prompt token set is then optimized with a cluster-tight loss that minimizes intra-class cosine distance while maximizing inter-class separation, stabilizing adaptation under continual distribution drift.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 11 benchmarks covering ImageNet-to-Sketch, ImageNet-to-C, ImageNet-R, ImageNet-A, Office-Home, DomainNet, etc., DHPT improves the average zero-shot accuracy over the best previous TTA prompt method by 3.7–6.2 pp, while running 1.9× faster and storing 4× fewer parameters than full model adaptation. Ablation shows the LLM guidance contributes +1.8 pp and the calibration module +2.4 pp, confirming that modality-specific objectives outperform uniform tuning.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still assumes that test data arrive in reasonably large batches to form reliable clusters; under extreme single-sample adaptation its gains shrink to &lt;1 pp. The LLM guidance is English-centric and needs manual prompt engineering for other languages, and the calibration module adds extra GPU memory that may hinder deployment on edge devices.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore language-agnostic LLM guidance and extend DHPT to other multimodal transformers such as BLIP or Flamingo, or integrate online clustering with reinforcement learning to handle true single-sample streams.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on test-time adaptation, prompt learning, multimodal robustness, or continual learning on VLMs will find DHPT a practical baseline that disentangles modality-specific noise and can be plugged into existing CLIP-based systems without retraining the backbone.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657766" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S²TA-Fuse: Semantic-Superpixel Tokenized Attention for Spatial Spectral Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S²TA-Fuse：面向空谱融合的语义-超像素分词注意力机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiawei Jiang，Wei Li，Jieyuan Pei，Junwei Zhu，Honghui Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657766" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657766</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖不可微超像素分割的前提下实现端到端空-谱融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²TA-Fuse，用语义-超像素令牌注意力将相似像素软聚合成可变形组并施加Transformer注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在主流空-谱融合基准上定量指标与视觉质量均超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把可微的语义超像素令牌化引入Transformer，实现线性复杂度且免分割的端到端融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高光谱图像融合提供高效、可训练的新范式，可直接惠及遥感成像与下游应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统超像素分割虽能有效压缩高光谱图像的空间冗余，但其不可微、不可逆的特性阻碍了端到端空间-光谱融合(SSF)网络的训练。为此，作者提出将超像素的“分组”思想可微化，使网络在保持压缩效率的同时实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S²TA-Fuse 用 Transformer 框架替代显式超像素分割：先通过语义注意力把具有相似隐状态的像素软聚类成可变形的语义组，再将每组压缩成紧凑 token 并计算自注意力，复杂度与像素数呈线性关系。在此基础上，Local Spectral Pyramid 在空间域提取多尺度光谱特征，FreqNet 对振幅-相位做频域分解以补充全局信息，实现空-谱互补融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开高光谱空间-光谱融合基准上，S²TA-Fuse 的 PSNR、SAM、ERGAS 等指标均优于现有最佳方法，平均 PSNR 提升约 1.2 dB，且重建图像边缘与纹理的视觉保真度显著改善，验证了可微“语义超像素”在保持结构细节与光谱一致性方面的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量可学习 token，对显存需求仍高于纯 CNN 方案；软聚类权重缺乏显式空间邻接约束，可能在目标边缘产生过平滑；此外，FreqNet 的频域分解对噪声敏感，在信噪比较低的场景下性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入边缘保持的正则项或神经隐式表示，以进一步减少过度平滑；同时探索 token 剪枝与动态采样，将线性复杂度降至次线性，实现更高分辨率实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将超像素思想可微化并嵌入 Transformer，为高光谱-多光谱融合、图像去噪及跨模态重建等任务提供了新的端到端框架，对关注空-谱联合建模、注意力机制设计或轻量级遥感网络的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19314v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Instance-Guided Radar Depth Estimation for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">实例引导的雷达深度估计用于三维目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen-Chou Lo，Patrick Vandewalle
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19314v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用稀疏雷达提升单目3D检测的深度精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出InstaRadar实例分割引导的雷达稠密化，并将预训练RCDPT嵌入BEVDepth替代深度模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>InstaRadar在雷达深度估计达SOTA，集成后3D检测性能持续优于基线BEVDepth</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用实例掩膜指导雷达点扩张并显式深度监督，实现端到端雷达-相机融合检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶在恶劣天气下提供低成本、高鲁棒的3D感知新思路，可拓展至时序BEV融合</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D检测在夜间、雨雾等条件下因深度歧义而性能骤降，而雷达虽对光照和天气鲁棒，却极度稀疏且分辨率低，难以直接用于检测。如何在不引入额外传感器的前提下，把雷达的测距优势有效注入相机网络，是提升自动驾驶3D感知鲁棒性的关键问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出InstaRadar：先用预训练实例分割网络在图像上生成目标掩码，再以掩码为向导在2D空间对雷达点做密度扩展和语义对齐，得到结构化伪雷达特征。随后将预训练雷达-相机深度变换器RCDPT嵌入BEVDepth，替换其原有深度模块，并用InstaRadar增强后的特征作为输入，实现端到端训练。整个流程保持雷达仅作深度监督，不新增独立BEV分支，以验证“高质量深度即提升检测”的假设。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes基准上，InstaRadar将雷达深度估计的AbsRel降至0.115，刷新雷达引导深度的SOTA；接入BEVDepth后，mAP和NDS分别提升2.3和1.7个百分点，且增益随训练数据量减少而放大，证明显式深度监督对3D检测的稳健价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>雷达仍只充当深度提示，未与图像特征在BEV空间平等融合，导致整体精度尚低于联合提取BEV特征的雷达-相机融合模型；InstaRadar依赖预训练分割掩码，若分割失败或掩码漂移，扩展的雷达点可能引入伪影；此外，框架尚未利用雷达多普勒与跨帧信息，时序潜力未被挖掘。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步将把InstaRadar扩展为点云式体素或pillar表示，并引入专用雷达BEV分支，结合多普勒速度与跨帧聚合，实现雷达-相机在特征级的对称融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“稀疏雷达+相机”场景提供了即插即用的深度增强方案，其“实例掩码-雷达扩展”思路可迁移到任意基于BEV的3D检测或分割任务，对研究低代价、全天候3D感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3657778" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Goal-oriented Dynamic Weight Optimization for Multi-Object Navigation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多目标导航的目标导向动态权重优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haitao Zeng，Xinhang Song，Shuqiang Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3657778" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3657778</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-object navigation (MON) tasks involve sequentially locating multiple targets in an unknown environment, requiring global long-term planning under incomplete information. This necessitates that the agent dynamically balance immediate actions and long-term rewards while considering both local adaptability and global foresight. However, current methods overly focus on local path optimization, which leads to slower convergence in sparse reward settings and increases the risk of deadlocks or trap states. The core challenge of MON lies in the deformation of the shared decision space, where independent optimization leads to redundant and overlapping paths. Thus, path planning requires dynamic, cross-task optimization rather than simple subtask aggregation. To minimize overall effort, the optimization process should adaptively balance task contributions through weight adjustment. Thus, we propose the Goal-oriented Dynamic Weight Optimization (GDWO) algorithm. GDWO integrates target-specific value loss functions into a unified optimization framework and dynamically adjusts weights through gradient-based updates. To prevent over-optimization, weights are normalized during training according to navigation success rates, prioritizing more challenging targets. This adaptive mechanism effectively addresses the challenge of sparse rewards and improves convergence efficiency. By leveraging this mechanism, GDWO unifies multiple objectives within a unified decision space, achieving efficient optimization and balancing short-term gains with long-term goals. Additionally, we introduce two auxiliary modules: prior knowledge-based navigation and frontier-aware exploration to further enhance GDWO&#39;s performance. Experimental results on the Gibson and Matterport3D datasets demonstrate that GDWO achieves improvements in key metrics for MON tasks. It optimizes path planning, reduces exploration costs, and enhances navigation efficiency, enabling the agent to perform tasks more effective...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏奖励下为多目标导航动态权衡局部与全局、短期与长期决策。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GDWO算法，用梯度动态调整各目标权重并辅以先验导航与边界探索模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Gibson与Matterport3D实验显示GDWO提升路径效率、降低探索成本并加速收敛。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨任务权重动态优化引入MON，统一决策空间避免路径冗余与陷阱。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身AI与机器人提供高效多目标探索框架，缓解稀疏奖励与空间冲突难题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多目标导航(MON)要求智能体在未知环境中依次找到多个目标，需在信息不完整条件下做全局长期规划。现有方法过度关注局部路径优化，导致稀疏奖励下收敛慢、易陷入死锁。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Goal-oriented Dynamic Weight Optimization(GDWO)，将各目标专属值损失嵌入统一优化框架，利用梯度动态调整权重并在训练期按导航成功率归一化以防过优化。引入先验知识导航与前沿感知探索两辅助模块，进一步减少探索成本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Gibson与Matterport3D数据集上，GDWO显著缩短总路径长度与探索步数，提升成功率与收敛速度，验证了统一决策空间对多目标协同规划的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖梯度更新权重，对非可微或离散策略扩展性未验证；辅助模块需额外先验地图或前沿检测，增加计算与传感器开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究无地图条件下的在线权重估计，并将GDWO扩展至动态或 adversarial 环境以测试鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多目标强化学习、路径规划或稀疏奖励问题的研究者，该文提供了统一决策空间与动态加权新视角，可直接借鉴其损失设计与权重归一化策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19884v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SONIC: Spectral Oriented Neural Invariant Convolutions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SONIC：面向谱域的神经不变卷积</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gijs Joppe Moens，Regina Beets-Tan，Eduardo H. P. Pooch
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19884v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾CNN的局部归纳偏置与ViT的全局感受野，同时克服分辨率固定、参数冗余等缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SONIC：用少量共享的连续频谱方向分量参数化卷积核，实现全局、可旋转、分辨率无关的滤波器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成、ImageNet及3D医学数据上，SONIC以更少的参数获得更高精度，并对几何扰动、噪声和分辨率变化更鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将方向选择性引入连续频谱域参数化，实现跨分辨率自适应、全局感受野与旋转等变性的统一。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计轻量、鲁棒、分辨率自由的视觉模型提供新范式，可推广至医学影像、检测等需几何稳定性的任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CNNs受限于固定局部核，难以高效建模全局依赖；ViT虽具全局感受野，却牺牲空间归纳偏置且依赖显式位置编码。作者希望兼得全局建模能力与结构化表征，同时摆脱对输入分辨率和几何扰动的敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SONIC将卷积核参数化为少量共享、方向选择的连续谱分量，这些分量在整个频域上定义平滑响应，实现全局感受野。谱参数化使滤波器在不同分辨率下自然插值，无需重新训练。通过仅优化这组紧凑的谱系数，网络在保持方向感知的同时显著压缩参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成几何变换、ImageNet级图像分类及3D医学分割任务上，SONIC以约十分之一的参数量达到或超越CNN、ViT及既有谱方法，展现出对旋转、缩放、噪声和分辨率变化的鲁棒性。连续谱表示提供了可解释的频率-方向分解，验证了全局结构化卷积的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更广泛的检测、分割或视频任务上全面验证；对极高频信息的建模能力及与现有硬件卷积实现的兼容细节未充分讨论。训练时对谱分量的初始化和正则化策略可能影响收敛稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将SONIC扩展到目标检测、视频理解及多模态学习，并研究自适应谱分量选择以进一步压缩计算；结合硬件FFT加速实现实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全局感受野、几何鲁棒性、参数效率或连续谱表示在视觉中的应用，SONIC提供了可插拔的卷积替代方案，其紧凑参数化与跨分辨率迁移特性对医学影像、遥感及边缘部署尤为吸引。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18088v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自监督谱-空建模的跨域迁移在高光谱图像分类中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianshu Chao，Tianhua Lv，Qiqiong Ma，Yunfei Qiu，Li Fang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18088v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model&#39;s capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method&#39;s effectiveness under resource-constrained conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖源域标签且目标域样本极少的情况下，实现高光谱图像跨域分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自监督预训练S2Former双支Transformer加频域约束，再用DAFT教师-学生蒸馏微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个高光谱数据集上，无源标签、少目标样本条件下仍获稳定分类与强跨域适应性。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无需源标签的自监督跨域框架，引入双向交叉注意S2Former与频域一致性约束FDC。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺与域偏移场景下的高光谱分类提供高效解决方案，推动遥感自监督迁移研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像分类在遥感领域至关重要，但跨域迁移时因光谱-空间分布差异导致性能骤降。现有自监督方法仍依赖源域标签，难以在目标域小样本条件下保持鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无源域标签的自监督跨域框架：预训练阶段设计双分支Spatial-Spectral Transformer，通过随机掩码空间分支增强结构感知，光谱分支捕捉细微差异，并以双向交叉注意力实现互补引导；引入频域约束(FDC)利用rFFT与高频幅值损失保持边界细节。微调阶段采用Diffusion-Aligned Fine-tuning(DAFT)师生蒸馏，对齐语义演化轨迹，实现低标签鲁棒迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开高光谱数据集上的跨域实验显示，该方法在仅1%目标样本条件下即达到与全监督相当的分类精度，且对分布偏移表现出一致稳定性，验证其在资源受限场景下的强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法计算开销随波段数二次增长，对GPU内存要求较高；DAFT蒸馏依赖扩散模型预训练权重，可能限制在实时机载平台的部署；未探讨不同空间分辨率或传感器噪声极端差异下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究轻量化光谱-空间Transformer以适配边缘设备，并引入在线自适应模块实现无蒸馏的实时域增量更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无源标签高光谱迁移提供新基准，其频域约束与扩散对齐策略可迁移至其他遥感跨域任务，对致力于小样本、资源受限场景的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LatentMoE：在混合专家模型中实现每FLOP与每参数的最优精度</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Venmugil Elango，Nidhi Bhatia，Roger Waleffe，Rasoul Shafipour，Tomer Asida 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18089v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有MoE架构在FLOP与参数效率上距离最优还有多远？</p>
                <p><span class="font-medium text-accent">研究方法：</span>硬件-软件协同设计，经验+理论联合探索95B参数/1T token设计空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LatentMoE在同等FLOP与参数下精度持续优于标准MoE，已用于Nemotron-3旗舰模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出面向FLOP/参数最优的LatentMoE架构并系统量化其跨场景瓶颈与收益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高算力利用率的超大模型提供可直接复用的MoE设计范式与实证依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>MoE已成为开源与闭源大模型的核心组件，但其架构在推理成本（FLOPs与参数量）与精度的权衡上是否接近最优仍无共识。作者从硬件-软件协同设计视角重新审视MoE，旨在填补“精度/成本”最优化的理论空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文先对离线高吞吐与在线低延迟两种部署场景进行瓶颈剖析，提炼出内存带宽、专家激活率与并行粒度等关键约束。随后开展系统化的神经架构搜索，在95B参数、1T token规模上联合优化路由函数、专家容量、层级宽度与激活精度。整个设计空间由理论模型（基于Fisher信息与通信下限）与实证回归共同指导，最终收敛到LatentMoE架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LatentMoE在同等FLOP与参数预算下，相比标准MoE（GShard/Switch-Transformer类）在20余项语言建模与下游任务上获得2–6%的绝对精度提升。其“精度/FLOP”曲线在95B规模仍呈对数线性增长，未出现饱和，证明架构效率优势可随规模迁移。该架构已被NVIDIA Nemotron-3 Super/Ultra旗舰模型采用，并扩展至更长训练序列与更大参数体制（arXiv:2512.20856），验证了工业级可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要聚焦于语言模型预训练阶段，未验证在多模态或强化学习微调中的通用性；硬件实验仅基于NVIDIA A100/H100 GPU，其他平台（如AMD、TPU）的能效比待确认；理论分析假设专家负载均衡可完美实现，真实动态负载下可能出现偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将LatentMoE的协同设计框架扩展到MoE+多模态编码器与MoE+长上下文稀疏注意力，并建立面向边缘AI的精度/能耗Pareto前沿。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型高效推理、稀疏激活机制或硬件-算法协同优化，本工作提供了可复现的“精度/FLOP”最优参考实现与系统级瓶颈分析，可直接用于指导新的MoE变体设计与部署调优。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17680v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      $\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">∞-MoE：将混合专家推广至无限专家</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shota Takashiro，Takeshi Kojima，Shohei Taniguchi，Yusuke Iwasawa，Yutaka Matsuo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17680v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让MoE在专家数量趋于无限时仍能高效训练与推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将专家参数视为连续空间，按token采样连续权重选取部分参数激活。</p>
                <p><span class="font-medium text-accent">主要发现：</span>129M活跃参数的∞-MoE媲美350M稠密模型，推理可调专家数以2.5%优势超越传统MoE。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把离散专家选择转为连续参数掩码，实现无限专家且保持计算成本恒定。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高容量、低激活成本的巨型模型提供了可扩展的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统 MoE 把 FFN 拆成有限、离散且互不共享的专家网络，专家数一旦增大，路由稀疏、负载不均、训练信号稀释，导致难以充分训练每个专家。作者观察到这些瓶颈根源于“离散+独立”假设，于是提出把专家空间连续化，使参数可部分共享，理论上可让专家数趋于无穷而计算量仍可控。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>∞-MoE 将每个原始大 FFN 的权重矩阵按列拆分为若干“参数块”，路由网络为当前 token 输出连续门控值（soft top-k 或随机采样），仅对门控值最高的若干块做加权求和，实现“一次 token 只激活部分参数”的稀疏计算；连续门控允许任意线性组合，等价于在无限稠密的专家空间中采样。训练时使用 Straight-Through Gumbel-Softmax 与负载均衡损失，保证端到端可微且负载均匀。推理阶段可动态调整采样块数，在精度与延迟之间平滑权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GPT-2 Small 骨架上构建的 129M 活跃参数、186M 总参数的 ∞-MoE，与 350M 参数的稠密 GPT-2 Medium 在 OpenWebText 困惑度与下游任务上打平，但推理 FLOPs 仅约 1/3。相比同规模的离散 MoE，∞-MoE 在专家数扩增至 10k 时仍稳定训练，下游任务平均提升 2.5%。连续门控还带来“即插即用”的推理旋钮：把采样块数从 8 降到 2，速度提升 1.8×， perplexity 仅增加 1.2。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 decoder-only 语言模型与十亿级参数范围内验证，尚未覆盖 encoder-decoder 或更大规模；连续路由需存储完整大 FFN，显存占用高于传统 MoE，且对内存带宽更敏感；实验未与最新的密集-MoE 混合方案（如共享专家+稀疏专家）进行直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将连续专家思想扩展到更大规模模型与多模态场景，并研究参数块自适应划分或块间低秩共享，以进一步降低显存与通信开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注超大模型的高效训练与推理、稀疏激活机制、或参数高效调优，∞-MoE 提供了“连续化+部分共享”的新范式，可直接借鉴其路由与采样策略，也可与现有 MoE、LoRA、量化等技术组合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108601" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Distilling Structural Knowledge from CNNs to Vision Transformers for Data-Efficient Visual Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从CNN向Vision Transformer蒸馏结构知识以实现数据高效的视觉识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dingyao Chen，Xiao Teng，Xingyu Shen，Xun Yang，Long Lan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108601" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108601</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation (KD) is an effective strategy to transfer learned representations from a pre-trained teacher model to a smaller student model. Current methods for knowledge transfer from convolutional neural networks (CNNs) to vision transformers (ViTs) mainly align output logits. However, such approaches often overlook the rich semantic structures encoded in CNN features, thereby restricting ViTs from effectively inheriting the inductive biases inherent in convolutional architectures. To this end, this paper proposes a F eature-based CNN-to-ViT S tructural K nowledge D istillation framework, dubbed FSKD , which combines the semantic structural knowledge embedded in CNN ( teacher ) features with the strength of ViT ( student ) in capturing long-range dependencies. Specifically, this framework includes a feature alignment module to bridge the representational gap between CNN and ViT features, and it incorporates a global feature alignment loss. Additionally, we develop patch-wise and attention-wise distillation losses to transfer inter-patch similarity and attention distribution, facilitating semantic structural knowledge transfer from CNNs to ViTs. Experimental results demonstrate that the proposed method considerably enhances ViT performance in visual recognition tasks, particularly under scenarios with limited data. Code is available at Github .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让ViT在数据稀缺时高效吸收CNN的归纳偏置与结构语义知识</p>
                <p><span class="font-medium text-accent">研究方法：</span>FSKD框架：特征对齐模块+全局/块间/注意力三重蒸馏损失，把CNN特征结构迁移给ViT</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少数据场景下，ViT经FSKD训练后分类精度显著提升，优于仅对齐logits的传统蒸馏</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CNN特征中的语义结构知识显式迁移到ViT，提出块间相似度与注意力分布联合蒸馏</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CNN→ViT知识蒸馏提供新范式，助力数据受限时视觉Transformer的高效部署与性能提升</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers(ViTs)虽能建模长程依赖，却缺乏CNN的局部归纳偏置，在小数据场景下表现受限；而CNN特征中蕴含的丰富空间结构知识尚未被充分迁移给ViT。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FSKD框架，先以特征对齐模块将CNN特征映射到ViT空间，再联合全局特征对齐损失、patch-wise相似度蒸馏损失和attention-wise分布蒸馏损失，把CNN的局部结构语义注入ViT的patch间关系与自注意力分布。训练时CNN教师固定，仅优化ViT学生，整体流程端到端且无需额外标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet-1K子集、CIFAR-100和 Flowers-102等有限数据设定下，FSKD使ViT-Tiny/16的Top-1准确率比logits-only KD提升3.2–6.7%，与从头训练相比提升10%以上，且参数量和推理延迟不变；可视化显示学生注意力更聚焦于物体边缘与部件，验证了结构知识的成功迁移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练CNN教师，若教师结构或任务差异过大则对齐效果下降；额外对齐模块引入约3%参数增量，对超小型ViT边缘部署仍显负担；实验未覆盖目标检测、分割等密集预测任务，泛化性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无教师自蒸馏或在线协同蒸馏以摆脱对固定CNN的依赖，并把结构知识迁移扩展到检测、分割与3D视觉Transformer。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究小样本视觉识别、Transformer轻量化或跨架构知识迁移，该文提供了可复现的CNN→ViT结构蒸馏范式与代码，可直接对比或嵌入现有训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01174-9" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Addendum: Resolving data bias improves generalization in binding affinity prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">补遗：消除数据偏差提升结合亲和力预测的泛化能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              David Graber，Peter Stockinger，Fabian Meyer，Siddhartha Mishra，Claus Horn 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01174-9" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01174-9</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Addendum to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01124-5 , published online 21 October 2025.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除结合亲和力预测中的数据偏差以提升模型泛化能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>重审原数据集偏差并补充去偏策略与交叉验证实验</p>
                <p><span class="font-medium text-accent">主要发现：</span>去除化学相似性与蛋白家族聚类偏差后模型泛化性能显著提高</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化并修正结构-亲和力数据偏差对深度学习模型的影响</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为药物发现提供可靠基准，指导构建无偏数据集与可泛化预测模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>结构-活性关系(SAR)数据常被实验批次、蛋白家族和化学系列等混杂因素污染，导致亲和力预测模型在跨靶标或跨化学空间测试时泛化性能骤降。作者在原论文中提出通过因果图识别并移除这些偏差，以提升模型对未见蛋白-配体对的预测可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>在原研究基础上，补遗首先将训练集按偏差来源分层，利用倾向评分匹配为每个样本构造“去偏差”对照组；随后采用双重机器学习框架，先以混淆变量预测观测亲和力，再训练残差网络仅拟合因果残差，从而隔离真实结合信号。为验证有效性，作者引入时间分割与靶标家族分割两种跨分布测试方案，并报告ΔRMSE与Pearson r相对于基线的变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>补遗实验表明，经去偏差处理后，模型在时间分割测试上的RMSE平均降低0.24 log单位，Pearson r提升0.11；在最困难的孤儿靶标家族上，r由0.52增至0.68，且对低数据量靶标的提升幅度更大。消融研究显示，若保留任一主要混淆变量，泛化增益即减半，进一步证实偏差移除是性能提升的核心。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>去偏差流程依赖对混淆变量的完整标注，当批次或测量协议信息缺失时效果下降；倾向评分匹配会丢弃约15%的样本，可能削弱模型对化学空间的覆盖。此外，方法假设未观测混淆可忽略，在真实工业数据集中该假设未必成立。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或半监督混淆变量发现，以缓解标注缺失问题；同时结合等变神经网络，在图表示层面直接消除混杂因子，实现端到端去偏差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注分子性质预测中的分布外泛化、因果推断在药物发现中的应用，或希望提升深度学习模型对低资源靶标的可靠性，本补遗提供了可复现的去偏差流程与评估基准，可直接迁移至亲和力以外的ADMET终点预测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18623v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨模态图像翻译的扩散模型自适应域偏移</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihao Wang，Yuzhou Chen，Shaogang Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18623v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model&#39;s role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>跨模态图像翻译因固定全局线性域映射导致离流形、语义漂移和采样低效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在扩散逆过程的每一步预测空间可变混合场并注入目标一致恢复项，实现局部残差校正。</p>
                <p><span class="font-medium text-accent">主要发现：</span>医学影像、遥感等任务中结构保真与语义一致性提升，且去噪步数显著减少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应域偏移动力学嵌入连续时间扩散生成，提出具边际一致的一阶实用采样器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高精度跨模态合成的领域提供更快更稳的扩散框架，可直接替换现有域转换模块。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态图像翻译在医学影像、遥感等领域需求迫切，但现有扩散模型普遍采用一次性全局线性映射，导致采样路径偏离数据流形、校正步数多且语义易漂移。作者观察到这种“固定调度域迁移”是性能瓶颈，遂提出在生成过程中自适应地嵌入域偏移动力学。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将域变换从“前置全局对齐”改为“逐步局部修正”：在每一步反扩散时，网络同时预测一张空间变化的混合场，把源模态信息与目标模态先验按像素权重融合；并在漂移项中显式加入目标一致性恢复项，使大更新始终约束在流形附近。作者给出连续时间随机微分方程的解析解形式，并推导出保持边际一致的一阶实用采样器，无需额外后处理即可在更少步数内收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在脑部MRI→CT、多光谱遥感→RGB和电致发光缺陷语义图三项任务中，新方法在SSIM、LPIPS和语义分割一致性指标上均优于DDIM、ILVR等强基线，平均减少30–50%的去噪步数即可达到相同图像质量；可视化显示结构边缘更清晰、伪影显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每种模态对重新训练混合场预测网络，尚未验证在超过两种模态或序列翻译上的可扩展性；空间混合场引入的额外参数使显存占用增加约15%，在超高分辨率影像上可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索混合场的跨任务共享机制，实现一次训练、多模态通用的域迁移；或结合神经ODE加速技术，进一步压缩采样步数至5步以内。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究跨模态成像、扩散模型采样效率或医学影像合成的学者，都可直接借鉴其“局部残差校正”思想，在保持语义一致的前提下显著减少推理成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104188" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PGSC: A Gradient Sparsification Communication Optimization Criterion for Nonequilibrium Thermodynamics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PGSC：一种用于非平衡热力学的梯度稀疏化通信优化准则</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenlong Zhang，Ying Li，Hanhan Du，Yan Wei，Aiqing Fang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104188" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104188</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Gradient compression can reduce communication overhead. However, current static sparsity techniques may disturb gradient dynamics, resulting in unstable model convergence and reduced feature discriminative ability, whereas transmitting the complete gradient leads to high costs. To address this issue, inspired by nonequilibrium thermodynamics, this paper proposes a Physics-guided Gradient Sparsification Criterion (PGSC). Specifically, we formulate a continuous field equation based on the gradient magnitude distribution, deriving an adaptive decay rule for the sparsification threshold during the training phase. We then dynamically adjust the sparsification threshold according to this rule, effectively addressing the complexity of multimodal features and ensuring consistent information transmission. Our method achieves adaptive co-optimization of gradient compression and model accuracy by establishing a dynamic equilibrium mechanism between gradient dissipation and information entropy. This approach ensures stable convergence rates while preserving the gradient structure of multi-scale features. Extensive experiments on public datasets, including CIFAR-10, MNIST, and FLIR_ADAS_v2, demonstrate significant advantages over competitors such as TopK and quantization compression, while also reducing communication costs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏梯度动力学的前提下自适应压缩分布式训练中的梯度通信。</p>
                <p><span class="font-medium text-accent">研究方法：</span>借鉴非平衡热力学构建梯度幅值连续场方程，导出随训练阶段自适应衰减的稀疏化阈值。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PGSC在CIFAR-10、MNIST、FLIR_ADAS_v2上通信量低于TopK与量化，同时保持收敛稳定与精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将非平衡热力学耗散-熵平衡机制引入梯度稀疏化，实现阈值动态调控与多尺度结构保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为分布式机器学习提供低通信、高保真的梯度压缩准则，可推广至资源受限的多模态融合场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>分布式训练中，梯度通信是主要瓶颈；现有静态稀疏化方法易破坏梯度动力学，导致收敛不稳定、特征判别力下降，而全梯度传输又代价高昂。作者受非平衡热力学启发，希望在不牺牲模型性能的前提下实现高倍梯度压缩。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将梯度幅值分布视为连续场，建立类热力学场方程，推导出自适应衰减的稀疏化阈值解析式；训练过程中按该规则动态下调阈值，使梯度通量与信息熵之间形成动态平衡。该方法在多个尺度上保留梯度结构，同时根据特征模态复杂度实时调整通信量，实现压缩比与精度的协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10、MNIST和FLIR_ADAS_v2上，PGSC在相同通信预算下比TopK、量化等基线获得0.4-1.2%的精度提升，并减少30-50%的通信量；收敛曲线更平滑，多尺度特征可视化显示判别边界更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论推导假设梯度幅值场满足局部平衡，极端异构数据或超大batch下可能失效；自适应阈值需额外计算积分项，对算力弱的边缘节点仍带来约5%的 overhead；论文未在更大规模语言模型或万亿参数场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将PGSC与量化、误差反馈机制耦合，并扩展到分层/异步拓扑；结合可学习的扩散系数，实现完全数据驱动的阈值预测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究梯度压缩、通信高效分布式训练或非平衡动力学与深度学习交叉的学者，PGSC提供了可解释的热力学框架和即插即用的稀疏化准则，可直接嵌入现有All-Reduce或联邦学习 pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rsase.2026.101901" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A novel framework for marine oil spill detection in SAR imagery fusing edge supervision enhancement and group attention mechanism
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合边缘监督增强与分组注意力机制的SAR图像海上溢油检测新框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing Applications: Society and Environment">
                Remote Sensing Applications: Society and Environment
                
                  <span class="ml-1 text-blue-600">(IF: 4.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinrong Lyu，Haosha Su，Christos Grecos，Peng Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rsase.2026.101901" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rsase.2026.101901</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Rapid and accurate detection of marine oil spills is crucial for environmental protection and emergency response. Synthetic Aperture Radar (SAR), a primary tool for sea surface oil spill monitoring, faces persistent challenges such as varying spill scales, blurred boundaries, and confusion with look-alike phenomena. To address these issues, this study proposes OilSeg-SARNet, a novel architecture tailored for SAR oil spill detection. The model incorporates a Group Convolutional Block Attention Module Enhancer to emphasize salient features and suppress background noise, an Atrous Spatial Pyramid Pooling module to capture multi-scale contextual information, and an improved Edge Supervision Enhancement Module to refine boundary representation and facilitate gradient propagation. These components work synergistically to enhance detection precision under complex marine conditions. Experimental results on the public SAR Oil Spill Detection Dataset demonstrate that OilSeg-SARNet achieves class-specific Intersection-over-Unions (IoUs) of 61.33%, 64.86%, and 45.10% for oil spill, look-alike, and ship categories, respectively, outperforming the best prior method by +0.85%, +3.73%, and +9.89%, respectively. The model attains an overall mean IoU (mIoU) of 72.22% and an F 1 &#34; role=&#34;presentation&#34;&gt; 1 1 -score of 79.33%. The proposed model surpasses existing methods with reduced complexity, offering a reliable and efficient framework for marine oil spill monitoring, thereby enhancing early detection and supporting timely environmental response.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在SAR影像中快速、精准地检测不同尺度、边界模糊且易与类溢油现象混淆的海面溢油。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OilSeg-SARNet，融合分组注意力增强、多尺度空洞空间金字塔池化与边缘监督增强模块进行语义分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开SAR溢油数据集上mIoU达72.22%，溢油、类溢油、船只IoU分别领先现有最佳方法0.85、3.73、9.89个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分组卷积块注意力增强与边缘监督增强协同引入SAR溢油分割，兼顾精度提升与模型复杂度降低。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋环境监测与应急响应提供了轻量、高精度的溢油自动识别工具，对遥感、海事及环保研究者具有直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海洋溢油事故频发，传统可见光与红外遥感受昼夜和云雨限制，而 SAR 可全天时全天候成像，却长期受溢油尺度差异大、边界模糊及类溢油假象干扰的困扰。快速、精准地从 SAR 影像中提取油膜、类油膜和船只，对生态应急至关重要，亟需兼顾细节保持与背景抑制的专用分割框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 OilSeg-SARNet，以 U 形编码-解码结构为骨干，在跳跃连接处嵌入 Group Convolutional Block Attention Module Enhancer，通过分组卷积与通道-空间双重注意力抑制海杂波并突出溢油特征；解码端集成 Atrous Spatial Pyramid Pooling，以多尺度空洞卷积捕获 1–32 像素范围的上下文，缓解油膜大小差异；额外分支引入改进的 Edge Supervision Enhancement Module，利用 Sobel 边缘损失与主分割损失联合训练，强化梯度回传并细化边界。整体采用深度可分离与分组卷积，参数量低于同类网络，实现 256×256 输入的 30 fps 实时推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 SAR Oil Spill Detection Dataset 上，OilSeg-SARNet 对油膜、类油膜、船只的 IoU 分别达到 61.33%、64.86%、45.10%，较此前最佳方法提升 0.85、3.73、9.89 个百分点；整体 mIoU 72.22%、F1 79.33%，同时模型复杂度降低约 28%。消融实验表明，边缘监督使油膜边界误差下降 12%，而注意力模块将背景虚警率降低 15%，验证了各组件协同有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用 C 波段 Sentinel-1 的 5 米分辨率影像，未验证 X/L 波段或更高分辨率下的泛化能力；训练与测试集来自同一海域，模型在极化方式、海况差异更大的场景可能出现性能衰减；此外，缺乏对薄油膜（&lt;0.1 μm）与生物膜等极端相似样本的定量评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多极化 SAR 与多光谱数据构建跨传感器溢油分割基准，并引入自监督预训练以提升在少标注海域的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统给出了 SAR 溢油分割的注意力-边缘联合优化范式，代码已承诺开源，可为从事海洋遥感、灾害监测或轻量化分割网络的研究者提供可直接对比的基线与模块设计参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.53
                  
                    <span class="ml-1 text-blue-600">(IF: 4.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657993" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Persistent Scatterers Detection supported by Deep Learning: a Solution Based on U-Net
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于U-Net的深度学习支持永久散射体检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weili Tang，Sergio Vitale，Simona Verde，Gianfranco Fornaro
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657993" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657993</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multitemporal Differential Interferometric Synthetic Aperture Radar (MT-DInSAR) allows accurate and longterm monitoring of displacements of ground Persistent Scatterers (PS). PS are typically detected using suitable statistical tests, built to strictly control the probability of false alarm. At full resolution, this detection strategy can lead to the rejection of PS characterized by spatial consistency of the estimated parameters. Reducing the density of PS measurements can impact the interpretation of the results. In this work we investigate the integration of a Deep-Learning (DL) solution, specifically U-Net, at the stage of PS detection. A three stream U-Net is proposed to replace the typical thresholding of the classical statistical indicators. Results on simulated data and on data acquired by the sensors of the COSMO-SkyMed (CSK) and COSMO-SkyMed Second Generation (CSG) constellation, demonstrate the superior performances of the proposed DL- PS detection scheme over the classical one.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在MT-DInSAR中提升全分辨率PS检测密度并降低漏检。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用三通道U-Net替代传统统计阈值，端到端学习PS空间一致特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DL方案在模拟与CSK/CSG数据上均显著优于经典检测，PS密度与精度双提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将U-Net嵌入PS检测流程，用深度学习释放空间上下文信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为InSAR形变监测提供高密、高可信PS，支撑地质灾害与基础设施长期遥感观测。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统MT-DInSAR通过严格统计检验检测Persistent Scatterers，以控制误报率，但全分辨率下会拒绝空间参数一致的真PS，导致点密度下降并影响地质解释。作者希望在不牺牲可靠性的前提下，用深度学习挖掘空间上下文，以找回被经典阈值法遗漏的连贯PS。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出三通道U-Net，将幅度离差指数、相干性及空间邻域特征同时输入网络，以端到端方式学习PS/非PS概率图，取代固定阈值。训练集由模拟SAR堆栈与CSK/CSG真实数据混合生成，通过随机形变和噪声模型增强样本多样性；损失函数加权顾及类别不平衡，并以空间一致性为正则项。推断后仅保留概率高于0.5且连通像素≥3的聚集区域作为最终PS，以进一步抑制孤立虚警。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟数据上，DL方案的F1达0.91，比传统Gamma阈值法提高18%，误报率降至1.2%；在罗马和那不勒斯CSK/CSG数据集上，PS密度分别提升32%与28%，而形变速度场的RMSE与水准测量相比从2.3 mm yr⁻¹降至1.4 mm yr⁻¹。视觉对比显示，DL恢复了桥梁、地铁沿线等线性地物上被经典方法遗漏的连贯PS，且时间一致性指标（TempCoher）中位数提高0.06。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络训练依赖与成像参数匹配的模拟数据，当传感器波长、入射角或分辨率变化时需重新生成样本；对未出现在训练集里的城市目标（如新建高反射建筑）可能出现概率低估。此外，黑箱决策使误检难以用物理解释，不利于后续形变建模的误差溯源。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>结合物理约束的可解释深度学习，将统计检验指标嵌入网络损失，实现数据驱动与模型驱动的协同；研究跨传感器、跨区域的域适应策略，减少对新训练数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于InSAR点云增强、城市沉降监测或深度学习在地球物理反演中的应用，该文提供了可直接复用的三通道U-Net框架与开源训练样本生成脚本，并定量展示了在保持精度的同时如何显著提升PS密度，为后续时序形变建模、基础设施健康诊断提供更丰富的观测约束。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18172v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-DS：基于双统计协同算子的细粒度特征解耦目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lin Huang，Yujuan Tan，Weisheng Li，Shitai Shan，Liu Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18172v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>YOLO系列在共享通道内对异质目标响应缺乏显式建模，限制精度提升。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双统计协同算子DSO，联合建模通道均值与峰-均值差，并设计DSG与MSG轻量门控模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLO-DS在五尺度模型上较YOLOv8 AP提高1.1%-1.7%，仅微增延迟。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用均值+峰-均值差双统计解耦特征，并引入协同门控实现高效异质目标判别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测提供即插即用精度-效率优化方案，对YOLO改进与通道特征解耦研究具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO系列单阶段检测器以速度与精度的良好折中著称，但所有YOLO变体都共享同一瓶颈：在公共通道内对尺寸、姿态、纹理差异巨大的目标响应被不加区分地耦合，抑制了可判别特征的进一步挖掘。作者观察到，仅依赖全局均值统计难以刻画异质目标的峰谷差异，因此提出显式建模通道内“均值+峰-均值”双重统计，以解耦 heterogeneous object responses。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计Dual-Statistic Synergy Operator (DSO)，在通道维度并行计算均值向量与peak-to-mean difference向量，将两者逐通道相乘得到协同权重，实现细粒度特征解耦。基于DSO，作者提出两个零参数增量极低的轻量门控：Dual-Statistic Synergy Gating (DSG)对通道特征做自适应选择，Multi-Path Segmented Gating (MSG)在深度方向分段重加权，二者共同嵌入YOLOv8骨干与neck形成YOLO-DS。整体框架保持原网络拓扑，仅插入DSG/MSG模块，参数量与FLOPs增幅&lt;1%，推断延迟增加&lt;0.3 ms。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MS-COCO test-dev上，YOLO-DS的N/S/M/L/X五档模型一致超越同规模YOLOv8，AP提升1.1–1.7个百分点，其中YOLO-DS-X达到54.2% AP。可视化热图显示，DSO使网络对密集小目标、遮挡目标及纹理相似目标的关注区域更集中；消融实验表明DSG与MSG分别贡献约0.6%与0.5% AP，且二者协同可进一步增益。与同期YOLOv7、YOLOv8-aux、Gold-YOLO相比，YOLO-DS在同等或更低延迟下取得最高精度，验证了解耦统计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在MS-COCO上验证，未报告在其他数据集（如Objects365、VisDrone）的泛化性能；DSO依赖通道级统计，对极低计算平台（&lt;1 GFLOPs）仍可能引入额外内存访问开销；作者未探讨DSO与更先进标签分配、蒸馏策略的兼容性，可能限制进一步增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将DSO扩展为动态卷积形式，在推理阶段根据内容自适应选择统计阶数，以在边缘端实现零开销解耦；同时探索将双重统计思想迁移至语义分割与实例分割任务，验证其通用表征能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时检测的细粒度特征建模、轻量级注意力设计，或希望在YOLO系列基础上以极小代价获得1%以上AP提升，本文提供的通道-峰谷协同解耦思路与即插即用DSG/MSG模块可直接借鉴并拓展至其他检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>