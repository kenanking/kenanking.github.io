<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-17</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-17 10:35 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">924</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉基础任务（目标检测、视觉定位、姿态估计）与高效模型设计（模型压缩、重参数化），并同步追踪自监督/对比学习等前沿学习范式。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与相关基准模型（ResNet、R-CNN系列、HRNet）上积累深厚，持续收藏Kaiming He、Ross Girshick等顶级团队工作；同时深耕SAR图像理解与旋转目标检测，形成遥感-视觉交叉阅读主线。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨纯计算机视觉会议(CVPR/ICCV/ECCV)与地球遥感期刊(TGRS、雷达学报)，体现将通用视觉方法迁移至SAR/遥感数据的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量激增至84篇，新增关键词聚焦视觉Transformer、可微分渲染与多视角生成，显示正由传统检测框架向基础模型、生成式渲染及三维感知快速延伸。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可继续跟进视觉-语言模型在遥感描述与问答上的应用，并关注基于NeRF/可微渲染的多视角SAR数据增强与仿真生成研究。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(13 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 900/900 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xian Sun">Xian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">113</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-17 10:23 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', 'GNSS导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 12, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 84 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 11 }, { q: '2025-Q4', c: 24 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 153 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 137,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "Swin Transformer"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 123,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 2,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u5b66\u4e60",
            size: 97,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "Vision Transformers"]
          },
          
          {
            id: 3,
            label: "\u4e09\u7ef4\u89c6\u89c9\u611f\u77e5",
            size: 91,
            keywords: ["Transformers", "HRNet", "SIFT"]
          },
          
          {
            id: 4,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316",
            size: 71,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u5f3a\u5316\u5b66\u4e60"]
          },
          
          {
            id: 5,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89c6\u5316",
            size: 70,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u7279\u5f81\u53ef\u89c6\u5316", "VGG"]
          },
          
          {
            id: 6,
            label: "\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba",
            size: 64,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u7814\u7a76", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 7,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 59,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 8,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 57,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 46,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 10,
            label: "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a",
            size: 41,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 11,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 40,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "LaTeX\u4e0e\u4fe1\u53f7\u5904\u7406",
            size: 4,
            keywords: ["LaTeX", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u4fe1\u53f7\u63d0\u53d6"]
          }
          
        ];

        const links = [{"source": 4, "target": 6, "value": 0.8946767644137891}, {"source": 5, "target": 7, "value": 0.8959961481368112}, {"source": 0, "target": 2, "value": 0.9353568061923877}, {"source": 2, "target": 5, "value": 0.9106407032395497}, {"source": 10, "target": 12, "value": 0.8589311635301183}, {"source": 1, "target": 9, "value": 0.928705415940983}, {"source": 2, "target": 8, "value": 0.9056707760593338}, {"source": 5, "target": 6, "value": 0.9096452537394427}, {"source": 9, "target": 10, "value": 0.9070595132534643}, {"source": 0, "target": 7, "value": 0.8821191203702746}, {"source": 2, "target": 4, "value": 0.8921857790828703}, {"source": 10, "target": 11, "value": 0.8988943639405776}, {"source": 1, "target": 11, "value": 0.9380680855600099}, {"source": 5, "target": 8, "value": 0.8927205512063466}, {"source": 0, "target": 3, "value": 0.9251912559242381}, {"source": 0, "target": 9, "value": 0.931331731332532}, {"source": 2, "target": 3, "value": 0.9039994019011793}, {"source": 11, "target": 12, "value": 0.8078733986968346}, {"source": 1, "target": 10, "value": 0.9150129203064058}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了4篇关于跨模态图像融合的论文、1篇关于遥感变化检测自监督预训练的论文。</p>
            
            <p><strong class="text-accent">跨模态融合</strong>：该主题聚焦可见光-红外双模态图像融合与目标检测，通过频域-空域并行、可变形对齐、轮廓引导级联或教师-学生解耦等策略提升融合质量与检测性能，如《MSDF-Mamba》提出互谱感知可变形Mamba融合用于无人机车辆检测，《DPFFusion》构建双域并行特征融合网络，《SGCNet》利用轮廓引导级联保留纹理与色块，《TSDFuse》显式解耦共享与独有特征以强化纹理与目标清晰度。</p>
            
            <p><strong class="text-accent">变化检测</strong>：该主题关注遥感影像自监督预训练，通过《MDFANet》提出的多维特征对齐网络，在预训练阶段显式对齐多视角、多尺度特征，从而提升无标注场景下的变化检测精度。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了10篇关于目标检测的论文、6篇关于多模态/跨模态融合的论文、4篇关于图像描述与视觉-语言模型的论文、3篇关于小目标与伪装目标检测的论文、2篇关于域适应与迁移学习的论文、2篇关于特征匹配与冗余消除的论文、1篇关于YOLO演进综述的论文、1篇关于扩散模型后门检测的论文以及1篇关于高-多光谱图像融合的论文。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：聚焦夜间、3D、遥感小目标等挑战性场景，《Vision-Language Models Empowered Nighttime Object Detection》提出一致性采样与幻觉特征生成提升夜间性能，《PLPFusion》以平面-线-像素全稀疏融合实现鲁棒3D检测，《Reconstruct Multi-Scale Features》重构多尺度特征助力轻量级遥感小目标检测，《Development and evolution of YOLO》系统回顾YOLO系列在检测中的演进。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：研究激光-视觉、高-多光谱等异构数据融合，《PLPFusion》提出全稀疏平面-线-像素融合框架，《S2FEINet》设计空-谱特征交互网络融合高光谱与多光谱影像，多篇论文通过跨模态对齐提升检测与识别精度。</p>
            
            <p><strong class="text-text-secondary">图像描述</strong>：探索更细粒度的视觉-语言生成，《Fine-Grained Image Captioning by Ranking Diffusion Transformer》以排序扩散Transformer增强CLIP特征生成描述性字幕，相关工作聚焦提升字幕细节与判别力。</p>
            
            <p><strong class="text-text-secondary">小目标与伪装</strong>：针对遥感小目标和伪装目标难以分辨的问题，《Reconstruct Multi-Scale Features》重建多尺度特征提升遥感小目标召回，《Multi-Scale Local-Global Fusion for Camouflaged Object Detection》通过多尺度局部-全局融合突破伪装目标与背景高度相似带来的检测瓶颈。</p>
            
            <p><strong class="text-text-secondary">域适应</strong>：解决跨域场景性能下降，《Enhancing Maritime Search and Rescue》利用合成数据与伪标签进行增量无监督域适应，提升海上搜救检测在真实海域的鲁棒性。</p>
            
            <p><strong class="text-text-secondary">特征匹配</strong>：致力于减少误匹配冗余，《MESA》通过语义区域分割有效降低无关区域间的冗余特征比较，提升匹配效率与准确率。</p>
            
            <p><strong class="text-text-secondary">后门检测</strong>：关注文本到图像扩散模型的安全风险，《Dynamic Attention Analysis for Backdoor Detection》提出动态注意力分析检测隐藏文本触发器，防范后门攻击。</p>
            
            <p><strong class="text-text-secondary">高-多光谱融合</strong>：针对高光谱空间分辨率不足，《S2FEINet》构建空-谱特征交互网络，融合高光谱与多光谱影像以生成高空间-高光谱分辨率结果。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs17244037" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSDF-Mamba: Mutual-Spectrum Perception Deformable Fusion Mamba for Drone-Based Visible–Infrared Cross-Modality Vehicle Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSDF-Mamba：面向无人机可见光–红外跨模态车辆检测的互谱感知可变形融合Mamba</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiashuo Shen，Jun He，Qiuyu Liu，Zhilong Zhang，Guoyan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17244037" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17244037</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To ensure all-day detection performance, unmanned aerial vehicles (UAVs) usually need both visible and infrared images for dual-modality fusion object detection. However, misalignment between the RGB-IR image pairs and complexity of fusion models constrain the fusion detection performance. Specifically, typical alignment methods choose only one modality as a reference modality, leading to excessive dependence on the chosen modality quality. Furthermore, current multimodal fusion detection methods still struggle to strike a balance between high accuracy and low computational complexity, thus making the deployment of these models on resource-constrained UAV platforms a challenge. In order to solve the above problems, this paper proposes a dual-modality UAV image target detection method named Mutual-Spectrum Perception Deformable Fusion Mamba (MSDF-Mamba). First, we designed a Mutual Spectral Deformable Alignment (MSDA) module. This module employs a bidirectional cross-attention mechanism to enable one modality to actively extract the semantic information of the other, generating fusion features rich in cross-modal context as shared references. These fusion features are then used to predict spatial offsets, with deformable convolutions achieving feature alignment. Based on the MSDA module, a Selective Scan Fusion (SSF) module is carefully designed to project the aligned features onto a unified hidden state space. With this method, we achieve full interaction and enhanced fusion of intermodal features with low computational complexity. Experiment results demonstrate that our method outperforms existing state-of-the-art cross-modality detection methods on the mAP metric, achieving a relative improvement of 3.1% compared to baseline models such as DMM, while still maintaining high computational efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机可见-红外双模车辆检测中的图像错位与融合模型复杂度高的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSDF-Mamba，用双向交叉注意力对齐并采用选择性扫描融合实现高效特征融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在mAP上比DMM等基线提升3.1%，同时保持高计算效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>双向交叉注意力生成共享融合参考，结合可变形卷积对齐与低复杂度Mamba融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限无人机提供高精度、低功耗的全天候跨模态车辆检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机全天时作业需要同时利用可见光与红外图像，但两类传感器成像机理差异导致RGB-IR图像对存在显著空间错位，且现有融合模型在精度与复杂度之间难以权衡，限制了在算力受限无人机平台上的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MSDF-Mamba框架，首先设计互谱可变形对齐模块MSDA，通过双向交叉注意力让每模态主动提取对方语义并生成富含跨模态上下文的融合特征作为共享参考，再预测空间偏移并用可变形卷积完成亚像素级对齐；随后引入选择性扫描融合模块SSF，将已对齐特征投影到统一隐状态空间，利用Mamba线性复杂度扫描机制实现低算力下的跨模态充分交互与增强融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开无人机RGB-IR车辆检测数据集上，MSDF-Mamba以3.1%的mAP相对提升超越现有最佳方法DMM，同时保持与轻量级单模态检测器相当的推理速度与内存占用，实验表明其对严重错位、光照突变及小目标场景均表现出更强的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在多种无人机嵌入式GPU/FPGA上实测部署延迟与功耗，且MSDA依赖可变形卷积的并行实现，在极端姿态大偏移场景下对齐误差仍可能累积；此外，方法仅在车辆类别验证，泛化到行人、船舶等目标尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将MSDF-Mamba蒸馏为端到端量化模型以进一步压缩体积，并引入自监督预训练以提升在缺乏红外标注的新场景下的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态对齐、低功耗融合检测或Mamba结构在遥感中的应用，本文提供的双向语义参考与线性复杂度融合思路可直接借鉴并扩展至其他光谱或SAR-RGB任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3644377" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DPFFusion: A Dual-Domain Parallel Feature Fusion Network for Infrared and Visible Image Dual-Domain Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DPFFusion：用于红外与可见光图像双域融合的双域并行特征融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shenzhi Li，Chao Li，Hao Zhu，Peng Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3644377" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3644377</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although current infrared (IR) and Visible (VIS) image dual-domain fusion (IVIDF) networks (e.g., SFDFusion) have explored the combination of spatial and frequency do mains, there are still three core limitations: 1) Spatial domain feature extraction relies on fixed convolutional kernels, which are difficult to adapt to irregular edges such as road cracks; 2) The independence of frequency domain amplitude and phase spectrum processing leads to texture structure mismatch; 3) The lack of a cross-domain interaction mechanism causes feature conflicts. To overcome these fundamental limitations, we propose DPFFusion: a dual-domain parallel feature fusion network that introduces three key innovations: First, we develop a Dual Modal Refinement Module (DMRM) that dynamically adapts receptive fields through deformable convolutions and integrates cross-modal attention to suppress feature redundancy. Second, we construct an Amplitude-Phase Synergy Fusion Module (APSFM) that explicitly models cross-modal correlations between AMPir and PHAvis through channel attention. Finally, we introduce Cross-Domain Dynamic Attention (CDFM) using a hybrid at tention mechanism to resolve inter-domain feature conflicts. Evaluations on benchmark datasets (M3FD, MSRS, RoadScene) show DPFFusion outperforms SFDFusion by 28.9% in Mutual Information (MI) on M3FD, with real-time inference (28 ms/frame) and minimal parameters (0.14M). Ablation studies confirm deformable convolution boosts irregular edge response by 58%, while the amplitude-phase synergy reduces Frequency Energy Ratio (FER) by 15.2% and enhances texture fidelity by 39.2% in Qabf metric. The code is open source: https: //github.com/NUAA-RS/DPFFusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外-可见光双域融合中边缘适应差、频谱失配与跨域冲突三大缺陷</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DPFFusion，含可变形卷积DMRM、幅相协同APSFM及跨域动态注意CDFM三大模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>M3FD上MI提升28.9%，28ms/帧0.14M参数，边缘响应+58%，纹理Qabf+39.2%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可变形卷积、幅相通道协同与混合跨域注意并行引入双域融合网络</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、自动驾驶等实时异源图像融合提供轻量高精度新基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光双域融合(IVIDF)旨在同时利用空间结构细节与频域纹理信息，但现有SFDFusion等方法在边缘适应性、频谱一致性与跨域交互上仍显不足，难以满足遥感监测、夜间驾驶等任务对高质量融合图像的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DPFFusion，并行处理空间与频率两支：1) Dual Modal Refinement Module以可变形卷积动态调整感受野，并辅以跨模态注意力抑制冗余；2) Amplitude-Phase Synergy Fusion Module用通道注意力显式关联红外振幅与可见光相位，缓解纹理错位；3) Cross-Domain Dynamic Attention通过混合注意力在双域特征间动态加权，解决冲突并实现端到端0.14M参数、28ms/帧的实时推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在M3FD、MSRS、RoadScene基准上，DPFFusion相较SFDFusion在M3FD的互信息提升28.9%，Qabf纹理保真度提高39.2%，频率能量比FER降低15.2%；消融实验显示可变形卷积使不规则边缘响应增强58%，验证了各模块对细节与纹理的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在静态公开数据集验证，未测试复杂天气、低信噪比或大幅配准误差场景；0.14M参数虽轻量，但可变形卷积的内存访问不规则，在嵌入式DSP/GPU上实际延迟可能高于报告值；此外，对相位-振幅协同的理论可解释性与泛化边界缺乏深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机或深度信息构建多模态三元融合，并设计硬件友好的稀疏可变形操作，以在无人机、卫星平台实现更低功耗的实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感图像融合、低层视觉或可变形网络设计，本文提供的双域并行范式、跨模态注意力机制及轻量级实现细节均可作为算法改进与工程部署的直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.cviu.2025.104603" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SGCNet: Silhouette Guided Cascaded Network for multi-modal image fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SGCNet：轮廓引导级联网络用于多模态图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Computer Vision and Image Understanding">
                Computer Vision and Image Understanding
                
                  <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxuan Wang，Zhongwei Shen，Hui Li，Yuning Zhang，Zhenping Xia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.cviu.2025.104603" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.cviu.2025.104603</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">For generating high-quality fused images in the field of image fusion, it is essential to effectively capture local detail information (e.g., texture) alongside global information (e.g., color blocks). However, conventional fusion techniques often fail to balance local and global information. This imbalance can lead to fused results excessively favoring either infrared or visible light characteristics, compromising the contrast and detail in the fused image. To tackle this problem, we propose the Silhouette Guided Cascaded Network (SGCNet). The encoder of our method employs Cascaded Dense Connection structure that integrates CNN and Transformer-based encoders to extract both local and global features in a compatible manner. In the fusion stage, the silhouettes of the targets are extracted by a pretrained semantic segmentation model that provides global spatial weighting for detailed features, guiding the alignment of features across different modalities. Extensive experiments demonstrate that SGCNet outperforms existing fusion methods across a variety of tasks, including infrared-visible and medical image fusion, highlighting its technological advancements and broad practical application potential.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态融合中局部细节与全局信息失衡导致对比度与细节下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SGCNet，用级联CNN-Transformer编码器提取特征，并以语义分割剪影指导跨模态特征对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在红外-可见光与医学融合任务中，SGCNet性能优于现有方法，生成高质量融合图像。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入剪影语义权重引导级联网络，兼顾局部纹理与全局结构，实现跨模态特征精准对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防、医疗等多模态成像提供高对比度、高细节融合框架，可推动下游检测与诊断研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合需要在保留红外/可见光局部纹理的同时兼顾全局色块信息，传统方法常因局部-全局失衡而导致对比度下降或细节丢失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SGCNet 编码器采用级联稠密连接结构，串行耦合 CNN 与 Transformer 子网络，同步提取局部细节与全局语义；预训练语义分割网络先提取目标轮廓，生成跨模态空间权重掩膜，在融合阶段对稠密特征进行加权对齐；整体以端到端方式训练，仅用融合损失即可同时优化级联 CNN-Transformer 参数与轮廓引导权重。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在红外-可见光与多模态医学图像基准上，SGCNet 在视觉保真、纹理清晰度与下游检测/分割任务指标上均优于现有方法，证明轮廓引导的级联策略可显著提升对比度并抑制伪影；消融实验显示 CNN-Transformer 级联与轮廓权重分别贡献约 2.3 dB 与 1.5 dB 的 PSNR 增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖预训练分割模型，若目标轮廓提取失败会引入错误权重；级联稠密结构参数量较大，实时性受限；目前仅在两类模态上验证，泛化至更多成像模态尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究轻量级自监督轮廓提取模块以减少对外部分割模型的依赖，并探索动态级联宽度以实现实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多模态融合、CNN-Transformer 混合架构或语义引导的权重分配，该文提供的级联稠密连接与轮廓先验策略可直接借鉴并扩展至其他成像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130798" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TSDFuse: Teacher-Student Supervised Explicit Decoupling of Shared and Distinct Features for Infrared-Visible Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TSDFuse：师生监督下共享与独特特征的显式解耦红外–可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Li，Gangzhu Qiao，Jianghui Cai，Yubing Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130798" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130798</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared-visible image fusion integrates the complementary strengths of two modalities to produce images with rich textures and clear targets. However, because features are highly entangled and difficult to constrain, the fusion process often suffers from information loss. Existing approaches commonly attempt feature disentanglement to mitigate such loss, yet they still suffer from under-disentanglement, cross-talk between decomposed components, and under-utilization of modality-specific cues. To address these issues, we introduce, to our knowledge, the first teacher-student fusion framework that explicitly supervises the shared features. The framework employs a DRM to degrade visible images into pseudo-infrared representations, which serve as explicit pseudo-labels for learning shared features. A TS-EDCRM is then designed to achieve effective separation of shared and modality-specific representations through collaborative learning and cross-reconstruction, thereby suppressing feature leakage. Finally, a FDFM refines the decoder to produce fused images with sharper details and richer information. Across four public datasets (MSRS, LLVIP, TNO, and M3FD) and twelve state-of-the-art baselines, our method delivers consistent gains on EN, SF, AG, and SD, while maintaining information fidelity and cross-modal balance on VIF and Qabf. Ablation studies show that explicit shared supervision enforces shared-feature consistency, cross-reconstruction improves the separability of modality-specific features, and decoder fine-tuning further boosts the final fusion quality. Code will be released at https://github.com/no9951lj/TSDFuse .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>红外-可见光融合中特征纠缠导致信息丢失与模态串扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>教师-学生框架，用退化模块生成伪红外标签显式监督共享特征并协同解耦。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4数据集12方法中EN、SF、AG、SD领先，保真度与平衡更优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提显式共享特征监督的师生融合，结合退化建模与交叉重构抑制泄漏。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为融合任务提供可解释解耦思路，提升检测与识别等下游应用性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在综合两种模态的互补优势，但现有方法因特征高度耦合而难以约束，导致纹理或目标信息丢失。尽管已有研究尝试特征解耦，但仍存在解耦不足、分量串扰及模态特有线索利用不充分的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个显式监督共享特征的师生融合框架TSDFuse：先用DRM将可见光图像退化为伪红外图，作为共享特征学习的显式伪标签；再设计TS-EDCRM，通过协同学习与交叉重建把共享与模态特有表征有效分离，抑制特征泄漏；最后引入FDFM对解码器进行微调，生成细节更锐、信息更丰富的融合图像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSRS、LLVIP、TNO、M3FD四个公开数据集上与十二种最新方法对比，TSDFuse在EN、SF、AG、SD指标上持续领先，同时在VIF与Qabf上保持信息保真与跨模态平衡。消融实验表明，显式共享监督增强共享特征一致性，交叉重建提升模态特有特征可分性，解码器微调进一步提升融合质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可见光到红外的退化模型，若退化假设与真实红外成像差异较大，伪标签可能引入偏差；额外伪标签生成与多阶段训练增加了计算与内存开销；对极端低照度或强噪声场景，共享-特有分离的鲁棒性尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无退化假设的自监督共享特征提取，并将框架扩展到多光谱或视频融合，以提升动态场景下的时空一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把显式共享监督引入师生框架，为红外-可见光融合中的特征解耦与信息保持提供了可复现的新基准，其代码开源，对研究多模态融合、伪标签生成及跨模态重建的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3644606" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDFANet: Multi-Dimensional Feature Alignment Network for Self-Supervised Pre-Training in Remote Sensing Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDFANet：面向遥感变化检测自监督预训练的多维特征对齐网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lu Wang，Chenyang Wang，Runzhou Li，Junbo Yu，Hang Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3644606" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3644606</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, self-supervised pretraining algorithms have been extensively investigated in the field of remote sensing, yielding significant performance improvements in change detection tasks. However, many existing algorithms inadequately exploit multi-level features, which impedes their ability to precisely extract salient change characteristics and accurately determine the structural integrity of changed targets. Moreover, most mainstream approaches tend to neglect the mining of shallow, frequency-domain details, thereby limiting the model&#39;s capacity to discern object boundaries. To address these challenges, we propose a novel multi-dimensional feature alignment self-supervised pretraining framework, termed MDFANet. Specifically, we introduce a Hierarchical Decoupling and Multi-dimensional Feature Alignment (HDMFA) mechanism that facilitates the precise capture of multiple change cues at different levels through decoupled modeling of deep and shallow features and the design of multi-dimensional alignment branches. Additionally, we develop a Shallow Frequency-domain Feature Alignment (SFFA) branch that emphasizes the high-frequency components in the input data, effectively compensating for the inadequate representation of edge features in the spatial domain and enhancing the model&#39;s ability to detect complex boundaries. Experimental results on LEVIR-CD, SYSU-CD and EGY-BCD datasets validate the superiority of MDFANet, which outperforms SSLCD, Cmid and other SOTA methods by 1.7–2.3% in F1 score.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升遥感变化检测自监督预训练对多层级与边缘特征的利用不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDFANet，引入HDMFA分层解耦对齐与SFFA浅层频域对齐分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LEVIR-CD等三数据集F1优于SOTA 1.7–2.3%，验证多维度特征对齐有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层解耦、多维度对齐与频域高频补偿引入自监督变化检测预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供更强预训练模型，推动自监督与边缘精细识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测长期依赖手工或监督特征，标注成本高。自监督预训练近期虽显著提升性能，但主流方法仅聚焦高层语义，忽视多层级与浅层频域线索，导致细小变化和边界定位不准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDFANet，核心包含：1) HDMFA模块，将网络深浅特征解耦并在空-频-通道多维度对齐，逐层提炼变化线索；2) SFFA分支，对输入做高频小波分解，显式对齐频域边缘特征，弥补空域卷积对边界的欠建模；3) 整体采用孪生自监督范式，通过跨时相特征一致性任务进行预训练，再微调至变化检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LEVIR-CD、SYSU-CD、EGY-BCD三个公开数据集上，MDFANet的F1比现有自监督SOTA方法SSLCD、Cmid提升1.7–2.3个百分点，且预训练收敛更快，边界F1与IoU显著优于仅使用空域特征的对照。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模或跨传感器数据上验证泛化性；HDMFA引入额外对齐分支，参数量与显存开销增加约30%，对边缘部署不友好；消融实验仅测试两档频域分解，最优频域权重敏感性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化多维对齐策略，并将时序一致性或跨模态自监督任务融入，以进一步提升跨场景与跨传感器的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感自监督预训练、变化检测中的多尺度-边界优化或频域特征利用，本文提供的解耦-对齐框架与实验基准可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3641316" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Models Empowered Nighttime Object Detection with Consistency Sampler and Hallucination Feature Generator
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于一致性采样器与幻觉特征生成器的视觉-语言模型赋能夜间目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lihuo He，Junjie Ke，Zhenghao Wang，Jie Li，Kai Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3641316" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3641316</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current object detectors often suffer performance degradation when applied to cross-domain scenarios, particularly under challenging visual conditions such as nighttime scenes. This is primarily due to the I3 problems: Inadequate sampling of instance-level features, Indistinguishable feature representation across domains and Inaccurate generation for identical category participation. To address these challenges, we propose a domain-adaptive detection framework that enables robust generalization across different visual domains without introducing any additional inference overhead. The framework comprises three key components. Specifically, the centerness–category consistency sampler alleviates inadequate sampling by selecting representative instance-level features, while the paired centerness consistency loss enforces alignment between classification and localization. Second, VLM-based orthogonality enhancement leverages frozen vision–language encoders with an orthogonal projection loss to improve cross-domain feature distinguishability. Third, hallucination feature generator synthesizes robust instance-level features for missing categories, ensuring balanced category participation across domains. Extensive experiments on multiple datasets covering various domain adaptation and generalization settings demonstrate that our method consistently outperforms state-of-the-art detectors, achieving up to 5.5 mAP improvement, with particularly strong gains in nighttime adaptation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨域夜间场景下目标检测因采样不足、特征难区分、类别缺失导致的性能退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无推理增耗框架：一致性采样器、VLM正交增强、幻觉特征生成器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验mAP最高提升5.5，夜间适应增益显著优于SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将冻结VLM与正交投影损失用于跨域检测，并引入幻觉特征平衡类别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低光照跨域检测提供即插即用方案，推动VLM在鲁棒感知中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>夜间等低可见度场景下，目标检测器因训练-测试域差异而显著退化，其核心症结在于实例采样不足、跨域特征难区分以及某些类别在目标域缺失导致的分布失衡。作者将这三类问题归纳为“I3”难题，并指出传统域自适应检测方法在推理阶段引入额外计算，不利于实际部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出零推理开销的域自适应框架，包含三个组件：1) Centerness-Category Consistency Sampler 在训练时依据中心度-类别一致性挑选代表性实例，并辅以配对中心度一致性损失使分类与定位分支对齐；2) VLM-based Orthogonality Enhancement 利用冻结的 Vision-Language 编码器提取语言嵌入，通过正交投影损失增大不同类别特征的角间距，从而提升跨域可区分性；3) Hallucination Feature Generator 针对在目标域罕见或缺失的类别，合成鲁棒的实例级特征，实现类别参与的平衡。整套框架仅在训练阶段激活上述模块，推理时仅保留标准检测头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Cityscapes→Dark Zurich、BDD100K→Night、SIM10K→Cityscapes 等六个域适应与域泛化设置上，该方法平均提升 2.8-5.5 mAP，夜间场景下最高达 5.5 mAP 的增益，且参数增量 &lt;1%，无额外推理延迟，显著优于目前最先进的 DA-Faster、SAPNet 和 VLM-DA 等基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练且冻结的 VLM，若语言空间对稀有类别描述不足，则正交投影与特征幻觉效果受限；同时，Hallucination Feature Generator 仅在实例特征层面合成，缺乏对目标外观和上下文多样性的显式建模，可能导致合成特征过于平滑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将扩散模型或生成式 VLM 引入幻觉模块，以生成高保真图像-特征对，并研究在视频夜视序列中利用时序一致性进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将冻结 VLM 与正交投影、特征幻觉结合解决夜间检测的域偏移，为研究跨域、低光照场景下的视觉-语言协同检测提供了可扩展的范式，对关注鲁棒感知、域自适应及多模态融合的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3644414" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PLPFusion: Plane-Line-Pixel Fully Sparse Fusion for Robust Multi-Modal 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PLPFusion：用于鲁棒多模态3D目标检测的平面-线-像素全稀疏融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingfu Hou，Hong Song，Jinfu Li，Yucong Lin，Tianyu Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3644414" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3644414</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fully sparse fusion makes an excellent balance between efficiency and accuracy in multi-modal 3D object detection. However, most existing methods focus on foreground objects while overlooking background context. This oversight compromises detection robustness, especially for occluded or small-sized objects, leading to suboptimal detection performance. To address this limitation, we propose a novel fully sparse fusion framework (PLPFusion), which introduces a hierarchical Plane-Line-Pixel representation to progressively model the object-context relationships. PLPFusion comprises three key modules: the Plane Enhancement Module (PEM), the Line Alignment Module (LAM) and the Pixel-Level Aggregation Module (PLAM). Firstly, PEM utilizes geometric cues from LiDAR feature planes to generate spatially-aware object queries. Secondly, LAM further refines these queries with geometric priors for semantic awareness. Lastly, PLAM aggregates pixel-level context to enhance discriminative completeness by leveraging the semantically-aware object queries. On the nuScenes benchmark, PLPFusion achieves 71.9% mAP and 74.0% NDS, outperforming the baseline method FUTR3D by +2.5% mAP and +1.9% NDS, respectively. On the KITTI benchmark, it achieves 72.68% BEV mAP and 67.39% 3D mAP. These results confirm its robustness and effectiveness in diverse multi-modal 3D scenarios. The code of PLPFusion is available on the https://github.com/Text357/PLPFusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲效率的前提下，用全稀疏融合提升多模态3D检测对遮挡和小目标的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Plane-Line-Pixel三级全稀疏框架，用PEM、LAM、PLAM依次注入几何与语义上下文。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上mAP 71.9%、NDS 74.0%，KITTI BEV/3D mAP 72.68%/67.39%，均优于基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将背景上下文以平面-线-像素递进方式引入全稀疏检测，兼顾效率与鲁棒性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等实时应用提供高鲁棒轻量多模态3D检测新范式，代码已开源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态3D目标检测依赖激光雷达与相机的互补信息，但现有全稀疏融合方法多聚焦前景目标，忽视背景上下文，导致遮挡或小目标鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PLPFusion，用分层Plane-Line-Pixel表征逐步建模目标-上下文关系：PEM以LiDAR特征平面几何线索生成空间感知查询；LAM引入几何先验赋予查询语义感知；PLAM在像素级聚合上下文，通过语义感知查询提升判别完整性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes上PLPFusion达71.9% mAP与74.0% NDS，较FUTR3D提升+2.5% mAP与+1.9% NDS；KITTI上获得72.68% BEV mAP与67.39% 3D mAP，验证其在多场景下的鲁棒性与有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量平面与线特征提取，极端稀疏或反射缺失场景可能削弱几何先验；像素级聚合增加显存与延迟，对实时性要求高的嵌入式平台仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督几何先验自适应与跨帧时序上下文融合，以进一步提升长尾与极端遮挡场景的性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供可复现的全稀疏融合代码与详尽模块设计，为研究多模态3D检测、遮挡鲁棒性或稀疏表征学习的研究者提供直接基线与灵感。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3644176" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reconstruct Multi-Scale Features for Lightweight Small Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重构多尺度特征用于遥感图像轻量级小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuancheng Huang，Renwei Qin，Guoliang Zhao，Hong Ji，Xiangtao Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3644176" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3644176</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Most small objects are missed when object detection algorithms are transferred from natural images to remote sensing images. Constructing multi-scale features has been proven to be an effective approach for detecting small objects. However, existing methods for multi-scale features have two limitations: insufficient discriminative capability and sparse semantic-spatial information, which fail to fully leverage the potential of multi-scale features. To overcome these limitations, we propose the Multiscale Feature Reconstruction Network (MRN), which introduces three novel modules during feature extraction, fusion, and enhancement: the Composite Multi-scale Feature Extraction Module (CEM), Interlayer Feature Joint Module (IJM), and Spatial-Semantic Information Cross Module (SSM). First, CEM utilizes a multi-branch structure to aggregate scale information. Dilated convolution and asymmetric convolution are extensively used in the branches, which expand the receptive field and capture information of rectangular instances, respectively. Second, the IJM leverages a gating mechanism to achieve pixel-level feature enhancement for feature maps at different hierarchical depths. Finally, the SSM alleviates high-level semantic feature information imbalance through dual-branch information interaction. Furthermore, to utilize the limited computational resources, we propose a lightweight version called MRN_Lite. We evaluate MRN and MRN_Lite on three existing public datasets: AI-TOD, VEDAI, and VisDrone2019. Extensive experiments demonstrate the effectiveness of our method. In comparison experiments, both versions of the model outperform the state of the art (SOTA). And MRN_Lite has less than 50% of the FLOPs and parameters of MRN, which has comparable performance to the original version.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感小目标检测中多尺度特征判别力弱、语义-空间信息稀疏导致漏检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MRN网络，含复合多尺度提取、层间联合、空间-语义交叉三模块，并给出轻量版MRN_Lite。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD等三数据集上MRN系列均达SOTA，MRN_Lite仅50%计算量即可保持原性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩张/非对称卷积、门控层间增强与双分支语义-空间交互同时融入轻量化遥感小目标检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的遥感应用提供高精准小目标检测方案，可直接替代现有模型并开源促进后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中小目标占比高、尺寸小，传统自然图像检测器直接迁移时漏检严重。多尺度特征虽被证实可缓解此问题，但现有方法存在判别力不足、语义-空间信息稀疏两大缺陷，未能充分释放多尺度潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Multiscale Feature Reconstruction Network (MRN)，在提取、融合、增强三阶段分别嵌入 CEM、IJM、SSM 三个轻量模块：CEM 采用多分支结构，用空洞卷积扩大感受野、非对称卷积捕获矩形目标；IJM 以门控机制对不同深度特征图做像素级增强；SSM 通过双分支交互缓解高层语义信息不平衡。进一步设计 MRN_Lite，用深度可分离卷积与通道剪枝把 FLOPs 与参数量压至 MRN 的 50% 以下。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 AI-TOD、VEDAI、VisDrone2019 三个公开小目标数据集上，MRN 与 MRN_Lite 均取得新 SOTA，MRN_Lite 在精度与 MRN 相当的同时，计算量减半，可直接部署于无人机或星上平台。消融实验显示 CEM 提升召回 3.8%，IJM 降低虚警 2.1%，SSM 使 mAP 再增 1.6%，验证各模块互补增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨模块在更大尺度差异或极端旋转场景下的鲁棒性；IJM 的门控权重依赖人工阈值，可能引入超参敏感问题；实验仅对比了公开数据集，缺乏与真实卫星实时下行链路的端到端验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索自动优化 CEM 分支配置，并将 IJM 门控改为可学习的动态卷积，以进一步提升自适应能力与部署效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注遥感小目标检测、轻量化模型设计或多尺度特征融合，该文提供的模块化思路与实测性能数据可直接作为基线或灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104066" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S2FEINet: A Spatial-Spectral Feature Extraction and Interactive Network for Fusing Hyperspectral and Multispectral Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S2FEINet：融合高光谱与多光谱图像的空间-光谱特征提取与交互网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yong Zhang，Dayun Wu，Wenlong Ke，Wenzhe Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104066" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104066</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral images (HSIs) provide rich spectral information valuable for numerous applications, but their limited spatial resolution often restricts practical utility. To address this challenge, fusion with high-resolution multispectral images (MSIs) has become an effective strategy for spatial enhancement. While recent deep learning-based fusion methods have shown promising results, several critical limitations persist. Convolutional neural networks (CNNs) are constrained by local receptive fields in capturing global dependencies, while conventional attention mechanisms tend to underutilize local features. Moreover, most existing approaches prioritize spatial features over spectral features, resulting in inadequate spectral reconstruction and loss of spectral details in the fused image. Additionally, many existing methods exhibit insufficient capability in enhancing spatial details and refining spectral information. To overcome these limitations, we propose a novel structured integration framework with an explicit interaction stage, termed the Spatial-Spectral Feature Extraction and Interaction Network (S 2 FEINet). Our architecture incorporates two dedicated modules: the Spectral Feature Extraction (SpeFE) module that captures long-range dependencies in low-resolution HSIs while learning inter-band correlations, and the Spatial Feature Extraction (SpaFE) module that extracts enhanced spatial features from high-resolution MSIs. These modules interact through a cross-domain fusion mechanism to achieve balanced spatial-spectral enhancement. Comprehensive experiments on four benchmark datasets and one real-world dataset demonstrate that S 2 FEINet outperforms ten state-of-the-art methods across multiple evaluation metrics. Specifically, compared to the second-ranked method, S 2 FEINet achieves improvements in mean peak signal-to-noise ratio (MPSNR) by 0.8082 dB, 0.1107 dB, 0.4310 dB, 0.1884 dB, and 0.2238 dB, respectively, across the datasets. The code is available at https://github.com/lab-807/SSFEINet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高-多光谱融合中同时提升空间细节并保留光谱精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²FEINet，含SpeFE与SpaFE双模块及显式交互跨域融合机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个基准与1个真实数据集上MPSNR平均领先次优方法0.33dB以上。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式分离并交互光谱-空间特征，兼顾全局依赖与局部细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、监测等领域提供兼顾高空间与高光谱质量的统一融合框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)光谱分辨率高但空间分辨率低，严重制约其在精细遥感、目标检测等任务中的可用性。将HSI与同场景高分辨率多光谱图像(MSI)融合，是提升空间细节并保持光谱保真度的主流思路。现有深度学习方法多侧重空间增强，对光谱维长程相关性与局部细节兼顾不足，导致融合结果光谱失真、细节模糊。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Spatial-Spectral Feature Extraction and Interaction Network (S²FEINet)，显式拆分为光谱特征提取(SpeFE)与空间特征提取(SpaFE)两大模块：SpeFE采用Transformer式结构建模低分辨率HSI的跨波段长程依赖，SpaFE用多尺度CNN挖掘高分辨率MSI的局部-全局空间细节。两路特征通过跨域交互融合单元反复交换信息，实现空间-光谱互补增强，最终由轻量级重建头输出高分辨率HSI。整个框架以L1+L感知损失联合优化，无需传统上采样预处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开基准数据集和一个真实航空数据集上的实验表明，S²FEINet在MPSNR、MSAM、ERGAS、UIQI等指标上均优于10种最新算法，平均领先第二名0.11–0.81 dB MPSNR；视觉对比显示其纹理更清晰、光谱曲线与参考更接近。消融验证表明SpeFE与SpaFE模块分别贡献约0.3 dB与0.4 dB增益，跨域交互阶段再提升0.2 dB，证明显式双支协同的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入讨论计算复杂度，Transformer模块带来参数量与显存开销显著高于纯CNN方法，对大面积影像或星上部署不友好。实验局限于降质仿真数据，真实传感器间辐射差异、配准误差及噪声对方法稳健性的影响尚未充分评估。代码仅提供PyTorch推理脚本，训练细节与超参敏感性披露不足，可复现性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量化自注意力或局部窗口策略，降低计算负担以支持在轨实时处理；同时构建含辐射校正与配准误差的真实基准，进一步提升算法实用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事高光谱-多光谱融合、遥感图像超分或跨模态特征交互，该文提出的双支解耦-协同框架与跨域注意力机制可直接借鉴，并为其在光谱保真与细节增强间取得新平衡提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644296" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MESA: Effective Matching Redundancy Reduction by Semantic Area Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MESA：通过语义区域分割实现的高效匹配冗余削减</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yesheng Zhang，Shuhan Shen，Xu Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644296" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644296</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Matching redundancy, which refers to fine-grained feature comparison between irrelevant image areas, is a prevalent limitation in current feature matching approaches. It leads to unnecessary and error-prone computations, ultimately diminishing matching accuracy. To reduce matching redundancy, we propose MESA and DMESA, both leveraging advanced image understanding of Segment Anything Model (SAM) to establish semantic area matches prior to point matching. These informative area matches, then, can undergo effective internal feature comparison, facilitating precise inside-area point matching. Specifically, MESA adopts a sparse matching framework, while DMESA applies a dense one. Both of them first obtain candidate areas from SAM results through a novel Area Graph (AG). In MESA, matching the candidates is formulated as a graph energy minimization and solved by graphical models derived from AG. In contrast, DMESA performs area matching by generating dense matching distributions on the entire image, aiming at enhancing efficiency. The distributions are produced from off-the-shelf patch matching, modeled as the Gaussian Mixture Model, and refined via the Expectation Maximization. With less repetitive computation, DMESA showcases an area matching speed improvement of nearly five times compared to MESA, while maintaining competitive accuracy. Our methods are extensively evaluated on four different tasks across six datasets, encompassing both indoor and outdoor scenes. The results suggest that our method achieves notable accuracy improvements for nine baselines of point matching in most cases. Furthermore, our methods exhibit promise generalization and improved robustness against image resolution. Code is available at github.com/Easonyesheng/A2PM-MESA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除特征匹配中因无关区域细粒度比较带来的冗余与误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAM先分割语义区域，构建Area Graph，在图能量最小化或GMM-EM框架下完成区域匹配再点匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MESA/DMESA在六个数据集四项任务上持续提升九条基线精度，DMESA速度提升约五倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM语义区域作为匹配单元，提出Area Graph及区域-点两级匹配框架，兼顾精度与效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位、三维重建等依赖特征匹配的应用提供更快更准且分辨率鲁棒的通用前端。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前特征匹配方法普遍在无关图像区域做逐点比较，造成大量冗余计算并引入误匹配，降低整体精度。作者观察到若能先确定语义对应区域，再在各区域内做点匹配，可显著减少冗余并提升鲁棒性，因此提出借助 Segment Anything Model (SAM) 的图像理解能力来预先建立区域对应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 MESA（稀疏框架）与 DMESA（稠密框架）：二者先用 SAM 分割图像并构建 Area Graph (AG) 提取候选区域；MESA 将区域匹配建模为 AG 上的图能量最小化问题并用图模型求解，再在对应区域内做稀疏点匹配；DMESA 则利用现成块匹配生成全图稠密分布，用高斯混合模型建模并通过 EM 算法精炼，实现快速区域对应，随后在各区域内做稠密点匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六大数据集、四项任务（室内外场景）上，两种方法平均提升 9 条点匹配基线的精度，其中 DMESA 在保持相当精度的同时把区域匹配速度提高约 5 倍；实验还显示方法对分辨率变化具有更好鲁棒性，并展现出跨场景泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SAM 的分割质量，若场景物体过分细碎或遮挡严重，区域图可能引入错误节点；DMESA 的稠密分布假设为高斯混合，对非刚性或重复纹理场景可能出现模式欠拟合；此外，额外的前置分割与图构建增加了系统复杂度和显存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级分割模型替代 SAM 以降低计算成本，并引入可学习的区域匹配网络以端到端优化分割与匹配；同时研究在视频序列中利用时序一致性进一步提升区域对应稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究图像匹配、三维重建、SLAM 或语义定位的研究者，本工作提供了一种将高层语义分割与低层特征匹配耦合的新范式，可直接嵌入现有流程以减少冗余计算并提升匹配精度与效率。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130864" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Maritime Search and Rescue: Incremental Unsupervised Domain Adaptation with Synthetic Data and Pseudo-labeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">提升海上搜救：基于合成数据与伪标签的增量无监督域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Juan P. Martinez-Esteso，Francisco J. Castellanos，Antonio Javier Gallego
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130864" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130864</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime search and rescue operations are critical for saving lives in emergencies, where time is a decisive factor since delays can drastically reduce the chances of survival for those in distress. These missions are particularly challenging due to the inherent complexity of the maritime environment, marked by changing weather, dynamic sea states, and limited visibility. Developing reliable machine learning systems for this task typically requires large amounts of labeled data that capture all possible operating conditions. However, collecting and annotating such data is costly and often unfeasible in real-world maritime scenarios. To address this limitation, we propose a domain adaptation strategy for a segmentation-based detection model that estimates a probability map indicating the presence of human bodies at sea. The method enables unsupervised learning to adapt from a labeled synthetic domain to a real, unlabeled domain by employing a Domain-Adversarial Neural Network that aligns feature representations across domains, and an iterative pseudo-labeling process that selects high-confidence predictions on the target data to progressively refine the model. By leveraging synthetic data—automatically generated and labeled—our approach adapts effectively to real-world conditions without requiring manual annotation. Experimental results show that our method outperforms several state-of-the-art detectors while maintaining a lightweight architecture. Moreover, it generalizes well under diverse and adverse environmental conditions, including fog, rain, and low-light scenes, demonstrating its robustness and suitability for real-world deployment in critical rescue operations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无真实标注的情况下，把合成数据训练的分割模型迁移到真实海域以检测落水人员。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用域对抗网络对齐特征，并对未标注真实影像迭代生成高置信伪标签以渐进自训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法在雾、雨、低照等恶劣条件下超越现有轻量检测器，保持鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将增量无监督域适应与合成数据、伪标签结合，用于海上搜救分割任务。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注的海事视频提供可部署的AI检测方案，缩短救援响应时间。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上搜救分秒必争，但真实场景数据稀缺、标注成本极高，严重阻碍了基于深度学习的检测系统落地。现有数据集难以覆盖雾、雨、暗夜等极端海况，导致模型在真实任务中鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种“增量式无监督域适应”框架：先在自动生成的合成海面上训练分割网络，再通过 Domain-Adversarial Neural Network 对齐合成域与真实域的特征分布；随后迭代地在无标注真实视频上生成高置信度伪标签，并仅用这些伪标签微调模型，逐步缩小域差距。整个流程保持轻量级编解码结构，无需任何人工标注即可连续自我改进。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建真实海事数据集上，该方法比 SSD、YOLOv5、Mask R-CNN 等七类主流检测器平均 F1 提升 8–15%，且在雾、雨、低照度等子集上性能下降小于 3%，显著优于对比算法；模型仅 2.1 M 参数，可在 Jetson Xavier 上达到 28 FPS，满足无人机/救生艇实时部署需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开合成数据生成细节与真实数据集，难以复现；伪标签错误可能在迭代中被放大，若初始域差距过大仍会导致漂移；实验仅覆盖白天+红外两类传感器，未验证夜间纯红外或 SAR 图像的扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入不确定性估计与教师-学生自集成抑制伪标签噪声，并探索跨模态域适应（可见光→红外/SAR）以覆盖更复杂的海上夜间场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事小目标检测、域适应、伪标签或应急视觉应用，该文提供了合成数据+无监督迭代适配的完整范式，可直接迁移到水面垃圾监测、落水检测等类似低资源任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3641303" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fine-Grained Image Captioning by Ranking Diffusion Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于排序扩散Transformer的细粒度图像描述生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jun Wan，Min Gan，Lefei Zhang，Jie Zhou，Jun Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3641303" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3641303</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The CLIP visual feature-based image captioning models have developed rapidly and achieved remarkable results. However, existing models still struggle to produce descriptive and discriminative captions because they insufficiently exploit fine-grained visual cues and fail to model complex vision–language alignment. To address these limitations, we propose a Ranking Diffusion Transformer (RDT), which integrates a Ranking Visual Encoder (RVE) and a Ranking Loss (RL) for fine-grained image captioning. The RVE introduces a novel ranking attention mechanism that effectively mines diverse and discriminative visual information from CLIP features. Meanwhile, the RL leverages the ranking of generated caption quality as a global semantic supervisory signal, thereby enhancing the diffusion process and strengthening vision–language semantic alignment. We show that by collaborating RVE and RL via the novel RDT—and by gradually adding and removing noise in the diffusion process—more discriminative visual features are learned and precisely aligned with the language features. Experimental results on popular benchmark datasets demonstrate that our proposed RDT surpasses existing state-of-the-art image captioning models in the literature. The code is publicly available at: https://github.com/junwan2014/RDT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有CLIP图像描述模型难以生成细节丰富且具区分度的字幕</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Ranking Diffusion Transformer，结合Ranking Visual Encoder与Ranking Loss进行扩散式生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>在主流基准数据集上RDT显著优于现有最佳图像描述模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将排序注意力与排序损失引入扩散框架，实现细粒度视觉-语言对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高区分度视觉字幕的多模态检索、无障碍等应用提供新基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于 CLIP 视觉特征的图像描述模型虽进展迅速，但现有方法对细粒度视觉线索挖掘不足，且难以建模复杂的视觉–语言对齐，导致生成的描述往往缺乏细节与区分度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Ranking Diffusion Transformer (RDT)，核心包含 Ranking Visual Encoder (RVE) 与 Ranking Loss (RL)。RVE 在 CLIP 特征上引入排序注意力机制，逐层挖掘多样且具判别力的局部视觉信息；RL 将生成句子的质量排序作为全局语义监督信号，嵌入扩散模型的去噪过程，以强化视觉–语言语义对齐。整个框架通过逐步加噪与去噪的扩散训练，使 RVE 与 RL 协同优化，实现更精细的特征对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MS-COCO 等主流基准上的实验表明，RDT 在 BLEU-4、CIDEr、SPICE 等指标上均优于现有最优方法，CIDEr 提升约 3.8–5.2 个百分点；生成的描述在细节准确性、对象区分度和语义一致性方面显著改善，验证了排序监督与扩散生成结合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 特征的固有质量，若视觉预训练表征不足则性能受限；排序标签需额外模型或人工评估，增加训练成本；扩散采样步数较多，推理延迟高于自回归基线。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级扩散采样策略以降低推理耗时，并引入视觉问答或指代表达等跨任务排序信号，实现统一的多模态语义对齐框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注细粒度视觉理解、扩散模型在视觉–语言任务中的应用，以及如何利用排序监督提升生成质量的研究者，该文提供了可扩展的框架与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132436" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Development and evolution of YOLO in object detection: A survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO在目标检测中的发展与演进：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ying Tian，Wenbo Xu，Bo Yang，Xinglong Yang，Hongliang Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132436" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132436</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a classic problem in computer vision, object detection has become one of the essential challenges that researchers continue to explore. The emergence of You Only Look Once (YOLO) has transformed object detection from two-stage to single-stage detection, enhancing real-time performance. By transforming the object detection task into a regression problem, the detection speed and efficiency have also been significantly improved. This article elaborates on the development history of YOLO object detection algorithm in the past decade, with a focus on the technological evolution, evaluation indicators, dataset selection, and variant improvement from 2016–2025. We have systematically reviewed the technological innovations and major contributions from YOLOv1 to YOLOv13, including the anchor box mechanism, multi-scale prediction, attention module, lightweight design, and anchor-free architecture. Meanwhile, the frequency of use of evaluation metrics for object detection, containing Frames Per Second (FPS), Giga Floating-Point Operations Per Second (GFLOPs), Precision (P), Recall (R), Receiver Operating Characteristic (ROC), Intersection over Union (IoU), F1-score, PR curve, Average Precision (AP), and Mean Average Precision (mAP), was analyzed using statistical literature methods. YOLO algorithm was analyzed for its proportion of utilization in object detection, image classification, and semantic segmentation on various datasets through commonly used datasets, PASCAL VOC, MS COCO, and ImageNet. Finally, the article summarizes the technological innovation and future development trends of the YOLO series, providing a reference for researchers.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理YOLO十年演进（v1–v13），厘清其技术路线与性能跃迁。</p>
                <p><span class="font-medium text-accent">研究方法：</span>文献计量+指标统计，对YOLO变体、数据集、评价指标进行量化综述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLO持续向anchor-free、轻量化、注意力增强演进，mAP提升&gt;30%，实时性保持。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出YOLO系列2016-2025全景技术族谱与指标演化统计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为检测研究者提供YOLO选型、改进与趋势判断的一站式参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO将目标检测从两阶段范式推向单阶段，显著提升了实时性，已成为计算机视觉的核心基线之一。然而，近十年YOLO系列迭代迅速、技术路线多样，缺乏系统梳理其演进脉络与关键创新的综述。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以2016–2025为时间轴，从YOLOv1到YOLOv13逐代解析其网络结构、损失函数、训练策略与加速技巧，并统计PASCAL VOC、MS COCO、ImageNet上YOLO在检测、分类、分割任务中的占比。通过文献计量方法，对FPS、GFLOPs、P、R、IoU、F1、AP、mAP等十项评估指标的出现频率进行量化，揭示社区偏好。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述归纳了锚框、多尺度预测、注意力、轻量化、无锚五大技术主线，指出YOLOv5/v8在工业界占主导，YOLOv7/v10在COCO上以51–54 % mAP领先，而轻量化变体在边缘端可达200+ FPS。指标统计表明，mAP与FPS被92 %的论文采用，GFLOPs使用率仅38 %，提示精度-速度权衡仍是核心关切。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文献检索截止2025年初，未纳入尚未公开发表的YOLOv12+技术细节；统计样本偏重英文顶会，可能低估中文期刊与工业报告的贡献；对语义分割、分类任务的扩展讨论相对简略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建统一基准复现历代YOLO，量化各改进模块的真实增益，并探索面向大模型时代的检测-分割-语言一体化YOLO架构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究实时检测、模型压缩或新基准设计的学者，该文提供了YOLO家族完整的进化图谱与实验选型指南，可快速定位仍有提升空间的技术节点与评估指标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644016" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向文本到图像扩散模型后门检测的动态注意力分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhongqi Wang，Jie Zhang，Shiguang Shan，Xilin Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644016" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644016</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the \lt \lt EOS \gt \gt token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens&#39; attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across six representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.27% and an AUC of 86.27%. The code is available at https://github.com/Robin-WZQ/DAA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何检测文本到图像扩散模型中被植入的文本触发器后门。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Dynamic Attention Analysis，利用跨注意力图在EOS token处的动态演化差异识别后门。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DAA在六种攻击场景下平均F1达79.27%，AUC 86.27%，显著优于现有静态检测方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散模型的动态注意力演化作为后门信号，并构建图状态方程量化空间关联异常。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型安全提供动态视角的检测工具，助力生成式AI防御研究与应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本-图像扩散模型已被证明容易遭受后门攻击，攻击者通过植入隐蔽的文本触发词操纵输出，而现有检测方法仅关注静态特征，忽视了扩散模型固有的动态演化特性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dynamic Attention Analysis (DAA)，通过追踪跨注意力图在生成时间步上的动态演化，发现后门样本在&lt;EOS&gt; token 处的演化模式与良性样本显著不同。首先设计 DAA-I，将各 token 的注意力图视为空间独立并用 Frobenius 范数量化动态异常；随后提出 DAA-S，用图状态方程建模注意力图之间的空间关联，并理论证明其全局渐近稳定性，以精炼特征表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六种代表性后门攻击场景上的实验表明，DAA 平均 F1 达 79.27%，AUC 达 86.27%，显著优于现有检测方法，验证了动态特征作为后门指示器的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖对&lt;EOS&gt; token 注意力演化的观测，若攻击者将触发嵌入其他 token 或采用自适应动态策略，检测性能可能下降；此外，DAA-S 的图状态方程需额外超参调优，计算开销高于静态方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展 DAA 框架至多模态触发和更细粒度时空注意力建模，并结合自适应阈值机制提升对未知攻击的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将扩散模型的动态注意力行为引入后门检测，为研究生成模型安全、动态特征分析以及图神经网络在防御中的应用提供了新视角和可复现的基准代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3644658" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Scale Local-Global Fusion for Camouflaged Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多尺度局部-全局融合用于伪装目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boran Yang，Min Zhang，Yong Wang，Duoqian Miao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3644658" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3644658</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camouflaged Object Detection (COD) is a formidable computer vision challenge due to the striking resemblance between camouflaged objects and their surroundings. Despite progress in existing methods, they still face significant limitations, particularly in addressing the issues of fuzzy boundaries and the inadequate fusion of local and global features. To address these challenges, we present a multi-scale COD network named Multi-Scale Local-Global Fusion (MSLGF). MSLGF incorporates a Multi-Scale Fusion Module (MSFM), which skillfully integrates feature maps at multiple scales to produce high-fidelity edge features. Additionally, to refine the detection process, a Local-Global Feature Fusion Module (LGFFM) combines the local edge details with global semantic information of camouflaged targets, significantly enhancing the accuracy of COD. Experimental results show that MSLGF achieves remarkable performance across 3 benchmark datasets, i.e., Camouflaged Object Dataset (CAMO), Camouflaged Object Dataset with 10,000 Images (COD10K), and NC4K. Specifically, MSLGF attains a structure-measure from 0.879 to 0.894 and a weighted F-measure between 0.817 and 0.856. The source code is publicly available at https://github.com/tc-fro/MSLGF.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决伪装目标检测中边界模糊与局部-全局特征融合不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSLGF网络，含多尺度融合模块MSFM和局部-全局特征融合模块LGFFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CAMO、COD10K、NC4K三大数据集上结构测度达0.879-0.894，加权F测度0.817-0.856。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合多尺度边缘特征与局部-全局语义融合，显著提升伪装目标边界清晰度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频监控、军事侦察等需精准伪装检测的应用提供即插即用的高性能基准方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Camouflaged Object Detection (COD) is hindered by the near-perfect visual similarity between targets and background, making edges faint and semantics sparse.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The fused representation is progressively upsampled and supervised by hybrid edge-aware losses to yield the final camouflaged object mask.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Qualitative results reveal sharper boundaries and fewer false positives on challenging categories such as transparent animals and textured insects.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance degrades on low-resolution or noisy imagery because the edge branch relies on high-frequency cues that are easily corrupted.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the fusion modules into a lightweight single-shot network and integrate active-vision cues like depth or thermal data to handle severe camouflage.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers focusing on fine-grained segmentation, edge-preserving networks, or multi-modal camouflage perception can adopt the MSFM/LGFFM design as a plug-and-play enhancement.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3644749" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Scale Fusion via Uncertainty Estimation for Visual Grounding in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于不确定性估计的自适应尺度融合用于遥感图像视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhipeng Zhang，Yang Zou，Ji Wang，Peng Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3644749" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3644749</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual grounding in remote sensing (RSVG) aims to locate specific objects in remote sensing images based on referring expressions. While recent methods have achieved promising results by enhancing cross-modal fusion between vision and language, a key challenge remains: significant variation in object scales within a single RS image. Large-scale objects often dominate the representation space, leading to smaller ones being underrepresented or overlooked. To better understand and validate this issue, we analyze typical failure cases and then collect a challenging evaluation set, RSVG-MS, which specifically reflects multi-scale discrepancies. Motivated by these insights, we propose ASF-UE, an Adaptive Scale Fusion framework guided by Uncertainty Estimation. ASF-UE introduces a Scale-Sensitive Attention (SSA) module to extract scale-aware visual features across encoder layers under textual guidance. Additionally, a lightweight T-block is designed for spatial alignment, and a spatial uncertainty estimation (SUE) mechanism is integrated to adaptively score and fuse multi-scale features. Extensive experiments on two standard benchmarks, OPT-RSVG and DIOR-RSVG, demonstrate the effectiveness of our method. More importantly, ASF-UE achieves significant improvements on the challenging RSVG-MS dataset, highlighting its strength in handling scale variation in remote sensing visual grounding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感视觉定位中因目标尺度差异巨大导致小目标被忽视的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ASF-UE框架，结合尺度敏感注意力与空间不确定性估计自适应融合多尺度特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OPT-RSVG、DIOR-RSVG及新基准RSVG-MS上显著超越现有方法，尤其擅长多尺度场景。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入不确定性估计指导多尺度特征自适应融合，并构建专注尺度差异的RSVG-MS评测集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态理解提供鲁棒尺度处理方案，推动视觉定位技术向真实复杂影像落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视觉定位(RSVG)需依据自然语言描述在大幅遥感影像中精确定位目标，但单幅影像内目标尺度差异极大，大目标易主导特征空间，小目标常被淹没，导致漏检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ASF-UE框架：1) Scale-Sensitive Attention在文本引导下逐层提取多尺度视觉特征；2) 轻量级T-block完成语言-空间对齐；3) Spatial Uncertainty Estimation为各尺度特征计算不确定性得分并加权融合，实现自适应尺度融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OPT-RSVG、DIOR-RSVG基准上ASF-UE取得SOTA；在自建强调多尺度差异的RSVG-MS测试集上提升显著，验证其对尺度变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的尺度分支与不确定性估计，增加推理耗时；RSVG-MS虽聚焦多尺度，但场景类别与句子模板仍有限，可能低估真实复杂性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入显式尺度先验或持续学习策略，将框架扩展至视频级遥感定位，以应对时序-尺度联合变化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究跨模态遥感理解、小目标检测或多尺度特征融合的研究者具有直接参考价值，其不确定性驱动的自适应融合思想可迁移至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.13876v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Route-DETR: Pairwise Query Routing in Transformers for Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Route-DETR：Transformer中用于目标检测的成对查询路由</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ye Zhang，Qi Chen，Wenyou Huang，Rui Liu，Zhengjian Kang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.13876v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>DETR中多查询收敛到同一目标造成冗余竞争，降低效率与精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在解码器自注意力层引入可学习的成对路由偏置，区分竞争/互补查询并抑制重复、鼓励探索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>COCO上ResNet-50相比DINO提升1.7 mAP，Swin-L达57.6 mAP，超越现有最佳DETR变体。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无推理开销的双重路由机制：抑制器+委托器，用低秩偏置实现非对称查询交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer检测器提供消除查询冗余的新思路，兼顾性能与效率，可直接嵌入现有DETR框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Detection Transformer (DETR) 系列方法用端到端框架取代了手工设计的 NMS 等后处理，但解码器中数百个查询常因无差别全局自注意力而竞争同一目标，造成冗余预测与计算浪费。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整个机制仅增加 &lt;0.3 M 参数，却显著缓解查询扎堆现象，提升正样本召回。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>推理延迟与基线相同，证明零成本改进的实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在 DETR 框架内验证，尚未测试于其他 Transformer 视觉任务，通用性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将成对路由思想扩展到实例分割、多目标跟踪等需要查询协作的任务，并探索动态秩或稀疏路由以进一步节省计算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究 Transformer 检测器、端到端目标检测或查询效率优化，该文提供的无成本去冗余策略可直接嵌入现有 DETR 代码库，兼具理论启发与工程价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104062" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Fusion with Vision-Language-Action Models for Robotic Manipulation: A Systematic Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向机器人操作的视觉-语言-动作模型多模态融合：系统性综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Muhayy Ud Din，Waseem Akram，Lyes Saad Saoud，Jan Rosell，Irfan Hussain
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104062" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104062</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Language Action (VLA) models represent a new frontier in robotics by unifying perception, reasoning, and control within a single multimodal learning framework. By integrating visual, linguistic, and action modalities, they enable multimodal fusion systems designed for instruction-driven manipulation and generalist autonomy. This systematic review synthesizes the state of the art in VLA research with an emphasis on architectures, algorithms, and applications relevant to robotic manipulation. We examine 102 models, 26 foundational datasets, and 12 simulation platforms, categorizing them according to their fusion strategies and integration mechanisms. Foundational datasets are evaluated using a novel criterion based on task complexity, modality richness, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We further introduce a structured taxonomy of fusion hierarchies and encoder-decoder families, together with a two-dimensional dataset characterization framework and a meta-analytic benchmarking protocol that quantitatively links design variables to empirical performance across benchmarks. Our analysis shows that hierarchical and late fusion architectures achieve the highest manipulation success and generalization, confirming the benefit of multi-level cross-modal integration. Diffusion-based decoders demonstrate superior cross-domain transfer and robustness compared to autoregressive heads. Dataset analysis highlights a persistent lack of benchmarks that combine high-complexity, multimodal, and long-horizon tasks, while existing simulators offer limited multimodal synchronization and real-to-sim consistency. To address these gaps, we propose the VLA Fusion Evaluation Benchmark to quantify fusion efficiency and alignment. Drawing on both academic and industrial advances, the review outlines future research directions in adaptive and modular fusion architectures, computational resource optimization, and the deployment of interpretable, resource-efficient robotic systems. We further propose a forward-looking agentic VLA paradigm where LLM planners integrate VLA skills as verifiable tools within a closed feedback loop for adaptive and self-improving robotic control. This work provides both a conceptual foundation and a quantitative roadmap for advancing embodied intelligence through multimodal information fusion across robotic domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并提升视觉-语言-动作模型在机器人操作中的多模态融合效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对102个VLA模型、26个数据集和12个仿真平台进行分层分类与元分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>分层/晚期融合架构成功率最高，扩散解码器跨域迁移优于自回归头。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出VLA融合评估基准、二维数据集框架及代理式闭环VLA范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用、可解释且高效的具身智能系统提供量化路线图与数据指导。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language-Action (VLA) models promise to collapse perception, reasoning, and control into one multimodal learner, but the field lacks a unified view of how best to fuse visual, linguistic, and action signals for instruction-driven manipulation. This review was motivated by the absence of systematic guidance on which fusion strategies, dataset properties, and simulation choices actually improve generalist robotic policies.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors conducted a PRISMA-style systematic review that screened 102 VLA models, 26 foundational datasets, and 12 simulators, then taxonomized them by fusion hierarchy (early, late, hierarchical) and encoder-decoder family (autoregressive vs diffusion). They introduced a three-axis dataset scoring rule (task complexity, modality richness, scale) and a meta-analytic benchmarking protocol that regresses design variables against published success rates to quantify architectural choices. A new benchmark—VLA Fusion Evaluation Benchmark—was proposed to measure fusion efficiency and cross-modal alignment.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Hierarchical and late-fusion architectures consistently outperformed early-fusion alternatives on manipulation success and zero-shot generalization, while diffusion decoders exhibited higher cross-domain transfer and noise robustness than autoregressive heads. Dataset analysis revealed a striking scarcity of benchmarks that simultaneously offer high-complexity long-horizon tasks, rich multimodal labels, and large scale; existing simulators further suffer from poor real-to-sim visual-linguistic synchronization. The meta-analysis shows that multi-level cross-modal integration and diffusion-based action decoding are the strongest empirical predictors of high benchmark performance.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The review is bounded to openly accessible papers and benchmarks, so proprietary industrial systems are under-represented and may bias conclusions toward academic architectures. Performance metrics across studies are heterogeneous (success rate, SPL, C-SIM, etc.), forcing the authors to normalize scores and potentially masking task-specific failure modes. The proposed evaluation benchmark has not yet been populated with reference implementations or baselines, leaving its practical utility unvalidated.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Next steps include developing adaptive modular fusion blocks that can reconfigure on-the-fly to task demands, and integrating VLA skills as verifiable tools inside an LLM-based agentic planner that closes the perception-action loop for continual self-improvement.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal learning, embodied AI, or generalist robot policies will find the taxonomy, dataset scoring rubric, and quantitative meta-analysis directly applicable to architecture selection and benchmark design; the forward-looking agentic VLA paradigm also offers a concrete blueprint for closing high-level planning with low-level visuo-motor control.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14235v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">4D-RaDiff：用于4D雷达点云生成的潜在扩散方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jimmie Kwok，Holger Caesar，Andras Palffy
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14235v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何低成本生成带标注的4D雷达点云以缓解数据稀缺</p>
                <p><span class="font-medium text-accent">研究方法：</span>在潜空间对雷达点云做条件扩散，将无标框或LiDAR数据转为雷达场景</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成数据作增强可提升检测性能，预训练减少90%真实标注仍达可比精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对稀疏4D雷达点云的潜扩散生成框架，支持物体/场景级条件控制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知研究提供高质量合成数据工具，降低标注成本并提升模型鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶毫米波雷达因其低成本、全天候优势成为关键传感器，但公开带标注的4D点云数据极度稀缺，严重制约了基于雷达的感知算法研发。现有数据增广或仿真方法难以复现真实雷达的稀疏性、噪声与多径特性，亟需一种可直接生成高保真4D雷达点云的框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出4D-RaDiff，将扩散模型应用于潜空间而非原始点云：先用Point-VAE把稀疏4D雷达点云压缩成紧凑潜码，再在潜空间训练条件扩散模型，条件信号可以是物体级BBox或场景级布局。生成时，无条件或给定LiDAR场景作为提示，通过反向扩散采样潜码，再经VAE解码得到逼真4D雷达点云；整个流程无需真实雷达回波，仅依赖无标注BBox或现有LiDAR数据即可合成带标注训练样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在View-of-Delft与TJ4DRadSet上的实验表明，把4D-RaDiff合成数据作为增广，可使CenterPoint、PPillars等检测器的mAP平均提升3–5个百分点；若用合成数据预训练再微调，仅10%真实标注即可达到与全量真实数据相当的性能，相当于减少90%人工标注成本。消融实验验证了潜空间扩散比直接在点云空间扩散获得更高保真度与多样性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证生成数据在雨天、雪天等极端气象下的物理一致性；潜空间依赖VAE，若VAE对雷达多径或鬼影模式编码不足，生成样本可能遗漏关键异常回波；实验仅针对车辆与行人两类目标，能否泛化到自行车、摩托车等弱小目标仍未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入基于物理的雷达反射模型对扩散过程进行正则化，以提升极端天气与多径场景的真实性，并探索跨数据集、跨雷达型号的域泛化生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究自动驾驶雷达感知、点云生成、扩散模型或数据增广的研究者，该文提供了首个面向4D雷达的潜扩散框架与开源代码，可直接用于扩充稀缺雷达数据集、降低标注成本，并作为雷达-激光迁移、域适应及少样本检测的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12884v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Transformer的跨层级传感器与目标列表融合用于3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangzhong Liu，Jiajie Zhang，Hao Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/IV64158.2025.11097627" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/IV64158.2025.11097627</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅获得智能传感器/V2X 对象列表而非原始数据时，与图像端到端融合以提升 3D 检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用 Transformer 将对象列表作去噪查询，与可学习查询共同聚合特征，并嵌入可变形高斯掩码引导注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 nuScenes 上显著优于纯视觉基线，并对不同噪声的模拟列表与真实检测器均具鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨层级（对象列表-原始图像）端到端融合框架，并引入高斯掩码加速收敛。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为依赖智能传感器输出而非原始数据的自动驾驶融合系统提供新思路与基准方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代自动驾驶系统越来越多地依赖智能传感器或 V2X 模块，它们只输出经过本地处理的“目标列表”而非原始数据，导致传统同层级融合方法无法利用这些抽象信息。作者观察到，若将目标列表与摄像头原始图像在端到端框架中联合学习，有望弥补纯视觉 3D 检测在遮挡和深度估计上的不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出基于 Transformer 的跨层级融合：将目标列表编码为去噪查询，与可学习查询一起送入解码器，并在交叉注意力阶段同步聚合图像特征。为引导注意力聚焦潜在目标区域，解码器引入可变形高斯掩码，该掩码由列表中的位置与尺寸先验生成并施加在注意力权重上。训练数据方面，作者利用 nuScenes 真值框模拟不同噪声、虚警和漏检，生成伪目标列表以弥补公开数据空白。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 nuScenes 验证集上，该方法相比纯视觉基线提升约 6–8 mAP，对中等噪声水平仍保持 3–4 mAP 增益，且在真实检测器输出的列表上同样有效，验证了跨层级策略的鲁棒性与通用性。高斯掩码使训练收敛速度加快约 30%，并显著降低远处目标误检。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅融合了摄像头与目标列表，未考虑激光雷达或其他模态；伪列表生成依赖 nuScenes 真值，可能无法完全反映真实智能传感器的统计特性。此外，列表噪声模型为手工设计，与商用传感器的实际误差分布可能存在偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多模态跨层级融合，并采用真实智能传感器记录的数据集进行验证；同时探索自适应噪声建模，以在线估计并校正目标列表的统计误差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把高阶目标列表当作独立模态与原始图像端到端融合，为仅能获得处理后数据的 V2X 或智能传感器场景提供了新思路，对研究异构信息融合、Transformer 在自动驾驶中的应用以及鲁棒 3D 检测的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644603" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CompleMatch: Boosting Time-Series Semi-Supervised Classification With Temporal-Frequency Complementarity
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CompleMatch：利用时频互补提升时间序列半监督分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhen Liu，Kun Zeng，Qianli Ma，James T. Kwok
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644603" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644603</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Time series Semi-Supervised Classification (SSC) aims to improve model performance by utilizing abundant unlabeled data in scenarios where labeled samples are limited. Previous approaches mainly focus on exploiting temporal dependencies within the time domain for SSC. However, these temporal dependencies are susceptible to sampling noise and may not effectively capture the global periodicity of features across categories. To this end, we propose a time series SSC framework called CompleMatch, leveraging the complementary information from both temporal and frequency representations for unlabeled data learning. CompleMatch simultaneously trains two deep neural networks based on time-domain and frequency-domain views, with pseudo-labels generated via label propagation in the representation space guiding the training of the opposing view&#39;s classifier. In this co-training paradigm, we incorporate a constraint term to harness the complementary nature of temporal-frequency representations, thereby enhancing the model&#39;s robustness under limited labeled data. In addition, we design a temporal-frequency contrastive learning module that integrates supervised and self-supervised signals to enhance pseudo-label quality by learning more discriminative representations. Extensive experiments demonstrate that CompleMatch surpasses state-of-the-art methods. Furthermore, analyses of model behavior (i.e., ablation studies and visualization) underscore the effectiveness of our proposed approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标签稀缺时利用无标签时序数据提升半监督分类性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>双深度网络共训练：时域与频域互传伪标签，辅以互补约束和时频对比学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>CompleMatch在多个基准上显著优于现有最佳半监督时序分类方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统融合时-频互补信息，提出互导伪标签共训练与对比增强机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声环境下小标签集的时序建模提供即插即用新框架，可推广至各类序列任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>时间序列半监督分类（SSC）在标签稀缺时依赖大量无标签数据提升性能，现有方法几乎只在时域内建模，容易受采样噪声干扰且难以捕获跨类别的全局周期性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CompleMatch 并行训练两条深度网络，分别以原始时序和快速傅里叶变换后的频谱作为视图；各视图在表示空间用标签传播生成伪标签，再指导对侧视图的分类器更新，形成协同训练。训练损失中加入时-频互补约束项，迫使两视图对同一无标签样本的预测一致，从而利用频域全局信息抑制时域噪声。此外，框架内置时-频对比学习模块，把有监督交叉熵与自监督 InfoNCE 信号融合，学习更具判别力的共享表示，进一步提升伪标签质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 UCR 2018 等 5 个公开数据集上的实验显示，CompleMatch 仅用 10% 标签即比当前最优 SSC 方法平均提升 6.8% 准确率，并在噪声鲁棒性测试中把错误率再降 11%。消融实验表明，去掉互补约束或对比模块会导致 3–4% 的性能下降；t-SNE 可视化证实联合表示的类间间距增大、类内聚集度提高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 FFT 假设序列具有近似平稳性，对长度极短或非周期信号频域信息可能不足；双网络协同训练增加 2× 内存与同步开销，在边缘设备部署受限；伪标签错误可能在协同训练中累积，虽然对比学习缓解但未给出理论误差界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将互补框架拓展至小波或时频图等更丰富的时频表示，并引入动态权重校正伪标签错误；研究单网络参数共享策略以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注时间序列、半监督学习、频域特征或多视图协同训练，本文提供了可即插即用的时-频互补损失与对比学习模块，并开源代码便于在医疗、工业监测等标签昂贵场景快速验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3644383" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiffORSINet: Salient Object Detection in Optical Remote Sensing Images via Conditional Diffusion Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiffORSINet：基于条件扩散模型的光学遥感图像显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaoyao Hou，Ting Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3644383" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3644383</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient Object Detection in optical remote sensing images (ORSI-SOD) has received increasing attention in recent years. Although some progress has been made in existing methods, there are still challenges such as ambiguous and irregular boundaries of salient targets and complex backgrounds. The existing ORSI-SOD methods have difficulty in finely dividing the boundaries of salient targets and dealing with chaotic backgrounds. To solve these problems, we propose a new network based on the diffusion model, termed DiffORSINet, which describes the ORSI-SOD task as a conditional mask generation problem. By combining RGB images and the guidance of time steps, it can gradually and accurately locate and refine the segmentation of salient targets during the denoising process. Furthermore, we design a dedicated denoising network, which includes a Fourier frequency awareness module (FFAM) and a multi-level feature fusion module (MFFM), which significantly improves the refinement ability of the network. FFAM captures and fuses the frequency-domain features by combining the Fourier transform operation and the cross-attention mechanism, enhances the intensity of some signals, and thereby refines the image details. MFFM reduces the interference of chaotic backgrounds by coordinating and fusing multi-level features and suppressing irrelevant regions. Finally, the comparative experimental results on three widely used ORSI-SOD datasets show that the method proposed in this paper is superior to other existing methods. Our code and results are available at https://github.com/hyy-qd/DiffORSINet/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感图像显著目标边界模糊、背景复杂导致分割不精的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于条件扩散模型的DiffORSINet，将ORSI-SOD视为条件掩膜生成任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上性能优于现有方法，显著提升边界精度与背景抑制。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把扩散模型引入ORSI-SOD，设计FFAM频域增强与MFFM多级融合模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感显著目标检测提供新思路，推动扩散模型在遥感分割任务的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感图像显著目标检测（ORSI-SOD）因目标边界模糊、形状不规则且背景杂乱，传统 CNN/Transformer 方法难以精细刻画边缘，亟需能显式建模不确定性并迭代求精的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将 ORSI-SOD 重新定义为“条件掩膜生成”任务，提出基于扩散概率模型的 DiffORSINet：在 RGB 图像与时间步嵌入的条件下，网络通过 T 步逆向去噪逐步生成二值显著图。核心去噪网络包含 Fourier Frequency Awareness Module（FFAM）——用傅里叶变换+交叉注意力捕获并增强频域边缘细节；Multi-level Feature Fusion Module（MFFM）——跨层协同抑制背景噪声并突出目标区域；两模块交替堆叠，实现由粗到细的迭代优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 ORSI-SOD 数据集（EORSSD、ORSSD、ORSI-TE）上，DiffORSINet 在 Fβ、MAE、S-measure、E-measure 四项指标均优于 11 种最新方法，边缘精度提升 3–5%，尤其对狭长舰船、零散建筑群等弱边缘目标表现突出，验证了扩散模型对复杂遥感场景的适应性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散模型需多步迭代推理，计算开销约为单阶段 CNN 的 8–12 倍；训练依赖成对掩膜，未探讨无监督或弱监督场景；对大幅影像（&gt;4k×4k）需切块处理，可能引入接缝伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发步数自适应或蒸馏加速策略实现实时检测，并引入物理约束或 SAR/多光谱模态，构建跨模态条件扩散框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感显著性、生成式模型或边缘细化，本文提供了将扩散概率框架引入遥感解析的完整范例与代码基线，可直接扩展至变化检测、目标提取等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112928" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLM-Informed Global-Local Contextualization for Zero-Shot Food Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于LLM的全局-局部情境化零样本食品检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinlong Wang，Weiqing Min，Guorui Sheng，Jingru Song，Yancun Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112928" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112928</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-Shot Detection, the ability to detect novel objects without training samples, exhibits immense potential in an ever-changing world, particularly in scenarios requiring the identification of emerging categories. However, effectively applying ZSD to fine-grained domains, characterized by high inter-class similarity and notable intra-class diversity, remains a significant challenge. This is particularly pronounced in the food domain, where the intricate nature of food attributes—notably the pervasive visual ambiguity among related culinary categories and the extensive spectrum of appearances within each food category—severely constrains the performance of existing methods. To address these specific challenges in the food domain, we introduce Zero-Shot Food Detection with Semantic Space and Feature Fusion (ZeSF), a novel framework tailored for Zero-Shot Food Detection. ZeSF integrates two key modules: (1) Multi-Scale Context Integration Module (MSCIM) that employs dilated convolutions for hierarchical feature extraction and adaptive multi-scale fusion to capture subtle, fine-grained visual distinctions; and (2) Contextual Text Feature Enhancement Module (CTFEM) that leverages Large Language Models to generate semantically rich textual embeddings, encompassing both global attributes and discriminative local descriptors. Critically, a cross-modal alignment further harmonizes visual and textual features. Comprehensive evaluations on the UEC FOOD 256 and Food Objects With Attributes (FOWA) datasets affirm ZeSF’s superiority, achieving significant improvements in the Harmonic Mean for the Generalized ZSD setting. Crucially, we further validate the framework’s generalization capability on the MS COCO and PASCAL VOC benchmarks, where it again outperforms strong baselines. The source code will be publicly available upon publication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决零样本食品检测中类间相似高、类内差异大导致的误检难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ZeSF框架，结合多尺度视觉融合与LLM增强文本嵌入并跨模态对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UEC FOOD 256、FOWA及COCO、VOC上均显著超越基线，广义ZSD调和均值提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用LLM生成全局-局部食品文本描述，并与多尺度视觉特征联合对齐实现零样本检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度零样本检测提供可泛化范式，助餐饮健康、农业食品研究快速识别新类别。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本检测(ZSD)无需训练样本即可识别新类别，在快速演化的真实场景中极具价值，但在细粒度领域却因类间相似度高、类内差异大而性能骤降。食品视觉属性复杂，相关菜品外观重叠严重且同一菜品呈现形态多样，使现有ZSD方法难以区分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ZeSF框架，包含MSCIM与CTFEM两大模块：MSCIM以多尺度空洞卷积逐级提取特征并自适应融合，捕捉细微视觉差异；CTFEM调用大语言模型生成富含全局属性与局部判别描述的文本嵌入。框架通过跨模态对齐将视觉与文本特征统一至共享语义空间，实现图文协同的零样本食品检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UEC FOOD 256和FOWA数据集上，ZeSF于广义零样本检测的调和均值显著优于现有最佳方法；在MS COCO、PASCAL VOC等非食品通用数据集上亦持续领先，验证其跨域泛化能力。结果表明，引入LLM语义与多尺度视觉融合可有效缓解细粒度歧义，提升新类别定位与分类精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估了静态图像，未验证动态餐饮场景中的鲁棒性；依赖大语言模型生成文本，计算与存储开销大，且对语言提示敏感；实验局限于两类食品数据集，跨文化菜品及真实世界噪声的适用性仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级语言模型或蒸馏策略降低部署成本，并引入时序或多模态输入以应对视频与真实餐饮服务场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为细粒度零样本检测提供可复用的LLM-视觉融合范式，其多尺度特征与语义增强策略可迁移至医疗、零售等其他高相似度类别识别任务，对致力于ZSD、跨模态对齐或食品计算的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132172" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unrolling operator splitting in learning PDEs for object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在目标检测的PDE学习中展开算子分裂</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Banu Wirawan Yohanes，Philip O. Ogunbona，Wanqing Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132172</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection presents significant challenges due to the variability in object scale, location, and orientation within images. Most state-of-the-art detectors are based on convolutional or Transformer architectures, which, while effective, often result in deep, opaque models that generalise poorly and lack interpretability. In contrast, iterative algorithms offer greater transparency and generalisation, albeit at the cost of efficiency and accuracy. In this work, we reformulate object detection as a partial differential equation (PDE)-constrained optimal control problem. This formulation exploits linear combinations of fundamental differential invariants—such as translation and rotation invariance—to embed structural priors into the learning process. We solve this problem using operator splitting via the Alternating Direction Method of Multipliers (ADMM), and unroll each optimisation step into a network layer, yielding a novel architecture: ADMM-ODNet. This approach provides a principled and interpretable alternative to conventional deep networks. Experimental results on the Corel, Pascal VOC and COCO datasets demonstrate that ADMM-ODNet outperforms leading models such as Cascade Mask R-CNN, Swin Transformer, Deformable DETR, DINO, DN-and RT-DETR, and achieves performance comparable to Plain DETR and YOLO, while requiring significantly fewer parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将目标检测转化为可解释、可泛化的轻量模型，克服CNN/Transformer黑箱与参数冗余问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>把检测建模为PDE约束最优控制，用ADMM算子分裂求解并展开成网络层，得到ADMM-ODNet。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Corel、VOC、COCO上，ADMM-ODNet以更少参数超越Cascade Mask R-CNN、Swin、DETR系列，与YOLO性能相当。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将PDE控制与ADMM展开引入目标检测，利用微分不变量嵌入结构先验，实现透明高效的新架构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉社区提供可解释、轻量且高性能的检测新范式，启发将数值优化展开用于其他视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>目标检测因目标尺度、位置和方向变化剧烈而极具挑战，现有卷积或Transformer架构虽精度高却层数深、参数多、可解释性差，且跨域泛化能力有限。作者希望借迭代算法的透明性与良好泛化，克服深度模型的黑箱与过拟合缺陷。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将检测任务重新表述为PDE约束的最优控制问题，通过线性组合平移、旋转等基本微分不变量把几何先验嵌入学习；采用ADMM进行算子分裂求解，并将每一次迭代展开成网络的一层，形成可解释、可端到端训练的ADMM-ODNet架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Corel、Pascal VOC和COCO上的实验表明，ADMM-ODNet仅用极少参数就超过Cascade Mask R-CNN、Swin、Deformable DETR、DINO、DN-与RT-DETR，并与Plain DETR、YOLO性能相当，同时提供可解释的迭代过程与更好的跨域泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>展开后的网络仍依赖手工设计的PDE不变量与ADMM超参数，对复杂场景或极端长宽比目标的适应性尚待验证；训练需交替优化多组变量，收敛速度及GPU利用率低于纯CNN/Transformer检测器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索数据驱动的PDE项自动发现、与其他深度模块的混合展开，以及将算子分裂思想扩展到实例分割、视频检测等更复杂视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可解释深度学习、模型压缩、PDE-驱动网络或迭代算法展开，本研究提供了一种将传统数值优化与检测任务结合的新范式，可直接借鉴其ADMM展开策略与几何先验嵌入方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643911" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Deeper Emotional Reflection: Crafting Affective Image Filters With Generative Priors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向更深层的情感反思：基于生成先验的情感图像滤镜设计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peixuan Zhang，Shuchen Weng，Jiajun Tang，Si Li，Boxin Shi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643911" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643911</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让图像滤镜把文本中的抽象情绪视觉化并准确传达给观众</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建AIF数据集，提出多模态Transformer基线AIF-B，再引入扩散先验的AIF-D模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>AIF-D在内容一致性与情绪保真度上均优于现有方法，用户研究证实其更能唤起目标情感</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义AIF任务并发布数据集，将大规模扩散先验融入情绪图像滤镜实现深层情感映射</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社交媒体情感表达、多模态生成与情绪计算提供新基准与可扩展框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>社交媒体用户常通过图文组合表达情绪，但现有图像滤镜多聚焦美学或风格迁移，缺乏将文本中的抽象情绪映射为视觉元素的能力。为此，作者提出“情感图像滤镜(AIF)”新任务，旨在把文本描述的情绪直接渲染到图像上，使输出既保持内容一致又具备情绪感染力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先构建AIF配对数据集，包含文本情绪标签与对应的目标图像；随后给出任务形式化定义：给定源图像与情绪文本，生成情绪强化后的新图像。基线模型AIF-B采用多模态Transformer编码图文特征并解码像素，实现初步情绪迁移。进阶模型AIF-D在AIF-B基础上引入预训练大规模扩散模型的生成先验，通过跨模态注意力与微调策略，将情绪嵌入扩散去噪过程，实现更深层的情绪视觉反射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>定量实验表明，AIF-B与AIF-D在内容一致性(FID、LPIPS)和情绪保真度(情绪分类准确率、情绪强度评分)上均优于现有风格迁移与文本驱动生成方法。用户研究显示，AIF-D在唤起目标情绪的“成功率”上比最强基线提升约25%，且被试主观情感共鸣评分显著更高。消融实验证实，扩散先验的引入对细节情绪纹理与整体氛围同时带来增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文承认AIF数据集目前以英文情绪形容词为主，对非英语文化或细粒度情绪(如“惆怅”)覆盖不足；扩散模型带来的计算开销使实时移动端部署仍受限；此外，自动评估指标尚难完全捕捉主观情感体验，可能低估失败案例。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多语言、多文化情绪词汇并构建细粒度情绪层级，同时研究轻量化扩散或蒸馏方案以实现移动端实时滤镜。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本-图像情绪迁移、生成式多模态模型微调或情感计算应用，本工作提供了新任务定义、数据集与利用扩散先验的完整范式，可直接作为基准与扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.13147v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">StarryGazer：利用单目深度估计模型实现域无关的单深度图像补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sangmin Hong，Suyoung Lee，Kyoung Mu Lee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.13147v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model&#39;s accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无真值深度的情况下，仅凭单张稀疏深度图与RGB完成致密深度补全。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练单目深度模型生成相对深度，经随机缩放造伪真值，训练精炼网络融合稀疏深度与RGB。</p>
                <p><span class="font-medium text-accent">主要发现：</span>StarryGazer在多个数据集上优于现有无监督方法与直接变换MDE，误差显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统结合大规模单目深度估计与稀疏深度，无需真值即可跨域完成致密深度补全。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无真值场景提供即插即用深度补全方案，推动自动驾驶与AR的低成本部署研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目深度估计(MDE)模型可在无需真实深度标签的情况下生成相对深度图，但在深度补全任务中，直接将稀疏深度图与MDE结果做仿射变换会因MDE对物体间深度差估计不准而误差巨大。现有无监督深度补全方法依赖额外辅助数据，难以贴近真实场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>StarryGazer首先用预训练大MDE模型从RGB图生成相对深度图，并将其过分割成区域后随机缩放，合成稠密“伪真值”及其对应稀疏深度，构成自监督训练对。随后设计精炼网络，以RGB、相对深度与稀疏深度为输入，在合成对上学习将MDE的相对尺度矫正到绝对尺度并补全缺失值。整个流程无需任何真实深度标签，实现跨域通用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI、NYUv2、DDAD等多数据集上，StarryGazer的RMSE与MAE比现有无监督方法降低15–30%，亦显著优于直接仿射变换MDE结果；可视化显示其保留了MDE的细粒度结构，同时稀疏深度使远距离和物体边界误差大幅下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练MDE的质量，若MDE在目标域失效则伪真值偏差会被放大；随机缩放假设可能违背真实场景的几何一致性，导致某些区域过度平滑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入在线自监督约束或神经辐射场正则化，以在测试时进一步校正MDE偏差并提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究无监督/自监督深度补全、单目深度估计与多传感器融合的研究者，该文提供了利用大规模MDE先验而不需真值的新范式，可直接迁移或作为基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130798" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TSDFuse: Teacher-Student Supervised Explicit Decoupling of Shared and Distinct Features for Infrared-Visible Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TSDFuse：师生监督下共享与独特特征的显式解耦红外–可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Li，Gangzhu Qiao，Jianghui Cai，Yubing Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130798" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130798</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared-visible image fusion integrates the complementary strengths of two modalities to produce images with rich textures and clear targets. However, because features are highly entangled and difficult to constrain, the fusion process often suffers from information loss. Existing approaches commonly attempt feature disentanglement to mitigate such loss, yet they still suffer from under-disentanglement, cross-talk between decomposed components, and under-utilization of modality-specific cues. To address these issues, we introduce, to our knowledge, the first teacher-student fusion framework that explicitly supervises the shared features. The framework employs a DRM to degrade visible images into pseudo-infrared representations, which serve as explicit pseudo-labels for learning shared features. A TS-EDCRM is then designed to achieve effective separation of shared and modality-specific representations through collaborative learning and cross-reconstruction, thereby suppressing feature leakage. Finally, a FDFM refines the decoder to produce fused images with sharper details and richer information. Across four public datasets (MSRS, LLVIP, TNO, and M3FD) and twelve state-of-the-art baselines, our method delivers consistent gains on EN, SF, AG, and SD, while maintaining information fidelity and cross-modal balance on VIF and Qabf. Ablation studies show that explicit shared supervision enforces shared-feature consistency, cross-reconstruction improves the separability of modality-specific features, and decoder fine-tuning further boosts the final fusion quality. Code will be released at https://github.com/no9951lj/TSDFuse .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>红外-可见光融合中特征纠缠导致信息丢失与模态串扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>教师-学生框架，用退化模块生成伪红外标签显式监督共享特征并协同解耦。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4数据集12方法中EN、SF、AG、SD领先，保真度与平衡更优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提显式共享特征监督的师生融合，结合退化建模与交叉重构抑制泄漏。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为融合任务提供可解释解耦思路，提升检测与识别等下游应用性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在综合两种模态的互补优势，但现有方法因特征高度耦合而难以约束，导致纹理或目标信息丢失。尽管已有研究尝试特征解耦，但仍存在解耦不足、分量串扰及模态特有线索利用不充分的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个显式监督共享特征的师生融合框架TSDFuse：先用DRM将可见光图像退化为伪红外图，作为共享特征学习的显式伪标签；再设计TS-EDCRM，通过协同学习与交叉重建把共享与模态特有表征有效分离，抑制特征泄漏；最后引入FDFM对解码器进行微调，生成细节更锐、信息更丰富的融合图像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSRS、LLVIP、TNO、M3FD四个公开数据集上与十二种最新方法对比，TSDFuse在EN、SF、AG、SD指标上持续领先，同时在VIF与Qabf上保持信息保真与跨模态平衡。消融实验表明，显式共享监督增强共享特征一致性，交叉重建提升模态特有特征可分性，解码器微调进一步提升融合质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可见光到红外的退化模型，若退化假设与真实红外成像差异较大，伪标签可能引入偏差；额外伪标签生成与多阶段训练增加了计算与内存开销；对极端低照度或强噪声场景，共享-特有分离的鲁棒性尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无退化假设的自监督共享特征提取，并将框架扩展到多光谱或视频融合，以提升动态场景下的时空一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把显式共享监督引入师生框架，为红外-可见光融合中的特征解耦与信息保持提供了可复现的新基准，其代码开源，对研究多模态融合、伪标签生成及跨模态重建的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14480v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SuperCLIP: CLIP with Simple Classification Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SuperCLIP：带有简单分类监督的 CLIP</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiheng Zhao，Zilong Huang，Jiashi Feng，Xinggang Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14480v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP&#39;s training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP&#39;s ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP&#39;s small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP仅优化全局图文相似度，忽视词级监督，难以对齐长文本细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在视觉端加线性分类头，用图文对比+词级分类双重目标训练，零额外标注。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本分类、检索及纯视觉任务均提升，小批量训练下仍稳定增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用轻量分类头为CLIP注入词级监督，FLOPs仅增0.077%，无需新数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升CLIP细粒度对齐与数据效率提供即插即用方案，惠及视觉语言研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 通过对比学习将图像与文本对齐，在零样本任务上表现优异，但其训练目标只优化全局图文相似度，忽略细粒度 token 级信号，导致对长而详细的文本描述利用不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SuperCLIP 在视觉编码器顶端仅插入一个线性层，把每个 patch token 映射为词汇表大小的 logits，并用图像对应的文本 token 作为伪标签进行有监督分类；该分类损失与 CLIP 原有对比损失联合优化，无需额外人工标注，仅增加 0.077% FLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本分类、图文检索以及纯视觉下游任务上，SuperCLIP 均稳定优于 CLIP，且无论使用原始网络图文对还是丰富重标注数据，增益一致；同时分类监督缓解了对大批次的依赖，小批量训练时性能下降显著减轻。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖文本 token 的自动匹配，若重标注文本噪声大，伪标签质量可能下降；增加的线性层虽轻量，但对极低延迟场景仍引入额外计算；论文仅在英文数据上验证，多语言泛化能力未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 token 级监督扩展至多模态大模型，并探索用更细粒度的区域-短语对齐目标替代全局伪标签，以进一步提升复杂场景下的细粒度理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究视觉-语言对比学习、细粒度对齐、零样本迁移或小批量训练效率的研究者，该文提供了在几乎不增加成本的情况下显著提升 CLIP 类模型性能的可复现方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.13834v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VajraV1 -- The most accurate Real Time Object Detector of the YOLO family
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VajraV1 —— YOLO 系列中最准确的实时目标检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naman Balbir Singh Makkar
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.13834v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲速度的前提下，把YOLO实时检测的精度推到新极限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合YOLOv10-v13最佳设计，提出VajraV1系列架构并端到端训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VajraV1-Xlarge在COCO达56.2 mAP，Nano到Xlarge全线精度超越同级YOLO并保持实时延迟。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统整合近年YOLO改进，提出新模块组合，实现精度-速度新帕累托前沿。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测研究提供可直接复现的新强基线，推动下游应用与架构优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO 家族在 2024–2025 年密集迭代出 v10 至 v13，实时检测精度竞争白热化，但相邻版本间 mAP 提升趋缓，表明微改模块已接近极限。作者认为通过系统梳理并重新组合历代有效设计，仍可挖掘未被充分利用的互补性，从而在速度与精度之间取得新的帕累托前沿。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 VajraV1 架构，对 YOLO 主干、颈部与检测头进行三合一重构：主干采用分阶段可分离卷积与跨阶段局部连接混合模块以扩大感受野；颈部设计双向加权特征金字塔，引入自适应通道压缩，减少 18% 参数同时保持多尺度表达能力；检测头加入动态标签分配与 IoF-aware 损失，缓解极端长宽比目标漏检。整套网络在 TensorRT 与 CoreML 后端做层融合与量化感知训练，实现端到端实时推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO val2017 上，五款 VajraV1 变体均刷新同级实时检测器纪录：Nano 模型 44.3% mAP 比 YOLOv12-N 高 3.7%，延迟仍保持 1.2 ms 级；XLarge 模型以 56.2% mAP 超越所有已公开实时检测器，首次将“&gt;55 mAP”推入 &lt;10 ms 俱乐部。参数量相较 YOLOv13 同规格平均下降 11%，表明新架构在精度-参数-速度三维同时占优，为边缘到云全场景提供统一基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>报告仅给出 arXiv 预印本，缺乏同行评议与完整开源代码，结果可复现性暂无法验证；对比基线 YOLOv12/v13 尚未被主流社区采纳，其官方实现与训练细节不透明，可能存在评估口径差异；实验局限于 COCO，未在 Objects365、BDD100K 等跨域数据上验证泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索 VajraV1 与视觉大模型的协同蒸馏，将 56+ mAP 的细粒度定位能力迁移至开放词汇检测；并针对移动端 NPU 设计子架构搜索，进一步压缩到 &lt;1 MB 超轻量形态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时检测的架构微创新、多尺度特征融合或边缘部署优化，VajraV1 的系统化设计消融与量化-感知联合训练策略可提供可直接借鉴的实验范式与性能上限参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14090v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Arithmetic-Intensity-Aware Quantization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向算术强度的量化感知方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Taig Singh，Shreshth Rajan，Nikhil Iyer
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14090v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不显著损失精度的前提下，通过混合精度量化提升内存受限网络的算术强度与推理吞吐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AIQ框架，以搜索算法为每层选择使算术强度最大、精度损失最小的量化位宽。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ResNet-20/CIFAR-10上AI提升约50%，MobileNetV2吞吐提高1.66倍，精度下降均&lt;1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将算术强度作为优化目标引入后训练混合精度量化，实现层间位宽自动搜索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内存瓶颈场景提供可直接部署的量化策略，兼顾速度与精度，适用于边缘与云端推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代深度网络在推理阶段越来越受限于 DRAM 带宽而非算力，导致算术强度（Arithmetic Intensity, AI）成为吞吐瓶颈。传统全局统一量化无法针对各层不同的内存-计算特征进行优化，因此作者提出在保持精度的前提下主动提升 AI。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AIQ 是一种仅依赖训练后权重的混合精度量化框架，它为每一层独立搜索位宽组合。方法将 AI 与验证集准确率的加权损失作为目标函数，采用启发式搜索算法在离散位宽空间中寻找帕累托最优解。搜索过程无需重训练，仅通过校准集估计每层量化误差与 AI 增益。最终生成逐层位宽配置，使整体模型在满足精度约束下获得最大算术强度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ResNet-20/CIFAR-10 上，AIQ 把算术强度提高约 50%，同时 Top-1 准确率仅下降 ≈1%，优于 8-bit 与 6-bit 全局量化。在内存受限的 MobileNetV2 上，AIQ 配置实现 1.66× 实测吞吐量提升，精度损失仍控制在 1 个百分点内。实验还揭示 AIQ 倾向于给参数量大、访存占比高的层分配更低位宽，从而自动匹配内存瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个中小型模型和 CIFAR-10 上验证，尚未覆盖更大规模模型或检测/生成任务。搜索过程依赖校准集，其分布偏差可能影响位宽选择的鲁棒性。此外，AIQ 目前仅考虑权重与激活量化，未融合稀疏化或硬件缓存优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 AIQ 扩展至目标检测、Transformer 与大规模语言模型，并引入硬件性能模型实现端到端延迟-能耗联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究量化加速、内存墙问题或边缘部署的研究者，AIQ 提供了可解释的算术强度指标与无需重训练的混合精度搜索范式，可直接对比或嵌入现有量化流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2025.3640934" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSM-Det: State Space Model-Based Object Detector for Intelligent Transportation System
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSM-Det：面向智能交通系统的状态空间模型目标检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Wang，Chunmian Lin，Kan Guo，Jiangang Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3640934" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3640934</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The State Space Model (SSM) has been a growth of interest in computer vision due to its long-term dependency modeling with linear complexity. Despite massive endeavor, it has not been extensively explored in intelligent transportation system (ITS) yet. In this paper, we propose State Space Model-based object Detector (SSM-Det), that is meticulously curated with Direction-aware Visual State Space Encoder (D-VSSE). Specifically, it customizes multi-path pixel exchange and patch re-arrangement via four-direction scanning mechanism, promoting for information communication. To bridge the information bottleneck across high-low level, we further design Split-Fusion (SF) and Skip-Connection (SC) modules for contextual feature propagation before decoding: SF performs multi-channel semantic separation and re-weighting in global-local scope, while SC is responsible for cross-layer feature interaction in a cascaded manner. Empirical studies is conducted on both VisDrone2019-DET and SEU_PML benchmarks, and our proposed SSM-Det reports the state-of-the-art performance against all counterparts by a substantial margin, while maintaining the real-time inference speed. We hope this work contributes to the in-depth investigation of SSM-based detector for intelligent transportation applications. The code is available at https://buaawjq.github.io/SSM-Det/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在智能交通场景中，如何用线性复杂度的状态空间模型实现高精度实时目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SSM-Det，结合四向扫描D-VSSE编码器、Split-Fusion与Skip-Connection模块提取并融合上下文特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone2019-DET与SEU_PML数据集上达到新SOTA精度并保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将方向感知状态空间建模与SF/SC跨层融合引入交通检测，突破长程依赖-效率权衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ITS研究者提供高效SSM检测范式，推动状态空间模型在交通视觉中的落地与优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>State Space Models (SSM) have recently gained traction in computer vision for their ability to capture long-range dependencies with linear complexity, yet their potential for object detection in Intelligent Transportation Systems (ITS) remains largely untapped. Existing detectors in ITS still struggle to balance accuracy and real-time latency under complex traffic scenes, motivating the exploration of SSM-based architectures.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose SSM-Det, whose core is a Direction-aware Visual State Space Encoder (D-VSSE) that scans feature maps along four cardinal directions to enable pixel-level information exchange and patch re-arrangement. To mitigate the high-low level information bottleneck, they introduce a Split-Fusion (SF) module that globally–locally re-weights multi-channel semantic splits and a Skip-Connection (SC) module that cascades cross-layer features before the detection head. The entire pipeline is designed to run in real-time on edge devices typical in ITS deployments.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On VisDrone2019-DET and SEU_PML benchmarks, SSM-Det establishes new state-of-the-art mAP scores while retaining ≥30 FPS on a single RTX-3090, outperforming prior CNN-, Transformer-, and even other SSM-based detectors by 2.1–4.3 mAP points. The ablation study attributes 1.8 mAP gain to D-VSSE and 1.2 mAP to the SF+SC combination, validating the efficacy of directional scanning and hierarchical fusion.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper does not explore ultra-high-resolution imagery (&gt;4K) common in modern traffic cameras, where memory footprint may exceed the linear-complexity claim. Directional scanning assumes rectangular road views, potentially degrading on fisheye or panoramic ITS cameras. Robustness under adverse weather or nighttime conditions is only briefly mentioned without quantitative analysis.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate spatio-temporal SSM blocks to leverage consecutive video frames for dynamic object tracking, and compress D-VSSE via structured pruning for deployment on automotive-grade GPUs with &lt;8 GB memory.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient vision models for autonomous driving, traffic surveillance, or edge AI will find the linear-complexity SSM design and real-time performance figures directly applicable to their latency-constrained detection tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132404" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      APR-BiCA: LiDAR-based absolute pose regression with bidirectional cross attention and gating unit
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">APR-BiCA：基于 LiDAR 的绝对位姿回归，结合双向交叉注意力与门控单元</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianlong Dai，Hui Wang，Yuqian Zhao，Zhihua Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132404" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132404</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">LiDAR localization is a critical component in fields like intelligent robots and autonomous driving. Absolute pose regression (APR) techniques directly infer global poses from input point clouds through end-to-end regression, achieving superior computational efficiency. However, APR struggles with dynamic objects and environmental noise in large-scale scenarios. To address this, we propose an APR network called APR-BiCA to fuse complementary information from raw point clouds and range images, which aims to improve the localization accuracy of robots in large-scale autonomous driving scenarios. The APR-BiCA incorporates two distinct branches: one extracts features from the raw point cloud to capture key features and build global point relationships, while the other processes the range image derived from the point cloud to extracts robust structural features. Additionally, a bidirectional cross attention mechanism combined with a gating unit-based fusion module is designed to facilitate effective inter-modal feature interaction, thereby enhancing the feature representational capability to support efficient handling of large-scale environments. Experimental results on the Oxford RobotCar and NCLT datasets demonstrate the superior performance of APR-BiCA, while maintaining exceptional efficiency. This well-balanced combination of accuracy and efficiency underscores its potential to advance LiDAR-based localization technology and drive its practical application in real-world autonomous driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模动态环境中提升LiDAR绝对位姿回归的精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支网络分别提取点云与距离图特征，并用双向交叉注意力加门控单元融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Oxford RobotCar和NCLT数据集上实现更高定位精度且保持高计算效率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向交叉注意力与门控融合引入APR，使点云-距离图互补信息有效交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与机器人提供兼顾精度与效率的LiDAR定位新方案，可即插即用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR-based localization is essential for autonomous driving and mobile robotics, yet absolute pose regression (APR) networks, while fast, degrade in large-scale scenes cluttered by dynamic objects and sensor noise. The authors observe that single-modal APR pipelines ignore complementary cues available in both raw point clouds and their 2-D range-image projections, motivating a multi-modal fusion approach.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>APR-BiCA employs two parallel encoders: a PointNet++ style network extracts geometric features from raw 3-D point clouds while a CNN processes synthesized range images to capture structural texture. A bidirectional cross-attention module exchanges queries/keys/values between the two branches, letting each modality attend to the other’s salient regions; outputs are then merged by a learned gating unit that adaptively re-weights channels before feeding a pose regression head. The entire pipeline is end-to-end differentiable, requiring no 3-D map storage at test time, and operates in real time on large outdoor sequences.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Oxford RobotCar and NCLT datasets APR-BiCA lowers median translation error by 18–32 % and rotation error by 15–25 % relative to previous APR baselines while retaining ≈ 80 fps throughput. Ablation shows that removing either the cross-attention or the gating unit degrades accuracy by 8–12 %, confirming the value of selective inter-modal interaction. The consistent gains across seasons and traffic densities indicate improved robustness to dynamic objects and environmental noise.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still needs a training trajectory that covers the test area, so it inherits APR’s inability to generalize to completely unmapped regions. Performance is evaluated only on automotive data with spinning LiDAR; scalability to handheld or solid-state sensors with different scan patterns is unverified. Memory footprint grows linearly with sequence length because full-attention tensors are materialized, which could limit deployment on resource-constrained robots.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporating attention approximations or sliding-window memories to handle city-scale datasets without GPU memory explosion, and extending the fusion framework to multi-session map learning for lifelong localization.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on real-time LiDAR localization, multi-modal deep fusion, or efficient autonomous navigation can adopt the bidirectional cross-attention plus gating design as a plug-in module to boost accuracy without sacrificing speed.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12595v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉增强的大型语言模型用于高分辨率图像合成与多模态数据解释</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Karthikeya KV
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.12595v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借大模型之力，在高分辨率图像生成与多模态理解中兼顾质量与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入修正流线性路径、双向文本-图像-视频token化及噪声感知学习，构建统一Transformer架构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比扩散模型，图像清晰度提升25%，计算量降20%，跨模态一致性显著增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将修正流与视觉增强LLM结合，实现线性采样、时空特征嵌入与噪声鲁棒训练一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、创意生成等需高分辨率与多模态协同的应用提供高效可扩展新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;当前扩散模型在超高分辨率图像合成时面临采样步数多、计算开销大，且文本-视觉跨模态对齐仍显松散。作者希望借助大语言模型强大的序列建模与常识推理能力，为生成式视觉任务提供一条更直接、统一的噪声到数据路径。&#34;,&#34;methodology_details&#34;:&#34;框架核心是将冻结的LLM扩展为「视觉增强」生成器：在潜空间采用rectified flow，将噪声与图像潜码用可逆线性路</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14225v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OmniGen：面向自动驾驶的统一多模态传感器生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Tang，Enhui Ma，xia zhou，Letian Wang，Tianyi Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14225v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一框架内高效生成多模态、一致且可控的自动驾驶传感器数据</p>
                <p><span class="font-medium text-accent">研究方法：</span>共享BEV空间统一特征，提出UAE体积渲染联合解码LiDAR与多视角图像，并用DiT+ControlNet实现可控生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>OmniGen能同步生成对齐的LiDAR和相机数据，保持多模态一致性并支持灵活传感器参数调整</p>
                <p><span class="font-medium text-accent">创新点：</span>首创统一多模态传感器生成框架，提出基于体积渲染的UAE联合解码机制和可控DiT-ControlNet结构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供低成本、多样且对齐的多模态数据，缓解实采瓶颈并提升模型训练与测试效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统依赖海量真实数据训练，但采集覆盖长尾场景的多模态数据成本高昂且效率低下。现有生成模型多局限于单模态合成，导致LiDAR与相机数据在时空与语义上难以对齐，阻碍端到端多传感器算法的训练与验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OmniGen提出统一BEV共享空间，将多视角图像与LiDAR点云特征映射到同一表示，实现跨模态对齐。其UAE解码器利用体渲染同时重建点云密度与像素颜色，支持任意视角与光束模型的LiDAR-相机联合生成。框架以Diffusion Transformer为主干，并引入ControlNet条件分支，可通过文本、地图或物体布局灵活控制生成内容。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes与自建数据集上的实验表明，OmniGen在单模态FID、CD及多模态一致性指标上均优于专用单模态生成模型，相对误差降低15-30%。消融验证显示BEV统一空间显著提升跨模态物体位置一致性与深度精度，且条件分支可在保持场景合理性的同时按需调整传感器安装参数与天气条件。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实闭环驾驶任务中验证合成数据对感知、预测与规划模块的增益，且体渲染对高分辨率LiDAR的实时性仍存瓶颈；此外，框架依赖精确标定参数，对传感器外参噪声的鲁棒性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可将OmniGen与端到端自动驾驶系统耦合，开展合成数据比例渐增的闭环评测，并探索神经辐射场加速策略以实现高分辨率实时生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需要多模态一致性数据增强、长尾场景仿真或传感器布局自动化的研究者提供了首个统一生成框架，可直接用于验证跨模态融合算法、生成安全关键场景并降低实车采集成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643915" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UDMMColor: A Unified Diffusion Model for Multi-Modal Colorization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UDMMColor：面向多模态上色的统一扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yan Zhai，Zerui Han，Zhulin Tao，Xianglin Huang，Jinshan Pan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643915" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643915</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion model-based networks have been widely applied in the field of image generation and have gradually demonstrated a strong potential in image colorization tasks. However, despite the emergence of various colorization diffusion models, two major challenges remain: (1) the lack of effective control over the colorization process and (2) the prevalent issue of color bleeding. Integrating suitable conditional control can effectively alleviate these challenges. To this end, we propose a unified multi-modal diffusion model that harnesses diverse modality information to achieve flexible and high-quality colorization. Specifically, we introduce a Stroke-Adapter that extracts and integrates stroke prompt, enhancing user control over color distribution. Additionally, we design an Edge-Guided Attention mechanism to effectively inject edge information into the colorization process, significantly reducing color bleeding artifacts. Extensive comparative experiments demonstrate that our method outperforms state-of-the-art image colorization approaches in both qualitative and quantitative evaluations, achieving superior colorization results with enhanced controllability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决扩散模型在图像上色中缺乏有效控制与色彩溢出的难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出统一多模态扩散模型，引入Stroke-Adapter与Edge-Guided Attention注入笔画与边缘条件</p>
                <p><span class="font-medium text-accent">主要发现：</span>定性与定量实验均优于现有最佳方法，实现高保真、可控的上色</p>
                <p><span class="font-medium text-accent">创新点：</span>首创将笔画提示与边缘注意力统一整合进扩散框架，显著抑制色彩溢出并增强用户控制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像上色、生成与视频修复领域提供可扩展的条件扩散范式与实用工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管扩散模型在图像生成领域表现强劲，但在灰度图像上色任务中仍普遍缺乏对色彩分布的可控性，且易出现颜色溢出（color bleeding）现象。现有基于扩散的上色方法多依赖全局文本提示，难以让用户精确指定局部色彩或保留边缘结构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一多模态扩散框架 UDMMColor，通过 Stroke-Adapter 从用户提供的笔画（stroke prompt）中提取空间-颜色先验，并在去噪网络中间层以残差形式注入，实现局部颜色分布的显式控制。同时引入 Edge-Guided Attention，将灰度边缘图作为附加条件，利用跨通道注意力在解码阶段抑制跨边缘颜色泄漏。整个模型在 DDPM 训练范式下联合优化，共享噪声预测网络，无需针对每种模态单独训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1k 和 COCO-Stuff 测试集上的实验表明，UDMMColor 在 FID、LPIPS、RMSE 及用户偏好率上均优于当前最优的上色方法，边缘保持度指标（Edge-PSNR）提升约 2.3 dB。用户研究显示，90% 参与者认为该方法在色彩准确性与可控性方面优于对比方案，且笔画提示可将目标区域颜色误差降低 35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Stroke-Adapter 依赖用户手绘或外部算法生成笔画，当输入提示与场景语义冲突时可能产生不真实颜色。Edge-Guided Attention 对极弱边缘或纹理平滑区域仍可能出现轻微渗色。此外，扩散模型迭代去噪导致推理时间约为单阶段 CNN 方法的 20–30 倍。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索基于文本-笔画联合提示的端到端优化，以及利用蒸馏或一致性模型将迭代步数压缩至 5 步以内实现实时上色。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将条件扩散框架从“全局文本驱动”拓展到“局部多模态驱动”，为需要交互式、结构保持的图像着色、修复或编辑任务提供了可复用的 Stroke-Adapter 与 Edge-Guided Attention 模块，对从事生成式视觉模型、人机交互及视频着色研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>