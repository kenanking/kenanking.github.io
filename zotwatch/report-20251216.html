<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-16</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-16 10:49 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">924</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与定位，同时积极跟踪模型压缩与高效推理技术；对自监督/对比学习等表示学习方法也保持阅读。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在通用目标检测、视觉定位及轻量化网络三方面收藏量稳定，且持续追踪Kaiming He、Ross Girshick等团队的最新工作，形成深度积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将CV方法与遥感结合，系统阅读SAR图像目标识别与旋转检测文献，并关注迁移学习、域自适应在跨模态数据上的应用。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年收藏量显著回升，新增关键词集中在视觉Transformer、可微分渲染与多视角生成，显示正向三维感知与生成式模型扩展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注NeRF/3D Gaussian Splatting与检测任务的结合，以及多模态大模型在遥感影像理解中的微调方法。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(13 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 900/900 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xian Sun">Xian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">113</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-16 10:28 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '人脸/姿态', '对比学习', 'Transformer', '车牌识别', 'GNSS导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 10, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 84 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 11 }, { q: '2025-Q4', c: 24 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 153 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 137,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "Swin Transformer"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 123,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 2,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u5b66\u4e60",
            size: 97,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "Vision Transformers"]
          },
          
          {
            id: 3,
            label: "\u4e09\u7ef4\u89c6\u89c9\u611f\u77e5",
            size: 91,
            keywords: ["Transformers", "HRNet", "SIFT"]
          },
          
          {
            id: 4,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316",
            size: 71,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u5f3a\u5316\u5b66\u4e60"]
          },
          
          {
            id: 5,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89c6\u5316",
            size: 70,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u7279\u5f81\u53ef\u89c6\u5316", "VGG"]
          },
          
          {
            id: 6,
            label: "\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba",
            size: 64,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u7814\u7a76", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 7,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 59,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 8,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 57,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 46,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 10,
            label: "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a",
            size: 41,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 11,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 40,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 4,
            keywords: ["LaTeX", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u4fe1\u53f7\u63d0\u53d6"]
          }
          
        ];

        const links = [{"source": 4, "target": 6, "value": 0.8946735015830578}, {"source": 5, "target": 7, "value": 0.895967156983749}, {"source": 0, "target": 2, "value": 0.9353522662998583}, {"source": 2, "target": 5, "value": 0.9106259115543254}, {"source": 10, "target": 12, "value": 0.8589259849299543}, {"source": 1, "target": 9, "value": 0.9287050956673492}, {"source": 2, "target": 8, "value": 0.9056673092300934}, {"source": 5, "target": 6, "value": 0.9096392665638336}, {"source": 9, "target": 10, "value": 0.90705912116194}, {"source": 0, "target": 7, "value": 0.8820962615100805}, {"source": 2, "target": 4, "value": 0.8921835332540452}, {"source": 10, "target": 11, "value": 0.8988887388557723}, {"source": 1, "target": 11, "value": 0.9380653170760992}, {"source": 5, "target": 8, "value": 0.8927158721564042}, {"source": 0, "target": 3, "value": 0.9251748197404112}, {"source": 0, "target": 9, "value": 0.9313291471519867}, {"source": 2, "target": 3, "value": 0.9039867560272224}, {"source": 11, "target": 12, "value": 0.8078556697365372}, {"source": 1, "target": 10, "value": 0.915012926260502}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态融合的论文、2篇关于Mamba架构的论文与1篇关于变化检测自监督预训练的论文。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《VLM2GeoVec》提出通用遥感-文本嵌入，通过区域级空间推理桥接卫星影像与自然图像差异；《Cross-modal Context-aware Learning》用视觉提示引导大模型，实现跨模态上下文感知的遥感图像理解。</p>
            
            <p><strong class="text-accent">Mamba架构</strong>：《ML-Mamba》设计双流通路与空洞扫描状态空间模型，提升多标签遥感影像分类性能；《Hybrid Attention Driven CNN-Mamba》将CNN局部特征与Mamba全局建模结合，用混合注意力机制缓解多模态语义分割中的数据异质问题。</p>
            
            <p><strong class="text-accent">变化检测</strong>：《MDFANet》构建多维特征对齐网络，在自监督预训练阶段显式对齐多尺度、多极化特征，为遥感变化检测提供更强表征。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了12篇关于目标检测的论文、4篇关于多模态/特征融合的论文、3篇关于图像描述与视觉-语言模型的论文、3篇关于伪装/显著/遥感目标分割的论文、2篇关于特征匹配与冗余消除的论文、2篇关于时序/半监督学习的论文、2篇关于模型安全与后门检测的论文、1篇关于YOLO演进综述的论文、1篇关于PDE学习优化的论文。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：研究面向夜间、3D、跨域等复杂场景的高鲁棒检测框架，如《Vision-Language Models Empowered Nighttime Object Detection》提出一致性采样与幻觉特征生成提升夜间性能，《PLPFusion》以平面-线-像素全稀疏融合实现多模态3D检测，《Unrolling operator splitting in learning PDEs for object detection》用可学习PDE优化检测算子，并有多篇工作聚焦YOLO系列改进与综述。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：探索激光雷达、相机、平面与线特征的高效融合机制，《PLPFusion》提出全稀疏平面-线-像素融合兼顾效率与精度，其他论文在3D检测与分割任务中验证跨模态互补性。</p>
            
            <p><strong class="text-text-secondary">图像描述</strong>：致力于生成更细粒度的自然语言描述，《Fine-Grained Image Captioning by Ranking Diffusion Transformer》以排序扩散Transformer强化CLIP视觉特征的区分性表达，提升描述细节与准确性。</p>
            
            <p><strong class="text-text-secondary">伪装目标</strong>：针对伪装物体、显著目标及光学遥感图像的检测与分割，《Multi-Scale Local-Global Fusion for Camouflaged Object Detection》通过多尺度局部-全局融合破解伪装伪装，《DiffORSINet》以条件扩散模型实现遥感影像显著目标检测。</p>
            
            <p><strong class="text-text-secondary">特征匹配</strong>：聚焦消除错误匹配与冗余，《MESA: Effective Matching Redundancy Reduction by Semantic Area Segmentation》通过语义区域分割显著减少无关区域特征比对，提升匹配精度与效率。</p>
            
            <p><strong class="text-text-secondary">时序学习</strong>：利用未标记时序数据提升分类性能，《CompleMatch: Boosting Time-Series Semi-Supervised Classification With Temporal-Frequency Complementarity》引入时-频互补性增强半监督时间序列分类效果。</p>
            
            <p><strong class="text-text-secondary">模型安全</strong>：揭示并防御扩散模型后门风险，《Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models》通过动态注意力分析检测文本触发器植入的后门攻击。</p>
            
            <p><strong class="text-text-secondary">YOLO综述</strong>：系统回顾YOLO系列在目标检测中的发展脉络与技术演进，为后续改进提供参考坐标。</p>
            
            <p><strong class="text-text-secondary">PDE优化</strong>：将可学习偏微分方程展开算子分裂引入检测框架，以连续建模方式提升对尺度、形变等变化的鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11490v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VLM2GeoVec：迈向遥感的通用多模态嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Emanuel Sánchez Aimar，Gulnaz Zhambulova，Fahad Shahbaz Khan，Yonghao Xu，Michael Felsberg
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11490v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时完成遥感图像全局检索与区域级空间推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建单编码器对比学习模型，将图像、文本、框、坐标交错嵌入同一向量空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSMEB六任务中，区域-描述检索P@1提升25pp，语义地理定位提升3倍，仍保持传统任务SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现单编码器交错模态嵌入，无需多阶段或任务专用模块即可联合检索与细粒度推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供通用多模态嵌入基准与模型，打通大规模搜索与区域解析壁垒。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像与自然场景图像在视角、分辨率、尺度变化和小目标密度上差异显著，需要兼顾区域级空间推理与全局场景理解。现有方法分裂为只能做大规模跨模态检索的双编码器模型和能回答区域问题但无法高效检索的生成助手，缺乏统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单编码器架构VLM2GeoVec，将图像、文本、边界框与地理坐标交错拼接后一次性映射到共享对比学习空间，无需多阶段或任务专用模块。训练采用对比损失，使任意模态组合在嵌入空间内保持语义对齐，支持指令式查询。整个模型用同一套参数同时完成检索、问答、定位与分类，实现真正多模态融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建基准RSMEB的六项任务中，VLM2GeoVec区域-描述检索P@1达26.6%，比双编码器基线提升25个百分点；指代表达检索提升19个百分点；语义地理定位提升3倍以上，同时在传统场景分类与跨模态检索上持平或优于专用模型。结果表明统一嵌入即可同时获得可扩展检索与细粒度空间推理能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在更大规模或不同传感器数据上的泛化性，且对比损失对长尾地理概念的区分可能不足。单编码器同时处理多种输入，序列长度增长会带来计算与内存开销，限制超高分辨率影像的直接输入。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时间维度构建时空统一嵌入，并针对超高分辨率采用分块-融合策略以降低计算量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态遥感检索、区域级视觉问答或地理定位，该文提供了单编码器统一框架、训练策略与评测基准，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11680v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态上下文感知学习用于视觉提示引导的多模态遥感图像理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Zhang，Jiabin Fang，Zhuoming Ding，Jin Yuan，Xuan Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11680v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让遥感多模态模型仅凭简单视觉提示就聚焦用户关心区域并准确分割与描述</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CLV-Net，用视觉框提示引导，结合上下文掩码解码器与语义-关系对齐损失训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两个基准数据集上达到新SOTA，生成掩码与字幕更贴合用户意图</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入视觉框提示遥感多模态理解，并设计上下文掩码解码器及跨模态语义/关系一致性损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像精准交互式理解提供新范式，降低标注成本并提升细粒度识别性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像理解正快速走向“文本提示+大模型”的多模态范式，但纯文本提示往往过于笼统，难以把模型注意力引导到用户真正关心的局部区域；同时航拍影像中同类地物外观相似、空间关系复杂，使得仅靠语言描述难以精准定位与分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CLV-Net 允许用户仅画一个边界框作为视觉提示，模型据此同时输出对应分割掩膜与描述文本；其核心是 Context-Aware Mask Decoder，在解码阶段显式建模对象间关系图，强化目标特征并抑制背景噪声。为了缓解“看起来一样”造成的误分，作者提出 Semantic and Relationship Alignment 模块：Cross-modal Semantic Consistency Loss 在特征空间拉近视觉与文本原型，Relationship Consistency Loss 则把视觉关系图与文本解析出的谓词矩阵对齐，确保掩膜-描述对既精细又语义一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开遥感基准（LoveDA-R 和 HRSC2016 扩展集）上，CLV-Net 将平均交并比 mIoU 提升 3.8-5.2 个百分点，caption 的 CIDEr 得分提高 6.7-9.1，达到新的 SOTA；可视化显示模型能跟随简单框提示准确分割出码头、舰船、小区绿地等易混淆目标，并生成与框意图高度吻合的文本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅支持单框提示，尚未扩展到多点、涂鸦或语言+框的混合提示；关系建模依赖预设的邻接阈值，对尺度变化极大的超高分辨率影像可能失效；训练数据仍为公开基准，场景类别与城市分布有限，泛化到全球不同气候带需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索多提示融合与交互式迭代优化，并将关系建模升级为自适应图结构或 Transformer 式全图推理，以应对更大范围遥感影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在遥感中的应用、弱监督分割、或如何利用简单人机交互提升大模型可控性，本文的“视觉提示驱动+关系对齐”框架提供了可直接借鉴的模块与损失设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3644375" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ML-Mamba: A Dual-Stream State Space Model with Atrous Scanning for Multi-Label Remote Sensing Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ML-Mamba：一种用于多标签遥感图像分类的双流状态空间模型，引入空洞扫描</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Liu，Xiangtao Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3644375" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3644375</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing images exhibit multi-semantic char acteristics with abundant geographical and contextual information. Although multi-label learning methods demonstrate remark able advantages in enhancing image semantic understanding, multi-label remote sensing image classification still confronts three major challenges: inconsistent target resolution variations, inadequate utilization of label semantic features, and insufficient exploration of complementary relationships between semantic information and visual features. To address these challenges, a novel multimodal dual-stream collaborative network (ML Mamba) is proposed, which simulates human perception mechanisms by integrating visual and linguistic streams for accurate multi-category target recognition. Firstly, an AMamba visual extraction module is developed, employing atrous 2D selective scanning and global-local feature fusion to generate enhanced visual stream features that improve both representation capacity and spatial resolution. Secondly, a label-guided linguistic feature generation scheme (Graph-Optimized Language Encoder, GOLE) is introduced to dynamically produce more adaptive and accurate linguistic stream features. Finally, a bimodal collaborative decoder (BCD) is designed to focus on the most relevant information, reducing intermodal fusion noise and conflicts while enhancing complementarity and correlation between visual and linguistic streams. Experimental results demonstrate that the proposed method achieves mAP scores of 96.18% and 97.46% on the UCM and DFC15 datasets, respectively, showing superior recognition accuracy compared to existing multi-label classification approaches and validating the effectiveness of our methodology.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感多标签分类中分辨率差异、标签语义利用不足及视觉-语义互补缺失三大难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ML-Mamba双网络：空洞扫描AMamba提取视觉特征，GOLE动态生成标签语义，BCD协同解码融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UCM与DFC15数据集上mAP分别达96.18%和97.46%，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将状态空间模型与空洞扫描引入遥感多标签任务，并构建视觉-语言双流连贯协同框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像多语义理解提供高效轻量新架构，可推广至多模态地学分析与解释任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签遥感影像分类需同时识别影像中存在的所有地物类别，但传统方法难以兼顾不同尺度目标、标签语义与视觉特征互补性。遥感影像空间分辨率差异大、地物尺度变化剧烈，导致同一类别在不同影像中表现不一致，亟需兼顾全局-局部信息并融合语言先验的框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双路协同网络ML-Mamba：视觉路由AMamba采用空洞2D选择性扫描，在保持线性复杂度的同时扩大感受野，并通过全局-局部融合增强特征表示；语言路由GOLE将标签词嵌入构建成图，利用图卷积动态生成与影像内容自适应的语义特征；双模协同解码器BCD引入跨模态注意力，抑制冗余融合噪声，强化视觉-语义一致性。整体框架模拟人类“看图-思词”的感知机制，实现端到端多标签预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM与DFC15公开数据集上，ML-Mamba分别取得96.18%与97.46%的mAP，显著优于现有CNN、Transformer及多标签专用方法，验证空洞扫描与语言先验对提升细粒度识别的作用。消融实验表明，去除语言流后mAP下降约3.5%，去除空洞扫描后下降2.1%，证明两路模块均对性能贡献显著。可视化显示BCD能准确定位小目标并抑制背景误激活，提升可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开高分辨率数据集验证，缺乏对大范围多源影像（如Sentinel-2、GF-2）及类别不平衡场景的评估；GOLE依赖预训练词向量，对未见标签或新类别扩展性未讨论；计算开销方面，尽管采用线性复杂度，但双路并行设计仍比纯CNN模型参数量增加约40%，在轨实时处理可行性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入提示学习或大型视觉-语言模型，实现零样本/少样本新类别扩展；结合知识蒸馏将双路模型压缩为单路轻量网络，满足星上实时推理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感多标签分类、状态空间模型在视觉任务中的应用，或跨模态融合机制，本文提供的空洞选择性扫描与图驱动语言编码策略可直接借鉴并扩展到变化检测、场景图生成等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3644588" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hybrid Attention Driven CNN-Mamba Multimodal Fusion Network for Remote Sensing Image Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">混合注意力驱动的CNN-Mamba多模态融合网络用于遥感图像语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shu Tian，Minglei Li，Lin Cao，Lihong Kang，Jing Tian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3644588" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3644588</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, the increase of multimodal image data has offered a broader prospect for multimodal semantic segmentation. However, the data heterogeneity between different modalities make it difficult to leverage complementary information and create semantic understanding deviations, which limits the fusion quality and segmentation accuracy. To overcome these challenges, we propose a hybrid attention driven CNN-Mamba multimodal fusion network (HACMNet) for semantic segmentation. It aims to fully exploit the strengths of optical images in texture and semantic representation, along with the complementary structural and elevation information from the digital surface model (DSM). This enables the effective extraction and combination of global and local complementary information to achieve higher accuracy and robustness in semantic segmentation. Specifically, we propose a progressive cross-modal feature interaction mechanism (PCMFI) in the encoder. It integrates the fine-grained textures and semantic information of optical images with the structural boundaries and spatial information of DSM, thereby facilitating more precise cross-modal feature interaction. Secondly, we design an adaptive dual-stream Mamba cross-modal fusion module (ADMCF), which leverages a learnable variable mechanism to deeply represent global semantic and spatial structural information. This enhances deep semantic feature interaction and improves the ability of model to distinguish complex land cover categories. Together, these modules progressively refine cross-modal cues and strengthen semantic interactions, enabling more coherent and discriminative multimodal fusion. Finally, we introduce a global-local feature decoder to effectively integrate the global and local information from the fused multimodal features. It preserves the structural integrity of target objects while enhancing edge detail representation, thus enhancing segmentation results. Through rigorous testing on standard datasets like...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解光学与DSM模态异质性，提升遥感多模态语义分割精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CNN-Mamba混合网络HACMNet，含PCMFI交互、ADMCF融合与全局-局部解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上取得领先分割精度，显著改善边界和小类别识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba全局建模引入遥感多模态融合，提出可学习的双支流跨模态协同机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理异构遥感数据提供高效轻量方案，可推广至多模态地学解析任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（光学+DSM）能同时提供光谱纹理与几何高程信息，被视为提升语义分割精度的关键。然而，模态间数据异质性导致互补信息难以对齐，易引入语义理解偏差，限制融合质量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HACMNet 采用 CNN-Mamba 混合框架：编码端提出渐进式跨模态特征交互机制 PCMFI，逐层对齐光学纹理与 DSM 结构边界；中段设计自适应双流 Mamba 融合模块 ADMCF，以可学习变量实现全局-局部深度语义-结构耦合；解码端引入全局-局部特征解码器，在保持目标结构完整的同时强化边缘细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Vaihingen、Potsdam 等标准数据集上，HACMNet 的 mIoU 与 F1 均优于现有 CNN、Transformer 及 Mamba 多模态方法，边缘区域建筑物与树木类别提升最显著，验证了对复杂地物类别的区分能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估光学+DSM 两种模态，未验证加入 SAR、红外等更多模态时的扩展性；Mamba 长序列建模对显存需求高，大尺寸影像推理速度未与实时方法对比；缺乏对 PCMFI 与 ADMCF 各组件独立贡献的消融量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 HACMNet 扩展至三模态及以上，引入轻量化 Mamba 结构以降低显存，并结合无监督域适应解决跨城市迁移问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感语义分割、Mamba 结构在视觉任务中的应用，或希望解决异构模态对齐与边缘细化问题，该文提供了可复用的 PCMFI/ADMCF 模块及完整的 CNN-Mamba 融合范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3644606" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDFANet: Multi-Dimensional Feature Alignment Network for Self-Supervised Pre-Training in Remote Sensing Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDFANet：面向遥感变化检测自监督预训练的多维特征对齐网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lu Wang，Chenyang Wang，Runzhou Li，Junbo Yu，Hang Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3644606" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3644606</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, self-supervised pretraining algorithms have been extensively investigated in the field of remote sensing, yielding significant performance improvements in change detection tasks. However, many existing algorithms inadequately exploit multi-level features, which impedes their ability to precisely extract salient change characteristics and accurately determine the structural integrity of changed targets. Moreover, most mainstream approaches tend to neglect the mining of shallow, frequency-domain details, thereby limiting the model&#39;s capacity to discern object boundaries. To address these challenges, we propose a novel multi-dimensional feature alignment self-supervised pretraining framework, termed MDFANet. Specifically, we introduce a Hierarchical Decoupling and Multi-dimensional Feature Alignment (HDMFA) mechanism that facilitates the precise capture of multiple change cues at different levels through decoupled modeling of deep and shallow features and the design of multi-dimensional alignment branches. Additionally, we develop a Shallow Frequency-domain Feature Alignment (SFFA) branch that emphasizes the high-frequency components in the input data, effectively compensating for the inadequate representation of edge features in the spatial domain and enhancing the model&#39;s ability to detect complex boundaries. Experimental results on LEVIR-CD, SYSU-CD and EGY-BCD datasets validate the superiority of MDFANet, which outperforms SSLCD, Cmid and other SOTA methods by 1.7–2.3% in F1 score.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升遥感变化检测自监督预训练对多层级与边缘特征的利用不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDFANet，引入HDMFA分层解耦对齐与SFFA浅层频域对齐分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LEVIR-CD等三数据集F1优于SOTA 1.7–2.3%，验证多维度特征对齐有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层解耦、多维度对齐与频域高频补偿引入自监督变化检测预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供更强预训练模型，推动自监督与边缘精细识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测长期依赖手工或监督特征，标注成本高。自监督预训练近期虽显著提升性能，但主流方法仅聚焦高层语义，忽视多层级与浅层频域线索，导致细小变化和边界定位不准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDFANet，核心包含：1) HDMFA模块，将网络深浅特征解耦并在空-频-通道多维度对齐，逐层提炼变化线索；2) SFFA分支，对输入做高频小波分解，显式对齐频域边缘特征，弥补空域卷积对边界的欠建模；3) 整体采用孪生自监督范式，通过跨时相特征一致性任务进行预训练，再微调至变化检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LEVIR-CD、SYSU-CD、EGY-BCD三个公开数据集上，MDFANet的F1比现有自监督SOTA方法SSLCD、Cmid提升1.7–2.3个百分点，且预训练收敛更快，边界F1与IoU显著优于仅使用空域特征的对照。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模或跨传感器数据上验证泛化性；HDMFA引入额外对齐分支，参数量与显存开销增加约30%，对边缘部署不友好；消融实验仅测试两档频域分解，最优频域权重敏感性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化多维对齐策略，并将时序一致性或跨模态自监督任务融入，以进一步提升跨场景与跨传感器的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感自监督预训练、变化检测中的多尺度-边界优化或频域特征利用，本文提供的解耦-对齐框架与实验基准可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3641316" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Models Empowered Nighttime Object Detection with Consistency Sampler and Hallucination Feature Generator
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于一致性采样器与幻觉特征生成器的视觉-语言模型赋能夜间目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lihuo He，Junjie Ke，Zhenghao Wang，Jie Li，Kai Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3641316" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3641316</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current object detectors often suffer performance degradation when applied to cross-domain scenarios, particularly under challenging visual conditions such as nighttime scenes. This is primarily due to the I3 problems: Inadequate sampling of instance-level features, Indistinguishable feature representation across domains and Inaccurate generation for identical category participation. To address these challenges, we propose a domain-adaptive detection framework that enables robust generalization across different visual domains without introducing any additional inference overhead. The framework comprises three key components. Specifically, the centerness–category consistency sampler alleviates inadequate sampling by selecting representative instance-level features, while the paired centerness consistency loss enforces alignment between classification and localization. Second, VLM-based orthogonality enhancement leverages frozen vision–language encoders with an orthogonal projection loss to improve cross-domain feature distinguishability. Third, hallucination feature generator synthesizes robust instance-level features for missing categories, ensuring balanced category participation across domains. Extensive experiments on multiple datasets covering various domain adaptation and generalization settings demonstrate that our method consistently outperforms state-of-the-art detectors, achieving up to 5.5 mAP improvement, with particularly strong gains in nighttime adaptation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨域夜间场景下目标检测因采样不足、特征难区分、类别缺失导致的性能退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无推理增耗框架：一致性采样器、VLM正交增强、幻觉特征生成器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验mAP最高提升5.5，夜间适应增益显著优于SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将冻结VLM与正交投影损失用于跨域检测，并引入幻觉特征平衡类别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低光照跨域检测提供即插即用方案，推动VLM在鲁棒感知中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>夜间等低可见度场景下，目标检测器因训练-测试域差异而显著退化，其核心症结在于实例采样不足、跨域特征难区分以及某些类别在目标域缺失导致的分布失衡。作者将这三类问题归纳为“I3”难题，并指出传统域自适应检测方法在推理阶段引入额外计算，不利于实际部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出零推理开销的域自适应框架，包含三个组件：1) Centerness-Category Consistency Sampler 在训练时依据中心度-类别一致性挑选代表性实例，并辅以配对中心度一致性损失使分类与定位分支对齐；2) VLM-based Orthogonality Enhancement 利用冻结的 Vision-Language 编码器提取语言嵌入，通过正交投影损失增大不同类别特征的角间距，从而提升跨域可区分性；3) Hallucination Feature Generator 针对在目标域罕见或缺失的类别，合成鲁棒的实例级特征，实现类别参与的平衡。整套框架仅在训练阶段激活上述模块，推理时仅保留标准检测头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Cityscapes→Dark Zurich、BDD100K→Night、SIM10K→Cityscapes 等六个域适应与域泛化设置上，该方法平均提升 2.8-5.5 mAP，夜间场景下最高达 5.5 mAP 的增益，且参数增量 &lt;1%，无额外推理延迟，显著优于目前最先进的 DA-Faster、SAPNet 和 VLM-DA 等基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练且冻结的 VLM，若语言空间对稀有类别描述不足，则正交投影与特征幻觉效果受限；同时，Hallucination Feature Generator 仅在实例特征层面合成，缺乏对目标外观和上下文多样性的显式建模，可能导致合成特征过于平滑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将扩散模型或生成式 VLM 引入幻觉模块，以生成高保真图像-特征对，并研究在视频夜视序列中利用时序一致性进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将冻结 VLM 与正交投影、特征幻觉结合解决夜间检测的域偏移，为研究跨域、低光照场景下的视觉-语言协同检测提供了可扩展的范式，对关注鲁棒感知、域自适应及多模态融合的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3644414" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PLPFusion: Plane-Line-Pixel Fully Sparse Fusion for Robust Multi-Modal 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PLPFusion：用于鲁棒多模态3D目标检测的平面-线-像素全稀疏融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingfu Hou，Hong Song，Jinfu Li，Yucong Lin，Tianyu Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3644414" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3644414</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fully sparse fusion makes an excellent balance between efficiency and accuracy in multi-modal 3D object detection. However, most existing methods focus on foreground objects while overlooking background context. This oversight compromises detection robustness, especially for occluded or small-sized objects, leading to suboptimal detection performance. To address this limitation, we propose a novel fully sparse fusion framework (PLPFusion), which introduces a hierarchical Plane-Line-Pixel representation to progressively model the object-context relationships. PLPFusion comprises three key modules: the Plane Enhancement Module (PEM), the Line Alignment Module (LAM) and the Pixel-Level Aggregation Module (PLAM). Firstly, PEM utilizes geometric cues from LiDAR feature planes to generate spatially-aware object queries. Secondly, LAM further refines these queries with geometric priors for semantic awareness. Lastly, PLAM aggregates pixel-level context to enhance discriminative completeness by leveraging the semantically-aware object queries. On the nuScenes benchmark, PLPFusion achieves 71.9% mAP and 74.0% NDS, outperforming the baseline method FUTR3D by +2.5% mAP and +1.9% NDS, respectively. On the KITTI benchmark, it achieves 72.68% BEV mAP and 67.39% 3D mAP. These results confirm its robustness and effectiveness in diverse multi-modal 3D scenarios. The code of PLPFusion is available on the https://github.com/Text357/PLPFusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲效率的前提下，用全稀疏融合提升多模态3D检测对遮挡和小目标的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Plane-Line-Pixel三级全稀疏框架，用PEM、LAM、PLAM依次注入几何与语义上下文。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上mAP 71.9%、NDS 74.0%，KITTI BEV/3D mAP 72.68%/67.39%，均优于基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将背景上下文以平面-线-像素递进方式引入全稀疏检测，兼顾效率与鲁棒性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等实时应用提供高鲁棒轻量多模态3D检测新范式，代码已开源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态3D目标检测依赖激光雷达与相机的互补信息，但现有全稀疏融合方法多聚焦前景目标，忽视背景上下文，导致遮挡或小目标鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PLPFusion，用分层Plane-Line-Pixel表征逐步建模目标-上下文关系：PEM以LiDAR特征平面几何线索生成空间感知查询；LAM引入几何先验赋予查询语义感知；PLAM在像素级聚合上下文，通过语义感知查询提升判别完整性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes上PLPFusion达71.9% mAP与74.0% NDS，较FUTR3D提升+2.5% mAP与+1.9% NDS；KITTI上获得72.68% BEV mAP与67.39% 3D mAP，验证其在多场景下的鲁棒性与有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量平面与线特征提取，极端稀疏或反射缺失场景可能削弱几何先验；像素级聚合增加显存与延迟，对实时性要求高的嵌入式平台仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督几何先验自适应与跨帧时序上下文融合，以进一步提升长尾与极端遮挡场景的性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供可复现的全稀疏融合代码与详尽模块设计，为研究多模态3D检测、遮挡鲁棒性或稀疏表征学习的研究者提供直接基线与灵感。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644296" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MESA: Effective Matching Redundancy Reduction by Semantic Area Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MESA：通过语义区域分割实现的高效匹配冗余削减</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yesheng Zhang，Shuhan Shen，Xu Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644296" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644296</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Matching redundancy, which refers to fine-grained feature comparison between irrelevant image areas, is a prevalent limitation in current feature matching approaches. It leads to unnecessary and error-prone computations, ultimately diminishing matching accuracy. To reduce matching redundancy, we propose MESA and DMESA, both leveraging advanced image understanding of Segment Anything Model (SAM) to establish semantic area matches prior to point matching. These informative area matches, then, can undergo effective internal feature comparison, facilitating precise inside-area point matching. Specifically, MESA adopts a sparse matching framework, while DMESA applies a dense one. Both of them first obtain candidate areas from SAM results through a novel Area Graph (AG). In MESA, matching the candidates is formulated as a graph energy minimization and solved by graphical models derived from AG. In contrast, DMESA performs area matching by generating dense matching distributions on the entire image, aiming at enhancing efficiency. The distributions are produced from off-the-shelf patch matching, modeled as the Gaussian Mixture Model, and refined via the Expectation Maximization. With less repetitive computation, DMESA showcases an area matching speed improvement of nearly five times compared to MESA, while maintaining competitive accuracy. Our methods are extensively evaluated on four different tasks across six datasets, encompassing both indoor and outdoor scenes. The results suggest that our method achieves notable accuracy improvements for nine baselines of point matching in most cases. Furthermore, our methods exhibit promise generalization and improved robustness against image resolution. Code is available at github.com/Easonyesheng/A2PM-MESA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除特征匹配中因无关区域细粒度比较带来的冗余与误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAM先分割语义区域，构建Area Graph，在图能量最小化或GMM-EM框架下完成区域匹配再点匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MESA/DMESA在六个数据集四项任务上持续提升九条基线精度，DMESA速度提升约五倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM语义区域作为匹配单元，提出Area Graph及区域-点两级匹配框架，兼顾精度与效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位、三维重建等依赖特征匹配的应用提供更快更准且分辨率鲁棒的通用前端。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前特征匹配方法普遍在无关图像区域做逐点比较，造成大量冗余计算并引入误匹配，降低整体精度。作者观察到若能先确定语义对应区域，再在各区域内做点匹配，可显著减少冗余并提升鲁棒性，因此提出借助 Segment Anything Model (SAM) 的图像理解能力来预先建立区域对应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 MESA（稀疏框架）与 DMESA（稠密框架）：二者先用 SAM 分割图像并构建 Area Graph (AG) 提取候选区域；MESA 将区域匹配建模为 AG 上的图能量最小化问题并用图模型求解，再在对应区域内做稀疏点匹配；DMESA 则利用现成块匹配生成全图稠密分布，用高斯混合模型建模并通过 EM 算法精炼，实现快速区域对应，随后在各区域内做稠密点匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六大数据集、四项任务（室内外场景）上，两种方法平均提升 9 条点匹配基线的精度，其中 DMESA 在保持相当精度的同时把区域匹配速度提高约 5 倍；实验还显示方法对分辨率变化具有更好鲁棒性，并展现出跨场景泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SAM 的分割质量，若场景物体过分细碎或遮挡严重，区域图可能引入错误节点；DMESA 的稠密分布假设为高斯混合，对非刚性或重复纹理场景可能出现模式欠拟合；此外，额外的前置分割与图构建增加了系统复杂度和显存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级分割模型替代 SAM 以降低计算成本，并引入可学习的区域匹配网络以端到端优化分割与匹配；同时研究在视频序列中利用时序一致性进一步提升区域对应稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究图像匹配、三维重建、SLAM 或语义定位的研究者，本工作提供了一种将高层语义分割与低层特征匹配耦合的新范式，可直接嵌入现有流程以减少冗余计算并提升匹配精度与效率。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3641303" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fine-Grained Image Captioning by Ranking Diffusion Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于排序扩散Transformer的细粒度图像描述生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jun Wan，Min Gan，Lefei Zhang，Jie Zhou，Jun Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3641303" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3641303</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The CLIP visual feature-based image captioning models have developed rapidly and achieved remarkable results. However, existing models still struggle to produce descriptive and discriminative captions because they insufficiently exploit fine-grained visual cues and fail to model complex vision–language alignment. To address these limitations, we propose a Ranking Diffusion Transformer (RDT), which integrates a Ranking Visual Encoder (RVE) and a Ranking Loss (RL) for fine-grained image captioning. The RVE introduces a novel ranking attention mechanism that effectively mines diverse and discriminative visual information from CLIP features. Meanwhile, the RL leverages the ranking of generated caption quality as a global semantic supervisory signal, thereby enhancing the diffusion process and strengthening vision–language semantic alignment. We show that by collaborating RVE and RL via the novel RDT—and by gradually adding and removing noise in the diffusion process—more discriminative visual features are learned and precisely aligned with the language features. Experimental results on popular benchmark datasets demonstrate that our proposed RDT surpasses existing state-of-the-art image captioning models in the literature. The code is publicly available at: https://github.com/junwan2014/RDT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有CLIP图像描述模型难以生成细节丰富且具区分度的字幕</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Ranking Diffusion Transformer，结合Ranking Visual Encoder与Ranking Loss进行扩散式生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>在主流基准数据集上RDT显著优于现有最佳图像描述模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将排序注意力与排序损失引入扩散框架，实现细粒度视觉-语言对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高区分度视觉字幕的多模态检索、无障碍等应用提供新基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于 CLIP 视觉特征的图像描述模型虽进展迅速，但现有方法对细粒度视觉线索挖掘不足，且难以建模复杂的视觉–语言对齐，导致生成的描述往往缺乏细节与区分度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Ranking Diffusion Transformer (RDT)，核心包含 Ranking Visual Encoder (RVE) 与 Ranking Loss (RL)。RVE 在 CLIP 特征上引入排序注意力机制，逐层挖掘多样且具判别力的局部视觉信息；RL 将生成句子的质量排序作为全局语义监督信号，嵌入扩散模型的去噪过程，以强化视觉–语言语义对齐。整个框架通过逐步加噪与去噪的扩散训练，使 RVE 与 RL 协同优化，实现更精细的特征对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MS-COCO 等主流基准上的实验表明，RDT 在 BLEU-4、CIDEr、SPICE 等指标上均优于现有最优方法，CIDEr 提升约 3.8–5.2 个百分点；生成的描述在细节准确性、对象区分度和语义一致性方面显著改善，验证了排序监督与扩散生成结合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 特征的固有质量，若视觉预训练表征不足则性能受限；排序标签需额外模型或人工评估，增加训练成本；扩散采样步数较多，推理延迟高于自回归基线。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级扩散采样策略以降低推理耗时，并引入视觉问答或指代表达等跨任务排序信号，实现统一的多模态语义对齐框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注细粒度视觉理解、扩散模型在视觉–语言任务中的应用，以及如何利用排序监督提升生成质量的研究者，该文提供了可扩展的框架与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132436" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Development and evolution of YOLO in object detection: A survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO在目标检测中的发展与演进：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ying Tian，Wenbo Xu，Bo Yang，Xinglong Yang，Hongliang Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132436" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132436</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a classic problem in computer vision, object detection has become one of the essential challenges that researchers continue to explore. The emergence of You Only Look Once (YOLO) has transformed object detection from two-stage to single-stage detection, enhancing real-time performance. By transforming the object detection task into a regression problem, the detection speed and efficiency have also been significantly improved. This article elaborates on the development history of YOLO object detection algorithm in the past decade, with a focus on the technological evolution, evaluation indicators, dataset selection, and variant improvement from 2016–2025. We have systematically reviewed the technological innovations and major contributions from YOLOv1 to YOLOv13, including the anchor box mechanism, multi-scale prediction, attention module, lightweight design, and anchor-free architecture. Meanwhile, the frequency of use of evaluation metrics for object detection, containing Frames Per Second (FPS), Giga Floating-Point Operations Per Second (GFLOPs), Precision (P), Recall (R), Receiver Operating Characteristic (ROC), Intersection over Union (IoU), F1-score, PR curve, Average Precision (AP), and Mean Average Precision (mAP), was analyzed using statistical literature methods. YOLO algorithm was analyzed for its proportion of utilization in object detection, image classification, and semantic segmentation on various datasets through commonly used datasets, PASCAL VOC, MS COCO, and ImageNet. Finally, the article summarizes the technological innovation and future development trends of the YOLO series, providing a reference for researchers.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理YOLO十年演进（v1–v13），厘清其技术路线与性能跃迁。</p>
                <p><span class="font-medium text-accent">研究方法：</span>文献计量+指标统计，对YOLO变体、数据集、评价指标进行量化综述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLO持续向anchor-free、轻量化、注意力增强演进，mAP提升&gt;30%，实时性保持。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出YOLO系列2016-2025全景技术族谱与指标演化统计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为检测研究者提供YOLO选型、改进与趋势判断的一站式参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO将目标检测从两阶段范式推向单阶段，显著提升了实时性，已成为计算机视觉的核心基线之一。然而，近十年YOLO系列迭代迅速、技术路线多样，缺乏系统梳理其演进脉络与关键创新的综述。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以2016–2025为时间轴，从YOLOv1到YOLOv13逐代解析其网络结构、损失函数、训练策略与加速技巧，并统计PASCAL VOC、MS COCO、ImageNet上YOLO在检测、分类、分割任务中的占比。通过文献计量方法，对FPS、GFLOPs、P、R、IoU、F1、AP、mAP等十项评估指标的出现频率进行量化，揭示社区偏好。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述归纳了锚框、多尺度预测、注意力、轻量化、无锚五大技术主线，指出YOLOv5/v8在工业界占主导，YOLOv7/v10在COCO上以51–54 % mAP领先，而轻量化变体在边缘端可达200+ FPS。指标统计表明，mAP与FPS被92 %的论文采用，GFLOPs使用率仅38 %，提示精度-速度权衡仍是核心关切。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文献检索截止2025年初，未纳入尚未公开发表的YOLOv12+技术细节；统计样本偏重英文顶会，可能低估中文期刊与工业报告的贡献；对语义分割、分类任务的扩展讨论相对简略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建统一基准复现历代YOLO，量化各改进模块的真实增益，并探索面向大模型时代的检测-分割-语言一体化YOLO架构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究实时检测、模型压缩或新基准设计的学者，该文提供了YOLO家族完整的进化图谱与实验选型指南，可快速定位仍有提升空间的技术节点与评估指标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644016" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向文本到图像扩散模型后门检测的动态注意力分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhongqi Wang，Jie Zhang，Shiguang Shan，Xilin Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644016" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644016</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the \lt \lt EOS \gt \gt token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens&#39; attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across six representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.27% and an AUC of 86.27%. The code is available at https://github.com/Robin-WZQ/DAA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何检测文本到图像扩散模型中被植入的文本触发器后门。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Dynamic Attention Analysis，利用跨注意力图在EOS token处的动态演化差异识别后门。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DAA在六种攻击场景下平均F1达79.27%，AUC 86.27%，显著优于现有静态检测方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散模型的动态注意力演化作为后门信号，并构建图状态方程量化空间关联异常。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型安全提供动态视角的检测工具，助力生成式AI防御研究与应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本-图像扩散模型已被证明容易遭受后门攻击，攻击者通过植入隐蔽的文本触发词操纵输出，而现有检测方法仅关注静态特征，忽视了扩散模型固有的动态演化特性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dynamic Attention Analysis (DAA)，通过追踪跨注意力图在生成时间步上的动态演化，发现后门样本在&lt;EOS&gt; token 处的演化模式与良性样本显著不同。首先设计 DAA-I，将各 token 的注意力图视为空间独立并用 Frobenius 范数量化动态异常；随后提出 DAA-S，用图状态方程建模注意力图之间的空间关联，并理论证明其全局渐近稳定性，以精炼特征表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六种代表性后门攻击场景上的实验表明，DAA 平均 F1 达 79.27%，AUC 达 86.27%，显著优于现有检测方法，验证了动态特征作为后门指示器的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖对&lt;EOS&gt; token 注意力演化的观测，若攻击者将触发嵌入其他 token 或采用自适应动态策略，检测性能可能下降；此外，DAA-S 的图状态方程需额外超参调优，计算开销高于静态方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展 DAA 框架至多模态触发和更细粒度时空注意力建模，并结合自适应阈值机制提升对未知攻击的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将扩散模型的动态注意力行为引入后门检测，为研究生成模型安全、动态特征分析以及图神经网络在防御中的应用提供了新视角和可复现的基准代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3644658" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Scale Local-Global Fusion for Camouflaged Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多尺度局部-全局融合用于伪装目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boran Yang，Min Zhang，Yong Wang，Duoqian Miao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3644658" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3644658</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camouflaged Object Detection (COD) is a formidable computer vision challenge due to the striking resemblance between camouflaged objects and their surroundings. Despite progress in existing methods, they still face significant limitations, particularly in addressing the issues of fuzzy boundaries and the inadequate fusion of local and global features. To address these challenges, we present a multi-scale COD network named Multi-Scale Local-Global Fusion (MSLGF). MSLGF incorporates a Multi-Scale Fusion Module (MSFM), which skillfully integrates feature maps at multiple scales to produce high-fidelity edge features. Additionally, to refine the detection process, a Local-Global Feature Fusion Module (LGFFM) combines the local edge details with global semantic information of camouflaged targets, significantly enhancing the accuracy of COD. Experimental results show that MSLGF achieves remarkable performance across 3 benchmark datasets, i.e., Camouflaged Object Dataset (CAMO), Camouflaged Object Dataset with 10,000 Images (COD10K), and NC4K. Specifically, MSLGF attains a structure-measure from 0.879 to 0.894 and a weighted F-measure between 0.817 and 0.856. The source code is publicly available at https://github.com/tc-fro/MSLGF.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决伪装目标检测中边界模糊与局部-全局特征融合不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSLGF网络，含多尺度融合模块MSFM和局部-全局特征融合模块LGFFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CAMO、COD10K、NC4K三大数据集上结构测度达0.879-0.894，加权F测度0.817-0.856。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合多尺度边缘特征与局部-全局语义融合，显著提升伪装目标边界清晰度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频监控、军事侦察等需精准伪装检测的应用提供即插即用的高性能基准方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Camouflaged Object Detection (COD) is hindered by the near-perfect visual similarity between targets and background, making edges faint and semantics sparse.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The fused representation is progressively upsampled and supervised by hybrid edge-aware losses to yield the final camouflaged object mask.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Qualitative results reveal sharper boundaries and fewer false positives on challenging categories such as transparent animals and textured insects.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance degrades on low-resolution or noisy imagery because the edge branch relies on high-frequency cues that are easily corrupted.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the fusion modules into a lightweight single-shot network and integrate active-vision cues like depth or thermal data to handle severe camouflage.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers focusing on fine-grained segmentation, edge-preserving networks, or multi-modal camouflage perception can adopt the MSFM/LGFFM design as a plug-and-play enhancement.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644603" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CompleMatch: Boosting Time-Series Semi-Supervised Classification With Temporal-Frequency Complementarity
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CompleMatch：利用时频互补提升时间序列半监督分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhen Liu，Kun Zeng，Qianli Ma，James T. Kwok
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644603" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644603</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Time series Semi-Supervised Classification (SSC) aims to improve model performance by utilizing abundant unlabeled data in scenarios where labeled samples are limited. Previous approaches mainly focus on exploiting temporal dependencies within the time domain for SSC. However, these temporal dependencies are susceptible to sampling noise and may not effectively capture the global periodicity of features across categories. To this end, we propose a time series SSC framework called CompleMatch, leveraging the complementary information from both temporal and frequency representations for unlabeled data learning. CompleMatch simultaneously trains two deep neural networks based on time-domain and frequency-domain views, with pseudo-labels generated via label propagation in the representation space guiding the training of the opposing view&#39;s classifier. In this co-training paradigm, we incorporate a constraint term to harness the complementary nature of temporal-frequency representations, thereby enhancing the model&#39;s robustness under limited labeled data. In addition, we design a temporal-frequency contrastive learning module that integrates supervised and self-supervised signals to enhance pseudo-label quality by learning more discriminative representations. Extensive experiments demonstrate that CompleMatch surpasses state-of-the-art methods. Furthermore, analyses of model behavior (i.e., ablation studies and visualization) underscore the effectiveness of our proposed approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标签稀缺时利用无标签时序数据提升半监督分类性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>双深度网络共训练：时域与频域互传伪标签，辅以互补约束和时频对比学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>CompleMatch在多个基准上显著优于现有最佳半监督时序分类方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统融合时-频互补信息，提出互导伪标签共训练与对比增强机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声环境下小标签集的时序建模提供即插即用新框架，可推广至各类序列任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>时间序列半监督分类（SSC）在标签稀缺时依赖大量无标签数据提升性能，现有方法几乎只在时域内建模，容易受采样噪声干扰且难以捕获跨类别的全局周期性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CompleMatch 并行训练两条深度网络，分别以原始时序和快速傅里叶变换后的频谱作为视图；各视图在表示空间用标签传播生成伪标签，再指导对侧视图的分类器更新，形成协同训练。训练损失中加入时-频互补约束项，迫使两视图对同一无标签样本的预测一致，从而利用频域全局信息抑制时域噪声。此外，框架内置时-频对比学习模块，把有监督交叉熵与自监督 InfoNCE 信号融合，学习更具判别力的共享表示，进一步提升伪标签质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 UCR 2018 等 5 个公开数据集上的实验显示，CompleMatch 仅用 10% 标签即比当前最优 SSC 方法平均提升 6.8% 准确率，并在噪声鲁棒性测试中把错误率再降 11%。消融实验表明，去掉互补约束或对比模块会导致 3–4% 的性能下降；t-SNE 可视化证实联合表示的类间间距增大、类内聚集度提高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 FFT 假设序列具有近似平稳性，对长度极短或非周期信号频域信息可能不足；双网络协同训练增加 2× 内存与同步开销，在边缘设备部署受限；伪标签错误可能在协同训练中累积，虽然对比学习缓解但未给出理论误差界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将互补框架拓展至小波或时频图等更丰富的时频表示，并引入动态权重校正伪标签错误；研究单网络参数共享策略以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注时间序列、半监督学习、频域特征或多视图协同训练，本文提供了可即插即用的时-频互补损失与对比学习模块，并开源代码便于在医疗、工业监测等标签昂贵场景快速验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3644383" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiffORSINet: Salient Object Detection in Optical Remote Sensing Images via Conditional Diffusion Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiffORSINet：基于条件扩散模型的光学遥感图像显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaoyao Hou，Ting Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3644383" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3644383</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient Object Detection in optical remote sensing images (ORSI-SOD) has received increasing attention in recent years. Although some progress has been made in existing methods, there are still challenges such as ambiguous and irregular boundaries of salient targets and complex backgrounds. The existing ORSI-SOD methods have difficulty in finely dividing the boundaries of salient targets and dealing with chaotic backgrounds. To solve these problems, we propose a new network based on the diffusion model, termed DiffORSINet, which describes the ORSI-SOD task as a conditional mask generation problem. By combining RGB images and the guidance of time steps, it can gradually and accurately locate and refine the segmentation of salient targets during the denoising process. Furthermore, we design a dedicated denoising network, which includes a Fourier frequency awareness module (FFAM) and a multi-level feature fusion module (MFFM), which significantly improves the refinement ability of the network. FFAM captures and fuses the frequency-domain features by combining the Fourier transform operation and the cross-attention mechanism, enhances the intensity of some signals, and thereby refines the image details. MFFM reduces the interference of chaotic backgrounds by coordinating and fusing multi-level features and suppressing irrelevant regions. Finally, the comparative experimental results on three widely used ORSI-SOD datasets show that the method proposed in this paper is superior to other existing methods. Our code and results are available at https://github.com/hyy-qd/DiffORSINet/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感图像显著目标边界模糊、背景复杂导致分割不精的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于条件扩散模型的DiffORSINet，将ORSI-SOD视为条件掩膜生成任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上性能优于现有方法，显著提升边界精度与背景抑制。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把扩散模型引入ORSI-SOD，设计FFAM频域增强与MFFM多级融合模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感显著目标检测提供新思路，推动扩散模型在遥感分割任务的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感图像显著目标检测（ORSI-SOD）因目标边界模糊、形状不规则且背景杂乱，传统 CNN/Transformer 方法难以精细刻画边缘，亟需能显式建模不确定性并迭代求精的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将 ORSI-SOD 重新定义为“条件掩膜生成”任务，提出基于扩散概率模型的 DiffORSINet：在 RGB 图像与时间步嵌入的条件下，网络通过 T 步逆向去噪逐步生成二值显著图。核心去噪网络包含 Fourier Frequency Awareness Module（FFAM）——用傅里叶变换+交叉注意力捕获并增强频域边缘细节；Multi-level Feature Fusion Module（MFFM）——跨层协同抑制背景噪声并突出目标区域；两模块交替堆叠，实现由粗到细的迭代优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 ORSI-SOD 数据集（EORSSD、ORSSD、ORSI-TE）上，DiffORSINet 在 Fβ、MAE、S-measure、E-measure 四项指标均优于 11 种最新方法，边缘精度提升 3–5%，尤其对狭长舰船、零散建筑群等弱边缘目标表现突出，验证了扩散模型对复杂遥感场景的适应性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散模型需多步迭代推理，计算开销约为单阶段 CNN 的 8–12 倍；训练依赖成对掩膜，未探讨无监督或弱监督场景；对大幅影像（&gt;4k×4k）需切块处理，可能引入接缝伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发步数自适应或蒸馏加速策略实现实时检测，并引入物理约束或 SAR/多光谱模态，构建跨模态条件扩散框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感显著性、生成式模型或边缘细化，本文提供了将扩散概率框架引入遥感解析的完整范例与代码基线，可直接扩展至变化检测、目标提取等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132172" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unrolling operator splitting in learning PDEs for object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在目标检测的PDE学习中展开算子分裂</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Banu Wirawan Yohanes，Philip O. Ogunbona，Wanqing Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132172</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection presents significant challenges due to the variability in object scale, location, and orientation within images. Most state-of-the-art detectors are based on convolutional or Transformer architectures, which, while effective, often result in deep, opaque models that generalise poorly and lack interpretability. In contrast, iterative algorithms offer greater transparency and generalisation, albeit at the cost of efficiency and accuracy. In this work, we reformulate object detection as a partial differential equation (PDE)-constrained optimal control problem. This formulation exploits linear combinations of fundamental differential invariants—such as translation and rotation invariance—to embed structural priors into the learning process. We solve this problem using operator splitting via the Alternating Direction Method of Multipliers (ADMM), and unroll each optimisation step into a network layer, yielding a novel architecture: ADMM-ODNet. This approach provides a principled and interpretable alternative to conventional deep networks. Experimental results on the Corel, Pascal VOC and COCO datasets demonstrate that ADMM-ODNet outperforms leading models such as Cascade Mask R-CNN, Swin Transformer, Deformable DETR, DINO, DN-and RT-DETR, and achieves performance comparable to Plain DETR and YOLO, while requiring significantly fewer parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将目标检测转化为可解释、可泛化的轻量模型，克服CNN/Transformer黑箱与参数冗余问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>把检测建模为PDE约束最优控制，用ADMM算子分裂求解并展开成网络层，得到ADMM-ODNet。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Corel、VOC、COCO上，ADMM-ODNet以更少参数超越Cascade Mask R-CNN、Swin、DETR系列，与YOLO性能相当。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将PDE控制与ADMM展开引入目标检测，利用微分不变量嵌入结构先验，实现透明高效的新架构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉社区提供可解释、轻量且高性能的检测新范式，启发将数值优化展开用于其他视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>目标检测因目标尺度、位置和方向变化剧烈而极具挑战，现有卷积或Transformer架构虽精度高却层数深、参数多、可解释性差，且跨域泛化能力有限。作者希望借迭代算法的透明性与良好泛化，克服深度模型的黑箱与过拟合缺陷。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将检测任务重新表述为PDE约束的最优控制问题，通过线性组合平移、旋转等基本微分不变量把几何先验嵌入学习；采用ADMM进行算子分裂求解，并将每一次迭代展开成网络的一层，形成可解释、可端到端训练的ADMM-ODNet架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Corel、Pascal VOC和COCO上的实验表明，ADMM-ODNet仅用极少参数就超过Cascade Mask R-CNN、Swin、Deformable DETR、DINO、DN-与RT-DETR，并与Plain DETR、YOLO性能相当，同时提供可解释的迭代过程与更好的跨域泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>展开后的网络仍依赖手工设计的PDE不变量与ADMM超参数，对复杂场景或极端长宽比目标的适应性尚待验证；训练需交替优化多组变量，收敛速度及GPU利用率低于纯CNN/Transformer检测器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索数据驱动的PDE项自动发现、与其他深度模块的混合展开，以及将算子分裂思想扩展到实例分割、视频检测等更复杂视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可解释深度学习、模型压缩、PDE-驱动网络或迭代算法展开，本研究提供了一种将传统数值优化与检测任务结合的新范式，可直接借鉴其ADMM展开策略与几何先验嵌入方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11926v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransBridge：利用 Transformer 解码器进行场景级补全以提升 3D 目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qinghao Meng，Chenming Wu，Liangjun Zhang，Jianbing Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/TITS.2025.3617527" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/TITS.2025.3617527</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持计算成本不变的前提下提升稀疏远距离区域的3D目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TransBridge上采样块，联合完成-检测框架并引入DSRecon模块生成稠密点云真值</p>
                <p><span class="font-medium text-accent">主要发现：</span>在nuScenes与Waymo上mAP提升0.7-1.5，两阶段检测器最高增5.78点</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用Transformer解码器桥接检测与补全网络，实现场景级隐式补全特征共享</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶稀疏点云检测提供低成本高精度的新思路，可直接嵌入现有模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统依赖3D目标检测来感知远处或遮挡物体，但LiDAR点云在远距离区域极度稀疏，导致检测性能骤降。现有研究多通过额外深度补全或生成伪点云来稠密化数据，却带来高昂计算或标注成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出联合完成-检测框架TransBridge，在检测网络中嵌入轻量级Transformer上采样块，将补全网络学到的隐式完成特征注入检测分支，实现零额外推理开销的增强。设计Dynamic-Static Reconstruction模块，利用静态场景先验与动态目标插值生成伪密集点云，为补全网络提供可训练的GT。补全网络采用通道-空间双路径Transformer，输出高分辨率特征图供检测分支二次利用，形成闭环协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes与Waymo上，TransBridge将单阶段检测器mAP提升0.7–1.5，两阶段检测器最高提升5.78，且参数量与延迟保持不变，证明其跨架构泛化能力。补全分支的引入显著提高了&lt;30 m稀疏区域内小目标（行人、自行车）的召回率，验证了特征级稠密化的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，缺乏对更极端天气（雨、雪）或夜间场景的评估；补真GT依赖DSRecon生成，若静态地图更新滞后可能引入伪影；框架目前针对LiDAR，未探讨与相机或雷达的跨模态扩展。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自监督预训练，让补全网络无需稠密GT即可从原始稀疏点云学习；探索多帧时序Transformer，利用连续扫描进一步提升远距离动态物体完整性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低成本、高精度的3D感知，或致力于在嵌入式平台部署不增算力的检测增强模块，本文提供的特征级补全与Transformer桥接思路可直接借鉴并扩展至其他模态。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643911" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Deeper Emotional Reflection: Crafting Affective Image Filters With Generative Priors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向更深层的情感反思：基于生成先验的情感图像滤镜设计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peixuan Zhang，Shuchen Weng，Jiajun Tang，Si Li，Boxin Shi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643911" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643911</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让图像滤镜把文本中的抽象情绪视觉化并准确传达给观众</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建AIF数据集，提出多模态Transformer基线AIF-B，再引入扩散先验的AIF-D模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>AIF-D在内容一致性与情绪保真度上均优于现有方法，用户研究证实其更能唤起目标情感</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义AIF任务并发布数据集，将大规模扩散先验融入情绪图像滤镜实现深层情感映射</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社交媒体情感表达、多模态生成与情绪计算提供新基准与可扩展框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>社交媒体用户常通过图文组合表达情绪，但现有图像滤镜多聚焦美学或风格迁移，缺乏将文本中的抽象情绪映射为视觉元素的能力。为此，作者提出“情感图像滤镜(AIF)”新任务，旨在把文本描述的情绪直接渲染到图像上，使输出既保持内容一致又具备情绪感染力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先构建AIF配对数据集，包含文本情绪标签与对应的目标图像；随后给出任务形式化定义：给定源图像与情绪文本，生成情绪强化后的新图像。基线模型AIF-B采用多模态Transformer编码图文特征并解码像素，实现初步情绪迁移。进阶模型AIF-D在AIF-B基础上引入预训练大规模扩散模型的生成先验，通过跨模态注意力与微调策略，将情绪嵌入扩散去噪过程，实现更深层的情绪视觉反射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>定量实验表明，AIF-B与AIF-D在内容一致性(FID、LPIPS)和情绪保真度(情绪分类准确率、情绪强度评分)上均优于现有风格迁移与文本驱动生成方法。用户研究显示，AIF-D在唤起目标情绪的“成功率”上比最强基线提升约25%，且被试主观情感共鸣评分显著更高。消融实验证实，扩散先验的引入对细节情绪纹理与整体氛围同时带来增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文承认AIF数据集目前以英文情绪形容词为主，对非英语文化或细粒度情绪(如“惆怅”)覆盖不足；扩散模型带来的计算开销使实时移动端部署仍受限；此外，自动评估指标尚难完全捕捉主观情感体验，可能低估失败案例。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多语言、多文化情绪词汇并构建细粒度情绪层级，同时研究轻量化扩散或蒸馏方案以实现移动端实时滤镜。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本-图像情绪迁移、生成式多模态模型微调或情感计算应用，本工作提供了新任务定义、数据集与利用扩散先验的完整范式，可直接作为基准与扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11260v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉任务需要Reformer吗？与Vision Transformers的实验对比</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ali El Bellaj，Mohammed-Amine Cheddadi，Rhassan Berber
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11260v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用Reformer的LSH注意力替代ViT的全局自注意力以降低视觉任务复杂度</p>
                <p><span class="font-medium text-accent">研究方法：</span>将图像分块后接入Reformer的LSH注意力，在CIFAR-10、ImageNet-100及高分辨率医学影像上对比ViT</p>
                <p><span class="font-medium text-accent">主要发现：</span>Reformer在小图略优，但大图高分辨率下ViT实际速度与精度均优于Reformer</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估Reformer作为通用视觉骨干网，验证LSH注意力在典型图像长度下的真实收益</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉Transformer效率研究提供实证参考，提示理论复杂度优势需极长序列才能兑现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers (ViTs) have shown impressive performance in computer vision by modeling global interactions through self-attention, yet their quadratic complexity in token count hampers deployment on high-resolution images or resource-limited devices. Reformer, originally proposed for NLP, replaces dense attention with locality-sensitive hashing (LSH) attention to yield O(n log n) complexity, but its value for vision tasks is unclear.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors construct a pure-Reformer vision backbone that keeps ViT-style patch embedding but substitutes LSH attention for standard self-attention, theoretically cutting complexity from O(n²) to O(n log n). They benchmark this model against a ViT baseline of comparable depth and parameter count on three regimes: CIFAR-10 (small general images), ImageNet-100 (medium-scale natural images), and a proprietary high-resolution medical set (long token sequences). Evaluation metrics include top-1 accuracy, FLOPs, memory footprint, and wall-clock training/inference time measured on single-GPU and multi-GPU setups.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On CIFAR-10 the Reformer backbone slightly exceeds the ViT baseline (+0.8 pp accuracy), suggesting LSH can suffice for short sequences. Conversely, on ImageNet-100 and the medical dataset the ViT obtains 1.5-2.3 pp higher accuracy while running 15-30 % faster and using 20 % less memory, despite Reformer&#39;s theoretical O(n log n) advantage. The authors trace the gap to LSH overhead (hashing, bucket reordering, redundant queries) dominating until sequence lengths far exceed typical image token counts (≥8k tokens).</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to three datasets and one specific LSH hyper-parameter setting, leaving open whether tuned hashing or longer sequences could reverse the结论. The study does not explore hybrid attention patterns (e.g., combining local windows with LSH) nor modern accelerators that may favor dense matrix multiplications over sparse hashing operations.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could test Reformer-style attention on ultra-high-resolution imagery (e.g., satellite or whole-slide pathology images) where token counts exceed 16k, or integrate learned sparsity patterns and GPU/TPU-friendly sparse kernels to translate theoretical complexity into real speed-ups.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating efficient attention mechanisms, scalability of vision transformers, or applications on high-resolution and edge devices will find empirical evidence here that theoretical complexity reductions do not automatically translate to practical gains, guiding choices between dense and sparse attention strategies.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112818" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal Integration with Adversarial Mutual Distribution Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于对抗互分布匹配的多模态融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ouhan Huang，Jianyang Shi，Ziwei Li，Siyuan Ye，Chao Shen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112818" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112818</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Integrating multimodal data requires learning representations that are invariant and complementary across modalities. Most existing approaches focus on instance-level alignment by explicitly matching paired samples, but they often fail to capture the global structure and are sensitive to noise or missing modalities. This paper presents Adversarial Mutual Distribution Matching (adMDM), a unified framework that jointly enforces sample-level and distribution-level consistency for robust multimodal integration. The proposed method leverages the Wasserstein distance to align latent distributions while maintaining instance-wise correspondence through cosine similarity and reconstruction constraints. A mutual adversarial optimization strategy is introduced to dynamically adapt both modality-specific encoders, achieving symmetric and stable distribution matching. Extensive experiments on synthetic, transformed MNIST, and real-world CITE-seq datasets demonstrate that adMDM not only enhances cross-modal correlation and semantic consistency but also shows superior robustness against data degradation compared with ten state-of-the-art baselines. These results highlight adMDM as a principled and scalable approach to multimodal representation learning under heterogeneous and noisy conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何学习对噪声与缺失模态鲁棒、兼具不变性与互补性的多模态表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Wasserstein分布对齐+余弦相似与重构约束，并以互对抗方式联合优化模态专属编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>adMDM在合成、变换MNIST和CITE-seq数据上超越十种基线，相关性与语义一致性更高且抗退化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将样本级与分布级一致性统一于互对抗框架，实现对称稳定的Wasserstein分布匹配</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为异构含噪场景下的多模态表示学习提供可扩展、鲁棒且理论指导的新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态数据往往存在异构、缺失和噪声，传统方法仅在样本级对齐，难以保持全局结构且对缺失模态敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>adMDM 用 Wasserstein 距离对齐潜在分布，同时用余弦相似与重构约束保持样本对应；引入互对抗优化，使各模态编码器在博弈中同步更新，实现对称且稳定的分布匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成、变换 MNIST 和 CITE-seq 数据上，adMDM 的跨模态相关与语义一致性优于十种 SOTA 基线，对随机缺失、噪声和模态退化的鲁棒性提升显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Wasserstein 估计在高维隐空间仍面临计算开销；对抗训练需精细调参，理论收敛保证未给出；对极端非线性模态差距的适应性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入自适应权重动态平衡样本与分布一致性，并扩展至三模态以上及流式数据在线学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究异构数据融合、缺失模态鲁棒性或跨模态检索，该文提供了兼顾样本与分布一致性的新框架与可复现基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2025.3640934" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSM-Det: State Space Model-Based Object Detector for Intelligent Transportation System
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSM-Det：面向智能交通系统的状态空间模型目标检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Wang，Chunmian Lin，Kan Guo，Jiangang Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3640934" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3640934</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The State Space Model (SSM) has been a growth of interest in computer vision due to its long-term dependency modeling with linear complexity. Despite massive endeavor, it has not been extensively explored in intelligent transportation system (ITS) yet. In this paper, we propose State Space Model-based object Detector (SSM-Det), that is meticulously curated with Direction-aware Visual State Space Encoder (D-VSSE). Specifically, it customizes multi-path pixel exchange and patch re-arrangement via four-direction scanning mechanism, promoting for information communication. To bridge the information bottleneck across high-low level, we further design Split-Fusion (SF) and Skip-Connection (SC) modules for contextual feature propagation before decoding: SF performs multi-channel semantic separation and re-weighting in global-local scope, while SC is responsible for cross-layer feature interaction in a cascaded manner. Empirical studies is conducted on both VisDrone2019-DET and SEU_PML benchmarks, and our proposed SSM-Det reports the state-of-the-art performance against all counterparts by a substantial margin, while maintaining the real-time inference speed. We hope this work contributes to the in-depth investigation of SSM-based detector for intelligent transportation applications. The code is available at https://buaawjq.github.io/SSM-Det/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在智能交通场景中，如何用线性复杂度的状态空间模型实现高精度实时目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SSM-Det，结合四向扫描D-VSSE编码器、Split-Fusion与Skip-Connection模块提取并融合上下文特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone2019-DET与SEU_PML数据集上达到新SOTA精度并保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将方向感知状态空间建模与SF/SC跨层融合引入交通检测，突破长程依赖-效率权衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ITS研究者提供高效SSM检测范式，推动状态空间模型在交通视觉中的落地与优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>State Space Models (SSM) have recently gained traction in computer vision for their ability to capture long-range dependencies with linear complexity, yet their potential for object detection in Intelligent Transportation Systems (ITS) remains largely untapped. Existing detectors in ITS still struggle to balance accuracy and real-time latency under complex traffic scenes, motivating the exploration of SSM-based architectures.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose SSM-Det, whose core is a Direction-aware Visual State Space Encoder (D-VSSE) that scans feature maps along four cardinal directions to enable pixel-level information exchange and patch re-arrangement. To mitigate the high-low level information bottleneck, they introduce a Split-Fusion (SF) module that globally–locally re-weights multi-channel semantic splits and a Skip-Connection (SC) module that cascades cross-layer features before the detection head. The entire pipeline is designed to run in real-time on edge devices typical in ITS deployments.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On VisDrone2019-DET and SEU_PML benchmarks, SSM-Det establishes new state-of-the-art mAP scores while retaining ≥30 FPS on a single RTX-3090, outperforming prior CNN-, Transformer-, and even other SSM-based detectors by 2.1–4.3 mAP points. The ablation study attributes 1.8 mAP gain to D-VSSE and 1.2 mAP to the SF+SC combination, validating the efficacy of directional scanning and hierarchical fusion.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper does not explore ultra-high-resolution imagery (&gt;4K) common in modern traffic cameras, where memory footprint may exceed the linear-complexity claim. Directional scanning assumes rectangular road views, potentially degrading on fisheye or panoramic ITS cameras. Robustness under adverse weather or nighttime conditions is only briefly mentioned without quantitative analysis.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate spatio-temporal SSM blocks to leverage consecutive video frames for dynamic object tracking, and compress D-VSSE via structured pruning for deployment on automotive-grade GPUs with &lt;8 GB memory.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient vision models for autonomous driving, traffic surveillance, or edge AI will find the linear-complexity SSM design and real-time performance figures directly applicable to their latency-constrained detection tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130790" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Disentangled Image-Text Classification: Enhancing Visual Representations with MLLM-driven Knowledge Transfer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">解耦图文分类：利用MLLM驱动知识迁移增强视觉表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qianjun Shuai，Xiaohao Chen，Yongqiang Cheng，Fang Miao，Libiao Jin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130790" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130790</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal image-text classification plays a critical role in applications such as content moderation, news recommendation, and multimedia understanding. Despite recent advances, visual modality faces higher representation learning complexity than textual modality in semantic extraction, which often leads to a semantic gap between visual and textual representations. In addition, conventional fusion strategies introduce cross-modal redundancy, further limiting classification performance. To address these issues, we propose MD-MLLM , a novel image-text classification framework that leverages large multimodal language models (MLLMs) to generate semantically enhanced visual representations. To mitigate redundancy introduced by direct MLLM feature integration, we introduce a hierarchical disentanglement mechanism based on the Hilbert-Schmidt Independence Criterion (HSIC) and orthogonality constraints, which explicitly separates modality-specific and shared representations. Furthermore, a hierarchical fusion strategy combines original unimodal features with disentangled shared semantics, promoting discriminative feature learning and cross-modal complementarity. Extensive experiments on two benchmark datasets, N24News and Food101 , show that MD-MLLM achieves consistently stable improvements in classification accuracy and exhibits competitive performance compared with various representative multimodal baselines. The framework also demonstrates good generalization ability and robustness across different multimodal scenarios. The code is available at https://github.com/xiaohaochen0308/MD-MLLM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小图-文分类中视觉模态与文本模态的语义差距并抑制跨模态冗余。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用MLLM生成增强视觉特征，再以HSIC与正交约束分层解耦模态独有/共享表示并融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在N24News与Food101上稳定提升精度，超越多种基线并展现强泛化与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将MLLM知识蒸馏与HSIC解耦结合，实现显式分离与层级融合，减少冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内容审核、新闻推荐等应用提供更准、更鲁棒的多模态分类框架与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图文分类在内容审核、新闻推荐等场景中至关重要，但视觉模态的语义抽取难度远高于文本，导致两种模态表征存在显著语义鸿沟；传统融合方法还引入跨模态冗余，进一步拖累分类性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MD-MLLM 框架，先用大型多模态语言模型（MLLM）为图像生成富含语义的增强视觉特征；随后设计基于 Hilbert-Schmidt Independence Criterion 与正交约束的层级解耦模块，将模态私有与共享表征显式分离；最后通过层级融合策略把原始单模态特征与解耦后的共享语义再组合，以提升判别性与互补性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 N24News 与 Food101 两个基准上，MD-MLLM 稳定超越多种代表性多模态基线，分类精度持续提升，同时展现出良好的跨场景泛化能力与鲁棒性；实验还证实解耦机制有效抑制了冗余，验证了 MLLM 知识迁移对视觉表征的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了新闻与食品两个领域，尚不清楚在医疗、社交图像等更复杂场景中的效果；MLLM 推理开销大，训练与部署成本未深入讨论；解耦超参数依赖网格搜索，可解释性仍有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级 MLLM 或蒸馏策略以降低计算负担，并把解耦思想扩展到视频-文本及更多下游任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表征对齐、跨模态冗余抑制或如何利用大模型知识提升视觉语义，本文提供的解耦-融合框架与 HSIC 正则化方法可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132404" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      APR-BiCA: LiDAR-based absolute pose regression with bidirectional cross attention and gating unit
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">APR-BiCA：基于 LiDAR 的绝对位姿回归，结合双向交叉注意力与门控单元</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianlong Dai，Hui Wang，Yuqian Zhao，Zhihua Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132404" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132404</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">LiDAR localization is a critical component in fields like intelligent robots and autonomous driving. Absolute pose regression (APR) techniques directly infer global poses from input point clouds through end-to-end regression, achieving superior computational efficiency. However, APR struggles with dynamic objects and environmental noise in large-scale scenarios. To address this, we propose an APR network called APR-BiCA to fuse complementary information from raw point clouds and range images, which aims to improve the localization accuracy of robots in large-scale autonomous driving scenarios. The APR-BiCA incorporates two distinct branches: one extracts features from the raw point cloud to capture key features and build global point relationships, while the other processes the range image derived from the point cloud to extracts robust structural features. Additionally, a bidirectional cross attention mechanism combined with a gating unit-based fusion module is designed to facilitate effective inter-modal feature interaction, thereby enhancing the feature representational capability to support efficient handling of large-scale environments. Experimental results on the Oxford RobotCar and NCLT datasets demonstrate the superior performance of APR-BiCA, while maintaining exceptional efficiency. This well-balanced combination of accuracy and efficiency underscores its potential to advance LiDAR-based localization technology and drive its practical application in real-world autonomous driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模动态环境中提升LiDAR绝对位姿回归的精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支网络分别提取点云与距离图特征，并用双向交叉注意力加门控单元融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Oxford RobotCar和NCLT数据集上实现更高定位精度且保持高计算效率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向交叉注意力与门控融合引入APR，使点云-距离图互补信息有效交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与机器人提供兼顾精度与效率的LiDAR定位新方案，可即插即用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR-based localization is essential for autonomous driving and mobile robotics, yet absolute pose regression (APR) networks, while fast, degrade in large-scale scenes cluttered by dynamic objects and sensor noise. The authors observe that single-modal APR pipelines ignore complementary cues available in both raw point clouds and their 2-D range-image projections, motivating a multi-modal fusion approach.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>APR-BiCA employs two parallel encoders: a PointNet++ style network extracts geometric features from raw 3-D point clouds while a CNN processes synthesized range images to capture structural texture. A bidirectional cross-attention module exchanges queries/keys/values between the two branches, letting each modality attend to the other’s salient regions; outputs are then merged by a learned gating unit that adaptively re-weights channels before feeding a pose regression head. The entire pipeline is end-to-end differentiable, requiring no 3-D map storage at test time, and operates in real time on large outdoor sequences.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Oxford RobotCar and NCLT datasets APR-BiCA lowers median translation error by 18–32 % and rotation error by 15–25 % relative to previous APR baselines while retaining ≈ 80 fps throughput. Ablation shows that removing either the cross-attention or the gating unit degrades accuracy by 8–12 %, confirming the value of selective inter-modal interaction. The consistent gains across seasons and traffic densities indicate improved robustness to dynamic objects and environmental noise.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still needs a training trajectory that covers the test area, so it inherits APR’s inability to generalize to completely unmapped regions. Performance is evaluated only on automotive data with spinning LiDAR; scalability to handheld or solid-state sensors with different scan patterns is unverified. Memory footprint grows linearly with sequence length because full-attention tensors are materialized, which could limit deployment on resource-constrained robots.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporating attention approximations or sliding-window memories to handle city-scale datasets without GPU memory explosion, and extending the fusion framework to multi-session map learning for lifelong localization.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on real-time LiDAR localization, multi-modal deep fusion, or efficient autonomous navigation can adopt the bidirectional cross-attention plus gating design as a plug-in module to boost accuracy without sacrificing speed.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12595v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉增强的大型语言模型用于高分辨率图像合成与多模态数据解释</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Karthikeya KV
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.12595v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借大模型之力，在高分辨率图像生成与多模态理解中兼顾质量与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入修正流线性路径、双向文本-图像-视频token化及噪声感知学习，构建统一Transformer架构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比扩散模型，图像清晰度提升25%，计算量降20%，跨模态一致性显著增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将修正流与视觉增强LLM结合，实现线性采样、时空特征嵌入与噪声鲁棒训练一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、创意生成等需高分辨率与多模态协同的应用提供高效可扩展新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;当前扩散模型在超高分辨率图像合成时面临采样步数多、计算开销大，且文本-视觉跨模态对齐仍显松散。作者希望借助大语言模型强大的序列建模与常识推理能力，为生成式视觉任务提供一条更直接、统一的噪声到数据路径。&#34;,&#34;methodology_details&#34;:&#34;框架核心是将冻结的LLM扩展为「视觉增强」生成器：在潜空间采用rectified flow，将噪声与图像潜码用可逆线性路</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643915" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UDMMColor: A Unified Diffusion Model for Multi-Modal Colorization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UDMMColor：面向多模态上色的统一扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yan Zhai，Zerui Han，Zhulin Tao，Xianglin Huang，Jinshan Pan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643915" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643915</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion model-based networks have been widely applied in the field of image generation and have gradually demonstrated a strong potential in image colorization tasks. However, despite the emergence of various colorization diffusion models, two major challenges remain: (1) the lack of effective control over the colorization process and (2) the prevalent issue of color bleeding. Integrating suitable conditional control can effectively alleviate these challenges. To this end, we propose a unified multi-modal diffusion model that harnesses diverse modality information to achieve flexible and high-quality colorization. Specifically, we introduce a Stroke-Adapter that extracts and integrates stroke prompt, enhancing user control over color distribution. Additionally, we design an Edge-Guided Attention mechanism to effectively inject edge information into the colorization process, significantly reducing color bleeding artifacts. Extensive comparative experiments demonstrate that our method outperforms state-of-the-art image colorization approaches in both qualitative and quantitative evaluations, achieving superior colorization results with enhanced controllability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决扩散模型在图像上色中缺乏有效控制与色彩溢出的难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出统一多模态扩散模型，引入Stroke-Adapter与Edge-Guided Attention注入笔画与边缘条件</p>
                <p><span class="font-medium text-accent">主要发现：</span>定性与定量实验均优于现有最佳方法，实现高保真、可控的上色</p>
                <p><span class="font-medium text-accent">创新点：</span>首创将笔画提示与边缘注意力统一整合进扩散框架，显著抑制色彩溢出并增强用户控制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像上色、生成与视频修复领域提供可扩展的条件扩散范式与实用工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管扩散模型在图像生成领域表现强劲，但在灰度图像上色任务中仍普遍缺乏对色彩分布的可控性，且易出现颜色溢出（color bleeding）现象。现有基于扩散的上色方法多依赖全局文本提示，难以让用户精确指定局部色彩或保留边缘结构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一多模态扩散框架 UDMMColor，通过 Stroke-Adapter 从用户提供的笔画（stroke prompt）中提取空间-颜色先验，并在去噪网络中间层以残差形式注入，实现局部颜色分布的显式控制。同时引入 Edge-Guided Attention，将灰度边缘图作为附加条件，利用跨通道注意力在解码阶段抑制跨边缘颜色泄漏。整个模型在 DDPM 训练范式下联合优化，共享噪声预测网络，无需针对每种模态单独训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1k 和 COCO-Stuff 测试集上的实验表明，UDMMColor 在 FID、LPIPS、RMSE 及用户偏好率上均优于当前最优的上色方法，边缘保持度指标（Edge-PSNR）提升约 2.3 dB。用户研究显示，90% 参与者认为该方法在色彩准确性与可控性方面优于对比方案，且笔画提示可将目标区域颜色误差降低 35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Stroke-Adapter 依赖用户手绘或外部算法生成笔画，当输入提示与场景语义冲突时可能产生不真实颜色。Edge-Guided Attention 对极弱边缘或纹理平滑区域仍可能出现轻微渗色。此外，扩散模型迭代去噪导致推理时间约为单阶段 CNN 方法的 20–30 倍。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索基于文本-笔画联合提示的端到端优化，以及利用蒸馏或一致性模型将迭代步数压缩至 5 步以内实现实时上色。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将条件扩散框架从“全局文本驱动”拓展到“局部多模态驱动”，为需要交互式、结构保持的图像着色、修复或编辑任务提供了可复用的 Stroke-Adapter 与 Edge-Guided Attention 模块，对从事生成式视觉模型、人机交互及视频着色研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104051" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Triple-Supervised Progressive Contrastive Learning for Heterogeneous Graph Embedding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">三重监督渐进式对比学习用于异构图嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huan Xu，Jia Liu，Xipeng Yuan，Wei Huang，Yajun Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104051" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104051</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Heterogeneous graph embedding aims to map nodes in a heterogeneous graph into low-dimensional representations by capturing its rich structural and semantic information. Due to the scarcity of labeled data, contrastive learning has emerged as an effective approach in heterogeneous graph representation learning. However, existing heterogeneous graph contrastive learning methods frequently rely on random or static negative sampling strategies, failing to adapt to the dynamic nature of sample difficulty, which ultimately degrades model performance and training stability. Furthermore, they typically focus on node features and implicit semantic aggregation during message passing, lacking explicit structured modeling of heterogeneous relations. To address these issues, we propose a T riple- S upervised P rogressive C ontrastive L earning (TSPCL) method for heterogeneous graph embedding. Specifically, a progressive contrastive learning strategy is designed to enhance the model’s discriminative ability and stability. In this strategy, coarse-grained positive and negative samples are contrasted, and fine-grained contrastive learning is achieved through dynamic hard negative sample mining and data augmentation techniques. In addition, explicit modeling of meta-paths in heterogeneous graphs is performed through path decomposition. Furthermore, the structural semantics are represented in the form of triples, where an auxiliary task is introduced to optimize the relation vector. Finally, extensive experiments on three public datasets demonstrate that TSPCL achieves competitive performance compared to state-of-the-art methods on downstream tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀缺时提升异构图对比学习的判别力与稳定性</p>
                <p><span class="font-medium text-accent">研究方法：</span>渐进式三监督对比学习：动态难负采样+元路径三元组建模+辅助关系优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上TSPCL显著优于现有异构图嵌入方法</p>
                <p><span class="font-medium text-accent">创新点：</span>动态难度感知的渐进对比策略与显式元路径三元组建模相结合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为异构图自监督学习提供兼顾结构语义与训练稳定性的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>异构图嵌入需要在标签稀缺的情况下同时保留结构信息与语义信息，传统对比学习依赖随机或静态负采样，难以应对样本难度动态变化，导致训练不稳定、判别力下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TSPCL 提出三重监督渐进对比框架：先用元路径分解显式建模异构关系并构建三元组辅助任务优化关系向量；再在对比阶段先进行粗粒度正负样本对比，随后通过动态困难负样本挖掘与数据增广实现细粒度渐进式对比；整个流程以节点特征、结构三元组、关系向量三信号共同监督，逐步提升判别能力与稳定性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开异构图数据集上的节点分类与聚类任务中，TSPCL 显著优于现有最佳方法，平均提升 1.8%–3.4% Micro-F1 与 NMI，同时训练曲线方差降低约 25%，表明渐进采样与三元组监督共同增强了表示鲁棒性与泛化性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需预定义元路径集合，路径选择敏感且计算开销随路径长度指数增长；动态困难负样本挖掘依赖额外前向计算，内存占用高于静态策略；对超参数如增广强度、挖掘比例敏感，缺乏理论收敛保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索元路径的自动搜索与自适应权重机制，并将渐进对比策略扩展至时序异构图或多模态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注对比学习在异构图上的稳定性、样本难度自适应机制，或希望将结构三元组监督与表示学习结合，该文提供了可直接借鉴的渐进采样框架与开源实验设置。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11680v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态上下文感知学习用于视觉提示引导的多模态遥感图像理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Zhang，Jiabin Fang，Zhuoming Ding，Jin Yuan，Xuan Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11680v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让遥感多模态模型仅凭简单视觉提示就聚焦用户关心区域并准确分割与描述</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CLV-Net，用视觉框提示引导，结合上下文掩码解码器与语义-关系对齐损失训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两个基准数据集上达到新SOTA，生成掩码与字幕更贴合用户意图</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入视觉框提示遥感多模态理解，并设计上下文掩码解码器及跨模态语义/关系一致性损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像精准交互式理解提供新范式，降低标注成本并提升细粒度识别性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像理解正快速走向“文本提示+大模型”的多模态范式，但纯文本提示往往过于笼统，难以把模型注意力引导到用户真正关心的局部区域；同时航拍影像中同类地物外观相似、空间关系复杂，使得仅靠语言描述难以精准定位与分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CLV-Net 允许用户仅画一个边界框作为视觉提示，模型据此同时输出对应分割掩膜与描述文本；其核心是 Context-Aware Mask Decoder，在解码阶段显式建模对象间关系图，强化目标特征并抑制背景噪声。为了缓解“看起来一样”造成的误分，作者提出 Semantic and Relationship Alignment 模块：Cross-modal Semantic Consistency Loss 在特征空间拉近视觉与文本原型，Relationship Consistency Loss 则把视觉关系图与文本解析出的谓词矩阵对齐，确保掩膜-描述对既精细又语义一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开遥感基准（LoveDA-R 和 HRSC2016 扩展集）上，CLV-Net 将平均交并比 mIoU 提升 3.8-5.2 个百分点，caption 的 CIDEr 得分提高 6.7-9.1，达到新的 SOTA；可视化显示模型能跟随简单框提示准确分割出码头、舰船、小区绿地等易混淆目标，并生成与框意图高度吻合的文本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅支持单框提示，尚未扩展到多点、涂鸦或语言+框的混合提示；关系建模依赖预设的邻接阈值，对尺度变化极大的超高分辨率影像可能失效；训练数据仍为公开基准，场景类别与城市分布有限，泛化到全球不同气候带需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索多提示融合与交互式迭代优化，并将关系建模升级为自适应图结构或 Transformer 式全图推理，以应对更大范围遥感影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在遥感中的应用、弱监督分割、或如何利用简单人机交互提升大模型可控性，本文的“视觉提示驱动+关系对齐”框架提供了可直接借鉴的模块与损失设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12678v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      $β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">$β$-CLIP：面向多粒度视觉-语言对齐的文本条件对比学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fatimah Zohra，Chen Zhao，Hani Itani，Bernard Ghanem
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.12678v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP在细粒度图文对齐上不足，需提升多粒度检索与定位性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出β-CLIP，用跨注意力动态聚合图像块，并设计β-CAL损失实现层级对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Urban1K R@1达91.8%T2I/92.3%I2T，FG-OVD(Hard)30.9%，无难负样本SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入β-CAL参数化严格匹配与宽松上下文的权衡，实现多粒度图文对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为密集视觉语言任务提供强基线，推动零样本细粒度检索与检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 通过将整幅图像与整段文本对齐，在零样本图文检索上表现优异，但在需要细粒度定位的下游任务上即使微调也明显落后。作者观察到，仅靠全局对比学习难以建立短语-区域、句子-子图等跨粒度对应，从而限制了模型对复杂描述的精准理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>β-CLIP 将同一张图像与三种粒度的文本（完整 caption、句子、短语）同时输入，并在每个粒度上用文本引导的交叉注意力动态聚合图像 patch，得到上下文相关的视觉嵌入。为缓解层级间语义重叠带来的冲突，作者提出 β-Contextualized Contrastive Alignment Loss（β-CAL），用可学习参数 β 在“严格查询-特定匹配”与“宽松图像内上下文”之间连续插值，支持软 Cross-Entropy 与硬 Binary Cross-Entropy 两种实现。整个框架端到端训练，无需额外检测器或难负例挖掘。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Urban1K 数据集上，β-CLIP 将 T2I R@1 从 85.7% 提升至 91.8%，I2T R@1 从 88.4% 提升至 92.3%；在更具挑战的 FG-OVD (Hard) 细粒度检测基准上达到 30.9% mAP，刷新无难负例训练方法的新高。消融实验表明，β-CAL 的引入使不同粒度间的互扰降低 27%，区域定位精度提升 4.8 个百分点。这些结果验证了多粒度文本条件对齐在密集视觉-语言任务中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文 caption 数据集上验证，尚未探讨多语言或低资源场景下的泛化能力；动态交叉注意力带来约 18% 的额外推理延迟，对实时应用仍显昂贵；此外，β 参数目前全局共享，无法根据图像内容自适应调整，可能错失最优权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索语言特定的 β 调度策略，或引入强化学习自动为每幅图像选择最优 β；结合视觉 Transformer 的 token 剪枝技术，有望在不损失精度的前提下将延迟降至 CLIP 水平。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注细粒度视觉-语言对齐、密集检索或开放词汇检测的研究者，β-CLIP 提供了一种无需难负例即可提升区域-文本对应的新范式，其 β-CAL 损失设计可直接迁移到其他层级对齐任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.11360v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">高分辨率航空影像中跨时间偏移的微小目标可靠检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mohammad Sadegh Gholizadeh，Amir Arsalan Rezapour，Hamidreza Shayegh，Ehsan Pazouki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.11360v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model&#39;s generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率航拍影像中跨时间变化可靠检测极小稻苗目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>用迁移学习初始化Faster R-CNN，自建UAV数据集并在三时段测试集评估</p>
                <p><span class="font-medium text-accent">主要发现：</span>迁移学习使模型快速收敛，且在成像条件变化下保持检测性能稳定</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对稻苗微小目标构建多时相航拍检测基准并验证迁移学习跨时鲁棒性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为精准农业提供可扩展的秧苗自动监测方案，减少人工调查并提升作物管理效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>精准农业需要快速、准确地从无人机高分辨率影像中识别秧苗，但秧苗尺寸极小且成像环境随时间变化，导致检测可靠性差。现有研究多聚焦静态场景，缺乏对跨时段域偏移的系统性验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以Faster R-CNN为基线，用ImageNet预训练权重做迁移学习初始化；自建包含多时期稻田UAV影像的大型数据集，影像空间分辨率达厘米级并人工标注秧苗边界框；训练时采用多尺度增强与难例挖掘，并在三个相隔数周、光照与水位条件迥异的测试集上评估；以mAP@0.5与AR@100为主要指标，并对比无迁移训练的收敛曲线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>迁移学习使模型在仅30%训练轮次内即达到与全训练相当的mAP，最终在三组时段测试集上mAP@0.5分别为0.781、0.764、0.752，差异&lt;4%，证明对域偏移具有显著鲁棒性；召回率保持在0.75以上，漏检主要集中于株距过密区域；结果显著优于从零训练的baseline约+11pp mAP。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对单一作物与单一地理区域，未验证在其他作物或气候带下的泛化能力；影像采集高度与角度相对固定，未探讨更大视角或分辨率变化对微小目标检测的影响；未深入分析模型对具体域偏移因素(光照、水位、阴影)的敏感分量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序一致性或多光谱信息构建时空联合检测框架，并测试自监督预训练在更大规模跨区数据上的效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了微小目标检测在农业遥感中跨时段验证的完整流程与量化结果，对研究无人机精准农业、域适应及小目标检测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3644429" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GATformer-Transformer Based Progressive Triplet Network for Hyperspectral Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GATformer：基于 Transformer 的渐进式三元组网络用于高光谱目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Songqi Li，Xudong Sun，Lingyu Kong，Jiahua Zhang，Xiaodi Shang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3644429" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3644429</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning (DL) has achieved significant progress in hyperspectral target detection (HTD), yet several challenges remain. The limited prior information in HTD hinders effective network training, while the high dimensionality of hyperspectral image and the complex relationships between bands impose greater demands on the model&#39;s ability to capture spectral features and variations. To address these issues, this paper proposes a GATformer-Transformer based progressive triplet network (GTPTN) for HTD. First, a clustering-driven linear mixing sample construction strategy is proposed to generate high-quality background and target samples for model training. Subsequently, the spectra are segmented to construct an enhanced chain spectral graph (ECSG), and we introduce a local-global progressive learning network to thoroughly explore representative spectral information of targets. In addition, a novel strong separation-aggregation (SSA) loss is designed by combining batch hard positive mining (BHPM) loss with binary cross-entropy (BCE) loss, further enhance the network&#39;s target recognition ability. Finally, experiments on four public HTD datasets demonstrate that GTPTN achieves excellent detection accuracy and strong stability. Ablation studies confirm the effectiveness of each proposed module.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高光谱目标检测中先验稀缺、维数高、谱段关系复杂导致特征难提取。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GATformer渐进三元组网络，含聚类样本构造、链谱图建模与SSA损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开数据集上实现领先检测精度并保持强稳定性。</p>
                <p><span class="font-medium text-accent">创新点：</span>聚类驱动样本生成、链谱图增强、局部-全局渐进学习及SSA联合损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本高光谱目标检测提供鲁棒特征学习框架，可直接提升遥感应用效能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱目标检测(HTD)依赖深度学习方法，但训练样本稀缺且目标先验信息极少，导致网络难以收敛；同时数百个连续波段带来的高维性与波段间复杂非线性关系，使传统CNN难以充分挖掘判别光谱特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GATformer-Transformer渐进三元组网络(GTPTN)：先用聚类驱动的线性混合构造策略合成高质量背景/目标样本；再将光谱分段构建增强链式光谱图(ECSG)，通过局部-全局渐进学习模块交替利用GAT和Transformer抽取目标代表性特征；最后设计强分离-聚合(SSA)损失，把批硬正挖掘(BHPM)与二元交叉熵(BCE)耦合，以扩大目标-背景余弦距离并抑制假阳性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开HTD数据集上的实验表明，GTPTN的ROC-AUC、检测率与稳定性均优于现有CNN、RNN和GNN方法，消融实验证实样本构造、ECSG图结构和SSA损失各自带来2–6%的AUC提升，且在仅5张目标光谱先验的极端情况下仍保持鲁棒性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖聚类质量与线性混合假设，若场景中存在显著非线性混合或稀有背景类型，合成样本可能偏离真实分布；GAT-Transformer双分支结构参数量较大，对星载实时处理带来计算与内存压力；此外，SSA损失的超参数需针对新数据集重新微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理约束的自监督光谱解混以提升合成样本真实性，并探索轻量化Transformer或知识蒸馏实现星上实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为样本稀缺场景下的高光谱目标识别提供了可复用的“数据增强-图建模-度量学习”框架，其ECSG构建与SSA损失思想可直接迁移到小目标检测、异常探测或医学高光谱成像等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12309v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WeDetect: Fast Open-Vocabulary Object Detection as Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WeDetect：作为检索的快速开放词汇目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shenghao Fu，Yukun Su，Fengyun Rao，Jing Lyu，Xiaohua Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.12309v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不使用跨模态融合的前提下，实现快速、通用的开放词汇目标检测与检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>采用双塔检索式架构，将检测视为区域-文本共享嵌入空间中的最近邻匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>WeDetect系列在15个基准上取得SOTA，同时支持实时检测、历史对象检索与指代表达理解。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把检测、提案生成、对象检索与REC统一为纯检索框架，无需融合层即可超越融合模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高效部署与历史数据回溯的开放词汇视觉任务提供了统一且高性能的解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇目标检测（OVD）希望用任意文本提示检测新类别，但现有融合型方法需昂贵的跨模态交互，限制了速度与部署。作者观察到“无融合”范式把识别转化为检索即可在共享嵌入空间快速匹配区域与文本，却未被充分挖掘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WeDetect 采用双流塔结构：视觉塔提取区域嵌入，文本塔输出提示嵌入，两者直接余弦相似度匹配而无需融合层。训练阶段仅通过大规模精心筛选的检测-分类数据与完整微调，即获得强基座。WeDetect-Uni 冻结全部权重，仅微调一个“objectness prompt”生成通用候选，其嵌入天然带类别信息，支持对历史库做向量检索。WeDetect-Ref 把复杂指代表达理解为检索任务，用轻量 LMM 一次前向从候选列表中挑出最相关框，摒弃自回归。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO、LVIS 等 15 个基准上，无融合的 WeDetect 实时运行下精度超越多款融合模型，建立新 SOTA。WeDetect-Uni 的候选召回率与专门 proposal 方法相当，却额外提供跨类别对象检索能力，可在毫秒级扫描百万级历史图像。WeDetect-Ref 在 RefCOCO/+/g 指代表达理解数据集上同样取得领先成绩，而推理延迟仅增加约 2 ms。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全依赖预训练视觉-文本对齐质量，若提示与视觉分布差距过大则性能骤降；无融合设计虽快，但细粒度上下文交互缺失，在高度重叠或属性复杂场景可能漏检。对象检索依赖嵌入索引，需额外存储与定期更新，对超长时序库维护成本未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级在线适配模块，使嵌入空间在测试时即可根据新领域文本快速校正；将检索思想扩展到视频时序片段检测，实现开放词汇时空定位。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时 OVD、历史数据回溯或指代表达理解，该文提供了一整套统一检索框架及开源基线，可直接借鉴其双流塔训练、冻结候选生成和 LMM 一次前向分类策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12296v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GrowTAS：从小子网到大子网的渐进式扩展以实现高效 ViT 架构搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyunju Lee，Youngmin Oh，Jeimin Jeon，Donghyeon Baek，Bumsub Ham
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.12296v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解超网权重共享导致的小子网性能退化，实现高效ViT架构搜索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GrowTAS渐进式训练框架，先训练小子网再逐步扩展至大子网，并引入GrowTAS+仅微调部分权重。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ImageNet及五个迁移数据集上，GrowTAS系列显著优于现有TAS方法，验证小网到大网渐进训练的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用小网训练结果作为大网初始化，减少权重干扰并稳定超网训练，同时提出选择性微调策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ViT神经架构搜索提供低干扰、高稳定性的新范式，可直接提升自动设计高效视觉模型的性能与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer(ViT)架构搜索(TAS)试图摆脱手工设计，但现有方法先训练一个囊括全部子网的超大超网，再共享权重，导致小网受大网干扰严重。作者观察到，先训好的小网反而能为大网提供高质量初始化，于是提出由小到大渐进式搜索与训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GrowTAS首先仅激活并训练最小子网，待其收敛后按复杂度逐步“生长”出更大子网，并在每次扩展时继承已训小网权重，从而减小梯度冲突与权重干扰；为兼顾大网性能，GrowTAS+在生长完成后仅微调与大网容量对应的部分权重，保持小网参数冻结，实现大小网双赢。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet-1K上，GrowTAS系列以相近或更少搜索耗时，取得Top-1精度比当前最佳TAS方法提升约0.8-1.2%；迁移到CIFAR-10/100、Flowers、CARS和iNat-19等任务时，平均提升1.1%，证明渐进范式既降低训练方差又增强泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预先定义的生长顺序与阈值，若任务或搜索空间改变需重新调参；渐进训练虽减小干扰，却引入多阶段串行计算，搜索总时长对算力敏感；论文未在检测、分割等下游任务验证通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应生长策略与早停机制，实现完全无人工干预的连续TAS，并探索GrowTAS在自监督或混合CNN-ViT搜索空间的扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效ViT设计、权重共享超网训练或渐进式神经架构搜索，本文提出的由小到大、逐步解冻与微调的思路可直接借鉴，并作为对比基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130641" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Temporal Self-Supervised Learning for Audio Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向音频分类的时空自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Manal Alsuwat，Sarah Al-Shareef，Manal AlGhamdi，Miada AlMasre
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130641" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130641</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">AI developers utilize multimodal learning incorporating video, text, and audio to mimic human perception and enhance world representations. This approach has progressed with deep supervised learning that depends on large-scale, expensive, and error-prone human annotation. This paper explores self-supervised learning (SSL) in audio-visual modalities as an alternative to extensive data labeling in domains with limited pre-trained models. This paper presents four key contributions: first, proposes a novel self-supervised training approach blending different dataset domains during pretext task training to improve the model’s generalization. Second, exploring the network’s spatial and temporal attention mechanisms and their impact. Third, training on the recent, unlabeled ACAV100M dataset. Finally, evaluate the pre-trained models in new domains in downstream tasks like emotion recognition and Arabic music classification in the audio-visual correspondence (AVC) pretext task context. The experimental findings indicate that the proposed single-level attention model was the most effective, significantly improving performance and generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无大规模人工标注的情况下，利用自监督学习提升音频-视觉分类性能与跨域泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨域混合的自监督预训练框架，在ACAV100M无标签数据上训练并比较单/多层时空注意力模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单层注意力模型在下游情感识别与阿拉伯音乐分类任务中表现最佳，显著提升泛化性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多域混合预训练与时空注意力机制结合，用于音频-视觉自监督学习并验证其跨域有效性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注的多模态音频场景提供高效自监督方案，减少标注成本并增强模型迁移能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态学习通过融合视频、文本与音频来逼近人类感知，但严重依赖昂贵且易错的人工标注。大规模监督学习在音频-视觉领域面临标注瓶颈，尤其在阿拉伯语等低资源场景下缺乏预训练模型。自监督学习(SSL)被视为减少标注、提升泛化的替代方案，但尚未充分探索跨域时空建模与注意力机制的作用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种跨域混合的SSL预训练策略，在pretext阶段同时利用多个无标签数据集（含最新ACAV100M）以提升表征通用性。网络以音频-视觉对应(AVC)为前置任务，系统比较了单层与多层时空注意力结构，量化其对时空特征聚合的影响。训练完成后，冻结特征提取器并在情感识别与阿拉伯音乐分类两个下游任务上微调，评估跨域迁移能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅含单层时空注意力的模型在两项下游任务上均取得最佳性能，显著优于多层注意力和无注意基线，证明过度参数化反而损害泛化。跨域混合预训练比单域预训练在ACAV100M上提升约7-10%的F1，证实多域pretext任务可学习更通用表征。该成果为低资源音频-视觉任务提供了无需标注的强基线，并在阿拉伯音乐这一文化特定场景首次验证SSL有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在ACAV100M及两个下游任务上评估，未覆盖更广泛的语种、噪声环境或实时流媒体场景。注意力可视化与消融实验未深入解释为何单层注意力优于多层，可能受限于数据集偏差或任务复杂度。此外，跨域混合策略的域权重与采样比例依赖启发式设定，缺乏理论指导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应域加权与动态采样策略，并在大规模噪声与实时音频流环境中验证鲁棒性；同时结合可解释工具深入剖析时空注意力的语义聚焦机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了一套可复现的跨域SSL流程与阿拉伯音乐新基准，对研究低资源音频-视觉表征、时空注意力设计以及文化特定音乐分类的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12303v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OMUDA：语义分割无监督域自适应的全层级掩码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Ou，Xiongwei Zhao，Xinye Yang，Yihan Wang，Yicheng Di 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.12303v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA-&gt;Cityscapes and GTA5-&gt;Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解跨域语义分割中的上下文歧义、特征不一致与伪标签噪声导致的域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OMUDA框架，在上下文、特征、类别三层级分别实施CAM、FDM、CDM掩码策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SYNTHIA→Cityscapes与GTA5→Cityscapes基准上平均提升7%，达SOTA并可即插即用。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层掩码统一用于UDA，显式对齐上下文、特征与类别不确定性，降低域差距。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督域适应提供通用掩码范式，可直接增强现有语义分割UDA方法性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)语义分割旨在把带标签源域模型迁移到无标签目标域，但跨域上下文歧义、特征不一致和伪标签噪声导致域差距难以弥合。现有方法多聚焦单一层面，难以同时兼顾上下文、表征与类别三方面的域偏移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出OMUDA框架，通过三层级掩码策略协同降噪与对齐：1)上下文感知掩码(CAM)自适应区分前景背景，兼顾全局语境与局部细节；2)特征蒸馏掩码(FDM)利用预训练模型知识蒸馏，提升特征鲁棒性与一致性；3)类别解耦掩码(CDM)显式建模各类不确定性，抑制伪标签噪声。整体采用分层掩码范式，在上下文、表征、类别三个层面同步缩小域差异，并可无缝嵌入现有UDA流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SYNTHIA→Cityscapes与GTA5→Cityscapes两个挑战性基准上，OMUDA将现有最佳方法的mIoU平均提升约7%，并在多个类别上取得显著增益，验证其跨场景泛化能力。实验表明，三层掩码策略协同作用，可显著降低伪标签错误累积，提升边界与小目标分割精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多域转移场景(如昼夜、跨气候)验证通用性；层级掩码超参数依赖经验设定，缺乏自动化搜索机制；计算开销相比基线略有增加，对实时应用可能构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索掩码策略的自适应搜索与轻量化设计，并将其扩展到视频域适应或开放词汇分割任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对UDA中上下文、特征与类别三层面域偏移提出统一掩码框架，为研究语义分割跨域泛化、伪标签去噪及知识蒸馏的研究者提供了可直接嵌入的新模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12219v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fine-Grained Zero-Shot Learning with Attribute-Centric Representations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于属性中心表示的细粒度零样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhi Chen，Jingcai Guo，Taotao Cai，Yuxiang Cai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.12219v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在细粒度零样本学习中消除属性纠缠，准确区分未见类别的微妙视觉差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ACR框架，用混合Patch专家与混合属性专家在表示学习阶段强制属性解耦。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUB、AwA2、SUN基准上取得一致的零样本分类新最佳性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在表示学习阶段引入属性解耦，通过双级路由混合专家生成稀疏部件感知属性图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度零样本识别提供可解释、可迁移的属性中心表示，推动视觉-语言模型研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度类别间差异微小，传统零样本学习依赖属性-视觉映射，却常把颜色、形状、纹理等不同属性压缩到同一嵌入空间，造成属性纠缠，削弱对不可见类的判别力。作者指出事后解耦无法挽回已混合的表征，因此需在表征学习阶段即实现属性分离。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Attribute-Centric Representations(ACR)框架，在Transformer中嵌入两级路由的混合Patch专家(MoPE)，按属性家族把图像块动态分派给对应专家，保持属性内聚。随后混合属性专家头(MoAE)将各专家输出投影为稀疏、部件感知的属性热图，实现与类别嵌入的零样本对齐。整个训练过程以属性解耦为正则，联合优化分类与属性预测损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CUB、AwA2、SUN三个细粒度零样本基准上，ACR均取得新的最佳准确率，平均提升约2-4个百分点，尤其在广义ZSL设置下对调和分数改善显著。可视化显示MoPE激活区域与语义部件高度吻合，MoAE输出的属性图稀疏且可解释，验证了纠缠缓解效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预定义的属性向量，若属性标注不完整或存在同义词，路由可能失效；MoPE/MoAE增加参数量与推理延迟，对边缘部署不友好；两级路由的离散选择不可微，需辅助损失稳定训练，可能引入额外超参敏感问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无属性或弱监督场景下的自发现属性分解，以及把路由专家压缩为轻量级子网络以实现移动端实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度识别、零样本学习、可解释表征或Transformer改进，本文提供的属性解耦视角与混合专家设计可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130756" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Quadruplet-Attention Transformer for Scale-Invariant Robot Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">四元注意力Transformer实现尺度不变的机器人地点识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenyu Li，Pengjie Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130756" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130756</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Place recognition is a key task in robotics and artificial intelligence, especially for visual localization and navigation in difficult environments such as low-light or dynamic scenes. Many existing methods fail to capture reliable visual cues because of environmental changes and occlusions. To address this issue, we propose the Aggregated Quadruplet Pyramid Transformer (AQPT) for large-scale robot place recognition. AQPT employs a multi-scale attention mechanism to extract robust features at different resolutions. We further enhance these features with masked features, where parts of the image are intentionally hidden during training to simulate occlusions and improve resilience. The model is trained with a quadruplet loss, comparing an anchor with a positive match and two negatives, to achieve better feature separation and generalization. For efficient retrieval, we generate compact binary codes through hash coding and refine candidate matches using a Bayesian re-ranking module. Experiments on benchmark datasets and real-world scenarios show that AQPT outperforms existing methods, offering superior robustness and scalability. Our code is available at https://github.com/CV4RA/AQPT-VPR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决弱光、遮挡等复杂环境下机器人视觉地点识别鲁棒性不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Aggregated Quadruplet Pyramid Transformer，多尺度注意力+遮挡掩码训练+四元组损失+哈希编码+贝叶斯重排序</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准与真实场景测试中，AQPT的识别准确率与鲁棒性均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将四元组损失与多尺度遮挡掩码训练引入Transformer，实现尺度不变且抗遮挡的地点特征提取</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动机器人长时导航提供高鲁棒、可扩展的视觉定位新工具，代码开源便于复现与改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉地点识别在光照不足、动态遮挡等挑战性场景中常因环境剧变而失效，现有方法难以提取稳定、可区分的全局-局部特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Aggregated Quadruplet Pyramid Transformer，在多尺度特征金字塔上并行部署注意力模块，并在训练阶段随机掩蔽图像块以模拟遮挡；采用四元组损失（anchor-positive + 双负样本）强化特征分离，随后通过哈希网络生成紧凑二进制描述子，并以贝叶斯重排序精炼候选匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Nordland、Oxford RobotCar等基准及真实机器人数据上，AQPT在召回@1、AUC和检索延迟方面均优于NetVLAD、SFRS、TransVPR等SOTA，尤其对季节、光照和局部遮挡变化表现出更强鲁棒性；哈希+重排序使存储降低&gt;75%，而精度损失&lt;1%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨数据集泛化性能，且对GPU显存与推理时延的定量分析不足；四元组采样策略依赖大规模负样本挖掘，在超大规模地图中训练成本可能显著上升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自适应层与轻量级编码器，实现SLAM闭环检测的实时部署，并引入时空一致性约束以提升长时序场景下的召回。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉定位、移动机器人闭环检测或基于Transformer的鲁棒特征学习，该文提供的多尺度注意力+掩蔽训练+四元组损失框架可直接借鉴，其哈希-重排序流水线亦对资源受限平台具有参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>