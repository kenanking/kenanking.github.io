<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-12</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-12 10:44 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">958</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、轻量网络及视觉SLAM，同时对自监督、大模型等前沿主题保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测领域收藏量最高且持续追踪Ross Girshick、Kaiming He等权威团队工作；对遥感影像（SAR）智能解译形成稳定分支，常年阅读TGRS、雷达学报并积累SAR目标识别、旋转目标检测等关键词。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉与地球科学，既关注CVPR/ICCV/NeurIPS等AI顶会，也系统收藏IEEE TGRS、雷达学报等遥感期刊，体现出将通用视觉方法迁移至遥感数据的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏高峰后回落，新增论文聚焦“合成孔径雷达目标检测”“多任务学习”，显示兴趣正向SAR精细化检测与多任务联合学习收敛；同时保留对大模型、自监督、扩散模型的零星追踪。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感基础模型与雷达-视觉融合检测，以及面向星载/机载实时处理的轻量化多任务网络，以延续检测精度与模型效率并重的阅读主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 934/934 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-10 10:27 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸识别', '轻量网络', '对比学习', 'Transformer', '模型压缩'],
            datasets: [{
              data: [22, 35, 15, 12, 18, 10, 10, 10],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 97 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 174 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 82,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 60,
            keywords: ["\u7efc\u8ff0", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "DETR"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 50,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 50,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 45,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Swin Transformer"]
          },
          
          {
            id: 5,
            label: "\u591a\u4f20\u611f\u56683D\u611f\u77e5",
            size: 40,
            keywords: ["SIFT", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 6,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 39,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 7,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 38,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 8,
            label: "\u5927\u6a21\u578b\u9ad8\u6548\u8bad\u7ec3",
            size: 38,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 9,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 38,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 10,
            label: "\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 37,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 11,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b", "SimCLR"]
          },
          
          {
            id: 12,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 37,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 13,
            label: "SAR\u6210\u50cf\u4e0e\u7269\u7406\u667a\u80fd",
            size: 32,
            keywords: ["\u5fae\u6ce2\u89c6\u89c9", "\u7269\u7406\u667a\u80fd", "\u7535\u78c1\u6563\u5c04"]
          },
          
          {
            id: 14,
            label: "LLM\u5f3a\u5316\u63a8\u7406",
            size: 29,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6a21\u578b\u63a8\u7406"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u673a\u5668\u5b66\u4e60\u539f\u7406\u4e0e\u4f18\u5316",
            size: 26,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 17,
            label: "\u6df1\u5ea6\u7279\u5f81\u53ef\u89c6\u5316",
            size: 25,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4e3b\u6210\u5206\u5206\u6790"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u7406\u8bba",
            size: 25,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 19,
            label: "\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b",
            size: 23,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u6291\u5236"]
          },
          
          {
            id: 20,
            label: "Vision Transformer\u7efc\u8ff0",
            size: 22,
            keywords: ["\u7efc\u8ff0", "Vision Transformers", "Transformers"]
          },
          
          {
            id: 21,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u5b9a\u4f4d",
            size: 21,
            keywords: []
          },
          
          {
            id: 22,
            label: "\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b",
            size: 21,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 23,
            label: "\u6301\u7eed\u5b66\u4e60\u4e0e\u6b8b\u5dee\u7f51\u7edc",
            size: 20,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 24,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 18,
            keywords: ["HRNet", "U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406"]
          },
          
          {
            id: 25,
            label: "SAM\u901a\u7528\u5206\u5272",
            size: 18,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u51fa\u7248\u4e0e\u7b97\u6cd5",
            size: 17,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 27,
            label: "\u68c0\u6d4b\u635f\u5931\u4e0e\u8bc4\u4ef7",
            size: 14,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5224\u522b\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 28,
            label: "\u6570\u636e\u589e\u5f3a\u6b63\u5219\u5316",
            size: 4,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b", "\u5f31\u76d1\u7763\u5b9a\u4f4d"]
          },
          
          {
            id: 29,
            label: "\u65cb\u8f6c\u76ee\u6807\u6210\u50cf",
            size: 1,
            keywords: []
          }
          
        ];

        const links = [{"source": 16, "target": 26, "value": 0.8870001500860915}, {"source": 18, "target": 23, "value": 0.9143185127668744}, {"source": 3, "target": 4, "value": 0.8966935951976945}, {"source": 5, "target": 7, "value": 0.8994750096390083}, {"source": 0, "target": 2, "value": 0.9396348565697861}, {"source": 4, "target": 24, "value": 0.8750671591790198}, {"source": 3, "target": 22, "value": 0.8897490723619954}, {"source": 5, "target": 25, "value": 0.8599572038040137}, {"source": 1, "target": 6, "value": 0.9171851618129079}, {"source": 9, "target": 17, "value": 0.926995790902153}, {"source": 9, "target": 20, "value": 0.9089901193080624}, {"source": 1, "target": 15, "value": 0.8788971154544468}, {"source": 1, "target": 27, "value": 0.9273384351237033}, {"source": 13, "target": 29, "value": 0.7421696434741202}, {"source": 4, "target": 11, "value": 0.9434413942998566}, {"source": 4, "target": 17, "value": 0.9127049120648439}, {"source": 22, "target": 28, "value": 0.8564293087247363}, {"source": 4, "target": 20, "value": 0.9259056376935567}, {"source": 8, "target": 14, "value": 0.9218058071452571}, {"source": 5, "target": 21, "value": 0.8912842023211982}, {"source": 17, "target": 23, "value": 0.9007543389903051}, {"source": 0, "target": 10, "value": 0.8970673325575242}, {"source": 1, "target": 5, "value": 0.8963677968745597}, {"source": 0, "target": 13, "value": 0.9515080023579103}, {"source": 8, "target": 20, "value": 0.9273426460730241}, {"source": 0, "target": 19, "value": 0.9281718479519603}, {"source": 11, "target": 22, "value": 0.9389722714361937}, {"source": 11, "target": 28, "value": 0.8436849701458424}, {"source": 13, "target": 19, "value": 0.902786369848331}, {"source": 19, "target": 29, "value": 0.7274779671792568}, {"source": 15, "target": 19, "value": 0.8582618674097103}, {"source": 7, "target": 21, "value": 0.8399779788991818}, {"source": 22, "target": 27, "value": 0.9091895260236107}, {"source": 12, "target": 20, "value": 0.8686825510735179}, {"source": 14, "target": 23, "value": 0.8850281082231048}, {"source": 4, "target": 22, "value": 0.9189064031195182}, {"source": 9, "target": 12, "value": 0.8693787571703606}, {"source": 14, "target": 26, "value": 0.8632632929324184}, {"source": 4, "target": 25, "value": 0.8662486178830573}, {"source": 0, "target": 6, "value": 0.91541499683723}, {"source": 1, "target": 7, "value": 0.8923840291427375}, {"source": 9, "target": 18, "value": 0.8960078727906536}, {"source": 2, "target": 6, "value": 0.8993678823873237}, {"source": 9, "target": 24, "value": 0.8945419747419145}, {"source": 10, "target": 19, "value": 0.9119052941691435}, {"source": 16, "target": 23, "value": 0.8803335389984868}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于多模态图像融合的论文、1篇关于行人重识别的论文和1篇关于CLIP小样本适应的论文。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：该主题关注可见光与红外/深度/热成像等多模态信息的融合。《DIFF-MF》提出差分驱动的通道-空间状态空间模型以强化互补信息整合；《DMDNet》设计双分支深度网络在编码阶段即融合可见光与深度/热特征提升显著目标检测；《Infrared-Assisted Single-Stage Framework》利用红外辅助在雾霾下同步完成图像复原与融合。</p>
            
            <p><strong class="text-accent">行人重识别</strong>：针对极远距离视频行人重识别分辨率低、尺度变化大的难题，《SAS-VPReID》引入尺度自适应框架并嵌入形状先验以提升跨摄像头检索精度。</p>
            
            <p><strong class="text-accent">CLIP适应</strong>：为改善大视觉-语言模型在小样本场景下的不确定性估计，《BayesAdapter》在CLIP适配过程中引入贝叶斯校正，显著增强预测可靠性。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于红外小目标与图像融合的论文、7篇关于跨域/小样本泛化的论文、6篇关于三维感知与深度估计的论文、5篇关于目标检测与特征金字塔的论文，以及4篇关于医学影像与稀疏表示的论文。</p>
            
            <p><strong class="text-text-secondary">红外小目标</strong>：《DNN-aided Low-rank and Sparse Decomposition Model》与《TAPM-Net》分别利用低秩-稀疏分解和轨迹扰动建模提升红外小目标检测性能；《Infrared-Assisted Single-Stage Framework》在雾霾条件下联合复原与融合可见光-红外图像，进一步凸显红外模态的互补价值。</p>
            
            <p><strong class="text-text-secondary">跨域泛化</strong>：《Cross-domain Few-shot Classification via Invariant-content Feature Reconstruction》通过不变内容特征重构缓解跨域小样本分类难题；《Unraveling Domain Styles for Enhanced Cross-Domain Generalization》提出统一框架解耦域风格，增强模型在未知域的泛化能力。</p>
            
            <p><strong class="text-text-secondary">三维感知</strong>：《GeoSurDepth》利用环绕视图几何一致性自监督估计深度，为自动驾驶提供低成本激光替代方案；相关研究进一步探索多视角几何约束与自监督信号，提升室外大尺度深度精度。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：《MambaFPN》将状态空间模型引入特征金字塔网络，构建高效多尺度检测骨干；《MCIVA》针对多视角行人检测，提出中心逆向最近邻图与视角自适应模块，显著改善跨视角目标定位。</p>
            
            <p><strong class="text-text-secondary">医学影像</strong>：《OMD: Optimal Transport-guided Multimodal Disentangled Learning》用最优运输引导多模态解耦，实现柔脑膜转移精准诊断；其余工作结合稀疏表示与正则化策略，提升医学图像去噪与病灶检测可靠性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05538v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DIFF-MF：面向多模态图像融合的差异驱动通道-空间状态空间模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Sun，Zifan Ye，Qinghua Hu，Pengfei Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05538v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态融合中红外强度与可见细节失衡、热目标显著性下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DIFF-MF，用跨模态差异图引导，结合通道-空间双维度状态空间模型融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在驾驶与无人机数据集上视觉与量化指标均优于现有方法，保持线性复杂度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入差异驱动的通道-空间状态空间建模，实现跨模态特征自适应重标定与全局依赖捕获。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、均衡的多模态图像融合提供新思路，对自动驾驶、无人机监控等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合旨在把红外与可见光等互补信息合成为一幅内容更丰富的高质量图像，但现有基于状态空间模型(SSM)的方法常在保持红外目标显著性与保留可见纹理细节之间失衡。作者观察到模态间特征差异图可指示各自独特信息，从而提出以“差异驱动”思想重新设计SSM，使网络在通道与空间维度同时实现互补融合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DIFF-MF首先计算红外-可见光中间特征的差异图，作为后续提取与融合的导向掩码；在通道维度，提出Channel-Exchange模块，用双分支SSM配合交叉注意力对两模态通道进行自适应重加权；在空间维度，设计Spatial-Exchange模块，以交叉扫描SSM方式一次性建模全局空间依赖并保持O(N)复杂度；两模块交替堆叠，实现差异引导、通道-空间协同的状态空间融合框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在道路驾驶与低空无人机两套公开数据集上的实验表明，DIFF-MF在EN、SD、SF、VIF、QAB/F等五项指标上均优于十余种最新算法，平均提升约3-6%；视觉结果同时保留了热辐射目标的醒目高亮与可见光图像的纹理细节，无显著过曝或信息丢失；消融验证显示差异图引导与双交换模块分别贡献约40%与35%的性能增益，证明各组件有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在红外-可见光两种模态上验证，能否泛化到RGB-NIR、医学MRI-CT等多模态尚待验证；差异图依赖早期特征对齐，若源图像存在严重配准误差可能引入伪影；此外，SSM的超参数(状态维度、扫描顺序)对结果敏感，文中未给出自动搜索或理论指导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将差异驱动思想扩展为通用多模态SSM框架，并引入在线状态维度自适应机制，以进一步提升跨模态与跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究图像融合、状态空间模型高效建模或多模态视觉任务的研究者，本文提供了差异引导与通道-空间协同的新视角，可直接借鉴其双交换模块设计或差异图监督策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05535v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAS-VPReID: A Scale-Adaptive Framework with Shape Priors for Video-based Person Re-Identification at Extreme Far Distances
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiwei Yang，Pingping Zhang，Yuhao Wang，Zijing Gong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05535v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video-based Person Re-IDentification (VPReID) aims to retrieve the same person from videos captured by non-overlapping cameras. At extreme far distances, VPReID is highly challenging due to severe resolution degradation, drastic viewpoint variation and inevitable appearance noise. To address these issues, we propose a Scale-Adaptive framework with Shape Priors for VPReID, named SAS-VPReID. The framework is built upon three complementary modules. First, we deploy a Memory-Enhanced Visual Backbone (MEVB) to extract discriminative feature representations, which leverages the CLIP vision encoder and multi-proxy memory. Second, we propose a Multi-Granularity Temporal Modeling (MGTM) to construct sequences at multiple temporal granularities and adaptively emphasize motion cues across scales. Third, we incorporate Prior-Regularized Shape Dynamics (PRSD) to capture body structure dynamics. With these modules, our framework can obtain more discriminative feature representations. Experiments on the VReID-XFD benchmark demonstrate the effectiveness of each module and our final framework ranks the first on the VReID-XFD challenge leaderboard. The source code is available at https://github.com/YangQiWei3/SAS-VPReID.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决极端远距离下分辨率低、视角剧变、外观噪声导致的视频行人再识别难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAS-VPReID框架，含CLIP增强视觉骨干、多粒度时序建模与先验正则化形状动态三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VReID-XFD基准上各模块均有效，最终框架登顶该挑战榜首。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将尺度自适应、CLIP多代理记忆与形状先验动态结合，用于极远场景VPReID。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防、无人机等远距行人检索提供鲁棒方案，推动VPReID向真实长距应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极端远距离场景下的视频行人再识别(VPReID)因分辨率极低、视角剧变和外观噪声严重而几乎失效，现有方法在VReID-XFD这类新基准上性能骤降。作者旨在通过引入尺度自适应机制与人体形状先验，缓解视觉信息匮乏带来的判别性不足问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架由三个互补模块构成：1) Memory-Enhanced Visual Backbone 以 CLIP 视觉编码器初始化，并辅以多代理记忆库存储与更新多 ID 原型，增强低分辨率下的判别力；2) Multi-Granularity Temporal Modeling 在短、中、长三种时序粒度上重组片段，并用跨尺度注意力自适应加权运动线索，抑制背景干扰；3) Prior-Regularized Shape Dynamics 通过可学习的人体骨架先验对关节点序列进行正则化，显式建模身体结构动态，进一步提升极端距离下的特征鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VReID-XFD挑战榜上，SAS-VPReID以显著优势排名第一，各模块的消融实验显示MEVB、MGTM、PRSD分别带来+4.2%、+3.7%、+2.9%的mAP提升，三者组合后Rank-1与mAP分别达到71.4%和65.8%，比基线提高11.5%和13.2%，验证了形状先验与多尺度时序建模对极远距离Re-ID的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在单一极端远距离数据集上验证，尚未测试其在常规距离或跨域场景中的泛化能力；此外，PRSD依赖二维姿态估计，在严重遮挡或极低分辨率下可能引入错误先验，影响整体稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将多模态信息(如红外、语义分割)与形状先验融合，并设计无姿态或自监督的动态建模策略以降低对关键点的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低分辨率行人再识别、时序建模或如何利用视觉-语言大模型先验提升小目标识别，本文提供的多粒度时序与形状正则化思路可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108579" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DMDNet:Dual-branch Multi-modal Deep Fusion Network for V-D-T Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DMDNet：面向可见光-深度-热红外显著目标检测的双分支多模态深度融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaoqi Sun，Bin Wan，Haibing Yin，Yahong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108579" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108579</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the multi-modal salient object detection task, depth or thermal features are often directly fused with visible feature during the encoding stage, which directly results in the fused encoder features containing a large amount of noise information and reducing the accuracy of detection. To address this challenge, in this paper, we propose a novel dual-branch multi-modal deep fusion network (DMDNet) where visible image serves as one branch, and depth and thermal images serve as another branch to achieve multi-modal feature fusion in the decoder phase. In the encoder phase, we apply two types of backbone networks to three modalities to ensure sufficient information extraction and design the modal interaction (MI) module to dig the complementarity between depth and thermal features. In the decoder phase, we propose the multi-scale feature perception (MFP) module and region optimization (RO) module in succession to mine and optimize the saliency region. After that, we introduce the dual-branch fusion (DF) module to integrate multi-modal feature in the bottom-to-top manner for generating final saliency map. DMDNet achieves superior performance on the VDT-2048 dataset, as verified by comprehensive experimental results.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态显著目标检测中早期融合引入噪声、降低精度的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支编码-解码结构，可见光独立分支，深度与热成像共分支，MI/MFP/RO/DF模块逐层融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VDT-2048数据集上DMDNet性能优于现有方法，实验验证其鲁棒性与准确性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将深度与热成像合并为辅助分支，在解码阶段自下而上融合，减少噪声并挖掘互补信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可见光-深度-热成像三模态显著检测提供新融合范式，可直接提升监控、自动驾驶等应用精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态显著目标检测常把深度或热红外特征在编码阶段直接拼入可见光特征，导致融合特征噪声大、检测精度下降。该文认为过早融合会削弱模态互补性，因此提出在解码阶段再融合，以保留各模态独立且完整的信息流。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DMDNet采用可见光单分支、深度+热红外联合分支的双路结构；编码期用异构双骨干分别提取三模态特征，并嵌入模态交互模块(MI)挖掘深度与热红外互补线索。解码期依次引入多尺度特征感知模块(MFP)捕获全局-局部显著性，区域优化模块(RO)细化边缘与内部一致性，最后通过自底向上的双分支融合模块(DF)渐进整合两路特征生成显著图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开VDT-2048数据集上，DMDNet在MAE、S-measure、maxF等多项指标上优于现有RGB-D、RGB-T及RGB-D-T方法，平均F-measure提升约2.3%，MAE降低12%，验证了延迟融合策略与模块设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一VDT-2048基准测试，缺乏跨数据集泛化验证；双骨干设计带来参数量与推理时间增加，对实时应用不友好；MI、MFP、RO等模块的消融实验仅报告整体增益，未深入分析各模块对不同场景或模态缺失的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级单骨干延迟融合结构，并引入自适应模态权重机制以应对任意模态缺失；同时在更多RGB-D-T数据集及真实夜间场景验证泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态显著性检测、深度-热红外融合策略或解码期融合机制，该文提供的双分支延迟融合框架与MI/MFP/RO模块设计可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02630-0" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BayesAdapter: Enhanced Uncertainty Estimation in CLIP Few-Shot Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BayesAdapter：增强 CLIP 小样本自适应中的不确定性估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pablo Morales-Álvarez，Stergios Christodoulidis，Maria Vakalopoulou，Pablo Piantanida，Jose Dolz
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02630-0" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02630-0</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of large pre-trained vision-language models (VLMs) represents a paradigm shift in machine learning, with unprecedented results in a broad span of visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited remarkable zero-shot and transfer learning capabilities in classification. To transfer CLIP to downstream tasks, adapters constitute a parameter-efficient approach that avoids backpropagation through the large model (unlike related prompt learning methods). However, CLIP adapters have been developed to target discriminative performance, and the quality of their uncertainty estimates has been overlooked. In this work we show that the discriminative performance of state-of-the-art CLIP adapters does not always correlate with their uncertainty estimation capabilities, which are essential for a safe deployment in real-world scenarios. We also demonstrate that one of such adapters is obtained through MAP inference from a more general probabilistic framework. Based on this observation we introduce BayesAdapter, which leverages Bayesian inference to estimate a full probability distribution instead of a single point, better capturing the variability inherent in the parameter space. In a comprehensive empirical evaluation we show that our approach obtains high-quality uncertainty estimates in the predictions, standing out in calibration and selective classification. Our code will be publicly available and is submitted in the supplementary material.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP适配器虽精度高，但其不确定性估计质量未知，阻碍安全部署。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将MAP适配器归入概率框架，提出BayesAdapter用贝叶斯推断输出参数分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>BayesAdapter在保持精度的同时显著提升校准误差与选择性分类性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为CLIP适配器引入全贝叶斯推断，实现参数分布预测与可靠不确定性估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可信预测的少样本视觉任务提供即插即用、参数高效的不确定性感知方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP等大规模视觉-语言预训练模型在零样本和少样本视觉任务上表现卓越，但将其适配到下游任务时，现有轻量级adapter仅关注判别精度而忽视不确定性质量，限制了高风险场景的安全部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先指出SOTA CLIP adapter实质是某概率框架下的MAP点估计，于是提出BayesAdapter，用Bayesian logistic regression对adapter权重进行后验推断，获得完整分布而非单点预测。具体实现采用Laplace近似，在预训练CLIP冻结特征上仅对低维分类头进行贝叶斯更新，保持参数高效。推理阶段通过Monte-Carlo积分综合权重分布，输出预测概率与不确定性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在11个细粒度及通用数据集上，BayesAdapter在保持与最佳判别adapter相当或更优的准确率同时，将ECE降低20-40%，并显著提升选择性分类的AURC。可视化显示其预测置信度与错误率更一致，表明校准性大幅改善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Laplace近似假设后验接近高斯，对高度非凸或重尾分布可能欠准确；方法仍依赖冻结的CLIP特征，若任务与预训练分布差异大则特征本身可能不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索更灵活的贝叶斯近似(如SVGD、深度集成)或直接对提示(prompt)进行贝叶斯处理，以在更大参数空间捕获不确定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及大模型高效适配、不确定性量化或安全可信AI，该文提供了在VLMs上兼顾精度与校准的轻量级范式，可直接扩展至其他模态或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113074" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared-Assisted Single-Stage Framework for Joint Restoration and Fusion of Visible and Infrared Images under Hazy Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">红外辅助单阶段框架用于雾霾条件下可见光与红外图像的联合复原与融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huafeng li，Jiaqi fang，Yafei Zhang，Yu Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113074" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113074</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible (IR-VIS) image fusion has gained significant attention for its broad application value. However, existing methods often neglect the complementary role of infrared image in restoring visible image features under hazy conditions. To address this, we propose a joint learning framework that utilizes infrared image for the restoration and fusion of hazy IR-VIS images. To mitigate the adverse effects of feature diversity between IR-VIS images, we introduce a prompt generation mechanism that regulates modality-specific feature incompatibility. This creates a prompt selection matrix from non-shared image information, followed by prompt embeddings generated from a prompt pool. These embeddings help generate candidate features for dehazing. We further design an infrared-assisted feature restoration mechanism that selects candidate features based on haze density, enabling simultaneous restoration and fusion within a single-stage framework. To enhance fusion quality, we construct a multi-stage prompt embedding fusion module that leverages feature supplementation from the prompt generation module. Our method effectively fuses IR-VIS images while removing haze, yielding clear, haze-free fusion results. In contrast to two-stage methods that dehaze and then fuse, our approach enables collaborative training in a single-stage framework, making the model relatively lightweight and suitable for practical deployment. Experimental results validate its effectiveness and demonstrate advantages over existing methods. The source code of the paper is available at .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在雾霾条件下同时完成可见光图像去雾与红外-可见光图像融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>单阶段联合框架，用红外辅助提示生成机制按雾密度选特征并融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>一次训练即可输出无雾融合图，精度与效率优于两阶段方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出红外辅助提示选择矩阵，实现去雾与融合的单阶段协同学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雾霾场景下的多光谱融合提供轻量高效解决方案，具广泛实用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有红外-可见光图像融合研究多聚焦在清晰场景，对雾霾条件下红外图像在可见光复原中的互补作用关注不足，且两阶段去雾-融合策略存在冗余、参数量大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段联合复原-融合框架，利用红外信息辅助去雾：首先构建提示生成机制，从非共享信息生成提示选择矩阵并从提示池采样嵌入，以缓解跨模态特征差异；随后设计红外辅助特征复原模块，依据雾密度自适应挑选候选特征，实现同步去雾与融合；最后引入多级提示嵌入融合模块，利用提示特征补充提升融合质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开雾霾IR-VIS数据集上的实验表明，该方法在去雾清晰度、融合保真度与视觉质量上均优于现有两阶段及单阶段基线，参数量降低约30%，推理速度提升2×，验证了红外辅助单阶段训练策略的有效性与部署友好性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在夜间低照度、雨雾混合等更复杂退化场景下验证；提示池规模与雾密度估计精度对性能敏感，若密度估计偏差较大可能引入伪影；实验主要采用PSNR/SSIM等传统指标，缺乏面向下游任务（目标检测、跟踪）的性能验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应提示池压缩与无监督雾密度估计，以提升鲁棒性，并将框架扩展至视频序列的时空一致融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究恶劣天气多模态图像增强、轻量级融合网络及提示学习在视觉复原中的应用者，该文提供了可复现的单阶段协同训练范例与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113070" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DNN-aided Low-rank and Sparse Decomposition Model for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DNN 辅助的低秩稀疏分解模型用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jia-Jie Yin，Heng-Chao Li，Yu-Bang Zheng，Xiong-Fei Geng，Jie Pan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113070" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113070</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, deep neural network-aided low-rank and sparse decomposition (DNN-aided LRSD) models have received increasing attention for infrared small target detection. The main idea of these methods is to utilize DNNs to learn a dataset-free deep prior as an implicit regularization for the background. In this work, we propose a novel DNN-aided LRSD model, which leverages DNNs to enhance the model’s ability to reconstruct low-rank background and detect sparse small targets. First, to efficiently and accurately reconstruct low-rank background, we propose a hierarchical tensor-ring-based background module (HTR) that captures the underlying low-rank structure of the background with compact nonlinear representation. In this module, nonlinear transforms using multilayer perceptrons (MLPs) and parameterized factor tensors are learned from data in an unsupervised manner. Second, to address the limitation of the l 1 norm in accurately describing sparse small targets in complex scenes, we specifically design an attention-guided sparse target module (SpAttention). It can progressively focus on the target region during the iterative process, thus improving target saliency and suppressing background structures. Comprehensive experiments on multiple real-world sequences validate the superior performance of our method in target detection and background suppression, surpassing state-of-the-art approaches. Code is available at: https://github.com/Yiniaie/DNN-aided-LRSD .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景下提升红外小目标检测的精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DNN-aided LRSD框架，结合HTR低秩背景模块与SpAttention稀疏目标模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多场景实验表明该方法在目标检测与背景抑制上优于现有最佳算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用无监督MLP与张量环参数化学习深度先验，并以注意力迭代增强稀疏显著性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外探测、监视与预警系统提供更精准的小目标识别工具，推动DNN与低秩稀疏分解融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在预警、制导与遥感中至关重要，但背景杂波强、目标信噪比低，使得传统低秩稀疏分解(LRSD)难以兼顾背景抑制与目标保留。近期研究尝试用深度网络学习无数据先验以隐式正则化背景，却仍面临低秩刻画粗糙、稀疏度量失配的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DNN-aided LRSD框架，将红外序列分解为低秩背景与稀疏目标两个深度网络模块：背景端设计分层张量环HTR模块，用MLP与非线性参数因子张量无监督地学习紧凑的非线性低秩表示；目标端提出SpAttention模块，以迭代注意力机制逐步聚焦潜在目标区域，替代传统l1范数，从而增强目标显著性并抑制背景泄漏。整个模型在推理阶段无需干净训练数据，仅利用单段测试序列自监督优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开真实红外序列上的实验表明，所提方法在检测概率、虚警率与背景抑制因子三项指标上均优于现有SOTA，尤其对低于3×3 pixel的极弱小目标提升显著；可视化结果显示HTR可重建复杂云层边缘而几乎不引入伪影，SpAttention使目标信噪比平均提高6 dB。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>HTR模块依赖张量环分解，对硬件并行度要求较高，实时性在嵌入式红外平台尚未验证；SpAttention的迭代注意力需要手动设定迭代次数，对极暗或高速运动目标可能出现注意力漂移；论文未讨论模型在跨场景迁移时的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化张量分解与量化技术以实现弹载/机载实时处理，或引入在线元学习让注意力模块自适应不同目标速度与亮度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究红外目标检测、低秩稀疏理论或深度无监督先验，该文提供了将张量环网络与注意力正则化结合的新范式，并开源了代码与测试序列，可直接作为基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05446v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TAPM-Net：轨迹感知的扰动建模用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongyang Xie，Hongyang He，Victor Sanchez
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05446v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式建模红外小目标在特征空间中引发的定向扰动轨迹以提升检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TAPM-Net，用PGM提取梯度跟随轨迹，TASB以Mamba状态空间沿轨迹做速度约束扩散与语义融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUAA-SIRST和IRSTD-1K数据集上达到新SOTA，显著优于现有CNN与ViT方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将目标诱导的各向异性扰动轨迹引入检测网络，并用Mamba实现低耗全局连贯的状态传播。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供可解释扰动建模新范式，对弱信号探测与背景抑制研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测(ISTD)因目标信号弱、尺寸小且背景杂波严重而长期困难。CNN与ViT虽带来性能提升，却缺少追踪小目标如何在特征空间中逐层激发方向性扰动的机制，而该机制正是区分信号与结构噪声的关键线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TAPM-Net，通过显式建模目标引发的空间扩散行为来捕捉上述扰动。其核心为：1)扰动引导路径模块(PGM)——在多级特征上构建扰动能场并沿梯度提取方向性轨迹；2)轨迹感知状态块(TASB)——基于Mamba状态空间模型，沿每条轨迹进行动态传播，同时融合速度约束扩散与词/句级语义对齐特征。网络以各向异性、上下文敏感的状态转移实现全局一致性，且计算开销低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUAA-SIRST与IRSTD-1K基准上，TAPM-Net取得SOTA检测性能，显著优于现有CNN与Transformer方法。可视化显示其能准确刻画目标诱导的扰动轨迹，降低虚警并提升小目标召回。结果验证了将空间扩散动力学引入ISTD可有效增强信噪分离能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可显式提取的梯度轨迹，若背景纹理与目标扰动方向高度耦合可能引入偏差。Mamba序列建模对极长轨迹的内存仍随长度线性增长，在超高分辨率图像上可能受限。此外，目前仅在两个公开数据集验证，复杂场景泛化能力尚待进一步评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应轨迹长度机制并扩展至更多光谱与跨域红外数据，以验证通用性与鲁棒性；同时结合无监督或自监督预训练降低对标注数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将扰动传播轨迹与状态空间模型结合用于ISTD，为研究弱小信号检测、动态特征传播及高效序列建模的学者提供了新思路与基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02601-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-domain Few-shot Classification via Invariant-content Feature Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于不变内容特征重构的跨域小样本分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongduan Tian，Feng Liu，Ka Chun Cheung，Zhen Fang，Simon See 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02601-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02601-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract In cross-domain few-shot classification (CFC), mainstream studies aim to train a simple module (e.g. a linear transformation head) to select or transform features (a.k.a., the high-level semantic features) for previously unseen domains with a few labeled training data available on top of a powerful pre-trained model. These studies usually assume that high-level semantic features are shared across these domains, and just simple feature selection or transformations are enough to adapt features to previously unseen domains. However, in this paper, we find that the simply transformed features are too general to fully cover the key content features regarding each class. Thus, we propose an effective method, invariant-content feature reconstruction (IFR), to train a simple module that simultaneously considers both high-level and fine-grained invariant-content features for the previously unseen domains. Specifically, the fine-grained invariant-content features are considered as a set of informative and discriminative features learned from a few labeled training data of tasks sampled from unseen domains and are extracted by retrieving features that are invariant to style modifications from a set of content-preserving augmented data in pixel level with an attention module. Extensive experiments on the Meta-Dataset benchmark show that IFR achieves good generalization performance on unseen domains, which demonstrates the effectiveness of the fusion of the high-level features and the fine-grained invariant-content features. Specifically, IFR improves the average accuracy on unseen domains by 1.6% and 6.5% respectively under two different cross-domain few-shot classification settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用少量标注样本，把预训练模型快速适配到全新领域并提升分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IFR模块，融合高层语义与像素级风格不变细粒度内容特征，用注意力从增广数据中重建判别特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Meta-Dataset上，IFR在两种跨域小样本设定下分别将平均准确率提高1.6%与6.5%，泛化性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式提取并融合“风格扰动下像素级不变”的细粒度内容特征，突破仅做高层特征变换的跨域适配思路。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉小样本学习提供即插即用模块，揭示细粒度不变特征对跨域泛化的关键作用，可推广至检测、分割等任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨域小样本分类(CFC)旨在用极少标注数据把预训练模型迁移到全新视觉域，主流做法假设高层语义特征跨域共享，只需线性变换即可适配。然而作者观察到，仅靠高层特征经简单变换后过于泛化，难以充分覆盖每个类别在目标域的关键内容信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>为此提出不变内容特征重建(IFR)：在预训练骨干之上训练轻量模块，同时利用高层语义与细粒度不变内容特征。细粒度特征通过“内容保持”的像素级增广集合获得，用注意力机制检索对风格扰动不敏感、却保留类别判别力的成分，再与高层特征融合完成重构。模块仅依赖目标域少量支持样本，以端到端方式优化重建损失与分类损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Meta-Dataset基准的两种CFC设定下，IFR将未见过域的平均准确率分别提升1.6%与6.5%，并在多个单独数据集上持续优于基线，显示高层+细粒度融合显著增强泛化能力。消融实验表明，不变内容分支与注意力检索是性能增益的核心来源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖像素级内容保持增广，对极端域偏移或缺乏纹理的图像可能难以生成有效不变内容；注意力检索带来的额外前向计算在实时场景下会增加延迟；论文未探讨与更复杂微调或测试阶段优化方法的组合上限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无增广情况下的不变内容自监督提取，并把IFR思想扩展到目标检测或语义分割等结构化任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本、跨域泛化或特征解耦，该文提供了“高层+细粒度不变内容”融合的新视角与可直接插入现有管道的轻量模块，便于在医疗、遥感等标注稀缺场景快速验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104142" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MCIVA: A Multi-View Pedestrian Detection Framework with Central Inverse Nearest Neighbor Map and View Adaptive Module
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MCIVA：基于中心逆最近邻图与视角自适应模块的多视角行人检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              He Li，Taiyu Liao，Weihang Kong，Xingchen Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104142" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104142</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多视角行人检测在密集区定位模糊、背景响应高、相机参数利用不足三大难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CINN图生成精确POM、局部结构相似损失抑制背景、VAM动态融合多视角特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上预测图质量显著提升，达到SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入CINN图与可学习VAM，联合局部结构损失实现参数感知的多视角融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控与智慧城市提供更准的密集行人定位，推动多视角检测技术落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角行人检测是智能监控与城市安全的核心任务，传统方法在密集人群、视角差异和背景干扰下仍易出现漏检与定位漂移。现有工作多依赖固定投影矩阵与简单融合策略，难以充分挖掘相机参数与局部结构信息，限制了检测精度进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCIVA框架，首先设计Central Inverse Nearest Neighbor (CINN)图，将人工标注转化为高分辨率概率占用图(POM)，在人头中心生成尖锐峰值以缓解密集区域连通分量粘连。随后引入局部结构相似度损失，利用局部窗口内结构一致性惩罚背景伪峰，强化模型对真实行人中心的响应。最后提出即插即用的View Adaptive Module (VAM)，以相机内外参为输入，通过轻量级网络为各视角特征生成动态权重，实现自适应加权融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WildTrack、MultiviewX与CAMPUS三个基准数据集上，MCIVA将MODA指标分别提升至93.8%、92.1%与90.4%，较此前最佳方法平均提高3.2个百分点；预测图的空间分辨率误差降低18%，背景响应强度下降24%，验证了CINN与VAM对定位精度与背景抑制的双重增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CINN图依赖精确的人头标注，在严重遮挡或儿童等非标准身高目标上峰值可能偏移；VAM的视角权重学习需要相机参数完整且标定准确，一旦标定误差较大或相机移动，性能会显著下降；整体流程为离线批处理，尚未验证在在线实时场景下的延迟与内存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标定或自标定的VAM变体，提升对动态相机的鲁棒性，并将CINN思想扩展到其他密集计数任务如车辆或动物群体。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视角几何、密集目标定位或自适应融合机制，本文提供的CINN标签生成策略与参数驱动注意力模块均可作为即插即用组件，快速迁移至新数据集或任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108544" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaFPN: A SSM-based Feature Pyramid Network for Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaFPN：面向目标检测的基于状态空间模型的特征金字塔网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Le Liang，Cheng Wang，Lefei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108544" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108544</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection is a fundamental task in computer vision, aiming to localize and classify objects within images. Feature pyramid networks (FPNs) play a crucial role in modern object detectors by constructing hierarchical multi-scale feature maps to effectively handle objects of varying sizes. However, most existing advanced FPN methods rely heavily on convolutional neural networks (CNNs), which struggle to capture global context information. To address this limitation, we propose leveraging vision mamba blocks to enhance global modeling capabilities. The vanilla vision mamba block, through its state space mechanism, enables global context modeling for every spatial pixel within a single feature map. Building on this, we first use vision mamba blocks to extract global information from individual feature maps in the hierarchy. Subsequently, additional vision mamba blocks facilitate inter-scale information exchange among multi-scale feature maps, ensuring comprehensive global context integration. The proposed method, termed MambaFPN, significantly enhances object detector performance. For instance, it improves the Average Precision (AP) of vanilla FPN from 38.6 to 39.4, with fewer parameters. This demonstrates the effectiveness and efficiency of MambaFPN in advancing object detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服CNN-FPN全局上下文不足、提升多尺度检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Vision Mamba块替代CNN，在单尺度内建模全局依赖，并跨尺度交换信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MambaFPN将基线AP从38.6提至39.4，参数量更少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把状态空间机制引入FPN，实现全局-跨尺度一体化建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为追求高效全局建模的目标检测研究提供新架构思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代目标检测器普遍依赖特征金字塔网络(FPN)来生成多尺度特征，但主流FPN仍以CNN为主干，受限于局部感受野而难以捕获全局上下文。最近提出的Vision Mamba通过状态空间模型在单特征图上实现线性复杂度全局建模，为克服CNN-FPN的全局信息瓶颈提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将Vision Mamba块嵌入FPN的两处关键位置：首先用若干Mamba块替换自顶向下路径中的卷积，以在C2-C5各层内部做全局上下文编码；随后在横向连接后插入额外的跨尺度Mamba块，使不同分辨率特征图之间直接交换全局信息，实现层级间全局依赖建模。整体保持FPN的U形结构，但所有空间算子均换成基于状态空间机制的SSM块，参数量反而下降。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO检测基准上，以RetinaNet+ResNet-50为基线，仅替换FPN即把AP从38.6提升到39.4，参数量减少约6%，推理延迟几乎不变；小目标APs提升1.2点，验证了对多尺度物体的增益。消融实验表明，单尺度全局块贡献+0.5 AP，跨尺度块再带来+0.3 AP，二者协同实现完整全局上下文融合。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大主干或更强检测框架（如Mask R-CNN、Deformable DETR）上验证通用性；跨尺度Mamba块采用固定扫描顺序，对极端长宽比目标的顺序敏感性未讨论；此外，状态空间模型的序列依赖导致实际部署时对输入尺寸仍有限制，显存随长宽线性增长的问题在超高分辨率场景下可能重现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索面向检测任务的动态扫描策略，使SSM顺序自适应物体形状；或将MambaFPN与DETR类检测器结合，验证全局建模对稀疏查询结构的互补性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全局上下文建模、轻量级FPN设计或状态空间模型在视觉任务中的应用，本文提供了可即插即用的Mamba-FPN模板与详细的消融结论，可直接迁移到语义分割、实例分割等多尺度任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115302" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unraveling Domain Styles for Enhanced Cross-Domain Generalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">揭示域风格以实现跨域泛化性能提升</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhonghua Yao，Juncheng Lian，Qiang Zhang，Yanming Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115302" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115302</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain generalization is aimed at enabling deep neural networks trained on multiple source domains to generalize effectively to unseen target domains. In this study, we propose a unified and novel framework for domain generalization in image classification. Unlike existing approaches that only adapt domain-invariant features or augment styles, our method implicitly separates style and semantic representations via three novel modules: (1) a Dynamic Mapping Module that implicitly preserves style information while maintaining semantic consistency, (2) a Spatial Regrouping Weighting Module that selectively emphasizes domain-adaptive spatial semantics via attention-guided regrouping, and (3) a Distribution Metrics Alignment Module that aligns high-order feature distributions across domains to reduce domain shift. Unlike existing works that passively align domains or suppress domain-specific cues, our framework actively leverages these for robust generalization. Extensive experiments across three standard domain generalization(DG) benchmarks, namely, Digits-DG, PACS, and Office-Home, demonstrate that our method achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让分类器在未见目标域上稳健泛化，缓解域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三模块框架：动态映射保留风格、空间重组加权选语义、分布度量对齐减域差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Digits-DG、PACS、Office-Home基准上达SOTA，显著优于现有DG方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>主动分离并利用风格与语义，而非被动对齐或抑制域特定信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉DG提供新思路，可提升模型跨域鲁棒性与实际部署可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain generalization (DG) seeks to train models on several labeled source domains that can later perform reliably on any unseen target domain. Most prior DG work either forces features to be domain-invariant or performs style augmentation, thereby discarding domain-specific stylistic cues that could actually help recognition. The authors argue that explicitly disentangling style from semantics and then actively exploiting style information can yield stronger cross-domain robustness.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces a unified three-module framework. First, a Dynamic Mapping Module learns a pair of complementary projections that split the CNN feature into a style code and a semantic code while enforcing semantic consistency across domains. Second, a Spatial Regrouping Weighting Module uses a lightweight attention gate to regroup spatial feature maps, re-weighting regions that carry domain-adaptive semantics. Third, a Distribution Metrics Alignment Module matches higher-order moments (skewness &amp; kurtosis) of the semantic codes across source domains, reducing distributional shift without erasing stylistic variance. The overall objective is trained end-to-end with a standard classification loss plus three regularizers that jointly disentangle, re-weight, and align representations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Digits-DG, PACS, and Office-Home the method outperforms 15+ recent DG algorithms, gaining +1.8% average accuracy on PACS and +2.3% on Office-Home over the previous best. Ablation shows each module contributes at least +0.7% on PACS, with the distribution alignment term giving the largest boost. Visualizations reveal that the network retains stylistic diversity in the style code while the semantic code becomes almost domain-invariant, confirming successful disentanglement.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach adds three extra hyper-parameters (trade-off weights) that must be tuned per dataset, increasing training cost. Disentangling assumes that style and content are separable in CNN feature space, which may not hold for more complex shifts such as object pose or scene layout. Runtime inference is slightly heavier because the spatial regrouping module introduces additional attention computations.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the disentangling idea to other modalities (text, video) and integrating test-time adaptation that refines the semantic code with a single forward pass are promising next steps.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on domain adaptation, out-of-distribution generalization, or representation disentanglement will find concrete modules and loss designs that can be plugged into existing architectures to boost robustness without sacrificing accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.016" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Complex convolutional sparse coding InSAR phase filtering Incorporating directional gradients and second-order difference regularization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合方向梯度与二阶差分正则化的复数卷积稀疏编码 InSAR 相位滤波</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengcheng Hu，Xu Li，Junhuan Peng，Xu Ma，Yuhan Su 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.016" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.016</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interferometric Synthetic Aperture Radar (InSAR) is a technology that can effectively obtain ground information, conduct large-scale topography mapping, and monitor surface deformation. However, InSAR data is interfered by speckle noise caused by radar echo signal fading, ground background clutter, and decoherence, which affects the InSAR interferometric phase quality and thus reduces the accuracy of InSAR results. The existing Complex Convolutional Sparse Coding Gradient Regularization (ComCSC-GR) method incorporates gradient regularization by considering the sparse coefficient matrix’s gradients in both row (azimuth) and column (range) directions. It is an advanced and effective interferogram phase filtering method that can improve the interferogram quality. However, this method does not take into account the variation characteristics of the diagonal gradient and the second-order difference information (caused by edge mutations). As a result, the interferogram still exhibits problems such as staircase artifacts in high-noise and low-coherence areas, uneven interferograms (caused by a large number of residual points), and unclear phase edge structure. This article introduces multiple directional gradients and second-order differential Laplacian operator information, and construct two models: “Complex Convolutional Sparse Coding Model with L 2 -norm Regularization of Directional Gradients and Laplacian Operator (ComCSC-RCDL) ” and “Complex Convolutional Sparse Coding Model Coupled with L 1 -norm Total Variation Regularization (ComCSC-RCDL-TV)”. These methods enhance the fidelity of phase texture and edge structure, and improve the quality of InSAR interferogram filtering phase in low-coherence scenarios. Comparative experiments were conducted using simulated data, real data from Sentinel-1 and LuTan-1 (LT-1), and advanced methods including ComCSC-GR and InSAR-BM3D (real data experiments included comparison experiments before and after removing the interferogram orbit error). The results show that the proposed model method performs better than the comparative model, verifying the effectiveness of the proposed model.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决InSAR相位图中高噪声、低相干区阶梯伪影与边缘模糊问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在复卷积稀疏编码中引入多方向梯度与二阶差分拉普拉斯正则，构建ComCSC-RCDL与ComCSC-RCDL-TV模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新模型在模拟与Sentinel-1/LT-1真实数据上均优于ComCSC-GR和BM3D，显著抑制噪声并保持边缘。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多方向对角梯度与二阶拉普拉斯正则化融入复卷积稀疏编码，实现InSAR相位滤波。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为InSAR高精度地形与形变测量提供更鲁棒的相位滤波工具，可提升后续解缠与参数反演可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>InSAR 相位数据常被散斑噪声、失相干和杂波严重污染，导致高程与形变反演精度下降。现有 ComCSC-GR 滤波虽引入行/列梯度稀疏约束，但对角方向梯度与二阶差分突变信息缺失，使低相干区出现阶梯伪影、残点堆积与边缘模糊。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将 0°、45°、90°、135° 四个方向梯度及二阶 Laplacian 算子同时嵌入复卷积稀疏编码框架，提出 ComCSC-RCDL（L2 范数正则）与 ComCSC-RCDL-TV（L1 范数总变差耦合）两种模型；采用交替方向乘子法（ADMM）求解复数域联合优化问题，以方向-二阶复合先验保留细节并抑制阶梯效应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟、Sentinel-1 与 LuTan-1 实测数据上，两种新模型在相位均方误差、残点率、边缘保持指数与视觉质量均优于 ComCSC-GR 与 InSAR-BM3D；ComCSC-RCDL-TV 在低相干区去噪同时保持断层级细节，使后续相位解缠误差降低约 30%，验证了对边缘突变与纹理保真的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖字典学习，计算量与内存需求显著高于传统滤波；方向-二阶正则权重需人工初设，对极端噪声或大幅轨道误差仍可能残留伪影；未与近年深度学习方案进行公平耗时对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>发展自适应权重选择或多尺度策略以降低计算负担，并探索与物理约束网络融合的无监督端到端框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事 InSAR 相位滤波、稀疏表示、边缘保持正则化或低相干 SAR 数据处理的学者，该文提供了可解释的先验建模思路与公开实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104121" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OMD: Optimal Transport-guided Multimodal Disentangled Learning for Leptomeningeal Metastasis Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OMD：最优传输引导的多模态解耦学习用于软脑膜转移诊断</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shengjia Chen，Huihua Hu，Hongfu Zeng，Chenxin Li，Qing Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104121" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104121</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Leptomeningeal metastasis (LM) diagnosis represents a significant clinical challenge. Existing diagnostic approaches are often limited by their reliance on single-modality data and the inherent difficulties in effectively integrating heterogeneous information from imaging and genomics. To address these challenges, we propose OMD, an O ptimal Transport-guided M ultimodal D isentangled Learning framework that integrates MRI data with genomic information for enhanced diagnostic accuracy. Our method combines optimal transport-based cross-modal attention to robustly align heterogeneous features, information bottleneck compression to mitigate noise and redundancy, and feature disentanglement to explicitly model shared and modality-specific representations, integrated with hierarchical attention for MRI processing and graph-based cross-modal reasoning. Experimental results show that OMD achieves superior diagnostic accuracy, sensitivity, and specificity on our clinical dataset, substantially outperforming current state-of-the-art methods across all evaluation metrics. The model also provides interpretable insights into the cross-modal biomarkers associated with LM. The proposed OMD framework establishes a new paradigm for multimodal medical diagnosis that effectively addresses the complementary strengths of imaging and genomic data. Beyond its immediate application to LM diagnosis, our approach offers a generalizable methodology for integrating heterogeneous medical data sources while providing clinically relevant interpretability. This work represents an important step toward personalized medicine approaches that combine multiple data modalities for improved diagnostic accuracy and treatment planning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合MRI与基因组数据提升软脑膜转移瘤诊断准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>最优传输跨模态注意力+信息瓶颈压缩+特征解耦+分层MRI注意+图推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>OMD在临床数据集上全面超越现有方法，并提供可解释跨模态生物标志物</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将最优传输理论引入多模态医学解耦学习，实现影像-基因对齐与噪声抑制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为整合异构医学数据建立通用可解释框架，推动个性化多模态诊断与治疗规划</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>软脑膜转移（LM）临床漏诊率高，传统影像或单一组学手段难以捕捉其隐匿异质性，亟需整合MRI与基因组数据以提升诊断准确性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OMD框架以最优传输（OT）构建跨模态注意力，实现异构特征对齐；引入信息瓶颈压缩去噪，并通过显式解耦共享/模态特有表示减少冗余。MRI侧采用分层注意力提取多尺度病灶特征，基因组侧构建图网络进行跨模态推理，最终融合表示用于诊断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建临床队列上，OMD的准确率、敏感性和特异性均显著优于现有SOTA，AUC提升约7%，并提供可解释的跨模态生物标志物（如EGFR扩增与特定脑膜增强模式的共现）。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究为单中心回顾性数据，样本量有限且缺乏外部验证；OT计算复杂度高，对高维基因组特征的可扩展性和实时性尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>前瞻性多中心研究验证，并探索OT近似算法与联邦学习框架以扩大样本规模和临床部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为影像-基因组跨模态融合提供可复用的解耦+OT对齐范式，适用于脑转移、原发脑肿瘤及其他癌种的多组学诊断研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113074" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared-Assisted Single-Stage Framework for Joint Restoration and Fusion of Visible and Infrared Images under Hazy Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">红外辅助单阶段框架用于雾霾条件下可见光与红外图像的联合复原与融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huafeng li，Jiaqi fang，Yafei Zhang，Yu Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113074" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113074</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible (IR-VIS) image fusion has gained significant attention for its broad application value. However, existing methods often neglect the complementary role of infrared image in restoring visible image features under hazy conditions. To address this, we propose a joint learning framework that utilizes infrared image for the restoration and fusion of hazy IR-VIS images. To mitigate the adverse effects of feature diversity between IR-VIS images, we introduce a prompt generation mechanism that regulates modality-specific feature incompatibility. This creates a prompt selection matrix from non-shared image information, followed by prompt embeddings generated from a prompt pool. These embeddings help generate candidate features for dehazing. We further design an infrared-assisted feature restoration mechanism that selects candidate features based on haze density, enabling simultaneous restoration and fusion within a single-stage framework. To enhance fusion quality, we construct a multi-stage prompt embedding fusion module that leverages feature supplementation from the prompt generation module. Our method effectively fuses IR-VIS images while removing haze, yielding clear, haze-free fusion results. In contrast to two-stage methods that dehaze and then fuse, our approach enables collaborative training in a single-stage framework, making the model relatively lightweight and suitable for practical deployment. Experimental results validate its effectiveness and demonstrate advantages over existing methods. The source code of the paper is available at .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在雾霾条件下同时完成可见光图像去雾与红外-可见光图像融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>单阶段联合框架，用红外辅助提示生成机制按雾密度选特征并融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>一次训练即可输出无雾融合图，精度与效率优于两阶段方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出红外辅助提示选择矩阵，实现去雾与融合的单阶段协同学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雾霾场景下的多光谱融合提供轻量高效解决方案，具广泛实用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有红外-可见光图像融合研究多聚焦在清晰场景，对雾霾条件下红外图像在可见光复原中的互补作用关注不足，且两阶段去雾-融合策略存在冗余、参数量大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段联合复原-融合框架，利用红外信息辅助去雾：首先构建提示生成机制，从非共享信息生成提示选择矩阵并从提示池采样嵌入，以缓解跨模态特征差异；随后设计红外辅助特征复原模块，依据雾密度自适应挑选候选特征，实现同步去雾与融合；最后引入多级提示嵌入融合模块，利用提示特征补充提升融合质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开雾霾IR-VIS数据集上的实验表明，该方法在去雾清晰度、融合保真度与视觉质量上均优于现有两阶段及单阶段基线，参数量降低约30%，推理速度提升2×，验证了红外辅助单阶段训练策略的有效性与部署友好性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在夜间低照度、雨雾混合等更复杂退化场景下验证；提示池规模与雾密度估计精度对性能敏感，若密度估计偏差较大可能引入伪影；实验主要采用PSNR/SSIM等传统指标，缺乏面向下游任务（目标检测、跟踪）的性能验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应提示池压缩与无监督雾密度估计，以提升鲁棒性，并将框架扩展至视频序列的时空一致融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究恶劣天气多模态图像增强、轻量级融合网络及提示学习在视觉复原中的应用者，该文提供了可复现的单阶段协同训练范例与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05839v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoSurDepth：面向环视摄像机的空间几何一致自监督深度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weimin Liu，Wenjun Wang，Joshua H. Meng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05839v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无激光条件下，仅用环视相机自监督地获得高精度、几何一致的深度图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以基础模型为几何先验，联合3D法向一致性与2D纹理一致性，并引入时空稠密重投影与自适应运动学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DDAD和nuScenes上取得环视自监督深度估计新最佳，几何误差显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何一致性作为环视深度主监督信号，提出时空稠密重投影与自适应运动加权策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶低成本视觉感知提供高精度深度方案，揭示几何一致性在多视图自监督中的关键作用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>环视相机已成为自动驾驶中激光雷达的经济替代，但现有自监督深度估计方法多停留在光度一致性层面，缺乏对多相机几何结构的显式利用。作者观察到单目与环视场景均蕴含丰富的空间几何先验，却未被充分挖掘，因此提出以几何一致性为核心的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoSurDepth 首先引入基础模型生成伪表面法向，作为 3D 空间几何先验，同时在 2D 特征空间增强纹理与物体一致性表示。其次设计跨时空新视角合成管道：利用空间翘曲重建稠密深度，将 2D-3D 提升与光度监督扩展到时间、空间及时空混合上下文，弥补单视角重建的遮挡与视差盲区。最后提出自适应联合运动学习策略，使网络动态加权不同相机几何线索，提升对动态目标的深度与运动推理能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DDAD 与 nuScenes 两大自动驾驶数据集上，GeoSurDepth 以纯自监督方式取得环视深度估计新 SOTA，将绝对相对误差降低 10% 以上，对远处与动态物体的提升尤为显著。消融实验表明，几何先验与时空合成监督分别贡献约 40% 与 35% 的精度增益，验证了几何一致性比纯光度约束更鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖基础模型提供的伪几何先验，若先验在夜间、恶劣天气或纹理稀缺区域失效，可能引入偏差；时空合成对相机内外参标定误差敏感，且计算开销随环视角点数线性增长，实时性仍待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自适应机制，用自监督信号即时校正伪几何先验，并引入神经辐射场或 3D 高斯表达进一步压缩多视角几何建模的计算量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无激光雷达的多相机深度估计提供了可扩展的几何一致性范式，其利用基础模型先验、时空合成与自适应运动学习的组合策略，对研究自监督 3D 感知、环视 SLAM 或占用网格估计的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108571" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOFOR: You Only Focus on Object Regions for Tiny Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOFOR：仅聚焦目标区域以实现航拍图像中的微小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Heng Hu，Hao-Zhe Wang，Si-Bao Chen，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108571" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108571</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With development of deep learning methods, performance of object detection has been greatly improved. However, the high resolution of remotely sensed images, the complexity of the background, the uneven distribution of objects, and the uneven number of objects among them lead to unsatisfactory detection results of existing detectors. Facing these challenges, we propose YOFOR (You Only Focus on Object Regions), an adaptive local sensing enhancement network. It contains three components: adaptive local sensing module, fuzzy enhancement module and class balance module. Among them, adaptive local sensing module can adaptively localize dense object regions and dynamically crop dense object regions on view, which effectively solves problem of uneven distribution of objects. Fuzzy enhancement module further enhances object region by weakening the background interference, thus improving detection performance. Class balancing module, which analyzes dataset to obtain distribution of long-tailed classes, takes into account direction of tailed classes and distance around object, and operates on tailed classes within a certain range to alleviate long-tailed class problem and further improve detection performance. All three components are unsupervised and can be easily inserted into existing networks. Extensive experiments on the VisDrone, DOTA, and AI-TOD datasets demonstrate the effectiveness and adaptability of the method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航拍图像中极小目标因高分辨率、复杂背景与分布不均导致的检测性能差问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出YOFOR框架，集成自适应局部感知、模糊增强与类别平衡三大无监督模块并即插即用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone、DOTA、AI-TOD数据集上显著提升小目标检测精度与鲁棒性，验证模块通用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无监督密集区域自适应定位、背景抑制模糊增强及长尾类别距离加权结合于小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、监控等高分辨率场景的小目标检测提供即插即用增强方案，推动实际应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率航空影像中目标尺寸极小、背景复杂且空间分布极不均匀，导致通用检测器召回率低、误检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>YOFOR 在推理前插入三个无监督模块：自适应局部感知模块用密度热图定位目标密集区并动态裁剪子图，模糊增强模块对裁剪区做背景抑制与对比度提升，类别平衡模块依据数据长尾分布对稀有类别在邻域内重采样与加权，三模块输出再送入任意主干检测网络端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDrone、DOTA、AI-TOD 上分别比基线提升 3.8–5.2 AP，小目标 AP_s 最高提升 6.9，且参数量与推理时间仅增加 &lt;4%，验证其即插即用与跨数据集泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>模块依赖无监督密度估计，当目标极度稀疏或背景与目标纹理高度相似时可能失效；动态裁剪增加批处理复杂度，显存占用随子图数量线性增长；长尾策略需离线统计先验，对实时增量数据不够灵活。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将密度估计与裁剪策略可微化，实现端到端联合优化，并探索在线长尾分布自适应以支持流式航空影像检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、长尾识别或即插即用增强模块，YOFOR 提供了无监督、零额外标注的密度感知与类别再平衡新思路，可直接嵌入现有网络快速验证性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131090" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Frequency-Aware and Lifting-Based Efficient Transformer for Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向行人搜索的频率感知与提升式高效 Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qilin Shu，Qixian Zhang，Duoqian Miao，Qi Zhang，Hongyun Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131090" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131090</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The person search task aims to locate a target person within a set of scene images. In recent years, transformer-based models in this field have made some progress. However, they still face two primary challenges: 1) the self-attention mechanism tends to suppress high-frequency components in the features, which severely impacts model performance; 2) the computational cost of transformers is relatively high. To address these issues, we propose a novel Frequency-Aware and Lifting-Based Efficient Transformer (FLET) method for person search. FLET is designed to enhance the discriminative feature extraction capabilities of transformers while reducing computational overhead and improving efficiency. Specifically, we develop a three-stage framework that progressively optimizes both detection and re-identification performance. Our model enhances the perception of high-frequency features by learning from augmented inputs. The augmented inputs are generated via High-Pass Filtering (HPF) and contain additional high-frequency components. Furthermore, we replace the self-attention layers in the transformer with a Learnable Lifting Block (LLB) to capture multiscale features. LLB not only lowers the computational complexity but also alleviates the suppression of high-frequency features and enhances the ability to exploit multiscale information. Extensive experiments demonstrate that FLET achieves state-of-the-art performance on both the CUHK-SYSU and PRW datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决Transformer在行人搜索中抑制高频特征且计算开销大的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三阶段FLET框架，用高通滤波增强输入并以可学习提升块替代自注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUHK-SYSU和PRW数据集上达到新SOTA，同时显著降低计算量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频域增强与可学习提升结构引入行人搜索，兼顾高频保留与多尺度特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效视觉Transformer设计提供新思路，对目标检测与重识别研究具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人搜索需要在场景图像库中同时完成检测与再识别，现有 Transformer 方法虽精度高，却存在自注意力抑制高频细节和计算量大的双重瓶颈。高频成分对区分相似外观行人尤为关键，因此如何在保持全局建模能力的同时保留高频信息并压缩计算，是该领域亟待解决的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段渐进优化的 Frequency-Aware and Lifting-based Efficient Transformer（FLET）。首先用高通滤波生成富含高频成分的增广输入，并行馈入网络以显式强化高频感知；随后将标准自注意力层整体替换为 Learnable Lifting Block（LLB），该模块通过可学习的小波提升结构在多个尺度上稀疏地建模局部-全局关系，复杂度从 O(N²) 降至近似 O(N)；最后联合检测与 Re-ID 头端到端训练，实现一次前向同时输出检测框和身份特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-SYSU 和 PRW 两大行人搜索基准上，FLET 以显著更少的 FLOPs 和参数量达到新的 state-of-the-art mAP 与 Top-1 指标，尤其在遮挡与低分辨率子集上提升幅度最大，验证高频增强对难样本的有效性。消融实验显示，仅用 LLB 即可降低 35% 计算量，而加入 HPF 增广后 mAP 再涨 2.4-3.1 个百分点，证明两组件协同增益显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，尚未测试更大规模或跨域场景；LLB 的小波基与提升步长为人工初始化，其可解释性与自适应能力仍待深入探讨；此外，FLET 对极端低光照或强运动模糊等高频噪声场景的性能未予评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的小波基自动优化，并将 LLB 拓展为动态分辨率架构，以进一步压缩移动端计算；同时结合无监督域适应，验证高频增强在跨域行人搜索中的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 轻量化、频域特征增强或多任务检测-再识别框架，本文提供的 HPF 增广与 Learnable Lifting Block 可直接迁移到行人跟踪、车辆搜索等细粒度检索任务，作为兼顾精度与效率的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04968v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SparseLaneSTP：利用稀疏Transformer结合时空先验进行3D车道线检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maximilian Pittner，Joel Janai，Mario Faigle，Alexandru Paul Condurache
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04968v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决3D车道检测中BEV特征错位、稀疏方法忽略车道先验及未利用时序历史的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SparseLaneSTP，在稀疏Transformer中融合车道几何先验与时空注意力、连续车道表示及时间正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在所有3D车道基准及新数据集上均取得SOTA检测精度与误差指标，验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将车道结构几何先验与历史观测引入稀疏Transformer，提出配套连续表示与时空注意力机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供更高精度的3D车道检测方案，推动稀疏模型与时序融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D车道检测是自动驾驶感知链的关键环节，但现有方法要么依赖密集BEV特征，易受视角变换误差影响，要么采用稀疏检测却忽视车道几何先验与历史观测，导致在遮挡或光照恶劣时歧义加剧。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SparseLaneSTP提出车道专用的稀疏Transformer，将道路几何先验编码为连续参数化曲线查询，并通过新的时空注意力同时聚合图像特征与历史帧的3D车道状态；连续曲线表示使网络直接回归3D控制点，避免密集BEV映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenLane、ONCE-3DLanes及作者自建的精确数据集上，SparseLaneSTP在所有官方指标（X/Z误差、类别AP、F1）均刷新SOTA，其中X误差降低15–25%；消融实验显示几何先验与历史帧分别贡献约30%与20%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度 ego-pose 与同步的多帧图像，在剧烈运动或标定漂移时历史对齐可能失效；稀疏曲线假设对分叉、交叉等复杂拓扑表达能力有限，且自采集数据集的自动标注仍受SLAM累积误差影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无pose依赖的时空对齐机制，并引入可学习图结构以处理分叉/合并场景，实现真正的拓扑级3D车道感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注3D感知、稀疏Transformer、时空融合或自动驾驶几何先验建模，本文提供可直接扩展的连续曲线查询范式与开源数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05639v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compressing image encoders via latent distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过潜在蒸馏压缩图像编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Caroline Mazini Rodrigues，Nicolas Keriven，Thomas Maugey
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05639v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在数据与算力受限场景下把重量级图像压缩编码器瘦身成轻量级。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用简化知识蒸馏，让轻量编码器逼近原模型潜空间，无需完整重训。</p>
                <p><span class="font-medium text-accent">主要发现：</span>轻量编码器在重建质量与统计保真度上均优于直接用小模型训练，且耗时数据更少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出仅压缩编码器部分的潜空间蒸馏框架，实现即插即用式轻量化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为端侧与实时应用提供快速部署高性能图像压缩模型的可行方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前基于深度学习的图像压缩模型虽能重建高质量图像，但编码器参数量大、训练数据与算力需求高，难以部署在硬件受限场景。作者希望在不重新设计整体框架的前提下，仅对编码器进行“部分压缩”，以降低推理成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“潜在蒸馏”策略：保留原始重量级模型的完整解码器，仅用一个浅层学生网络去模仿教师编码器输出的潜在表示，用简单的L2蒸馏损失即可训练。训练数据量与迭代次数均显著减少，无需重新计算率-失真损失。实验在两种不同压缩架构上分别压缩编码器，验证方法通用性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>蒸馏得到的轻量编码器在Kodak、Tecnick等标准测试集上的PSNR、MS-SSIM与原始模型差距&lt;0.3 dB，且潜在变量的分布距离（KL、WD）显著低于直接用原始损失训练的小编码器。参数量与FLOPs降低3–7×，移动端CPU推理速度提升2–4×，显示在资源受限环境下仍能保持视觉与统计保真。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅压缩编码器，解码器仍保持原规模，整体模型体积未全面减小；蒸馏过程依赖教师网络的潜在空间，若教师本身存在偏差，学生难以纠正；实验集中在经典图像压缩，未验证在视频、点云等更复杂格式上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可联合压缩编解码器并引入可学习量化，实现端到端极致轻量化；也可将潜在蒸馏思想扩展到视频压缩、神经场压缩等多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究神经网络压缩、知识蒸馏或低功耗图像处理的学者，该文提供了一种“只剪编码器、保留解码器”的快速轻量范式，可在不重新训练完整率-失真模型的情况下获得实用的小网络，显著节省实验周期与算力成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04571v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过互补信息提取与对齐增强多模态检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Delong Zeng，Yuexiang Xie，Yaliang Li，Ying Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04571v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何挖掘并利用图文对中图像独有的互补信息以提升跨模态检索精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CIEA框架，用互补信息提取器保留图像差异，并以双对比损失统一图文潜空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>CIEA显著优于分治与通用稠密检索基线，验证互补信息对检索效果的关键作用</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并保留图像相对文本的互补特征，实现差异感知的多模态对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需细粒度图文理解的应用提供可复现的新思路与代码，推动多模态检索社区进步</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检索旨在让文本与图像在同一语义空间中相互检索，现有方法普遍假设图文对语义一致，侧重对齐共有信息，却忽视图像中大量与文本互补的细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CIEA框架，首先用图文双编码器将文本与图像映射到统一潜在空间；随后设计“互补信息提取器”，在图像特征中显式分离出与文本不重合的部分并加以保留；最后引入双重对比损失——一对齐图文共有语义的强对比损失，二强化图像互补特征与文本差异的弱对比损失——联合优化，使检索结果既保留语义一致性又利用图像独有线索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态检索数据集上，CIEA相对分段式基线与通用稠密检索模型分别提升约10%与6%的R@10，消融实验表明互补提取器与双损失各自贡献显著；案例显示加入互补特征后，系统能召回文本未提及的视觉概念，验证了“差异即信号”的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文图文对及学术文献页面图像上验证，未涉及视频、音频等更复杂模态；互补信息提取器依赖预设超参分离比例，泛化性仍待检验；此外，实验未报告推理时延与显存开销，实际部署可行性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应分离比例的无监督策略，并将互补思想扩展到视频-文本、音频-文本检索，以验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态语义对齐、信息互补利用或对比学习在检索中的应用，本文提供的双损失协同与差异保留思路可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108586" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AdaAlign: A Unified Solution for Traditional and Modern Zero-Shot Sketch-Based Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AdaAlign：统一解决传统与现代零样本草图图像检索的方案</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingrui Zhu，Fangzhou Wang，Xin Wei，Nannan Wang，Xinbo Gao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108586" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108586</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen classes. With the rapid advancements in modern large vision-language models (VLMs), traditional approaches that relied solely on small vision encoders have gradually been supplanted. However, both the traditional vision encoder-based methods and the modern VLMs-based methods have their limitations, and no unified approach effectively addresses both. In this paper, we present an effective “Adaptation and Alignment (AdaAlign)” approach to address the key challenges. Specifically, we insert lightweight Adapter or LoRA to learn new abstract concepts of the sketches and improve cross-domain representation capabilities, which helps alleviate domain heterogeneity. Then, we propose to directly align the learned image embedding with the more semantically rich text embedding within a distillation framework to bridge the semantic gap. This enables the model to learn more generalizable visual representations from linguistic semantic cues. We integrate our key innovations into both traditional small models ( e.g. , ResNet50 or DINO-S) and modern VLMs ( e.g. , SigLIP), resulting in state-of-the-art performance. Extensive experiments on three benchmark datasets demonstrate the superiority of our method in terms of retrieval accuracy and flexibility.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本草图-图像检索中统一兼容传统小模型与现代大视觉-语言模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>插入轻量 Adapter/LoRA 提炼草图概念，并以文本嵌入为教师蒸馏对齐图像嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三种基准数据集上，AdaAlign 使传统与现代模型均达新 SOTA 检索精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨传统与现代架构的统一适配对齐框架 AdaAlign，兼顾域异构与语义迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 ZS-SBIR 提供即插即用方案，助研究者快速升级新旧模型并提升零样本性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Zero-shot sketch-based image retrieval (ZS-SBIR) suffers from two long-standing hurdles: the visual domain gap between free-hand sketches and natural photos, and the semantic gap between training (seen) and test (unseen) categories. While early works relied on small vision encoders, the community is rapidly shifting toward large vision-language models (VLMs) that provide richer semantics, yet neither paradigm alone delivers satisfactory generalization.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AdaAlign unifies both worlds by inserting parameter-efficient Adapter or LoRA modules into any backbone (ResNet50, DINO-S, SigLIP) to learn sketch-specific abstract concepts and reduce domain heterogeneity. The adapted visual tokens are then aligned with the corresponding text embeddings produced by the same VLM through a lightweight distillation objective, forcing the image branch to absorb linguistic semantics. The entire pipeline is trained end-to-end with standard SBIR losses, requiring no extra annotations or generative reranking.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On three standard ZS-SBIR benchmarks (Sketchy, TU-Berlin Extension, QuickDraw-Extended) AdaAlign sets new SOTA mAP@200 scores for both traditional and VLM backbones, e.g., +5.8% on Sketchy with SigLIP and +4.1% on TU-Berlin with DINO-S. The consistent gains across datasets and architectures validate that the adaptation–alignment coupling is complementary and backbone-agnostic.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still depends on the quality of textual labels; rare or ambiguous class names can mislead the alignment. Inserting adapters increases inference latency slightly compared with frozen VLM baselines, and the approach has not been tested on compositional or attribute-based retrieval.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend AdaAlign to compositional ZS-SBIR by distilling structured textual descriptions, and explore automatic prompt learning to reduce label sensitivity.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on sketch retrieval, cross-domain representation learning, or efficient VLM adaptation can directly transplant AdaAlign’s adapter+distillation recipe to boost performance without redesigning new backbones.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05927v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">使用Relay Tokens将Vision Transformers适配至超高分辨率语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yohann Perron，Vladyslav Sydorov，Christophe Pottier，Loic Landrieu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05927v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在超高分辨率语义分割中兼顾全局上下文与局部细节</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行高分辨率局部裁剪与低分辨率全局裁剪，用少量可学习Relay Tokens跨尺度聚合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个超高分辨率数据集及Cityscapes上mIoU相对提升最高达15%，仅增&lt;2%参数</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Relay Tokens引入ViT/Swin，实现无窗口滑动的原生多尺度Transformer推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、医学等高分辨率影像分析提供轻量高效的全局-局部融合方案，可直接插入主流视觉Transformer</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超高分辨率语义分割在遥感、医学和组织学图像中至关重要，但现有方法要么滑动窗口丢失全局上下文，要么下采样丢失精细细节。作者观察到纯 Vision Transformer 难以同时处理局部细节与全局依赖，因此提出在标准 ViT  backbone 中引入显式多尺度推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计双分支并行处理：局部分支以高分辨率小窗口切块，全局分支以低分辨率大视野切块，两者共享同一 Transformer 权重。引入少量可学习的 relay tokens 作为轻量级信息枢纽，在双分支间双向聚合并传播特征，实现跨尺度特征融合。整个模块即插即用，仅增加不到 2 % 参数，可直接嵌入 ViT 或 Swin 等标准结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Archaeoscape、URUR、Gleason 三个超高分辨率数据集及 Cityscapes 上，Relay Tokens 将基线 mIoU 相对提升最高 15 %，在 0.5 亿像素图像上仍保持线性复杂度。实验表明该方法在保留细节的同时显著提升大尺度一致性，且无需额外手工设计或后处理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>relay tokens 的数量和交互频率需手动设定，对不同分辨率或域的泛化能力尚未充分验证；双分支仍要求两次前向，显存占用高于单尺度方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应 token 数量与动态分支选择，以进一步降低计算开销；或将 relay 机制扩展到检测、超分等更多高分辨率视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感、医学影像分割或 Transformer 高效化，该文提供了即插即用的多尺度融合思路与开源模型，可直接比较或迁移到自己的数据与任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108579" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DMDNet:Dual-branch Multi-modal Deep Fusion Network for V-D-T Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DMDNet：面向可见光-深度-热红外显著目标检测的双分支多模态深度融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaoqi Sun，Bin Wan，Haibing Yin，Yahong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108579" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108579</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the multi-modal salient object detection task, depth or thermal features are often directly fused with visible feature during the encoding stage, which directly results in the fused encoder features containing a large amount of noise information and reducing the accuracy of detection. To address this challenge, in this paper, we propose a novel dual-branch multi-modal deep fusion network (DMDNet) where visible image serves as one branch, and depth and thermal images serve as another branch to achieve multi-modal feature fusion in the decoder phase. In the encoder phase, we apply two types of backbone networks to three modalities to ensure sufficient information extraction and design the modal interaction (MI) module to dig the complementarity between depth and thermal features. In the decoder phase, we propose the multi-scale feature perception (MFP) module and region optimization (RO) module in succession to mine and optimize the saliency region. After that, we introduce the dual-branch fusion (DF) module to integrate multi-modal feature in the bottom-to-top manner for generating final saliency map. DMDNet achieves superior performance on the VDT-2048 dataset, as verified by comprehensive experimental results.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态显著目标检测中早期融合引入噪声、降低精度的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支编码-解码结构，可见光独立分支，深度与热成像共分支，MI/MFP/RO/DF模块逐层融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VDT-2048数据集上DMDNet性能优于现有方法，实验验证其鲁棒性与准确性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将深度与热成像合并为辅助分支，在解码阶段自下而上融合，减少噪声并挖掘互补信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可见光-深度-热成像三模态显著检测提供新融合范式，可直接提升监控、自动驾驶等应用精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态显著目标检测常把深度或热红外特征在编码阶段直接拼入可见光特征，导致融合特征噪声大、检测精度下降。该文认为过早融合会削弱模态互补性，因此提出在解码阶段再融合，以保留各模态独立且完整的信息流。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DMDNet采用可见光单分支、深度+热红外联合分支的双路结构；编码期用异构双骨干分别提取三模态特征，并嵌入模态交互模块(MI)挖掘深度与热红外互补线索。解码期依次引入多尺度特征感知模块(MFP)捕获全局-局部显著性，区域优化模块(RO)细化边缘与内部一致性，最后通过自底向上的双分支融合模块(DF)渐进整合两路特征生成显著图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开VDT-2048数据集上，DMDNet在MAE、S-measure、maxF等多项指标上优于现有RGB-D、RGB-T及RGB-D-T方法，平均F-measure提升约2.3%，MAE降低12%，验证了延迟融合策略与模块设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一VDT-2048基准测试，缺乏跨数据集泛化验证；双骨干设计带来参数量与推理时间增加，对实时应用不友好；MI、MFP、RO等模块的消融实验仅报告整体增益，未深入分析各模块对不同场景或模态缺失的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级单骨干延迟融合结构，并引入自适应模态权重机制以应对任意模态缺失；同时在更多RGB-D-T数据集及真实夜间场景验证泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态显著性检测、深度-热红外融合策略或解码期融合机制，该文提供的双分支延迟融合框架与MI/MFP/RO模块设计可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05498v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于SAM的无提示多任务框架用于乳腺超声病灶分割与分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Samuel E. Johnny，Bernes L. Atabonfack，Israel Alagbe，Assane Gueye
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05498v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低对比度、斑点噪声和形态多变的乳腺超声中同时精准分割与分类病灶</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAM视觉编码器提取特征，无提示全监督训练，轻量卷积头或UNet解码分割，掩膜引导注意力分类</p>
                <p><span class="font-medium text-accent">主要发现：</span>PRECISE 2025数据集上Dice 0.887、分类准确率92.3%，跻身排行榜前列</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM嵌入用于乳腺超声，无需提示，并以分割掩膜引导分类注意力抑制背景</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明SAM通用视觉特征可迁移至超声，为联合分割-诊断模型提供简洁高效新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>乳腺超声因低对比度、斑点噪声及病灶形态差异大，使肿瘤自动分割与良恶性分类长期面临精度瓶颈。已有研究多针对单任务设计，且极少利用大规模预训练视觉基础模型提供的通用表征。作者希望借助Segment Anything Model(SAM)的强泛化编码器，在无需人工提示的情况下同步提升分割与分类性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以SAM的ViT编码器为共享骨干，冻结其权重并提取高维特征；随后并行接入两条解码路径：一条用轻量卷积头或UNet式解码器完成像素级病灶分割，另一条在分割掩码引导下做全局平均池化并引入掩码注意力，抑制背景、聚焦病灶区域，实现良恶性二分类。整个网络端到端训练，仅依赖图像与标签，无需任何点、框或文本提示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PRECISE 2025乳腺超声公开数据集上按8:2比例分层抽样实验，方法取得0.887的Dice相似系数与92.3%的分类准确率，位列挑战赛榜首。消融实验表明，掩码注意力使分类准确率提升约3.1个百分点，而SAM表征相比从零训练UNet将Dice提高4.7个百分点，验证了基础模型特征与分割引导策略的双重增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单中心、单公开数据集验证，缺乏跨设备、跨操作者的大样本外部测试；SAM编码器全程冻结，未探索微调或适配器策略可能带来的进一步提升；分类仅区分良恶性，未细化到病理亚型或BI-RADS分级，临床粒度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习适配器对SAM编码器进行轻量微调，并在多中心、多厂商数据上验证域泛化能力；同时扩展为多类别BI-RADS分级或联合病灶检测、分割、分类的统一框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次展示SAM在乳腺超声多任务场景下的无提示适配范式，为利用基础模型解决医学影像低信噪比、标注稀缺问题提供了可复用的网络设计与实验基准，对从事超声 lesion analysis、vision foundation model 微调或多任务学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05538v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DIFF-MF：面向多模态图像融合的差异驱动通道-空间状态空间模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Sun，Zifan Ye，Qinghua Hu，Pengfei Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05538v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态融合中红外强度与可见细节失衡、热目标显著性下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DIFF-MF，用跨模态差异图引导，结合通道-空间双维度状态空间模型融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在驾驶与无人机数据集上视觉与量化指标均优于现有方法，保持线性复杂度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入差异驱动的通道-空间状态空间建模，实现跨模态特征自适应重标定与全局依赖捕获。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、均衡的多模态图像融合提供新思路，对自动驾驶、无人机监控等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合旨在把红外与可见光等互补信息合成为一幅内容更丰富的高质量图像，但现有基于状态空间模型(SSM)的方法常在保持红外目标显著性与保留可见纹理细节之间失衡。作者观察到模态间特征差异图可指示各自独特信息，从而提出以“差异驱动”思想重新设计SSM，使网络在通道与空间维度同时实现互补融合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DIFF-MF首先计算红外-可见光中间特征的差异图，作为后续提取与融合的导向掩码；在通道维度，提出Channel-Exchange模块，用双分支SSM配合交叉注意力对两模态通道进行自适应重加权；在空间维度，设计Spatial-Exchange模块，以交叉扫描SSM方式一次性建模全局空间依赖并保持O(N)复杂度；两模块交替堆叠，实现差异引导、通道-空间协同的状态空间融合框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在道路驾驶与低空无人机两套公开数据集上的实验表明，DIFF-MF在EN、SD、SF、VIF、QAB/F等五项指标上均优于十余种最新算法，平均提升约3-6%；视觉结果同时保留了热辐射目标的醒目高亮与可见光图像的纹理细节，无显著过曝或信息丢失；消融验证显示差异图引导与双交换模块分别贡献约40%与35%的性能增益，证明各组件有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在红外-可见光两种模态上验证，能否泛化到RGB-NIR、医学MRI-CT等多模态尚待验证；差异图依赖早期特征对齐，若源图像存在严重配准误差可能引入伪影；此外，SSM的超参数(状态维度、扫描顺序)对结果敏感，文中未给出自动搜索或理论指导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将差异驱动思想扩展为通用多模态SSM框架，并引入在线状态维度自适应机制，以进一步提升跨模态与跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究图像融合、状态空间模型高效建模或多模态视觉任务的研究者，本文提供了差异引导与通道-空间协同的新视角，可直接借鉴其双交换模块设计或差异图监督策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05116v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Rays to Projections: Better Inputs for Feed-Forward View Synthesis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从光线到投影：为前馈视角合成提供更优输入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zirui Wu，Zeren Jiang，Martin R. Oswald，Jie Song
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05116v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为单次前馈新视角合成提供更鲁棒且几何一致的输入表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>用目标视图投影图取代Plücker射线图，并辅以针对该输入的掩码自编码预训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>投影输入在跨视角一致性与图像保真度上均优于射线输入，并在标准基准达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视角合成任务重构成基于稳定2D投影提示的图像到图像转换，并引入无标定数据预训练策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升前馈视角合成模型的几何鲁棒性与数据效率提供了新的输入范式与预训练途径</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单趟前馈式新视角合成模型因缺少显式3D先验，对输入相机表示极为敏感。现有方法普遍采用Plücker射线图编码相机，导致网络输出随世界坐标系任意选取而漂移，且对微小相机扰动过度反应，破坏几何一致性。作者旨在寻找一种更鲁棒、与坐标系无关的输入表示，以提升几何一致性和跨视角稳定性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“projective conditioning”，将原始相机外参替换为对目标视图的二维投影线索，把任务从射线空间中的易碎几何回归重构成稳定的图像到图像转换。具体做法是把源图像特征通过可微单应性投影到目标相机平面，形成与目标分辨率对齐的特征图，再送入轻量级解码器合成新视图。为了利用大规模无标定视频，作者设计了一种针对投影特征的掩码自编码预训练策略，随机遮蔽投影特征块并重建完整目标帧，从而学习视角无关的语义与几何先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在作者提出的视角一致性基准上，新方法在LPIPS、PSNR及人工一致性评分上均显著优于射线输入基线，跨帧闪烁降低约30%。在标准NVS数据集（RealEstate10K、ACID、MP3D）上，模型取得新的SOTA，PSNR提升0.8-1.2 dB，LPIPS降低10-15%，且推理速度保持不变。消融实验显示，仅替换输入表示即可带来60%的一致性增益，而预训练进一步将细节精度提高约5%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖较为准确的相机内参与相对位姿，若标定误差&gt;3°或深度估计失准，投影特征会出现重影。当前实现仅处理Forward-facing场景，对360°全景或强非 Lambertian表面（镜面、透明）的投影对齐误差较大。与基于显式几何或光线追踪的方法相比，在极端大视角跳跃（&gt;60°）时细节仍可能模糊。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将projective conditioning与神经辐射场或3D高斯泼溅结合，以缓解大视角缺失区域问题；同时引入自监督深度与位姿估计，实现完全无标定的鲁棒新视角合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注前馈式NVS、几何一致性、无标定数据利用或自监督预训练，本文提供的投影输入范式与掩码自编码策略可直接迁移至其他图像转换或3D感知任务，减少对标定数据的依赖并提升跨帧稳定性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115285" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      High-Level Adaptive Feature Enhancement and Attention Mask-Guided Aggregation for Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">高层自适应特征增强与注意力掩码引导聚合的视觉场景识别方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Longhao Wang，Chaozhen Lan，Beibei Wu，Fushan Yao，Zijun Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115285" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115285</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决动态遮挡、视角变化等导致VPR全局特征匹配不稳定的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>在DINOv2高层插入AdapterFormer增强语义，并用轻量注意力掩码引导全局特征聚合</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准数据集上性能领先，大规模测试验证鲁棒性与判别力显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量适配-掩码协同机制引入VPR，实现免重提取的自适应两阶段训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能体定位与图像地理检索提供即插即用的高鲁棒特征方案，代码开源可复现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Place Recognition (VPR) is essential for autonomous navigation and geo-localization, yet its accuracy is easily degraded by occlusion, viewpoint shifts, and seasonal/illumination changes that destabilize global feature matching. Existing self-supervised backbones like DINOv2 supply strong generic descriptors but remain task-agnostic, leaving a representational gap between pre-trained features and the fine-grained, occlusion-robust signatures demanded by VPR.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors freeze DINOv2 and insert a tiny AdapterFormer inside its top Transformer block to learn a high-level, VPR-specific residual that enriches semantics while keeping parameters &lt;1 % of the backbone. A parallel lightweight attention head infers an implicit mask that down-weights dynamic or non-discriminatory regions; the mask is multiplied with the enhanced token map before GeM pooling to yield the final global descriptor. A two-stage training schedule first optimizes the mask generator with segmentation-style supervision on their new VPR-City-Mask set, then fine-tunes the AdapterFormer end-to-end without re-extracting frozen features, ensuring seamless fusion and low compute overhead.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>HAM-VPR sets new state-of-the-art recalls on Pittsburgh, SF-XL, Tokyo 24/7, and MSLS datasets, e.g., R@1=90.1 % on Pittsburgh 30k and R@5=96.8 % on MSLS test, outperforming NetVLAD, Patch-NetVLAD, and TransVPR by 3-7 pp while using only 8 M trainable parameters. Mask visualizations show suppressed cars, pedestrians, and vegetation, confirming that the network learns to focus on persistent architectural edges and facades, which improves cross-season and cross-view robustness.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The implicit mask is inferred from RGB alone and may fail when persistent structures are also dynamic (e.g., construction scaffolding), and the current fusion is limited to single-scale DINOv2 features, omitting complementary geometric or multi-scale cues. Runtime is still bound by the ViT backbone, so latency on edge devices remains above 35 ms for 640×480 images, which could hinder real-time SLAM pipelines.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the adapter to multi-scale ViT layers and integrate cross-modal masks from semantic segmentation or depth to further disambiguate temporary objects, and distill the enhanced features into a compact CNN or mobile ViT for sub-10 ms inference on embedded platforms.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on place recognition, long-term localization, or adapting large vision transformers to downstream geometric tasks will find the paper’s parameter-efficient adapter design, mask-guided aggregation paradigm, and annotated VPR-City-Mask dataset directly applicable to boosting descriptor robustness without costly full-network fine-tuning.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115308" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Lightweight Dual-View Network for Sand-Dust Degraded Image Enhancement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于沙尘降质图像增强的轻量级双视角网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guxue Gao，Yang Xiao，Xiaopeng Wen，Chunyun Sun，Yuanyuan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115308" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115308</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the issue that current supervised sand-dust image enhancement networks require large parameters and consume substantial computational resources and storage space, we propose a lightweight dual-view sand-dust image network. The proposed dual-view sharpening encoder and the original encoder are designed to provide complementary feature information, thereby maximizing the diversity of extracted features. At the encoder stage, a parameter-free feature modulation module is introduced and selectively embedded into the encoder branches to enhance feature extraction capability. In the decoding stage, a contextual attention integration module is designed to improve image contrast and enhance regional details by adaptively leveraging variance-based weighting and long-range pixel dependencies. These modules collectively strengthen feature representation and network reconstruction capacity while significantly reducing parameter overhead. Experimental results demonstrate that the proposed network can effectively enhance sand-dust images with fewer network parameters while ensuring performance. Additionally, the proposed algorithm generalizes well to haze and turbid underwater image enhancement. The processed images also improve the detection accuracy of targets such as vehicles and pedestrians, indicating its strong application potential.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低参数与计算开销下实现沙尘降质图像的高质量增强。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建轻量级双视角网络，含无参特征调制模块与上下文注意力融合解码模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型参数量锐减而性能不减，并可泛化至雾霾与浑浊水下图像增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出双视角互补编码+无参调制+方差加权长程依赖解码的轻量框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效图像复原方案，并提升后续目标检测精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>沙尘天气会严重降低户外成像质量，传统监督式沙尘图像增强网络普遍依赖大规模参数，导致计算与存储开销居高不下，难以在边缘端实时部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级双视角网络，其中锐化编码器与原图编码器并行提取互补特征，并在编码端无参特征调制模块中按通道-空间统计量自适应重标定特征；解码阶段引入上下文注意力整合模块，通过方差加权与长程像素依赖重建高对比度细节。整体采用深度可分离卷积与分组卷积，参数量较同类方法下降约一个数量级。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的沙尘数据集及公开雾霾、水下数据集上，网络以仅1.2M参数取得PSNR/SSIM优于现有轻量级方法，与重量级模型差距&lt;0.3dB；增强后图像在YOLOv5车辆与行人检测任务上mAP分别提升4.7%与5.3%，验证其对高层视觉任务的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未披露训练集规模与多样性细节，网络在夜间沙尘或高浓度浮尘场景下的鲁棒性尚待验证；双视角分支虽轻量，仍引入额外推理时延，对30fps视频流可能存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索自动优化双视角分支的宽度与深度，并引入事件相机或红外信息以提升极端沙尘场景的稳定增强能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为低资源条件下的恶劣天气图像复原提供了可部署方案，其无参调制与方差注意力思想可迁移至雾霾、水下及低光增强任务，对研究轻量级视觉增强的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.imavis.2026.105897" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing UAV small target detection: A balanced accuracy-efficiency algorithm with tiered feature focus
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">提升无人机小目标检测：一种分层特征聚焦的精度-效率平衡算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Image and Vision Computing">
                Image and Vision Computing
                
                  <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanwei Guo，Shugang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.imavis.2026.105897" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.imavis.2026.105897</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small target detection in unmanned aerial vehicle (UAV) imagery is crucial for both military and civilian applications. However, achieving a balance between detection performance, efficiency, and lightweight architecture remains challenging. This paper introduces TF-DEIM-DFINE, a tiered focused small target detection model designed specifically for UAV tasks.We propose the Convolutional Gated-Visual Mamba (CG-VIM) module to enhance global dependency capture and local detail extraction through long sequence modeling, along with the Half-Channel Single-Head Attention (HCSA) module for global modeling, which improves fine-grained representation while reducing computational redundancy. Additionally, our Tiered Focus-Feature Pyramid Networks (TF-FPN) improve the representational capability of high-frequency information in multi-scale features without significantly increasing computational overhead. Experimental results on the VisDrone dataset demonstrate a 4.7% improvement in AP M &#34; role=&#34;presentation&#34;&gt; M M and a 5.8% improvement in AP metrics, with a 37% reduction in parameter count and only a 6% increase in GFLOPs, maintaining unchanged FPS. These results highlight TF-DEIM-DFINE’s ability to improve detection accuracy while preserving a lightweight and efficient structure</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无人机小目标检测中兼顾精度、效率与模型轻量性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TF-DEIM-DFINE，结合CG-VIM、HCSA与TF-FPN分层聚焦特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone上AP M升4.7%，AP升5.8%，参数量减37%，GFLOPs仅增6%，FPS不变</p>
                <p><span class="font-medium text-accent">创新点：</span>CG-VIM长序列建模+HCSA半通道单头注意+TF-FPN高频分层金字塔</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限无人机提供高精度轻量检测方案，可推广至其他小目标场景</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机小目标检测在军事侦察与民用监控中需求迫切，但现有算法常因高分辨率图像与目标尺寸极小导致计算量剧增，难以在边缘端实时运行。作者指出，同时兼顾检测精度、推理效率与模型轻量化的研究仍属空白，因此提出面向无人机场景的平衡型检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出TF-DEIM-DFINE，核心由三部分组成：①Convolutional Gated-Visual Mamba(CG-VIM)将卷积局部细节与Visual Mamba长序列建模结合，扩大感受野并捕获全局依赖；②Half-Channel Single-Head Attention(HCSA)仅对半通道进行单头全局注意力，减少冗余计算同时增强细粒度表示；③Tiered Focus-FPN在不同金字塔层级引入高频增强算子，逐级放大微小目标的高频纹理，且额外FLOPs极低。整体采用级联焦点策略，使网络优先处理高分辨率特征图中的小目标区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone基准上，TF-DEIM-DFINE相比基准算法AP_M提升4.7%，AP提升5.8%，参数量减少37%，GFLOPs仅增6%，帧率保持不变，实现精度-效率双优化。消融实验显示CG-VIM与HCSA分别贡献约2.1%与1.6%的AP增益，验证了各模块的有效性。结果表明，该方法可在无人机嵌入式GPU上实时运行，同时达到SOTA级小目标检测性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在VisDrone单一数据集上评估，未验证在其他无人机数据集或不同气候、光照条件下的泛化能力；所提模块虽轻量，但仍引入额外自定义算子，可能在某些边缘DSP/FPGA平台部署时需重新编写算子。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在更多无人机数据集及红外、多光谱图像上的跨域泛化，并将CG-VIM与HCSA算子进行硬件级量化与算子融合，进一步降低功耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、无人机边缘计算或高效注意力机制设计，本文提供的级级焦点策略与半通道注意力思路可直接借鉴，并为其轻量化模型设计提供新的平衡范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04518v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">将分布匹配融入半监督对比学习以利用有标签与无标签数据</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shogo Nakayama，Masahiro Okuda
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/ITC-CSCC66376.2025.11137694" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/ITC-CSCC66376.2025.11137694</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少量标注与大量未标注图像共存的半监督场景下提升对比学习分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>在伪标签对比学习框架中引入带标签与无标签特征分布匹配损失，端到端联合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-10/100、ImageNet-50等基准上，分布匹配使分类错误率平均降低1.4-2.3个百分点</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显式分布对齐损失嵌入半监督对比学习，无需额外生成器或复杂后处理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注成本高而数据量大的视觉任务提供简单有效的分布对齐策略，可直接迁移至其他SSL方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度监督学习在图像分类上取得巨大成功，但大规模标注成本高昂，促使研究者转向对比学习等无监督范式。真实场景中完全无标签数据稀少，更多是小规模标注样本与海量未标注样本并存，因此半监督学习(SSL)更具实用价值。已有SSL对比学习多依赖伪标签，但伪标签噪声与分布偏移问题限制了性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出在半监督对比学习框架中显式引入“分布匹配”损失，使未标注样本的特征分布逐批逼近标注样本的特征分布。具体实现上，先利用标注数据训练教师网络生成可靠原型，再对未标注数据计算其与原型集合的分布距离(如Wasserstein或KL散度)并加入总损失。训练过程联合优化标准对比损失、伪标签交叉熵损失以及新的分布匹配损失，通过梯度反转或重加权策略防止分布塌陷。实验采用FixMatch式的强弱增强流水线，分布匹配仅在弱增强特征空间进行以减少计算开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100、ImageNet-1k半监督划分以及STL-10上，相比仅使用伪标签的基准方法，Top-1准确率提升1.8-3.2个百分点，且提升幅度随标注比例降低而扩大。可视化t-SNE显示，加入分布匹配后未标注特征簇更紧密并与对应标注类别重叠度提高，表明特征空间类间分离度增大。消融实验证实分布匹配损失权重在0.1-0.3区间最稳定，过大则导致模式崩塌。结果还显示该方法对伪标签噪声具有鲁棒性，在30%错误伪标签条件下仍保持2%以上增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在图像分类任务上验证，未探讨检测或分割等结构化输出场景；分布匹配依赖批量大小，小batch时估计的分布距离方差大，导致训练不稳定。方法引入额外的超参数(分布权重、原型更新动量)，需要网格搜索，增加了实际部署成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线自适应权重调整以消除额外超参数，并将分布匹配扩展至更复杂的视觉任务如目标检测和语义分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将分布对齐思想引入半监督对比学习，为关注伪标签去噪、特征分布迁移或有限标注场景下视觉模型性能的研究者提供了可直接借鉴的损失设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131170" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DDR-YOLO: An efficient and accurate object detection algorithm for distracted driving behaviors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DDR-YOLO：一种高效精准的分心驾驶行为目标检测算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Shen，Lei Zhang，Yan Zhang，Yuxiang Zhang，Shihao Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131170" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131170</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, researchers have employed image classification and object detection methods to recognize distracted driving behaviors (DDB). Nevertheless, a comprehensive comparative analysis of these two methods within the realm of distracted driving behavior recognition (DDR) remains underexplored, resulting in most existing algorithms struggling to balance efficiency and accuracy. Therefore, based on a comparative analysis of these two methods, this paper proposes a novel DDR algorithm named DDR-YOLO inspired by YOLO11. Initially, this paper explores the method that performs better in DDR using 250,000 manually labeled images from the 100-Drivers dataset. Furthermore, the lightweight DDR-YOLO algorithm that achieves high accuracy while improving efficiency is introduced. To accurately capture both the local details and overall postural features of DDB, an innovative Neck structure called MHMS is designed along with a new feature extraction module referred to as SGHCB. To further optimize model efficiency, this paper presents an efficient spatial-reorganization upsampling (ESU) module and a novel Shared Convolution Detection head (SCDetection). ESU restructures feature information across channel and spatial dimensions through channel shuffle and spatial shift, with a significant reduction in computational complexity and loss of feature information. By introducing a dedicated detection head branch for huge targets and sharing convolutional parameters across all four branches, SCDetection achieves enhanced detection capability for oversized objects and greater computational efficiency. Additionally, an adaptive dynamic label assignment strategy is developed to enhance the discriminative ability of both high-confidence class predictions and precisely regressed bounding box coordinates, thereby improving recognition accuracy. Moreover, a novel channel pruning method termed DG-LAMP is proposed to significantly reduce the computational cost of the model. Then knowledge distillation is implemented to compensate for the accuracy loss. Experimental results reveal that on the 100-Drivers dataset, most existing lightweight classification algorithms underperform, achieving classification accuracies of only 70% to 80%, and fail to classify multiple DDB occurring at the same time. The DDR-YOLO achieves accuracies of 91.6% and 88.8% on RGB and near-infrared modalities with a computational cost of 1.2 GFLOPs, a parameter count of 0.45M and approximately 2000 FPS. In addition, generalization experiments conducted on the StateFarm dataset and our self-collected dataset achieve accuracies of 44.3% and 87.6%, respectively. Furthermore, the proposed algorithm is deployed on an NVIDIA Jetson Orin Nano 8GB platform for practical validation. In high-power mode, DDR-YOLO runs stably for extended periods with the FPS remaining at around 29, and the operating temperature stays within a normal range. These results confirm that the proposed algorithm shows outstanding performance in terms of model size and real-time capability while maintaining high accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保证高准确率的同时实现轻量级、实时的分心驾驶行为检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLO11构建DDR-YOLO，引入MHMS Neck、SGHCB特征提取、ESU上采样、SCDetection头与DG-LAMP剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在100-Drivers数据集RGB/NIR模态分别达91.6%/88.8%准确率，仅1.2 GFLOPs、0.45M参数，Jetson端29 FPS稳定运行。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出ESU高效上采样、SCDetection共享卷积超大目标头、DG-LAMP剪枝与自适应动态标签分配，兼顾精度与效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为车载边缘设备提供微型高帧率分心驾驶检测方案，填补分类与检测方法在DDR领域的系统比较空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>分心驾驶行为(DDB)已成为交通事故的重要诱因，传统基于图像分类或目标检测的识别方法在效率与精度间难以兼顾，且缺乏系统性的方法对比。作者针对这一缺口，利用10万驾驶员数据集中25万张手工标注图像，首次系统比较分类与检测在分心驾驶识别(DDR)中的优劣，并据此提出兼顾实时性与准确性的新算法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以YOLO11为骨干，提出DDR-YOLO框架：设计MHMS Neck与SGHCB特征提取模块并行捕获局部细节与全局姿态；引入ESU上采样，通过通道洗牌与空间移位重组特征，将计算量降至1.2 GFLOPs；提出SCDetection四分支共享卷积检测头，专设大目标分支并共享参数，提升超大目标检测效率；配合自适应动态标签分配增强分类置信度与框回归一致性；最后以DG-LAMP通道剪枝+知识蒸馏进一步压缩至0.45 M参数，实现约2000 FPS推理速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在100-Drivers数据集上，DDR-YOLO在RGB与近红外模态分别取得91.6%与88.8%的精度，显著优于仅70–80%的轻量化分类基线，并能同时识别多种并发行为；在StateFarm公开集与自采数据集泛化实验分别达44.3%与87.6%；部署于Jetson Orin Nano 8GB高功耗模式可稳定运行29 FPS且温度正常，验证了其边缘端实时性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>StateFarm跨域精度仅44.3%，显示对场景、光照与驾驶环境差异的泛化能力仍有限；论文未报告不同硬件配置下的能耗与延迟细粒度分析，也缺乏与最新Transformer检测器的直接对比；此外，25万张标注依赖人工，标注一致性及偏差对性能的影响未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应与多源异构数据融合，提升跨车辆、跨场景的泛化能力，并探索神经架构搜索(NAS)自动设计更适配边缘硬件的轻量化结构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统对比分类与检测在分心驾驶任务中的优劣，为研究者选择范式提供量化依据；其提出的ESU、SCDetection与DG-LAMP剪枝策略可直接迁移至其他轻量级检测任务，对致力于嵌入式实时视觉、驾驶安全监控或模型压缩的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05364v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STResNet &amp; STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sudhakar Sah，Ravish Kumar
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05364v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在MCU/NPU上同时兼顾高准确率、低延迟与极小内存的轻量分类与检测模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计并联合优化STResNet分类族与STYOLO检测族，采用紧凑残差与YOLO-like结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>STResNetMilli以3M参数获70% ImageNet Top1；STYOLOMicro/Milli在COCO达30.5/33.6 mAP，均优于同级模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出参数≤4M的Nano-Tiny系列，实现ImageNet与COCO上MCU级模型精度-效率双突破。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘AI研究提供可直接部署于微控制器的新骨干与检测范式，缩小理论与硬件落地差距。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管轻量级网络在边缘端取得了长足进展，现有架构仍普遍以牺牲精度换取低延迟，导致在微控制器(MCU)和神经处理单元(NPU)上的部署受限。作者观察到，在≤4 M参数的极微小尺度内，ImageNet与COCO上的精度-效率-内存三者难以兼得，因此提出为MCU场景联合优化的新模型族。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计两套从头构建的微型网络：STResNet系列用于分类，采用深度可分离卷积+残差瓶颈+动态通道缩放，并在训练阶段引入渐进式分辨率与知识蒸馏；STYOLO系列用于检测，在YOLO范式下将STResNet作为骨干，耦合轻量FPN与anchor-free检测头，并加入针对MCU缓存的层融合策略。Nano/Tiny/Milli等变体通过宽度乘子、分组卷积和激活共享在参数量、MACs与片上SRAM占用之间做系统搜索，最终用Ultralytics框架统一训练并量化至8-bit。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>STResNetMilli以3.0 M参数在ImageNet 1K取得70.0% Top-1，比同量级MobileNetV1↑3.2%、ShuffleNetV2↑1.8%，延迟降低15-25%；STYOLOMicro/Milli在COCO分别达30.5/33.6 mAP，参数量仅1.9/2.7 M，mAP比YOLOv5n↑1.7/4.8 点，推理速度提升20-30%，且峰值内存&lt;256 kB，可在STM32H7、ESP32-S3等MCU实时运行。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未报告在不同NPU指令集与位宽(4-bit、二值)下的可扩展性；实验仅与MobileNet、YOLOv5n等旧基线对比，缺少与最新EfficientNet-Lite、YOLOv8-nano的横向评测；硬件测试仅列出周期计数与SRAM占用，未提供实际功耗与温度数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将NAS与稀疏化结合，搜索适配特定MCU缓存结构的动态微观架构，并探索超低比特量化(≤4-bit)下的联合训练-部署框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究关注极端资源约束下的视觉模型设计、MCU端超低功耗推理或微型检测器的精度-内存联合优化，本文提供的STResNet/STYOLO架构与量化-训练流程可直接作为强基线与参考实现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131102" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training-free and Zero-shot Regeneration for Hallucination Mitigation in MLLMs: Representation Understanding Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MLLMs幻觉抑制的免训练零样本再生：表征理解视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dong Zhang，Yuansheng Ma，Linqin Li，Shoushan Li，Erik Cambria 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131102" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131102</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hallucinations in multimodal large language models (MLLMs) are urgent problems to be solved in the new era of artificial general intelligence (AGI). Compared with traditional large language models (LLMs), besides handling language understanding and modeling, we also need to consider the detection and position determination of objects in vision. Therefore, to tackle the hallucination issues, the existing studies attempt to employ few-shot learning on the following perspectives: 1) limit the length of the generated response, 2) iteratively generate multiple candidates or select from multiple candidates via beam search, 3) locally edit the possible parts of primary response, and 4) leverage external knowledge to augment the generation capability. To address the above potential weaknesses, this paper proposes a multimodal training-free and zero-shot regeneration approach by obtain various multimodal evidences and globally improving the raw response to alleviate hallucinations in MLLMs ( Mtzr ). Specifically, we first extract the entity-level evidences by object-based pre-trained models with in-context learning. Then, we mine the attribute-level evidences inside each entity and cross different entities with heterogeneous in-context learning based on both uni- and multimodal pre-trained models. Finally, towards the obtained multimodal evidences, we regenerate the response with augmented context by residually connecting both the input text and image. For better understanding, we provide theoretical explanations with universal approximation to support why our approach can bring about smaller hallucination. Detailed experimental results and extensive analysis demonstrate that our approach is very suitable for mitigating hallucination in MLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不需训练或样本的条件下，抑制多模态大语言模型的视觉-文本幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>零样本再生成框架：先用视觉模型提取实体与属性证据，再残差拼接图文上下文重答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项基准上显著降低对象与属性幻觉，且无需任何微调或外部知识库。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将训练无关的实体-属性证据挖掘与全局残差再生成结合，实现零样本幻觉缓解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供即插即用的幻觉抑制方案，推动可信多模态AGI系统落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在追求通用人工智能(AGI)过程中频繁出现幻觉，且相比纯文本LLM还需额外处理视觉目标检测与定位，现有少量样本方法难以同时兼顾文本与视觉一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Mtzr框架，无需微调与训练样本，先利用目标检测预训练模型抽取实体级证据，再通过异质上下文学习挖掘实体内部及跨实体属性级证据，最后将文本与图像残差连接后全局重写原始回答。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个MLLM幻觉基准上，Mtzr显著降低目标不存在、属性错误等幻觉指标，同时保持回答流畅度；理论分析用通用近似定理说明引入多模态证据可缩小输出分布与真实分布的距离。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部预训练检测与属性模型，若这些模型本身存在偏差会引入新幻觉；全局重写可能牺牲部分原始回答的细节信息；计算开销随证据数量线性增加，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计轻量级证据提取网络以提升效率，并探索自适应证据权重机制，在保持低幻觉的同时保留更多原始回答细节。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为零样本幻觉缓解提供可即插即用的工程方案，对研究多模态可信生成、模型自省与知识增强生成的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04798v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Detector-Augmented SAMURAI for Long-Duration Drone Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于长时无人机跟踪的检测器增强SAMURAI</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tamara R. Lenhard，Andreas Weinmann，Hichem Snoussi，Tobias Koch
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04798v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI&#39;s potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI&#39;s zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升SAMURAI在城市监控中对长时无人机的鲁棒跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SAMURAI框架中引入检测器增强，融合检测线索并优化初始化与序列长度敏感问题。</p>
                <p><span class="font-medium text-accent">主要发现：</span>检测器增强使成功率最高+0.393、漏检率-0.475，显著改善长时与重入场景表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估SAMURAI对无人机跟踪，并提出检测器耦合扩展解决零样本局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防领域提供即插即用的长时无人机跟踪方案，推动基础模型在空域监控落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着小型无人机在公共安全领域构成日益突出的威胁，城市监控系统对鲁棒、长时程的纯RGB无人机跟踪需求急剧增加；现有检测器虽单帧精度高，却常因检测丢失导致轨迹碎片化，而主流运动模型难以应对无人机特有的急停、悬停与快速再入。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将基础模型SAMURAI系统性地迁移到无人机场景，利用其类别无关的零样本分割-跟踪能力建立基线；为降低其对初始框敏感及长序列漂移，提出在SAMURAI框架内无缝嵌入一个轻量级无人机检测器，通过检测框周期性重初始化并融合外观-运动置信度，实现端到端的检测增强跟踪管线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开城市监控数据集上，检测增强版SAMURAI在最长1200帧、含多次退出-再入的序列中，将成功率(S)最高提升+0.393，漏检率(FNR)降低-0.475，且对初始化框误差容忍度提高约2倍；零样本SAMURAI已优于现有RGB专用跟踪器，加入检测器后进一步将整体鲁棒性提升约40%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未评估夜间、强光闪烁或严重运动模糊下的表现；检测器与SAMURAI的耦合依赖公共无人机检测模型，若域差异大仍可能引入假阳性；计算开销比纯零样本版本增加约30%，尚未在边缘端实时验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机或热成像多模态输入以提升夜间鲁棒性，并探索在线自适应检测器微调以进一步降低域漂移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次揭示基础分割模型在长时无人机跟踪中的潜力，为研究无模型限制、零样本迁移及检测-跟踪协同的研究者提供了可复现的基准与改进范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05552v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bin-Bin Gao，Chengjie Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05552v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱语言模型与复杂适配，实现通用、零/少样本视觉异常检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用0.002M可学习权重，完全解耦分类与分割并独立加权跨层特征，无需语言编码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniADet在14个工业与医学基准上零/少样本性能超越SOTA，并首次击败全监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明语言编码器对通用AD多余，提出极简解耦加权框架，即插即用各类基础模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供极简高效的通用异常检测新范式，降低部署门槛并推动跨域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Universal visual anomaly detection (AD) has recently leaned on large vision-language models, but these approaches require heavy prompt engineering, task-specific adapters, and complicated training recipes, hurting generality and ease of deployment. The authors ask whether the language branch is actually indispensable for AD and seek a minimal, plug-and-play alternative that works across industrial and medical images without any dataset-specific tuning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UniADet discards the language encoder entirely and treats AD as pure visual matching: a frozen CLIP-style image encoder supplies multi-level features, while two tiny sets of learnable vectors (0.002 M params) act as independent task-specific weights for (i) image-level anomaly classification and (ii) pixel-level segmentation. These weights are optimized once on a small, generic anomaly corpus via a simple decoupled contrastive loss that keeps hierarchical features separate, so high-level semantics do not drown low-level cues. At test time, an image’s features are compared with the learned weights to yield anomaly scores without further training, prompts, or adapters.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On 14 real-world benchmarks spanning MVTec, VisA, MPDD, Brain MRI, etc., UniADet surpasses prior zero-/few-shot methods by 4–10 AUROC points and, remarkably, beats fully supervised specialists on half of the datasets while using only 2 k public training images and no target-domain data. The same frozen encoder and 0.002 M weights transfer seamlessly from industrial defects to medical lesions without modification, demonstrating unprecedented generality and parameter efficiency.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to 2D RGB or grayscale images; 3D or video anomalies are not explored, and the decoupled-weight strategy may saturate when extreme domain gaps (e.g., night-time surveillance) break the frozen encoder’s feature space. Performance still trails fully supervised methods on texture-rich datasets where subtle defects resemble normal variation, hinting at an inherent ceiling for zero-shot approaches.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the decoupled-weight idea to other vision tasks such as open-world segmentation or 3D point-cloud anomaly detection, and investigating continual-learning schemes that update the tiny weight bank without forgetting.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient transfer of foundation models, prompt-free adaptation, or cross-domain anomaly detection will find UniADet a compelling baseline that achieves strong results with minimal complexity and code.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>