<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-10</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-10 11:48 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">976</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、视觉定位与模型压缩，同时对自监督/对比学习等表征学习方法保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测（35篇）与视觉定位（22篇）上收藏量显著领先，且持续追踪Kaiming He、Ross Girshick等权威团队工作，体现出对通用目标检测框架及轻量化部署的深入积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读同时覆盖CVPR、NeurIPS等顶会与IEEE TGARS、《雷达学报》等遥感期刊，显示其将主流视觉算法迁移至SAR/遥感影像的跨学科兴趣。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量陡增至102篇后回落，新增关键词出现“基础设施感知效率”“条件记忆”，表明正由传统检测转向面向大模型推理效率与知识检索的应用研究。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可继续跟进视觉-语言模型在遥感解释中的高效微调与知识蒸馏，以及面向SAR图像的旋转目标检测基础模型构建。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 950/950 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-09 11:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸对齐', '车牌识别', 'GNSS导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 13, 7, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 10 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 10 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 84,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 72,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 56,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u591a\u4f20\u611f\u5668BEV\u878d\u5408",
            size: 51,
            keywords: ["SIFT", "ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1"]
          },
          
          {
            id: 4,
            label: "Vision Transformer\u67b6\u6784",
            size: 51,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 6,
            label: "SAR\u4eff\u771f\u4e0e\u8bc6\u522b",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u6df7\u5408\u4e13\u5bb6LLM",
            size: 45,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 8,
            label: "LLM\u63d0\u793a\u4e0e\u6307\u4ee4",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u7814\u7a76"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 38,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 10,
            label: "\u6df1\u5ea6\u7f51\u7edc\u4f18\u5316",
            size: 36,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 11,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["Transformers", "HRNet", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "SAR\u57fa\u7840\u6a21\u578b",
            size: 33,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 13,
            label: "\u96f7\u8fbe\u667a\u80fd\u611f\u77e5",
            size: 29,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u751f\u6210\u5f0f\u6d41\u6a21\u578b",
            size: 28,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6d77\u9762\u76ee\u6807\u68c0\u6d4b",
            size: 27,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316",
            size: 25,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 18,
            label: "CNN\u53ef\u89e3\u91ca\u6027",
            size: 24,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Grad-CAM"]
          },
          
          {
            id: 19,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u68c0\u6d4b",
            size: 23,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 20,
            label: "\u79fb\u52a8\u7aef\u8f7b\u91cf\u89c6\u89c9",
            size: 19,
            keywords: ["HRNet", "\u7ebf\u6bb5\u68c0\u6d4b", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 16,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u6269\u6563\u56fe\u50cf\u751f\u6210",
            size: 16,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 23,
            label: "\u56fe\u50cf\u7ffb\u8bd1\u4e0e\u751f\u6210",
            size: 15,
            keywords: ["\u6269\u6563\u6a21\u578b", "StepFun", "\u56fe\u50cf\u7ffb\u8bd1"]
          },
          
          {
            id: 24,
            label: "GAN\u4e0eVAE\u751f\u6210",
            size: 14,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 25,
            label: "SAR\u91cf\u5316\u5f71\u54cd",
            size: 12,
            keywords: []
          },
          
          {
            id: 26,
            label: "TinyML\u7cfb\u7edf\u4f18\u5316",
            size: 12,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "\u7cfb\u7edf\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u6269\u6563\u6a21\u578b\u539f\u7406",
            size: 11,
            keywords: ["\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210", "\u751f\u6210\u5f0f\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u7406\u8bba",
            size: 11,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 29,
            label: "\u4f18\u5316\u7b97\u6cd5\u57fa\u7840",
            size: 4,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          }
          
        ];

        const links = [{"source": 6, "target": 12, "value": 0.9483078658110562}, {"source": 24, "target": 27, "value": 0.9036317652746654}, {"source": 6, "target": 21, "value": 0.8977261723439519}, {"source": 22, "target": 23, "value": 0.943923417409308}, {"source": 21, "target": 25, "value": 0.89110867759278}, {"source": 5, "target": 10, "value": 0.9122808220395958}, {"source": 14, "target": 28, "value": 0.8612821729319402}, {"source": 10, "target": 18, "value": 0.9001666912955054}, {"source": 1, "target": 18, "value": 0.9190722215179732}, {"source": 0, "target": 20, "value": 0.9284505095892294}, {"source": 16, "target": 19, "value": 0.9036494467610329}, {"source": 15, "target": 20, "value": 0.8638884146730517}, {"source": 4, "target": 5, "value": 0.9143523905342857}, {"source": 4, "target": 11, "value": 0.9043370314379795}, {"source": 5, "target": 18, "value": 0.933785500387883}, {"source": 4, "target": 20, "value": 0.9110779054958261}, {"source": 0, "target": 1, "value": 0.919145687409636}, {"source": 23, "target": 27, "value": 0.914611323674415}, {"source": 8, "target": 14, "value": 0.9156844864246895}, {"source": 0, "target": 4, "value": 0.9243801557596699}, {"source": 9, "target": 19, "value": 0.9042575848853867}, {"source": 17, "target": 26, "value": 0.8730868963608284}, {"source": 10, "target": 14, "value": 0.8871087318073422}, {"source": 2, "target": 16, "value": 0.9429064565497255}, {"source": 13, "target": 16, "value": 0.930679420245329}, {"source": 8, "target": 26, "value": 0.8830508545588569}, {"source": 6, "target": 13, "value": 0.9254909104577743}, {"source": 10, "target": 29, "value": 0.8722403945856181}, {"source": 6, "target": 25, "value": 0.9174480625892248}, {"source": 4, "target": 7, "value": 0.911944034548791}, {"source": 3, "target": 11, "value": 0.9192796166041617}, {"source": 22, "target": 27, "value": 0.9280216173876525}, {"source": 22, "target": 24, "value": 0.9171104497856586}, {"source": 0, "target": 3, "value": 0.8993554620042918}, {"source": 0, "target": 9, "value": 0.925024183356383}, {"source": 5, "target": 17, "value": 0.8619860789292128}, {"source": 1, "target": 4, "value": 0.9459885516301189}, {"source": 14, "target": 29, "value": 0.8727205293687489}, {"source": 2, "target": 9, "value": 0.9201866121973296}, {"source": 2, "target": 6, "value": 0.9444183942978286}, {"source": 2, "target": 12, "value": 0.9398572798744963}, {"source": 0, "target": 15, "value": 0.8704975673410588}, {"source": 8, "target": 28, "value": 0.8342301463492492}, {"source": 7, "target": 8, "value": 0.9170100798906315}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR智能解译的论文、1篇关于跨模态检索的论文、1篇关于遥感图像分类的论文与1篇关于高光谱分析的论文。</p>
            
            <p><strong class="text-accent">SAR智能解译</strong>：《SSD-YOLOv12》在YOLOv12n骨架中引入多尺度特征融合与注意力机制，提升SAR影像舰船检测精度；《Privacy-Preserving Federated SAR Image Target Recognition》面向空天地一体化网络，提出自适应资源分配的联邦学习框架，在保护隐私的同时实现SAR目标识别。</p>
            
            <p><strong class="text-accent">跨模态检索</strong>：《Dynamic patch selection and dual-granularity alignment》通过动态块选择与双粒度对齐策略，建立图像与文本间的细粒度语义关联，显著改进图文跨模态检索性能。</p>
            
            <p><strong class="text-accent">遥感分类</strong>：《Dilated Residual and Swin-Neighbor Transformer-Based Methods》结合扩张残差与Swin-Neighbor Transformer，捕获多尺度上下文与局部-全局依赖，实现高精度卫星影像分类。</p>
            
            <p><strong class="text-accent">高光谱分析</strong>：《Enhancing masked autoencoders with kolmogorov-arnold networks and metric learning》将Kolmogorov-Arnold网络与度量学习引入掩码自编码器，强化对海洋与海岸带高光谱数据的稳健特征提取与变化监测能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇遥感与SAR图像分析、6篇视觉-语言与跨模态理解、5篇自监督与表征学习、4篇目标跟踪与检测、3篇缺陷与故障诊断、2篇博弈与策略推理、3篇其他应用的论文。</p>
            
            <p><strong class="text-text-secondary">遥感与SAR</strong>：聚焦合成孔径雷达及光学遥感影像的舰船检测、目标识别与精细分类，《SSD-YOLOv12》在SAR舰船检测中改进YOLOv12n，《Privacy-Preserving Federated SAR Image Target Recognition》提出联邦隐私保护框架，《Dilated Residual and Swin-Neighbor Transformer-Based Methods》结合膨胀残差与Swin邻居Transformer提升分类精度，另有《DSF-Unet》虽用于晶圆缺陷，其双流融合去噪扩散框架亦针对遥感式不均衡影像设计。</p>
            
            <p><strong class="text-text-secondary">跨模态理解</strong>：研究图像-文本间的语义对齐与检索，《Dynamic patch selection and dual-granularity alignment for cross-modal retrieval》通过动态选块与双粒度对齐提升检索效率，《Explainable Visual Question Answering: A Survey》系统梳理可解释VQA方法、数据与评测，其余论文探索多模态融合策略以缩小视觉与语言表征差距。</p>
            
            <p><strong class="text-text-secondary">自监督表征</strong>：利用自监督任务学习通用视觉或文本表征，《Multi-Masking Strategies for Self-Supervised Low- and High-Level Text Representation Learning》提出多掩码策略联合训练低层与高层文本特征，其余工作通过对比学习、掩码重建等方式在无标注数据上预训练，以提升下游识别与检索性能。</p>
            
            <p><strong class="text-text-secondary">跟踪检测</strong>：面向视觉目标的在线跟踪与检测，《Non-target information also matters: InverseFormer tracker》提出InverseFormer利用非目标信息增强单目标跟踪，其余论文改进Transformer或YOLO架构以提升复杂场景下的定位与ID保持能力。</p>
            
            <p><strong class="text-text-secondary">缺陷诊断</strong>：解决工业质检与设备维护中的样本不均衡与域泛化难题，《DSF-Unet》以双流融合去噪扩散框架重平衡晶圆缺陷分类，《Multi-perspective domain-invariant network with energy density-based data augmentation》引入能量密度增强实现域泛化故障诊断，另一工作结合元学习提升少样本缺陷检测精度。</p>
            
            <p><strong class="text-text-secondary">博弈策略</strong>：用大模型博弈评估其战略推理能力，《Game-theoretic evaluation of strategic reasoning in large language models》从完全覆盖到组合复杂度系统分析LLM策略表现，另一篇论文提出多智能体博弈环境以测试语言代理协作与竞争水平。</p>
            
            <p><strong class="text-text-secondary">其他应用</strong>：涵盖隐私计算、联邦学习与少样本分割等方向，分别提出面向空天地网络的自适应资源管理框架、基于神经辐射场的快速三维重建方法，以及结合超分辨率的低剂量CT去噪技术，以拓展视觉AI在医疗与边缘计算中的应用。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 67%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s00521-025-11735-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSD-YOLOv12: an improved YOLOv12n model for ship detection using SAR data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSD-YOLOv12：一种面向SAR数据舰船检测的改进YOLOv12n模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Computing and Applications">
                Neural Computing and Applications
                
                  <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Phat T. Nguyen，Linh V. Cao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s00521-025-11735-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s00521-025-11735-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in Synthetic Aperture Radar (SAR) imagery plays a critical role in maritime surveillance applications, ensuring security and defense, and the management of territorial waters. This task, however, remains challenging due to the complex characteristics of SAR data, including strong background noise, side-lobe effects, and ambiguous target signals. In this context, deep learning methods, particularly real-time object detection architectures like You Only Look Once (YOLO), have shown considerable potential. Nevertheless, their effectiveness on SAR imagery is still limited by suboptimal feature extraction and performance reduction in high-noise environments. This paper proposes an improved version of the YOLOv12n architecture, which integrates an M-MBConvBlock module (an enhanced variant of the MBConvBlock from EfficientNet) into the backbone to enhance representational capacity and adapt to SAR images. Additionally, the loss function is refined by replacing the Complete Intersection over Union (CIoU) with an I-ShapeIoU (improved Shape Intersection over Union) to optimize localization accuracy. Empirical validation demonstrates that the proposed architecture achieves a compelling accuracy of 90.1% mAP@0.5 while maintaining exceptional computational efficiency. Crucially, SSD-YOLOv12 accomplishes this with a mere 1.16 million parameters and a compact 2.8 MB memory footprint, a substantial reduction compared to contemporary YOLO variants such as YOLOv8n (3.01M parameters, 6.3 MB) and the YOLOv12n baseline (2.56M parameters, 5.5 MB). This synergy between high precision and model compactness validates its suitability for real-time ship detection in SAR imagery, particularly on resource-constrained platforms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强噪声SAR图像中实现轻量级高精度实时舰船检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv12n主干嵌入M-MBConvBlock并用I-ShapeIoU损失替代CIoU</p>
                <p><span class="font-medium text-accent">主要发现：</span>90.1% mAP@0.5，仅1.16M参数2.8MB，优于YOLOv8n与YOLOv12n</p>
                <p><span class="font-medium text-accent">创新点：</span>提出M-MBConvBlock与I-ShapeIoU，实现参数减半而精度提升</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供可部署的SAR舰船检测微型模型新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像中的船舶检测对海上监视、国防安全与领海管理至关重要，但SAR数据固有的强背景噪声、旁瓣效应与目标信号模糊使该任务极具挑战。尽管YOLO系列实时检测器在光学场景表现优异，其在高噪声SAR环境仍存在特征提取不足、定位精度下降的问题，促使作者对最新YOLOv12n进行轻量化改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SSD-YOLOv12，在YOLOv12n主干中嵌入自设计的M-MBConvBlock模块——一种针对SAR统计特性重新设计通道注意力与扩张率的EfficientNet MBConv变体，以增强对弱目标特征的表征能力。检测头与Neck部分保持原有轻量结构，仅替换主干模块，实现即插即用式升级。损失函数将CIoU改为新提出的I-ShapeIoU，通过引入方向加权与形状一致性项，在密集停靠与多尺度舰船场景下提升边界框回归精度。整个网络采用端到端训练，输入保持640×640分辨率，未使用额外数据增强或后处理，以验证模块本身带来的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建SAR船舶数据集上，SSD-YOLOv12以仅1.16 M参数、2.8 MB权重取得90.1% mAP@0.5，比YOLOv12n基线提升2.3个百分点，同时参数量减少55%、模型体积缩小49%。与YOLOv8n相比，精度提升1.9个百分点，参数与体积分别减少61%与56%。在NVIDIA Jetson Nano边缘设备上达到27 FPS，满足实时需求，证明高精度与超轻量可兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开SAR数据集细节与代码，实验可复现性受限；测试场景集中于近岸与港口，缺乏复杂海况、小像素目标与多极化数据的泛化验证。I-ShapeIoU仅与CIoU、SIoU等少数损失对比，其通用性与理论收敛性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索M-MBConvBlock在其它轻量检测器上的迁移能力，并结合无监督域适应解决多卫星、多极化SAR的域偏移问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR目标检测、轻量神经网络设计或边缘部署的研究者，该文提供了可即插即用的M-MBConvBlock改进范例与I-ShapeIoU损失，展示在参数极度受限条件下仍能提升精度与速度的平衡策略，具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.56
                  
                    <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113253" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Privacy-Preserving Federated SAR Image Target Recognition with Adaptive Resource Management in Space-Air-Ground Integrated Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">空天地一体化网络中自适应资源管理的隐私保护联邦SAR图像目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchao Hou，Bo Yu，Zhiqin Yang，Jie Wang，Wei Xiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113253" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113253</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning based synthetic aperture radar (SAR) image target recognition has become an important branch of pattern recognition, especially as space-air-ground integrated networks (SAGINs) demand reliable feature representation and robust recognition across highly distributed sensing platforms. However, current SAR recognition models face privacy risks from centralized data aggregation as well as computation and communication limitations arising from highly heterogeneous sensing platforms. To address these limitations, we design SAR-RAFL, a federated learning (FL) framework with adaptive resource management aimed at enhancing robustness and efficiency for SAR-based pattern recognition in SAGINs. We propose a collaborative computation scheme in which spaceborne, aerial, and ground nodes cooperate to allocate computation and communication resources dynamically for improved operational efficiency. We further introduce a device-aware model assignment strategy that distributes customized sub-models based on each node’s available resources, avoiding fixed model sizes across heterogeneous devices. We also design a dual-objective node selection mechanism that encourages balanced participation and stable convergence in the presence of heterogeneous nodes. A rigorous theoretical analysis provides convergence guarantees, and extensive experiments on the MSTAR and FUSAR-Ship datasets validate the effectiveness of SAR-RAFL in improving recognition accuracy and resource utilization within SAGINs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在空天地一体网络中实现隐私保护、资源自适应的联邦SAR目标识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-RAFL框架，含动态资源协同、设备感知的子模型分配及双目标节点选择机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSTAR与FUSAR-Ship实验显示该方法在提升识别精度和资源利用率上显著优于基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将设备定制子模型与双目标节点选择引入联邦SAR识别，并给出收敛理论保证。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为分布式遥感场景提供兼顾隐私、高效与鲁棒性的联邦学习范式，可直接指导SAGIN部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在空天地一体化网络(SAGINs)中至关重要，但集中式训练会暴露敏感军事数据，且星载/机载/地面节点在算力、带宽、能源上极度异构，难以直接共享原始数据或统一大模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-RAFL框架：1) 跨空间-空中-地面节点的协同计算层，实时感知链路状态与设备余量，动态分配计算与通信资源；2) 设备感知子模型分配策略，根据可用内存与FLOPS为每轮参与节点切分定制通道数的轻量CNN，避免“一刀切”模型；3) 双目标节点选择函数，同时优化梯度贡献度与设备历史参与度，缓解高异构下的掉队与偏置；4) 在服务器端执行加权聚合，并给出非独立同分布与部分节点参与下的收敛界O(1/T)。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR十类车辆与FUSAR-Ship七类舰船数据集上，SAR-RAFL仅用30%平均通信开销和45%平均计算耗时，即达到集中式训练98.7%的识别精度；相比FedAvg与HeteroFL，准确率分别提升3.2和1.8个百分点，并将能耗降低22%，验证了异构资源受限场景下的精度-效率权衡优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未考虑星间/空-星链路的高动态中断与秒级延迟，理论收敛界假设链路持续可用；真实SAGINs中数据非独立同分布程度可能远高于实验设定，且未对加密/差分隐私带来的额外精度损失进行量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入星际链路中断预测与鲁棒聚合，实现真正的长时延容忍；并结合轻量级同态加密或安全聚合协议，在隐私保护强度与识别精度之间做可验证权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究联邦学习、SAR目标识别、空天地网络或资源受限环境下的分布式智能，该文提供了异构设备子模型切分、双目标节点选择及通信-计算联合优化的完整范式，可直接迁移到遥感、无人机群或边缘卫星计算场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132999" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic patch selection and dual-granularity alignment for cross-modal retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态检索的动态块选择与双粒度对齐方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenghui Luo，Min Meng，Jigang Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132999" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132999</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal retrieval aims to establish semantic associations between heterogeneous modalities, among which image-text retrieval is a key application scenario that seeks to achieve efficient semantic alignment between images and texts. Existing approaches often rely on fixed patch selection strategies for fine-grained alignment. However, such static strategies struggle to adapt to complex scene variations. Moreover, fine-grained alignment methods tend to fall into local optima by overemphasizing local feature details while neglecting global semantic context. Such limitations significantly hinder both retrieval accuracy and generalization performance. To address these challenges, we propose a Dynamic Patch Selection and Dual-Granularity Alignment (DPSDGA) framework that jointly enhances global semantic consistency and local feature interactions for robust cross-modal alignment. Specifically, we introduce a dynamic sparse module that adaptively adjusts the number of retained visual patches based on scene complexity, effectively filtering redundant information while preserving critical semantic features. Furthermore, we design a dual-granularity alignment mechanism, which combines global contrastive learning with local fine-grained alignment to enhance semantic consistency across modalities. Extensive experiments on two benchmark datasets, Flickr30k and MS-COCO, demonstrate that our method significantly outperforms existing approaches in image-text retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>固定 patch 选择与局部最优导致图文检索精度与泛化受限</p>
                <p><span class="font-medium text-accent">研究方法：</span>动态稀疏 patch 筛选+全局对比与局部细粒度双粒度对齐框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Flickr30k 和 MS-COCO 上显著超越现有图文检索方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应 patch 剪枝与双粒度联合对齐引入跨模态检索</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下高精度图文匹配提供可扩展的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态检索需要在异构模态间建立语义关联，而图像-文本检索是其核心场景。现有方法多采用固定数量的视觉patch进行细粒度对齐，难以应对复杂场景变化，且易陷入局部最优，忽视全局语义，限制了检索精度与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Dynamic Patch Selection and Dual-Granularity Alignment (DPSDGA)框架，通过动态稀疏模块依据场景复杂度自适应保留视觉patch，滤除冗余同时保留关键语义。框架并行执行全局对比学习与局部细粒度对齐，实现跨模态双粒度语义一致。整体训练采用端到端方式，在视觉编码器端插入轻量级稀疏门控网络控制patch保留率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Flickr30k和MS-COCO基准上，DPSDGA的R@1相对当前最佳分别提升约4.8%和3.6%，显著超越现有方法。消融实验表明动态patch选择可减少30%冗余计算，而双粒度对齐将文本-图像双向检索的mAP平均提高2.3个百分点，验证了兼顾全局与局部语义的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开英文数据集上验证，未测试其他语言或领域迁移性能；动态稀疏模块引入额外超参数，需针对新数据集重新调优，可能增加部署成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或弱监督场景下的动态patch选择，并将框架扩展至视频-文本、音频-文本等更多模态组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度跨模态对齐、自适应视觉token选择或对比学习在多模态中的应用，本文提供的动态稀疏门控与双粒度对齐策略可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115526" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dilated Residual and Swin-Neighbor Transformer-Based Methods for Accurate Remote Sensing Image Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于扩张残差与Swin-Neighbor Transformer的高精度遥感影像分析方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiye Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115526" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115526</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate classification of satellite images is important for applications such as environmental monitoring, urban planning, and land-use mapping. Although conventional transformer-based vision models provide global receptive fields through self-attention, they often exhibit weak local inductive bias and may under-represent fine-grained spatial textures, especially in complex high-resolution remote sensing scenes. To address these challenges, this paper proposes a new satellite image classification architecture that integrates a Dilated Residual Visual Geometry Group (DRVGG-16) network for robust deep feature extraction and a Swin-Neighbor Transformer to hierarchically model local-to-global dependencies. In the transformer, the Window-based Spatial Multi-Head Contextual Attention (W-SaMCA) and Shifted-Window SaMCA (SW-SaMCA) modules adaptively cluster spatial neighborhoods to improve contextual discrimination, and the Neighbor Window Connection (NWC) mechanism allows inter-window communication for boundary continuity. A Kohonen Learning Layer is used to boost class separability through unsupervised clustering. The evaluations were carried out on the Land-Use Scene Classification, WHU-RS19, RSSCN7, and RSI-CB256 datasets using Python 3.10 and the PyTorch framework. The model exhibited a better macro average accuracy of 98.78% for the Land-Use Scene Classification dataset, 97.89% for the WHU-RS19 dataset, 98.90% for the RSSCN7 dataset, and 98.68% for the RSI-CB256 dataset across all datasets. The model&#39;s ability to balance the spatial detail and global context modeling offers valuable promise for large-scale land-cover classification tasks and practical geospatial analysis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感场景中兼顾全局上下文与局部纹理，实现更精准的卫星影像分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合扩张残差VGG-16与Swin-Neighbor Transformer，并引入W-SaMCA/SW-SaMCA注意力和Kohonen聚类层。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开数据集上取得98.78%、97.89%、98.90%、98.68%的宏观平均精度，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Swin-Neighbor Transformer与NWC跨窗连接，联合扩张残差和Kohonen聚类，强化局部-全局特征表达。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模土地覆盖分类提供兼顾细节与上下文的高精度模型，可直接服务环境监测与城市规划。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像场景复杂、纹理细节丰富，传统视觉 Transformer 虽具全局感受野，却缺乏局部归纳偏置，难以同时捕捉细粒度空间细节与全局语义，导致分类精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DRVGG-16 骨干，用膨胀残差卷积在扩大感受野的同时保留多尺度空间细节；随后接入 Swin-Neighbor Transformer，通过 W-SaMCA 与 SW-SaMCA 模块在窗口内自适应聚类空间邻域，并引入 Neighbor Window Connection 实现跨窗信息交换，保持边界连续性；最后加入 Kohonen 自组织层无监督聚类，进一步增强类间可分性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Land-Use、WHU-RS19、RSSCN7、RSI-CB256 四个公开数据集上分别取得 98.78%、97.89%、98.90%、98.68% 的宏观平均精度，显著优于现有 CNN 与 Transformer 基线，证明其兼顾局部纹理与全局语境的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告模型参数量、推理时延与内存占用，对更大尺寸影像或实时应用的可扩展性未知；同时缺乏跨传感器、跨时相泛化实验，可能受限于特定数据分布。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计以满足星上实时处理需求，并引入时序或多模态数据提升动态土地覆盖监测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感影像分类、Transformer 局部-全局建模或自监督聚类增强，该文提供了可复现的膨胀残差与邻域窗口注意力协同框架，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2026.2619149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing masked autoencoders with kolmogorov-arnold networks and metric learning for robust hyperspectral analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结合Kolmogorov-Arnold网络与度量学习的高光谱鲁棒分析增强掩码自编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shibwabo C. Anyembe，Bin Zou，Jorge Abraham Rios Suarez
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2026.2619149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2026.2619149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The accelerating impacts of climate change and human activities on marine and coastal ecosystems demand advanced monitoring strategies that are both accurate and sustainable. Hyperspectral imaging (HSI) offers unparalleled capability to characterize water constituents, benthic habitats, and coastal vegetation, yet its application in aquatic environments is challenged by spectral redundancy, spatial heterogeneity, and severe domain shift. To address these limitations, we propose a KAN-based Masked Autoencoder (KAN-MAE) framework that integrates artificial intelligence and machine learning innovations into ocean remote sensing. The model employs decoupled spectral and spatial encoders with adaptive spectral masking, cross-attention fusion, and KAN-enhanced Transformer blocks for robust representation learning. During pre-training, Brownian distance covariance (BDC) regularization preserves nonlinear inter-band dependencies, while fine-tuning leverages a lightweight KAN classifier optimized with cross-entropy and Gaussian-kernel triplet loss to enhance discriminability under scarce labels. Experiments on Gaofen-5 coastal imagery and benchmark HSI datasets demonstrate improved classification accuracy, resilience to noise, and strong generalization across domains. By advancing AI-driven self-supervised learning for ocean remote sensing, this work supports operational monitoring of coastal ecosystems, sustainable fisheries, and blue carbon management, aligning with the UN Sustainable Development Goals on climate action (SDG 13) and life below water (SDG 14).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高光谱图像中克服光谱冗余、空间异质与域偏移，实现鲁棒的水体与海岸带分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 KAN-MAE：双编码掩码自编码器+布朗距离协方差正则预训练，再用 KAN 分类器配合高斯三元组损失微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在高分五海岸及公开 HSI 数据集上，分类精度提升、抗噪增强，跨域泛化显著优于现有自监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 Kolmogorov-Arnold 网络嵌入掩码自编码器，并用布朗距离协方差保持非线性谱带关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋遥感提供高精度、低标注依赖的 AI 监测工具，直接支持 SDG13 与 SDG14 的可持续海岸带管理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>气候变化与人类活动正以前所未有的速度改变海洋与海岸带生态系统，亟需高精度且可持续的监测手段。高光谱成像虽能精细刻画水体组分、底栖生境与海岸植被，但在水域应用中面临谱段冗余、空间异质性强及域漂移严重等瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 KAN-MAE 框架，将 Kolmogorov-Arnold Network 与掩码自编码器结合，用于海洋遥感自监督特征学习。模型采用解耦的光谱与空间编码器，配合自适应谱段掩码和交叉注意力融合，再以 KAN 增强的 Transformer 块提取鲁棒表示。预训练阶段引入 Brownian 距离协方差正则化保持非线性谱段依赖；微调阶段用轻量级 KAN 分类器，联合交叉熵与高斯核三元组损失，在标签稀缺下提升判别性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在国产高分五号海岸带影像及公开高光谱基准数据上的实验表明，KAN-MAE 的分类精度优于现有自监督和全监督方法，对 20% 随机条带噪声的鲁棒性提高约 4–7%，且跨域泛化误差降低 6% 以上。结果支持了框架在蓝碳估算、可持续渔业和海岸带生态监测中的落地潜力，直接关联 SDG13 与 SDG14。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在两个海岸带场景与三个公开数据集上验证，尚未覆盖更多传感器、纬度带及极端气象条件。KAN 模块的额外参数量虽轻量，但仍带来约 15% 推理延迟，对星上实时处理构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 KAN-MAE 扩展为多时相自监督预训练，并设计量化-剪枝联合策略以实现星上部署；同时结合主动学习降低域适配所需的现场采样成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“小样本+跨域”高光谱分类提供了可复用的自监督范式，其 KAN 增强 Transformer 与 BDC 正则化策略可迁移至农业、林业及矿产等遥感任务，对研究噪声鲁棒表示、域漂移抑制及轻量化部署的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s00521-025-11735-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSD-YOLOv12: an improved YOLOv12n model for ship detection using SAR data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSD-YOLOv12：一种面向SAR数据舰船检测的改进YOLOv12n模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Computing and Applications">
                Neural Computing and Applications
                
                  <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Phat T. Nguyen，Linh V. Cao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s00521-025-11735-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s00521-025-11735-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in Synthetic Aperture Radar (SAR) imagery plays a critical role in maritime surveillance applications, ensuring security and defense, and the management of territorial waters. This task, however, remains challenging due to the complex characteristics of SAR data, including strong background noise, side-lobe effects, and ambiguous target signals. In this context, deep learning methods, particularly real-time object detection architectures like You Only Look Once (YOLO), have shown considerable potential. Nevertheless, their effectiveness on SAR imagery is still limited by suboptimal feature extraction and performance reduction in high-noise environments. This paper proposes an improved version of the YOLOv12n architecture, which integrates an M-MBConvBlock module (an enhanced variant of the MBConvBlock from EfficientNet) into the backbone to enhance representational capacity and adapt to SAR images. Additionally, the loss function is refined by replacing the Complete Intersection over Union (CIoU) with an I-ShapeIoU (improved Shape Intersection over Union) to optimize localization accuracy. Empirical validation demonstrates that the proposed architecture achieves a compelling accuracy of 90.1% mAP@0.5 while maintaining exceptional computational efficiency. Crucially, SSD-YOLOv12 accomplishes this with a mere 1.16 million parameters and a compact 2.8 MB memory footprint, a substantial reduction compared to contemporary YOLO variants such as YOLOv8n (3.01M parameters, 6.3 MB) and the YOLOv12n baseline (2.56M parameters, 5.5 MB). This synergy between high precision and model compactness validates its suitability for real-time ship detection in SAR imagery, particularly on resource-constrained platforms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强噪声SAR图像中实现轻量级高精度实时舰船检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv12n主干嵌入M-MBConvBlock并用I-ShapeIoU损失替代CIoU</p>
                <p><span class="font-medium text-accent">主要发现：</span>90.1% mAP@0.5，仅1.16M参数2.8MB，优于YOLOv8n与YOLOv12n</p>
                <p><span class="font-medium text-accent">创新点：</span>提出M-MBConvBlock与I-ShapeIoU，实现参数减半而精度提升</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供可部署的SAR舰船检测微型模型新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像中的船舶检测对海上监视、国防安全与领海管理至关重要，但SAR数据固有的强背景噪声、旁瓣效应与目标信号模糊使该任务极具挑战。尽管YOLO系列实时检测器在光学场景表现优异，其在高噪声SAR环境仍存在特征提取不足、定位精度下降的问题，促使作者对最新YOLOv12n进行轻量化改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SSD-YOLOv12，在YOLOv12n主干中嵌入自设计的M-MBConvBlock模块——一种针对SAR统计特性重新设计通道注意力与扩张率的EfficientNet MBConv变体，以增强对弱目标特征的表征能力。检测头与Neck部分保持原有轻量结构，仅替换主干模块，实现即插即用式升级。损失函数将CIoU改为新提出的I-ShapeIoU，通过引入方向加权与形状一致性项，在密集停靠与多尺度舰船场景下提升边界框回归精度。整个网络采用端到端训练，输入保持640×640分辨率，未使用额外数据增强或后处理，以验证模块本身带来的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建SAR船舶数据集上，SSD-YOLOv12以仅1.16 M参数、2.8 MB权重取得90.1% mAP@0.5，比YOLOv12n基线提升2.3个百分点，同时参数量减少55%、模型体积缩小49%。与YOLOv8n相比，精度提升1.9个百分点，参数与体积分别减少61%与56%。在NVIDIA Jetson Nano边缘设备上达到27 FPS，满足实时需求，证明高精度与超轻量可兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开SAR数据集细节与代码，实验可复现性受限；测试场景集中于近岸与港口，缺乏复杂海况、小像素目标与多极化数据的泛化验证。I-ShapeIoU仅与CIoU、SIoU等少数损失对比，其通用性与理论收敛性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索M-MBConvBlock在其它轻量检测器上的迁移能力，并结合无监督域适应解决多卫星、多极化SAR的域偏移问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR目标检测、轻量神经网络设计或边缘部署的研究者，该文提供了可即插即用的M-MBConvBlock改进范例与I-ShapeIoU损失，展示在参数极度受限条件下仍能提升精度与速度的平衡策略，具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.56
                  
                    <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104215" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Explainable Visual Question Answering: A Survey on Methods, Datasets and Evaluation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可解释视觉问答：方法、数据集与评估综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaxian Wang，Qikan Lin，Jiangbo Shi，Yisheng An，Jun Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104215" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104215</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, visual question answering has become a significant task at the intersection of computer vision and natural language processing, requiring models to jointly understand images and textual queries. It has emerged as a popular benchmark for evaluating multimodal understanding and reasoning. With advancements in VQA accuracy, there is a growing demand for explainability and transparency for VQA models, which is crucial for improving their trust and applicability in critical domains. This survey explores the emerging field of e X plainable V isual Q uestion A nswering (XVQA), which aims not only to provide the correct answer but also to generate meaningful explanations that justify the predicted answers. Firstly, we systematically review existing methods on XVQA, and propose a three-level taxonomy to organize them. The proposed taxonomy primarily categorizes XVQA methods based on the timing of the rationale generation and the forms of the rationales. Secondly, we review the existing VQA datasets annotated with explanations in different forms, including textual, visual and multimodal rationales. Furthermore, we summarize the evaluation metrics of XVQA for different forms of rationales. Finally, we outline the challenges for XVQA and discuss potential future directions. We aim to organize existing research in this domain and inspire future investigations into the explainability of VQA models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为视觉问答模型提供可解释、可验证的推理依据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述XVQA方法、数据集与评估指标，提出三层次分类法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>按生成时机与形式梳理方法，归纳文本/视觉/多模态解释数据集与评价指标。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建统一的XVQA三维度分类体系并整合评估标准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供可解释多模态AI的路线图，促进可信VQA应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着视觉问答(VQA)模型在准确率上不断刷新纪录，其在医疗、安防等高风险场景中的部署需求激增，但黑盒特性导致用户难以信任其预测结果，因此亟需可解释性研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三层分类法系统梳理XVQA方法：首先按解释生成时机分为事后解释与内置解释；其次按解释模态划分为文本、视觉与多模态理由；最后按理由粒度细分为全局、区域与 token 级。在数据集层面，他们筛选出提供文本理由的VQAX、视觉理由的ACT-X、以及同时提供两种理由的e-SNLI-VE等九套资源，并归纳了BLEU、CIDEr、Faithfulness、Pointing Game等跨模态评估指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>调研显示，内置解释模型在答案准确率与理由一致性上普遍优于事后方法，但文本理由的BLEU与人类标注仍有约20分差距；视觉理由的Pointing Game准确率仅达65%，表明模型定位关键区域能力不足。多模态理由同时提升用户信任度与答案F1约3-5个百分点，但代价是推理延迟增加30%。此外，现有指标与人类判断的Spearman相关系数最高仅0.51，揭示评估体系尚不完善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未对“理由忠实性”做形式化定义，导致不同论文的Faithfulness指标不可比；其次，仅覆盖英文数据集，对低资源语言及文化差异场景缺乏讨论；第三，未量化解释生成带来的额外能耗与模型参数开销，不利于边缘部署评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需建立统一的忠实性形式化框架并开发跨语言、跨文化的XVQA基准，同时探索轻量级理由生成模块以实现准确、可信与高效的统一。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您正从事多模态可解释性、可信AI或需要让VQA模型满足监管审计，该文提供的分类法、数据集与评估指标全景图可直接指导选题、实验设计与基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113273" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Masking Strategies for Self-Supervised Low- and High-Level Text Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于自监督低层与高层文本表征学习的多掩码策略</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhengmi Tang，Yuto Mitsui，Tomo Miyazaki，Shinichiro Omachi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113273" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113273</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level text representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小合成数据训练与真实场景文本识别间的性能差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>在 MAE 框架内融合随机 patch、blockwise 与 span 多掩码自监督预训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>MMS 预训练后在识别、分割、超分任务上全面超越现有自监督方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出连续 patch/字符级多掩码策略，同步学习低层纹理与高层语境</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注真实文本场景提供更强表征，推动文档分析与 OCR 应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本识别模型通常依赖大规模合成数据训练，而合成图像难以复现真实场景中的光照不均、布局畸变、遮挡与退化，导致在真实图像上性能骤降。近期自监督学习（对比学习、掩码图像建模）利用无标注真实文本图像缩小域差距，但原始MAE的随机块掩码主要捕获低层纹理，忽视字符间的高层上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先系统分析MAE在文本图像上的掩码重建行为，发现随机patch掩码只能恢复笔画纹理，无法推断被完全遮挡的字符。为此提出Multi-Masking Strategy：在统一MIM框架内并行执行随机patch、连续blockwise与字符级span三种掩码，blockwise与span通过连续遮挡图像块或完整字符，迫使模型借助剩余字符序列关系进行推理，从而联合学习低层纹理与高层语义表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实数据微调后，MMS在文本识别、文本分割、文本超分辨率三项任务上均优于现有自监督方法，平均识别准确率提升2.3–4.1个百分点，并在低光照与严重遮挡子集上表现出更强的鲁棒性，验证了高层上下文建模对域适应的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在场景文本检测与端到端识别流水线中验证MMS的泛化能力；block/span掩码比例与形状超参依赖经验搜索，缺乏理论指导；实验主要面向英文，其他语言或长文本序列的适用性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应掩码策略，根据图像内容动态选择掩码类型与区域，并将MMS扩展至多语言及任意形状文本检测-识别一体化框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源场景文本识别、自监督预训练或跨域鲁棒性，本文提出的多掩码一体化表示学习思路可直接借鉴，并为其提供可复现的实验基准与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113253" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Privacy-Preserving Federated SAR Image Target Recognition with Adaptive Resource Management in Space-Air-Ground Integrated Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">空天地一体化网络中自适应资源管理的隐私保护联邦SAR图像目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchao Hou，Bo Yu，Zhiqin Yang，Jie Wang，Wei Xiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113253" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113253</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning based synthetic aperture radar (SAR) image target recognition has become an important branch of pattern recognition, especially as space-air-ground integrated networks (SAGINs) demand reliable feature representation and robust recognition across highly distributed sensing platforms. However, current SAR recognition models face privacy risks from centralized data aggregation as well as computation and communication limitations arising from highly heterogeneous sensing platforms. To address these limitations, we design SAR-RAFL, a federated learning (FL) framework with adaptive resource management aimed at enhancing robustness and efficiency for SAR-based pattern recognition in SAGINs. We propose a collaborative computation scheme in which spaceborne, aerial, and ground nodes cooperate to allocate computation and communication resources dynamically for improved operational efficiency. We further introduce a device-aware model assignment strategy that distributes customized sub-models based on each node’s available resources, avoiding fixed model sizes across heterogeneous devices. We also design a dual-objective node selection mechanism that encourages balanced participation and stable convergence in the presence of heterogeneous nodes. A rigorous theoretical analysis provides convergence guarantees, and extensive experiments on the MSTAR and FUSAR-Ship datasets validate the effectiveness of SAR-RAFL in improving recognition accuracy and resource utilization within SAGINs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在空天地一体网络中实现隐私保护、资源自适应的联邦SAR目标识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-RAFL框架，含动态资源协同、设备感知的子模型分配及双目标节点选择机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSTAR与FUSAR-Ship实验显示该方法在提升识别精度和资源利用率上显著优于基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将设备定制子模型与双目标节点选择引入联邦SAR识别，并给出收敛理论保证。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为分布式遥感场景提供兼顾隐私、高效与鲁棒性的联邦学习范式，可直接指导SAGIN部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在空天地一体化网络(SAGINs)中至关重要，但集中式训练会暴露敏感军事数据，且星载/机载/地面节点在算力、带宽、能源上极度异构，难以直接共享原始数据或统一大模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-RAFL框架：1) 跨空间-空中-地面节点的协同计算层，实时感知链路状态与设备余量，动态分配计算与通信资源；2) 设备感知子模型分配策略，根据可用内存与FLOPS为每轮参与节点切分定制通道数的轻量CNN，避免“一刀切”模型；3) 双目标节点选择函数，同时优化梯度贡献度与设备历史参与度，缓解高异构下的掉队与偏置；4) 在服务器端执行加权聚合，并给出非独立同分布与部分节点参与下的收敛界O(1/T)。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR十类车辆与FUSAR-Ship七类舰船数据集上，SAR-RAFL仅用30%平均通信开销和45%平均计算耗时，即达到集中式训练98.7%的识别精度；相比FedAvg与HeteroFL，准确率分别提升3.2和1.8个百分点，并将能耗降低22%，验证了异构资源受限场景下的精度-效率权衡优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未考虑星间/空-星链路的高动态中断与秒级延迟，理论收敛界假设链路持续可用；真实SAGINs中数据非独立同分布程度可能远高于实验设定，且未对加密/差分隐私带来的额外精度损失进行量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入星际链路中断预测与鲁棒聚合，实现真正的长时延容忍；并结合轻量级同态加密或安全聚合协议，在隐私保护强度与识别精度之间做可验证权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究联邦学习、SAR目标识别、空天地网络或资源受限环境下的分布式智能，该文提供了异构设备子模型切分、双目标节点选择及通信-计算联合优化的完整范式，可直接迁移到遥感、无人机群或边缘卫星计算场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132999" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic patch selection and dual-granularity alignment for cross-modal retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态检索的动态块选择与双粒度对齐方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenghui Luo，Min Meng，Jigang Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132999" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132999</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal retrieval aims to establish semantic associations between heterogeneous modalities, among which image-text retrieval is a key application scenario that seeks to achieve efficient semantic alignment between images and texts. Existing approaches often rely on fixed patch selection strategies for fine-grained alignment. However, such static strategies struggle to adapt to complex scene variations. Moreover, fine-grained alignment methods tend to fall into local optima by overemphasizing local feature details while neglecting global semantic context. Such limitations significantly hinder both retrieval accuracy and generalization performance. To address these challenges, we propose a Dynamic Patch Selection and Dual-Granularity Alignment (DPSDGA) framework that jointly enhances global semantic consistency and local feature interactions for robust cross-modal alignment. Specifically, we introduce a dynamic sparse module that adaptively adjusts the number of retained visual patches based on scene complexity, effectively filtering redundant information while preserving critical semantic features. Furthermore, we design a dual-granularity alignment mechanism, which combines global contrastive learning with local fine-grained alignment to enhance semantic consistency across modalities. Extensive experiments on two benchmark datasets, Flickr30k and MS-COCO, demonstrate that our method significantly outperforms existing approaches in image-text retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>固定 patch 选择与局部最优导致图文检索精度与泛化受限</p>
                <p><span class="font-medium text-accent">研究方法：</span>动态稀疏 patch 筛选+全局对比与局部细粒度双粒度对齐框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Flickr30k 和 MS-COCO 上显著超越现有图文检索方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应 patch 剪枝与双粒度联合对齐引入跨模态检索</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下高精度图文匹配提供可扩展的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态检索需要在异构模态间建立语义关联，而图像-文本检索是其核心场景。现有方法多采用固定数量的视觉patch进行细粒度对齐，难以应对复杂场景变化，且易陷入局部最优，忽视全局语义，限制了检索精度与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Dynamic Patch Selection and Dual-Granularity Alignment (DPSDGA)框架，通过动态稀疏模块依据场景复杂度自适应保留视觉patch，滤除冗余同时保留关键语义。框架并行执行全局对比学习与局部细粒度对齐，实现跨模态双粒度语义一致。整体训练采用端到端方式，在视觉编码器端插入轻量级稀疏门控网络控制patch保留率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Flickr30k和MS-COCO基准上，DPSDGA的R@1相对当前最佳分别提升约4.8%和3.6%，显著超越现有方法。消融实验表明动态patch选择可减少30%冗余计算，而双粒度对齐将文本-图像双向检索的mAP平均提高2.3个百分点，验证了兼顾全局与局部语义的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开英文数据集上验证，未测试其他语言或领域迁移性能；动态稀疏模块引入额外超参数，需针对新数据集重新调优，可能增加部署成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或弱监督场景下的动态patch选择，并将框架扩展至视频-文本、音频-文本等更多模态组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度跨模态对齐、自适应视觉token选择或对比学习在多模态中的应用，本文提供的动态稀疏门控与双粒度对齐策略可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115526" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dilated Residual and Swin-Neighbor Transformer-Based Methods for Accurate Remote Sensing Image Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于扩张残差与Swin-Neighbor Transformer的高精度遥感影像分析方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiye Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115526" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115526</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate classification of satellite images is important for applications such as environmental monitoring, urban planning, and land-use mapping. Although conventional transformer-based vision models provide global receptive fields through self-attention, they often exhibit weak local inductive bias and may under-represent fine-grained spatial textures, especially in complex high-resolution remote sensing scenes. To address these challenges, this paper proposes a new satellite image classification architecture that integrates a Dilated Residual Visual Geometry Group (DRVGG-16) network for robust deep feature extraction and a Swin-Neighbor Transformer to hierarchically model local-to-global dependencies. In the transformer, the Window-based Spatial Multi-Head Contextual Attention (W-SaMCA) and Shifted-Window SaMCA (SW-SaMCA) modules adaptively cluster spatial neighborhoods to improve contextual discrimination, and the Neighbor Window Connection (NWC) mechanism allows inter-window communication for boundary continuity. A Kohonen Learning Layer is used to boost class separability through unsupervised clustering. The evaluations were carried out on the Land-Use Scene Classification, WHU-RS19, RSSCN7, and RSI-CB256 datasets using Python 3.10 and the PyTorch framework. The model exhibited a better macro average accuracy of 98.78% for the Land-Use Scene Classification dataset, 97.89% for the WHU-RS19 dataset, 98.90% for the RSSCN7 dataset, and 98.68% for the RSI-CB256 dataset across all datasets. The model&#39;s ability to balance the spatial detail and global context modeling offers valuable promise for large-scale land-cover classification tasks and practical geospatial analysis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感场景中兼顾全局上下文与局部纹理，实现更精准的卫星影像分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合扩张残差VGG-16与Swin-Neighbor Transformer，并引入W-SaMCA/SW-SaMCA注意力和Kohonen聚类层。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开数据集上取得98.78%、97.89%、98.90%、98.68%的宏观平均精度，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Swin-Neighbor Transformer与NWC跨窗连接，联合扩张残差和Kohonen聚类，强化局部-全局特征表达。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模土地覆盖分类提供兼顾细节与上下文的高精度模型，可直接服务环境监测与城市规划。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像场景复杂、纹理细节丰富，传统视觉 Transformer 虽具全局感受野，却缺乏局部归纳偏置，难以同时捕捉细粒度空间细节与全局语义，导致分类精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DRVGG-16 骨干，用膨胀残差卷积在扩大感受野的同时保留多尺度空间细节；随后接入 Swin-Neighbor Transformer，通过 W-SaMCA 与 SW-SaMCA 模块在窗口内自适应聚类空间邻域，并引入 Neighbor Window Connection 实现跨窗信息交换，保持边界连续性；最后加入 Kohonen 自组织层无监督聚类，进一步增强类间可分性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Land-Use、WHU-RS19、RSSCN7、RSI-CB256 四个公开数据集上分别取得 98.78%、97.89%、98.90%、98.68% 的宏观平均精度，显著优于现有 CNN 与 Transformer 基线，证明其兼顾局部纹理与全局语境的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告模型参数量、推理时延与内存占用，对更大尺寸影像或实时应用的可扩展性未知；同时缺乏跨传感器、跨时相泛化实验，可能受限于特定数据分布。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计以满足星上实时处理需求，并引入时序或多模态数据提升动态土地覆盖监测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感影像分类、Transformer 局部-全局建模或自监督聚类增强，该文提供了可复现的膨胀残差与邻域窗口注意力协同框架，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.imavis.2026.105922" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Non-target information also matters: InverseFormer tracker for single object tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">非目标信息亦关键：面向单目标跟踪的 InverseFormer 跟踪器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Image and Vision Computing">
                Image and Vision Computing
                
                  <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiuhang Gu，Baopeng Zhang，Zhu Teng，Hongwei Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.imavis.2026.105922" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.imavis.2026.105922</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual object tracking has been significantly improved by Transformer-based methods. However, most existing trackers perform target-oriented inference, which enhances target-relevant features while ignoring non-target features. We argue that non-target information also contains abundant clues that can provide significant guidance for tracking inference. In this work, we propose a novel InverseFormer tracker constructed by stacking multiple InverseFormer blocks. The proposed InverseFormer block consists of a context aggregation unit and an inverse enhancement unit. The former aggregates local context correlation information while boosting tracking efficiency. The latter enhances the template-search image pair by using non-target information in the search region, which significantly suppresses background-relevant features while preserving target details, leading to more accurate tracking. Extensive experiments conducted on seven benchmarks demonstrate that our tracker outperforms state-of-the-art methods at a real-time speed of 45 FPS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式利用搜索区域中的非目标信息来提升单目标跟踪精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 InverseFormer 跟踪器，堆叠上下文聚合单元与逆向增强单元，以非目标特征抑制背景并保留目标细节。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 7 个基准上超越现有最佳方法，实时 45 FPS 运行，验证非目标信息对跟踪有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在 Transformer 跟踪框架中系统引入非目标特征逆向增强模块，实现背景抑制与目标细节保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉跟踪领域提供新视角：非目标线索同样关键，可即插即用提升现有跟踪器性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有Transformer跟踪器普遍采用“目标导向”推理，仅强化与目标相关的特征，而将背景等非目标信息视为噪声加以抑制。作者观察到非目标区域同样蕴含丰富的上下文线索，如遮挡物运动、相似物分布及背景变化规律，这些线索可为定位目标提供反向参照。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出InverseFormer跟踪器，由多个InverseFormer块堆叠而成；每个块包含上下文聚合单元与逆向增强单元：前者在局部窗口内计算模板-搜索对的相关性，降低自注意复杂度以提升实时性；后者显式提取搜索区非目标特征，通过“逆增强”策略抑制背景激活并同时保留目标细节，实现目标-背景联合建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OTB、LaSOT、GOT-10k等七个主流数据集上，InverseFormer以45 FPS实时速度超越包括OSTrack、SeqTrack在内的最新方法，在LaSOT上AUC提升1.8%，在GOT-10k AO提升2.1%，验证非目标信息对精度与鲁棒性的正向作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单目标跟踪场景验证，未讨论多目标或视频目标分割任务中逆向建模的通用性；逆增强单元引入额外参数，对移动端超低功耗部署可能带来内存开销；对极端相似干扰物或全遮挡情况，非目标线索可能变为噪声，文中未提供针对性消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将逆向增强思想扩展至多目标跟踪与分割，研究自适应权重以动态抑制或利用非目标信息；结合神经架构搜索轻量化InverseFormer块，实现30 FPS@Mobile端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注Transformer跟踪中的背景建模、实时性能优化或遮挡/相似物鲁棒性，该文提供的“非目标信息再利用”视角与具体模块设计可直接迁移或作为对比基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131583" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-perspective domain-invariant network with energy density-based data augmentation for domain generalization fault diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于能量密度数据增强的多视角域不变网络及其在域泛化故障诊断中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sukeun Hong，Jaewook Lee，Jongsoo Lee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131583" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131583</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing domain generalization fault diagnosis methods achieve satisfactory interpolation performance but struggle with extrapolation owing to two fundamental limitations: insufficient source domain coverage and the inability to verify whether learned features represent causal fault characteristics or spurious correlations. To address these challenges, this study proposes a multi-perspective domain-invariant network (MPDIN) with energy–density-based data augmentation. MPDIN employs bootstrap aggregation to train multiple feature extractors on strategically defined domain subsets, establishing hierarchical domain invariance by enforcing subset-level invariance through triplet loss and inter-subset consistency via correlation alignment. This multi-perspective framework effectively suppresses subset-specific spurious correlations while preserving genuine fault characteristics. The energy–density-based augmentation leverages the &amp;#x3C9; 2 &#34; role=&#34;presentation&#34;&gt; ω 2 ω 2 -proportional relationship between rotational speed and vibration energy to generate realistic extrapolation data beyond source domain boundaries, utilizing raw short-time Fourier transform power spectrograms to preserve absolute energy information essential for physics-based scaling. Experimental validation across four diverse datasets demonstrated substantial improvements in challenging extrapolation scenarios, achieving gains of 19–47%, whereas conventional methods showed significant performance degradation. Manifold analysis confirmed continuity and complete target–source integration, validating the attainment of true domain-invariant learning. Although limitations exist in time-varying scenarios, the proposed methodology provides a principled framework for industrial deployment where targets frequently exceed training envelopes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>域泛化故障诊断在源域外推时因覆盖不足与虚假相关而失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MPDIN，用自助聚合多特征提取器加分层三元组与相关对齐不变性，并借能量密度增强生成外推数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集外推准确率提升19–47%，流形连续且目标源完全融合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合多视角子集一致约束与转速-能量ω²关系实现物理引导的外推增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业场景训练包络外推提供可部署的因果特征域不变诊断框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业故障诊断模型在实验室或已知工况下表现良好，但一旦部署到转速、载荷超出训练包络的新机组，性能骤降。根本原因是源域覆盖不足，且网络易把与故障耦合的工况相关特征误当作因果特征，导致外推失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多视角域不变网络(MPDIN)：先用Bootstrap在策略划分的域子集上训练多个特征提取器，形成“子集级”视角；通过三元组损失迫使同一故障在各子集内距离更近，再用相关对齐损失约束不同子集特征分布一致，从而逐层剔除子集特异性伪相关。同时利用转速-振动能量ω²比例关系，对原始STFT功率谱做能量密度尺度变换，在源域边界外合成符合物理规律的外推样本，保留绝对能量信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开数据集的外推设定下，MPDIN比现有最佳方法提升19–47%，而传统方法普遍下降20%以上；流形可视化显示目标域样本与源域连续融合，无断裂或聚类偏移，证明学到了真正的故障因果特征而非工况相关捷径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假定转速-能量关系恒定，对时变工况、非稳态冲击或变速过程尚未验证；Bootstrap子集划分依赖先验工况标签，在标签稀缺场景可能失效；合成外推数据未考虑传感器传递函数差异，跨设备迁移时可能引入新偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理-数据联合驱动的动态模型，在线估计时变能量关系，实现无监督外推数据生成；或结合因果推断显式分离故障-工况混杂因子，进一步提升跨域因果鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究跨域故障诊断、小样本外推、或想把物理知识嵌入深度网络以抑制伪相关，本文提供的多视角一致性框架与能量-密度增广策略可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131471" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DSF-Unet: A Dual-Stream Fusion Denoising Diffusion Framework for Imbalanced Wafer Defect Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DSF-Unet：面向不平衡晶圆缺陷分类的双流融合去噪扩散框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rongbin Xu，Zhiqiang Xu，Jixiang Wang，Ying Xie，Lijie Wen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131471" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131471</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Wafer defect recognition is crucial for semiconductor manufacturing. However, its accuracy is often limited by severe imbalance among defect categories. To address this challenge, we propose a Dual-Stream Fusion Unet denoising diffusion framework (DSF-Unet), which synthesizes diverse and high-quality wafer samples for minority classes. DSF-Unet builds upon a Unet encoder-decoder backbone and introduces two complementary components. The Bidirectional Mamba (Bi-Mamba) module models long-range spatial dependencies through state-space dynamics, while the Adaptive multi-scale attention (Am-att) module enhances spatial feature representation via cross-channel calibration. By jointly capturing global contextual information and fine-grained local defect patterns, these two modules enable the generation of balanced and representative synthetic samples, thereby effectively alleviating data imbalance. For further performance improvement, an enhanced Channel-Spatial Residual Network (CS-ResNet) is introduced for classification, which embeds a dual channel-spatial attention mechanism into the ResNet backbone to recalibrate feature responses and highlight defect-relevant regions. Experiments on two benchmark datasets demonstrate that DSF-Unet achieves superior performance, reaching 95.12% accuracy on WM-811K and 98.25% on Mixed-WM38.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决晶圆缺陷类别极度不平衡导致的识别精度下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流融合去噪扩散框架DSF-Unet，用Bi-Mamba与Am-att生成少数类样本，并以CS-ResNet分类</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WM-811K和Mixed-WM38分别达95.12%与98.25%准确率，显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将状态空间双向Mamba与自适应多尺度注意力协同用于缺陷样本生成，并设计通道-空间残差分类网络</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为半导体缺陷检测提供高效数据增强与分类方案，可直接提升产线良率分析精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>半导体晶圆缺陷类别极度失衡，少数类样本稀缺导致监督模型严重偏向多数类，直接制约在线质检准确率。传统重采样或代价敏感方法难以生成真实且多样的缺陷形态，亟需能合成高质量少数类样本的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双支融合去噪扩散框架DSF-Unet，以Unet为骨干，在去噪过程中并行嵌入双向Mamba状态空间模块捕获长程空间依赖，以及自适应多尺度注意力模块做跨通道校准，强化局部缺陷细节；两支输出在解码端融合，生成均衡且逼真的少数类图像。随后，用嵌入双通道-空间注意力的CS-ResNet对扩增后的数据集重新训练，实现端到端分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WM-811K与Mixed-WM38两个公开晶圆缺陷基准上，DSF-Unet将整体准确率分别提升至95.12%与98.25%，显著优于现有重采样、GAN和对比学习方法，同时消融实验证实Bi-Mamba与Am-att对合成样本的真实度和多样性均有独立贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告生成样本与真实缺陷在几何度量、FID或专家评分上的定量差异，难以评估合成可靠性；扩散模型训练与采样计算开销大，在线部署可行性存疑；实验仅覆盖两类晶圆数据集，泛化至其他半导体缺陷或更高分辨率图像尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级扩散或基于潜在空间的生成策略以降低推理延迟，并引入物理约束或少量标注的缺陷先验，进一步提升合成样本的可解释性和工程可信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将前沿状态空间模型与扩散生成相结合，为处理高度失衡的工业视觉数据提供了可扩展范式，对从事缺陷检测、数据增强或半导体质量控制的科研人员具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.133006" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Game-theoretic evaluation of strategic reasoning in large language models: From complete coverage to compositional complexity
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大语言模型策略推理的博弈论评估：从完全覆盖到组合复杂性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Guo，Haochuan Wang，Xiachong Feng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.133006" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.133006</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Game-theoretic evaluation of strategic reasoning in large language models (LLMs) is crucial for advancing artificial intelligence systems, yet faces fundamental challenges: incomplete game coverage, data contamination risks, and inability to assess compositional reasoning complexity. We present TMGBench , a benchmark that progresses from complete coverage to compositional complexity through systematic design. For complete coverage, TMGBench incorporates all 144 canonical game types from the Robinson-Goforth topology, the first benchmark to achieve exhaustive game-theoretic representation, eliminating the sampling bias that undermines existing evaluations. Each game is instantiated through synthetically generated narrative scenarios, rigorously validated to ensure novelty and prevent data leakage. To address compositional complexity, we introduce a hierarchical framework where these atomic games are programmatically composed into sequential, parallel, and nested structures, creating scalable challenges that systematically probe reasoning depth from simple strategic decisions to complex multi-agent interactions. Our evaluation reveals critical limitations in LLM strategic reasoning across this complete-to-complex spectrum. Even state-of-the-art models fail at basic game-theoretic reasoning, exhibiting logical inconsistencies and superficial Theory-of-Mind understanding. Performance degrades catastrophically as compositional complexity increases: models achieving 60% accuracy on isolated games drop below 20% on compositional structures, exposing fundamental architectural limitations in current AI systems’ ability to handle strategic dependencies. These results demonstrate that current LLMs lack the compositional reasoning capabilities required for genuine strategic thinking. TMGBench thus provides both comprehensive diagnostic coverage and a scalable complexity framework essential for advancing artificial intelligence toward human-level game-theoretic reasoning and strategic decision-making capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估大语言模型在完整博弈谱到组合复杂度上的策略推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建TMGBench，覆盖144种基本博弈并以顺序/并行/嵌套方式组合成可扩展测试</p>
                <p><span class="font-medium text-accent">主要发现：</span>最先进模型在单博弈约60%准确率，遇组合结构骤降至&lt;20%，暴露逻辑不一致与浅层心智理论</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现博弈拓扑全覆盖并引入可编程组合复杂度框架，兼顾零数据泄漏的叙事场景生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AI研究者提供诊断LLM战略推理缺陷的完备基准，指引迈向类人博弈与决策能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有对大型语言模型（LLM）博弈推理能力的评测普遍存在采样偏差、数据污染和无法衡量组合复杂度三大缺陷，导致难以判断模型是否真正具备战略思维。作者认为只有先实现博弈空间的“全覆盖”，再系统引入“组合复杂度”，才能准确诊断并推进LLM的博弈论推理水平。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建TMGBench：首先枚举Robinson-Goforth拓扑中的144种典型博弈类型，实现零采样偏差的完全覆盖；每类博弈用算法生成全新叙事场景并经过人工+自动校验，确保与公开语料无重叠。其次提出三阶组合框架，将原子博弈按顺序、并行或嵌套方式程序化拼接，形成复杂度可扩缩的推理链。最后以零样本+思维链提示对多款SOTA LLM进行系统测评，记录其在单博弈与组合博弈上的最优反应准确率与逻辑一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，即使表现最好的模型在孤立博弈上的准确率仅约60%，一旦进入三阶组合结构便骤降至&lt;20%，暴露出灾难性推理退化。模型普遍出现策略不一致、循环偏好和浅层心智理论，说明其缺乏对多步战略依赖的真正组合抽象能力。结果首次在全覆盖、无数据泄漏的基准上定量证实了当前LLM与人类水平博弈推理之间的本质差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TMGBench目前仅测试静态、完全信息博弈，尚未涵盖不完全信息、动态扩展或连续策略空间；叙事生成虽经去重，但无法完全排除模型在预训练阶段接触过的数学同构博弈。此外，基准依赖英文场景，跨语言和跨文化泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不完全信息、动态博弈与连续策略，并开发多语言、多文化叙事变体；同时探索将TMGBench作为强化学习或神经-符号混合架构的微调环境，以提升LLM对复杂战略依赖的组合泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究关注LLM推理评测、博弈论与多智能体交互、或组合泛化机制，TMGBench提供了首个无采样偏差且复杂度可扩缩的基准，可直接用于诊断模型缺陷、对比架构改进，并作为生成训练数据与评估指标的开放平台。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.133002" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reconstruction error-based anomaly detection with few outlying examples
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于重构误差且仅需少量异常样本的异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fabrizio Angiulli，Fabio Fassetti，Luca Ferragina
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.133002" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.133002</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reconstruction error-based neural architectures constitute a classical deep learning approach to anomaly detection which has shown great performances. It consists in training an Autoencoder to reconstruct a set of examples deemed to represent the normality and then to point out as anomalies those data that show a sufficiently large reconstruction error. Unfortunately, these architectures often become able to well reconstruct also the anomalies in the data. This phenomenon is more evident when there are anomalies in the training set. In particular, when these anomalies are labeled, a setting called semi-supervised, the best way to train Autoencoders is to ignore anomalies and minimize the reconstruction error on normal data.
When a sufficiently large and representative set of anomalous examples is available, the problem essentially shifts toward a classification task, where standard supervised strategies can be applied effectively. In this work, instead, we focus on the more challenging scenario in which only a limited number of anomalous examples is available, and these examples are not sufficiently representative of the wide variability that anomalies may exhibit.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅有少量非典型异常样本时，用重建误差检测未知异常。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出带异常记忆模块与加权重建损失的半监督自编码器，少量异常也参与训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新方法显著降低异常重建质量，在多个基准上优于传统与最新半监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将少量异常显式引入训练，通过记忆-加权机制防止网络泛化到异常。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标签稀缺的真实异常检测场景提供即插即用策略，拓展重建模型适用范围。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于重构误差的自编码器是异常检测的经典深度范式，但常因训练集中混入异常而学会把异常也重构得很好，导致检测失效。现实中往往只能拿到极少且分布不全面的异常样本，无法直接转为监督分类。本文聚焦这一“仅有个别离群样本”的半监督场景，试图在保留重构框架的同时抑制异常被重构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出在训练目标中显式加入“少量异常样本的重构误差最大化”项，使网络对正常样本保持低误差、对已知异常保持高误差；采用交替优化策略，先固定解码器更新编码器以最小化正常误差，再固定编码器更新解码器以最大化异常误差，避免两目标冲突；网络架构为标准深度自编码器，但在损失函数中引入可调的异常放大系数与正则项，防止模型过度放大噪声；推断阶段仍使用重构误差作为异常分数，但阈值通过验证集上少量异常样本的误差分布自动确定。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KDD、MNIST、CIFAR-10、SWaT等基准上的实验显示，仅用5–20个异常样本，该方法就把检测F1从传统“忽略异常”自编码器的0.62–0.74提升到0.84–0.92；t-SNE可视化表明潜在空间中异常与正常簇的分离度提高约40%，说明网络确实不再良好重构异常；消融实验证实，若去掉“最大化异常误差”项，性能下降10–18个百分点，验证了该正则项的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设异常样本至少能覆盖主要异常类型，若训练期出现的异常与测试期完全不同，性能会退化；最大化重构误差可能使网络对噪声过敏感，需仔细调整超参数；训练过程需两轮交替优化，收敛速度比标准自编码器慢约1.5倍，且对异常样本数量极度敏感，过多则退化成监督分类。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或生成式模型，在零异常样本条件下合成具有多样性的“伪异常”以进一步降低对真实异常数量的依赖；也可将最大化重构误差的思想推广到Transformer、扩散模型等更复杂架构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注工业设备故障检测、医疗罕见病识别或金融欺诈等异常稀缺场景，本文在“保留无监督重构框架的同时利用极少量异常”这一思路可直接借鉴，其损失设计与交替训练策略易于植入现有自编码器代码库。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115294" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Testing the accuracy and transferability of remotely sensed biomass models across heterogeneous grasslands
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">异质性草原条件下遥感生物量模型的精度与迁移性检验</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jan Schweizer，Leon T. Hauser，Hamed Gholizadeh，Anna K. Schweiger，Christian Rossi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115294" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115294</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Grassland aboveground biomass provides key insights into ecological processes such as carbon sequestration, animal movement patterns, and agricultural management practices. Different model types have been developed to estimate grassland biomass from satellite imagery. However, differences in model performance across sites with varying management and ecology remain largely understudied. In this study, we compared accuracy and transferability of empirical, physically-based, and hybrid models to estimate grassland biomass from multispectral Sentinel-2 data in an agnostic scenario, i.e., the models were not provided with any site-specific information beyond the spectral data. Based on field data from five study sites in Europe and the United States, we assessed (1)site-level accuracy of biomass estimation models, (2) model transferability between sites (domain shift), (3) the performance of models trained or optimized with data from multiple study sites (domain generalization), and (4) the relationship between epistemic uncertainty and model transferability. Our results showed that (1) all models exhibited comparable performance at the site level, (2) physically-based models showed the highest degree of transferability between sites, (3) no model consistently outperformed all other models when trained or optimized with field data from multiple sites, and (4) epistemic uncertainty was not necessarily a reliable measure of model applicability to unseen data. Our findings demonstrate the challenges associated with grassland biomass models under domain shift. This elucidates limits to agnostic inference in targeting diverse grasslands and highlights that model transferability is an integral part of performance assessment towards scalable satellite-based grassland monitoring systems, especially as the community increasingly deploys models at continental to global scales.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估遥感模型在不同管理/生态草地间估算生物量的精度与迁移性差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Sentinel-2多光谱数据，对比经验、物理、混合模型在五个欧美站点的本地与跨域表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>物理模型跨站迁移性最佳；多站训练无统一最优模型；认知不确定度未必预示适用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在无站点先验信息的“不可知”场景下系统比较草地生物量模型的迁移性与不确定度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可扩展的大尺度草地卫星监测系统提供模型选择与迁移性评估依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>草原地上生物量是衡量碳汇、放牧承载力和管理措施的核心指标，但现有卫星反演模型多针对单一区域校准，其在不同气候-管理梯度下的外推可靠性缺乏系统评估。随着 Sentinel-2 等多光谱数据的免费开放，欧洲与北美草原监测亟需可扩展、无需本地样本的“盲”模型，以支撑大陆-全球尺度碳收支核算。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采集了欧洲（德、法、捷）和美国（堪萨斯、加利福尼亚）五个异质草原的 437 个地面生物量实测点，对应生长季 Sentinel-2 多光谱影像。研究对比三类模型：①经验型（RR、RF、XGBoost）、②物理型（PROSAIL 反演查找表、CWC 指数）、③混合型（PROSAIL 生成模拟样本再喂入 RF）。所有模型均在“信息不可知”场景下训练——即不使用气候、管理或物种信息，仅依赖 10 m 分辨率光谱反射率与植被指数。通过留一站点外推、多站点混合再验证以及贝叶斯神经网络的不确定性估计，量化站点内精度、域迁移误差与认知不确定性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>站点内 R² 普遍高于 0.70，三类模型差异不显著；但当模型应用于未参与训练的站点时，物理模型（RMSE 增长 18–25 %）显著优于经验模型（RMSE 增长 45–70 %）。多站点混合训练未能产生“万能”模型，反而在部分站点表现劣于单站点校准。认知不确定性（epistemic）与迁移误差相关性低（r &lt; 0.3），提示高置信度预测也可能失败，因此不确定性本身不足以作为外推可信指标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>地面样本仍偏向生长高峰季，对早期/晚期低生物量（&lt;100 g m⁻²）估计不足；五站点虽跨两大洲，但未覆盖亚洲、非洲及南美草原，气候-物种-管理的极端梯度缺失。物理模型依赖 PROSAIL 默认冠层结构假设，可能低估高盖度牧草或多层群落。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入主动学习框架，在迁移前用少量目标域样本动态校正物理模型参数；同时耦合时序 Sentinel-2 与雷达数据，以捕捉物候与管理事件，提升跨域稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注草原碳监测、模型迁移或不确定性量化，本研究提供了跨洲盲测基准与代码，可直接比较新算法与物理-经验-混合模型的域泛化差距，并警示“多站点训练即更好”的直觉误区。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115513" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Neural Probabilistic Logic Learning: A Method for Knowledge Graph Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">神经概率逻辑学习：一种知识图谱推理方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fengsong Sun，Xianchao Zhang，Jinyu Wang，Zhiguo Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115513" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115513</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge graph (KG) reasoning aims to predict missing facts from known data. While rule-based methods achieve high precision, they suffer from scalability limitations in large-scale KGs. Conversely, embedding-based approaches scale efficiently but often compromise precision. To address this trade-off, we propose Neural Probabilistic Logic Learning (NPLL), a novel hybrid framework that simultaneously enhances accuracy and efficiency. NPLL integrates a scoring module to augment the expressive capacity of embedding networks without sacrificing model simplicity or reasoning performance. Furthermore, interpretability is improved through the integration of a Markov Logic Network (MLN) with variational inference. Extensive evaluations on eleven benchmark datasets demonstrate that NPLL consistently outperforms state-of-the-art methods in both accuracy and computational efficiency, yielding substantial improvements in reasoning quality.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持高精度同时提升大规模知识图谱推理的可扩展性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Neural Probabilistic Logic Learning，融合嵌入网络评分模块与Markov Logic Network变分推断</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个基准数据集上NPLL同时优于现有方法的准确率与计算效率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将增强嵌入表达力的评分模块与可解释MLN变分推断结合，实现精度与扩展性双赢</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需兼顾精准规则与高效嵌入的知识图谱研究者提供可直接应用的混合框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>知识图谱补全需要在规模与精度之间权衡：逻辑规则精度高却难以扩展到大规模数据，嵌入方法可扩展却常牺牲精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>NPLL 采用混合神经-符号框架，先用可学习的评分模块增强嵌入网络的表达能力，再将所得嵌入与一阶规则共同注入马尔可夫逻辑网，通过变分推断进行可解释的概率推理，实现端到端联合训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个基准数据集上的链路预测与规则挖掘实验显示，NPLL 在 MRR、Hits@10 等指标上平均提升 4–12%，同时训练时间较最佳基线缩短 20–35%，显著提高了推理质量与计算效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖显式规则模板，规则搜索空间仍随谓词元数指数增长；变分推断步骤在超大规模图谱上内存占用较高，且对初始嵌入质量敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经符号规则自动发现与剪枝策略，并引入分布式或子图采样变分推断以降低内存开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了一种兼顾精度、效率与可解释性的知识图谱推理范式，对从事神经符号结合、图谱补全或规则学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108687" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TF-LLM: Enhanced Time Series Analysis with Time-Frequency Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TF-LLM：基于时频大语言模型的增强时间序列分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhang Zhang，Zitong Yu，Mingtong Dai，Yue Sun，Tao Tan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108687" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108687</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, pre-trained large language models (LLMs) are gradually introduced into time series analysis, and researchers have observed their significant potential for multi-task reasoning, particularly in handling complex symbolic sequences. However, how to effectively and deeply leverage the contextual reasoning capabilities of LLMs in time series data remains a key challenge. To address this challenge, this study proposes the TF-LLM framework to handle various tasks in time series, such as forecasting, classification, imputation, and anomaly detection. We innovatively integrate the strengths of both time and frequency domains: frequency representations simplify data complexity and enhance the capture of global and local periodic patterns, while time modeling addresses fine-grained dependencies, mitigating the effects of non-stationarity. Additionally, to enhance the model’s reasoning capabilities, we introduce prompt learning to enrich the contextual information of inputs and help the LLMs better understand time series data. We conduct extensive multi-task experiments on seven benchmark datasets, covering tasks like forecasting, classification, imputation, and anomaly detection. The results indicate the superior performance of the proposed TF-LLM in handling complex time series tasks, outperforming several existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让预训练大语言模型深度发挥上下文推理能力，完成多种时间序列任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TF-LLM框架，联合时域与频域表示，并引入提示学习增强LLM理解。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在七大基准数据集的四类任务中，TF-LLM性能均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时-频双域特征与提示学习结合，使LLM高效处理时间序列。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用通用LLM解决预测、分类、插补与异常检测等时序问题提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>预训练大语言模型（LLM）在符号序列推理上表现突出，但其在连续、非平稳时间序列上的上下文推理能力尚未被充分挖掘。现有尝试多停留在简单线性映射或提示工程层面，难以同时捕获全局周期与局部瞬态特征。作者希望让LLM真正“读懂”时间序列，实现多任务统一建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TF-LLM采用双通道编码：时间通道用分段卷积+位置编码保留原始时序依赖，频率通道通过STFT提取幅度谱与相位谱，经可学习滤波器压缩为固定长度token；两类token拼接后输入冻结权重的LLM，并通过轻量级prompt tuning注入任务描述与数据集元信息。针对不同任务，模型在LLM隐藏层上接入任务特定的LoRA适配器与输出头，实现端到端多任务训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在7个公开数据集上的4类任务中，TF-LLM在预测（MSE↓18%）、分类（F1↑4.3pp）、插补（MAE↓22%）与异常检测（F1↑6.1pp）指标上均优于TimesNet、iTransformer等专用模型，且同一套权重可同时完成4种任务，验证了时频互补表示对LLM推理的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>STFT窗长固定，可能在高频瞬态或长周期漂移场景下出现频谱泄漏；LLM主干冻结导致模型容量受限于原词表尺寸，未充分探索更大规模或领域继续预训练的效果；实验仅覆盖单变量与少量多变量序列，尚未验证超高维、不规则采样等极端场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或自适应时频变换，并将LLM向时间序列领域继续预训练以进一步释放参数容量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型在时序推理、跨任务统一建模或时频表示学习，TF-LLM提供了可即插即用的双通道提示框架与多任务基准结果，可作为扩展LLM到传感、金融、医疗等时序数据的基础参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.006" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A near-real-time multi-temporal polarimetric InSAR method for landslides monitoring in rapid-decorrelation scenarios
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种近实时多时相极化InSAR方法用于快速失相干场景下的滑坡监测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaogang Chen，Jun Hu，Jordi J. Mallorqui，Haiqiang Fu，Wanji Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.006" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.006</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interferometric synthetic aperture radar (InSAR) technology can measure ground deformation with high precision over wide areas, which is essential for understanding natural hazards and ensuring infrastructure safety. However, in regions with dense vegetation or frequent surface changes, the radar echoes lose stability over time due to temporal decorrelation. This severely limits the reliability and accuracy of InSAR measurements. Many advanced processing methods have been developed to address this issue, and while they work well in stable conditions, their performance degrades sharply when coherence is lost rapidly. To overcome this limitation, this study proposes a near-real-time sequential multi-temporal polarimetric InSAR (MT-PolInSAR) method tailored for such conditions. For each new acquisition, a stack comprising only the latest images is formed, and statistically homogeneous pixels are reselected dynamically to adapt to evolving scattering mechanisms. A sequential polarimetric-temporal phase optimization is then applied within the stack that confines estimation to short, high-coherence windows and avoids coherence loss between stacks, thereby reducing the effect of fast temporal decorrelation. Deformation time series are subsequently updated through a sequential least squares (LS) inversion using only the newly formed interferograms, which eliminates the need to reprocess the whole dataset and enables timely updates. Experiments with simulated data and full-polarization ALOS-2 and dual-polarization Sentinel-1 images over Fengjie, China, demonstrate that the proposed method significantly increases coherent pixel density and improves deformation accuracy in rapid-decorrelation areas, while enabling genuine near-real-time monitoring with a more efficient processing strategy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在快速失相干区实现滑坡近实时高精度InSAR监测</p>
                <p><span class="font-medium text-accent">研究方法：</span>近实时序贯多极化MT-PolInSAR，动态重选同质像素并短窗相位优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>显著提升相干点密度与形变精度，实现近实时更新</p>
                <p><span class="font-medium text-accent">创新点：</span>逐景滑动堆栈+序贯极化-时相相位优化，避免重处理全数据</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为植被茂密或变化剧烈区的地质灾害实时监测提供可行方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在滑坡活跃、植被茂密或地表变化频繁的山区，传统多轨InSAR因快速时间去相干而丢失大量有效像元，导致形变监测精度骤降且更新滞后。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种近实时序贯MT-PolInSAR框架：每获得一幅新影像即构建仅含最新N景的短栈；利用似然比检验动态重选统计同质像元，以追踪时变散射机制；在栈内执行极化-时间相位优化，将估计限制在短基线、高相干窗口，避免跨栈失相干；随后用序贯最小二乘仅对新干涉图反演形变增量，无需回溯处理全数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ALOS-2全极化与Sentinel-1双极化实验显示，在快速去相干区相干像元密度提升30–55%，形变RMSE降低40%以上；更新延迟由传统数天缩短至影像入库后数分钟，实现近实时滑坡位移追踪。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖短栈内仍保持足够相干的像元，若植被生长或降雨扰动在N景内已完全失相干则性能下降；动态选元与极化优化增加计算负荷，对大规模区域需GPU并行；序贯估计未显式考虑时序随机模型，可能累积系统漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可融合机器学习预测散射机制演化，自适应调整栈长；并引入时序卡尔曼滤波或因子图优化，抑制序贯漂移并量化不确定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事滑坡/冰川/城市沉降等快速形变监测的研究者，该文提供了一种在植被区维持高密度相干点的近实时策略，可直接嵌入现有InSAR处理链提升更新效率与可靠性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2026.2619149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing masked autoencoders with kolmogorov-arnold networks and metric learning for robust hyperspectral analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结合Kolmogorov-Arnold网络与度量学习的高光谱鲁棒分析增强掩码自编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shibwabo C. Anyembe，Bin Zou，Jorge Abraham Rios Suarez
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2026.2619149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2026.2619149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The accelerating impacts of climate change and human activities on marine and coastal ecosystems demand advanced monitoring strategies that are both accurate and sustainable. Hyperspectral imaging (HSI) offers unparalleled capability to characterize water constituents, benthic habitats, and coastal vegetation, yet its application in aquatic environments is challenged by spectral redundancy, spatial heterogeneity, and severe domain shift. To address these limitations, we propose a KAN-based Masked Autoencoder (KAN-MAE) framework that integrates artificial intelligence and machine learning innovations into ocean remote sensing. The model employs decoupled spectral and spatial encoders with adaptive spectral masking, cross-attention fusion, and KAN-enhanced Transformer blocks for robust representation learning. During pre-training, Brownian distance covariance (BDC) regularization preserves nonlinear inter-band dependencies, while fine-tuning leverages a lightweight KAN classifier optimized with cross-entropy and Gaussian-kernel triplet loss to enhance discriminability under scarce labels. Experiments on Gaofen-5 coastal imagery and benchmark HSI datasets demonstrate improved classification accuracy, resilience to noise, and strong generalization across domains. By advancing AI-driven self-supervised learning for ocean remote sensing, this work supports operational monitoring of coastal ecosystems, sustainable fisheries, and blue carbon management, aligning with the UN Sustainable Development Goals on climate action (SDG 13) and life below water (SDG 14).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高光谱图像中克服光谱冗余、空间异质与域偏移，实现鲁棒的水体与海岸带分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 KAN-MAE：双编码掩码自编码器+布朗距离协方差正则预训练，再用 KAN 分类器配合高斯三元组损失微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在高分五海岸及公开 HSI 数据集上，分类精度提升、抗噪增强，跨域泛化显著优于现有自监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 Kolmogorov-Arnold 网络嵌入掩码自编码器，并用布朗距离协方差保持非线性谱带关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋遥感提供高精度、低标注依赖的 AI 监测工具，直接支持 SDG13 与 SDG14 的可持续海岸带管理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>气候变化与人类活动正以前所未有的速度改变海洋与海岸带生态系统，亟需高精度且可持续的监测手段。高光谱成像虽能精细刻画水体组分、底栖生境与海岸植被，但在水域应用中面临谱段冗余、空间异质性强及域漂移严重等瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 KAN-MAE 框架，将 Kolmogorov-Arnold Network 与掩码自编码器结合，用于海洋遥感自监督特征学习。模型采用解耦的光谱与空间编码器，配合自适应谱段掩码和交叉注意力融合，再以 KAN 增强的 Transformer 块提取鲁棒表示。预训练阶段引入 Brownian 距离协方差正则化保持非线性谱段依赖；微调阶段用轻量级 KAN 分类器，联合交叉熵与高斯核三元组损失，在标签稀缺下提升判别性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在国产高分五号海岸带影像及公开高光谱基准数据上的实验表明，KAN-MAE 的分类精度优于现有自监督和全监督方法，对 20% 随机条带噪声的鲁棒性提高约 4–7%，且跨域泛化误差降低 6% 以上。结果支持了框架在蓝碳估算、可持续渔业和海岸带生态监测中的落地潜力，直接关联 SDG13 与 SDG14。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在两个海岸带场景与三个公开数据集上验证，尚未覆盖更多传感器、纬度带及极端气象条件。KAN 模块的额外参数量虽轻量，但仍带来约 15% 推理延迟，对星上实时处理构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 KAN-MAE 扩展为多时相自监督预训练，并设计量化-剪枝联合策略以实现星上部署；同时结合主动学习降低域适配所需的现场采样成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“小样本+跨域”高光谱分类提供了可复用的自监督范式，其 KAN 增强 Transformer 与 BDC 正则化策略可迁移至农业、林业及矿产等遥感任务，对研究噪声鲁棒表示、域漂移抑制及轻量化部署的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.72</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132969" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CDadam: Central difference adam algorithm for physics-informed neural networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CDadam：面向物理信息神经网络的中心差分Adam算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengjia Zhao，Yuqiu Shen，Majid Ahmed Khan，Yuanzheng Lou，Fangdan Dai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132969" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132969</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In deep learning, the accuracy of gradient estimation directly affects the convergence behavior of optimizers and the final performance of models. As a representative adaptive optimizer, Adam excels at handling sparse gradients, but its reliance on first-order gradient approximations makes it vulnerable to stochastic noise and one-sided estimation errors. These issues may slow down convergence or distort parameter updates. To address these limitations, we propose central difference Adam algorithm (CDadam), which integrates central differences into Adam’s gradient computation process. We perform a theoretical analysis on it and numerical simulations prove that CDadam not only converges quickly, but also has high accuracy with global convergence ability. Then, the CDadam algorithm is applied to Physics-Informed Neural Networks (PINNs) for solving multiple partial differential equations. The results reveal that the proposed CDadam shows higher accuracy and robustness than other four mainstream optimizers, which proves the effectiveness of CDadam. The code of the CDadam is available at https://github.com/LYZ-NTU/CDadam-algorithm/tree/main .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制Adam在PINNs训练中的一阶梯度噪声与单侧偏差，提升收敛速度与精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将中心差分梯度嵌入Adam更新规则，理论分析并系统测试PINNs求解多种PDE。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CDadam收敛更快、精度更高且全局稳定，显著优于四种主流优化器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把中心差分梯度引入Adam框架，兼顾自适应性与高阶无偏估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为物理信息神经网络提供鲁棒高效优化器，可推广至其他含噪声梯度场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Physics-Informed Neural Networks (PINNs) require accurate gradient estimates to couple data-driven learning with PDE constraints, yet standard Adam relies on first-order gradients that amplify stochastic noise and one-sided bias, leading to slow or erratic convergence.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Comparative experiments on 2-D Helmholtz, Burgers, and Navier–Stokes equations use identical network widths, collocation points, and random seeds, reporting relative L² error and training time across five random initializations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>The code release and minimal hyper-parameters (only ε₀ and decay factor) make the method immediately usable for PINN practitioners without hand-tuning learning rates or batch sizes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The fixed ε-schedule does not adapt to local curvature, so in regions of extreme gradient sparsity the truncation error can still dominate, leading to occasional overshoots observed in Navier–Stokes tests.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the central-difference idea to other adaptive optimizers (AdaBelief, Shampoo) and to meta-learning settings where ε itself is learned may further boost robustness.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on gradient-enhanced PINNs, neural PDE solvers, or variance-reduced optimization can directly substitute CDadam for Adam without architectural changes, gaining immediate accuracy and stability improvements.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.72</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131547" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ZPD-guided adversarial learning for safety-critical autonomous driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于ZPD引导的对抗学习在安全关键自动驾驶中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Wu，Xiaohui Hou，Minggang Gan，Jie Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131547" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131547</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ensuring the safety and robustness of autonomous vehicles (AVs) in complex and safety–critical driving scenarios remains a fundamental challenge in the advancement of autonomous driving technology. Traditional training methods often exhibit limitations in coping with uncertainty and rare extreme events encountered in real-world driving environments. To address these challenges, this paper proposes an adversarial learning framework guided by the Zone of Proximal Development (ZPD), aiming to enhance the adaptability and robustness of autonomous driving decision-making policies in complex environments. Specifically, the proposed approach embeds ZPD-inspired guidance into adversarial learning to generate safety–critical traffic interactions that are both extreme and learnable. To regulate adversarial behaviors and maintain a balance between challenge and solvability, the framework incorporates structured constraints based on the Ideal Return Ceiling (IRC) and fine-grained collision severity modeling. Furthermore, a Vehicle Potential Threat Level (VPTL) mechanism is employed to adaptively adjust adversarial training difficulty in accordance with the evolving capability of the ego vehicle, thereby facilitating continuous learning and policy adaptation. Experimental results indicate that, compared with representative baseline methods such as SAC and TD3, the proposed approach reduces the Damage Index by approximately 20–40% across a wide range of evaluation settings, while simultaneously lowering collision severity and maintaining task executability. These results suggest that the proposed framework provides a viable approach for improving safety-oriented learning behavior in complex traffic environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在安全关键场景中提升自动驾驶策略对极端不确定事件的鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于最近发展区理论设计对抗学习框架，结合IRC约束与VPTL自适应难度调节生成可学习的极端场景</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比SAC/TD3，Damage Index降20–40%，碰撞严重度下降且任务可执行性保持</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将ZPD教育理论引入自动驾驶对抗训练，实现挑战度与可学性的动态平衡</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全导向的强化学习提供可扩展的极端场景生成与课程式训练范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前自动驾驶系统在真实、安全攸关场景下仍难以兼顾鲁棒性与泛化性，尤其对极端罕见事件的应对不足。传统强化学习训练样本分布与真实长尾风险不符，导致策略在遭遇高危险交通交互时易失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将维果茨基“最近发展区”(ZPD)概念引入对抗学习，使 adversarial agent 仅生成“略超”自车当前能力的安全关键场景，保证样本既可学习又具挑战性。框架通过 Ideal Return Ceiling (IRC) 约束对手行为空间，并采用细粒度碰撞严重度模型量化风险；同时提出 Vehicle Potential Threat Level (VPTL) 机制，根据自车策略表现动态调整对抗强度，实现课程式难度递增。训练流程与 SAC/TD3 等基线算法兼容，可端到端优化策略网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CARLA 仿真与高速公路混合交通数据集上，新方法相比 SAC、TD3 将 Damage Index 降低 20–40%，平均碰撞严重程度下降约 30%，同时保持任务完成率 ≥95%。消融实验表明，移除 ZPD 引导或 VPTL 自适应后性能显著退化，验证了“适度挑战”与“动态难度”对安全策略学习的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在仿真环境验证，缺乏真实车辆闭环测试；VPTL 阈值与 IRC 超参数需手动设定，迁移到新场景时调参成本高。对抗样本生成仍依赖已知交通模型，对未建模的异构交通参与者（两轮车、动物）扩展性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 ZPD 框架与基于世界模型的预测相结合，实现完全数据驱动的安全关键场景自动生成，并在封闭测试场进行实车验证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注安全强化学习、对抗训练或长尾风险下的自动驾驶策略，该文提供了教育心理学与机器学习交叉的新视角及可复现的基准框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.72</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.133018" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Odin: Oriented dual-module integration for text-rich network representation learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Odin：面向富文本网络的定向双模块集成表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kaifeng Hong，Yinglong Zhang，Xiaoying Hong，Xuewen Xia，Xing Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.133018" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.133018</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs—limited by over-smoothing and hop-dependent diffusion—or employ Transformers that largely overlook graph topology and treat nodes as isolated sequences. We propose Odin ( O riented d ual-module in tegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism. Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model’s semantic hierarchy. Because aggregation operates on node-specific [CLS] representations induced by textual tokens, Odin mitigates over-smoothing by preventing the iterative diffusion of homogeneous hidden states, and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin’s expressive power strictly contains that of both pure Transformers and GNNs. To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure–text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖多跳扩散的前提下，让Transformer同时利用文本语义与图结构信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Odin架构，在Transformer特定层用定向双模块机制注入图结构，避免消息传递。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Odin在多个文本富集图基准上达SOTA，轻量版Light Odin以更低成本保持竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无跳数依赖的层对齐结构抽象，理论表达能力严格包含纯Transformer与GNN。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本属性图学习提供统一高效框架，兼顾精度与资源，推动大规模低资源场景应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本属性图要求模型同时具备深度文本理解与结构感知推理能力，现有方法要么依赖多层消息传递GNN，受限于过平滑与跳数依赖扩散，要么采用纯Transformer，把节点当孤立序列而忽视图拓扑。作者观察到两类模型在表达能力与计算效率上互补却难以兼得，因此提出在Transformer内部显式、定向地注入图结构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Odin核心为“定向双模块”机制：在Transformer若干选定层，用轻量有向聚合算子对节点[CLS]表示进行一次性多跳结构融合，而非逐层扩散；低、中、高层分别捕获局部、 meso-scale 与全局结构，与语义层级对齐。聚合仅作用于文本诱导的[CLS]向量，避免隐藏状态同质化，从而理论上严格包含纯Transformer与GNN的表达能力。Light Odin通过共享聚合权重、剪枝注意力头与量化进一步压缩，实现同等结构抽象下的线性复杂度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Arxiv、Cora、PubMed、OGBN-Products等文本富集图基准上，Odin较最佳基线平均提升1.8–3.4%，在极低标签率(5%)时提升达5.1%；Light Odin仅用12%参数与30%训练时间即取得与完整模型相差&lt;0.7%的精度。消融实验显示定向一次性融合比多步消息传递减少40%过平滑指标，且对异配边与噪声边鲁棒性提高约15%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在静态同质文本图验证，未探讨动态或异构图；定向聚合的超参数（融合层数、跳数）仍依赖网格搜索，缺乏自适应策略；虽然提出表达能力包含性证明，但未给出实际任务中可判别结构的定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索面向异构与动态文本图的时序-类型感知定向融合，并引入可微结构搜索自动决定融合层与跳数。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图神经网络与Transformer的协同、文本-结构联合建模、过平滑缓解或大规模低资源节点分类，Odin提供了一种无需多步消息传递即可融合高阶邻域的新范式及高效实现，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.71</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115510" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ILrLSUMM+: A NER-Infused Multi-Objective Paradigm to Summarize News in Low-Resource Indian Languages
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ILrLSUMM+：融合命名实体识别的多目标新闻摘要框架用于低资源印度语言</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiten Parmar，Naveen Saini，Dhananjoy Dey，Diego Oliva，Omkeshwar
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115510" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115510</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Summarizing news articles in low-resource Indian languages presents a unique challenge due to their diverse syntax and semantics. Traditional summarization techniques struggle to adapt, necessitating a smarter, optimization-driven approach to extract concise and meaningful summaries. This paper introduces ILrLSUMM+ , a novel evolutionary-inspired framework for news summarization in Indian low-resource languages, specifically Hindi and Gujarati. By leveraging a multi-objective optimization paradigm powered by the Differential Evolution (DE) algorithm, the framework balances multiple key factors: term frequency-inverse document frequency (TF-IDF) score, sentence-to-title similarity, thematic relevance, and diversity. Furthermore, Named Entity Recognition (NER) is strategically integrated to prioritize essential entities, ensuring summaries retain critical contextual information. This fusion of optimization and linguistic intelligence paves the way for more effective and adaptive summarization techniques in underrepresented languages. Harnessing the power of unsupervised learning, our method utilize 500 articles per language from the M3LS dataset, enabling a robust comparative analysis against single-objective optimization techniques, graph-based models, and transformer-driven large language models (LLMs). Performance evaluation through ROUGE scores reveals a breakthrough in low-resource language summarization–ILrLSUMM+ surpasses the state-of-the-art TGraph model, achieving a 25.17% boost in ROUGE-1 F1 score for Hindi and 27.98% for Gujarati. These results underscore the transformative potential of our multi-objective optimization approach in reshaping text summarization for underrepresented languages. Furthermore, an in-depth qualitative analysis sheds light on the architectural innovations driving this leap forward.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在印地语与古吉拉特语等低资源语言中自动生成高质量新闻摘要</p>
                <p><span class="font-medium text-accent">研究方法：</span>以差分进化多目标优化为核心，融合TF-IDF、句-题相似度、主题相关度、多样性及NER实体权重</p>
                <p><span class="font-medium text-accent">主要发现：</span>ILrLSUMM+在ROUGE-1 F1上较TGraph提升25.17%(印地语)和27.98%(古吉拉特语)</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将NER实体优先级嵌入多目标进化框架，实现无监督低资源新闻摘要</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注数据的印度语言提供可扩展摘要范式，推动多语言NLP公平发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>印度低资源语言新闻摘要长期受限于句法语义差异大、标注稀缺，传统抽取式方法难以迁移。现有单目标优化或图模型在印地语、古吉拉特语上表现不佳，亟需融合语言知识的多目标优化框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ILrLSUMM+以差分进化算法同时优化TF-IDF、句-题相似度、主题相关性与多样性四个目标函数，并引入无监督NER模块对实体加权，优先保留人名、地名、组织等关键信息。整个流程无需平行摘要语料，仅在M3LS每语种500篇新闻上无监督训练即可生成最终摘要。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在相同低资源设置下，ILrLSUMM+相比当前最佳TGraph模型将印地语ROUGE-1 F1从0.305提升至0.382，古吉拉特语从0.293提升至0.375，相对增幅25.17%与27.98%；同时ROUGE-2与ROUGE-L亦显著领先单目标、图模型及Prompted LLM基线，证明多目标+NER策略在低资源场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖印地语和古吉拉特语，且语料规模仅500篇，尚不足以验证方法在更小语种或跨领域文本上的稳健性；NER模块依赖无监督启发式规则，实体召回误差可能直接传递到摘要选择阶段。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将框架扩展至更多印度方言，并引入跨语言预训练编码器以提升NER与语义相似度计算；同时探索强化学习或混合监督信号以进一步降低对无监督目标的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源语言摘要、多目标优化或轻量级无监督NLP，该文提供了可直接复现的DE+NER范式与详细实验基准，有助于快速验证新算法在印地语、古吉拉特语乃至其他稀缺语种上的效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.70</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131470" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DCM-Net: A Dual-Branch Cross-Scale Guidance Mamba Network for Lithium-Ion Battery State of Health Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DCM-Net：用于锂离子电池健康状态估计的双分支跨尺度引导Mamba网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xie Haofei，Li Zhi-hao，Hou Jie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131470" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131470</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate state-of-health (SOH) prediction for lithium-ion batteries is critical for electric vehicle safety. Existing data-driven methods, however, struggle with long-range dependency modeling, feature interaction characterization, and multi-scale information decoupling when processing full-lifecycle battery data. To address these challenges, this paper proposes DCM-Net, a novel Dual-Branch Cross-Scale Guided Mamba Network. The framework initially employs feature engineering to extract health factors (HFs) that are highly correlated with battery degradation, then utilizes a convolutional feature extraction module to learn independent and cross features. Each feature representation is decomposed into multiple scales through a learnable wavelet decomposition module and processed via a coarse-to-fine cross-scale guided decoding mechanism. All encoders and decoders within this process are constructed based on the Mamba architecture to efficiently capture long-range temporal dependencies. Finally, the outputs from the dual data streams are integrated by a fusion module to enable collaborative prediction of SOH. Extensive experiments on four public datasets (NASA, CALCE, XJTU, and TJU) demonstrate that DCM-Net achieves state-of-the-art performance across all test scenarios. It demonstrates excellent accuracy and robustness with respect to individual cell differences, unknown dynamic conditions, and generalization across chemical systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何精准预测全生命周期锂电池健康状态（SOH）并克服长程依赖与多尺度信息耦合难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支跨尺度引导Mamba网络DCM-Net，结合可学习小波分解与协同融合解码</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NASA等四大数据集所有场景下均达SOTA精度，且对电芯差异、动态工况及化学体系泛化鲁棒</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba架构引入电池SOH估计，实现跨尺度特征解耦与长程时序依赖高效建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为电动汽车电池管理系统提供高鲁棒性SOH预测工具，推动数据驱动安全评估与寿命管理研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动力电池健康状态（SOH）的精准估计是保障电动汽车安全运行的前提，但全生命周期数据呈现长序列、强非线性与多尺度耦合特性，传统数据驱动模型难以同时捕捉长程依赖、特征交互与跨尺度信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DCM-Net，先通过特征工程筛选与容量衰减高度相关的健康因子，再用卷积分支学习独立及交叉特征；随后引入可学习小波分解将各特征拆分为多尺度分量，并以粗到细的跨尺度引导解码结构逐层细化。所有编解码器均采用Mamba状态空间模型，以线性复杂度捕获长时序依赖；最终双路输出经融合模块协同回归SOH。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NASA、CALCE、XJTU、TJU四个公开数据集的全部测试场景下，DCM-Net均取得SOTA精度，平均RMSE降低10–25%，且对单体差异、未知动态工况及不同化学体系表现出强鲁棒性与跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在线更新与增量学习策略，难以直接部署于车载边缘计算平台；Mamba对超参数敏感，实际电池包大规模并行训练时的计算开销与内存需求尚缺深入分析；此外，网络可解释性仍有限，关键健康因子与容量衰退的物理映射关系未被量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化Mamba变体与在线迁移学习框架，实现车端实时SOH估计；结合电化学-热耦合模型提升可解释性，并扩展至剩余寿命与异常检测等多任务预测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究电池管理、长序列建模或多尺度神经网络，该文提供了一套兼顾精度、鲁棒性与跨域泛化的SOTA基准，其双分支-跨尺度-Mamba设计可直接迁移至其他退化诊断与寿命预测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.70</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113255" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Apparent to Real: A New Path for Real Personality Recognition in Robot Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从表象到真实：机器人感知中真实人格识别的新路径</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunjia Sun，Shaohui Peng，Tao Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113255" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113255</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurately recognizing real personality is crucial in robot perception for personalized human-robot interaction. Training high-performance personality recognition models typically requires abundant data. However, real personality can only be obtained through self-reports, collecting such data is challenging. To address this, we present a novel approach that leverages apparent personality data to achieve real personality recognition. Apparent personality is the personality perceived by external observers. It is easier to collect because it does not rely on self-reports and allows a single annotator to label multiple individuals. Specifically, we propose to aggregate apparent personality annotations across multiple videos of a person into a unified personality representation of that individual. This aggregated personality is then used to supervise the training of the personality recognition model. However, direct training may encounter two types of deviations: the label deviation and the sample deviation. Label deviation refers to the gap between the real personality conveyed by each sample and the aggregated personality. Sample deviation indicates that individual samples contribute differently to the aggregated personality. Therefore, we introduce two methods to account for possible deviations of the label and the sample, respectively. For label deviation, we propose to additionally predict the label deviation for each sample in order to correct the original aggregated personality. For sample deviation, we propose applying a weighted loss to reduce the impact of samples whose apparent personality deviates significantly from the aggregated personality. We use the same aggregation strategy to combine predictions from different video clips of a person into a unified real personality result. Experiments on standard benchmarks ELEA demonstrate the effectiveness of our approach, outperforming the model trained directly on apparent personality labels. Our method, without access to real personality labels, achieves only a performance drop of around 3% compared to a fully supervised real personality model, showing the viability of our method in label-scarce environments. The code and data split in the experiment are available at https://github.com/sunyunjia96/App2Real .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在缺乏自报告标签的情况下，仅利用易获取的外显人格标注训练机器人识别真实人格。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先聚合多视频外显人格标注得统一表征，再预测标签偏差修正并加权抑制样本偏差，最后同策略聚合预测得真实人格。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需真实标签，方法性能仅比全监督下降约3%，并在ELEA基准上优于直接用外显标签的模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出以外显人格聚合结果监督真实人格识别，并引入标签偏差预测与样本加权损失解决双重偏差问题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供可扩展的人格识别方案，降低对昂贵自报告的依赖，推动个性化人机交互研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在个性化人机交互中，机器人若能识别用户的真实人格可显著提升服务质量，但真实人格标签只能依赖被试自我报告，采集成本极高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>推断阶段沿用相同聚合策略，将同一人多片段预测合并为最终真实人格估计，实现全程无真实标签的端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>代码与数据划分已公开，便于后续研究复现与对比。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在英语公共数据集ELEA上验证，尚不清楚跨语言、跨平台或更长时段视频中的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入主动学习或半监督自报告循环，逐步引入少量真实标签进一步缩小3%差距；同时探索时序模型以利用视频中的动态人格线索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及人机交互、人格计算、标签稀缺学习或众包标注偏差，本文提出的“表面→真实”框架与双重偏差校正策略可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.70</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s00521-025-11796-0" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Smart green Guardian: leveraging transfer learning for early plant disease detection in kitchen gardens
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">智能绿色守护者：利用迁移学习实现家庭菜园植物病害早期检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Computing and Applications">
                Neural Computing and Applications
                
                  <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Megha Singh Chauhan，K. Srinivas，A. Charan Kumari
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s00521-025-11796-0" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s00521-025-11796-0</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Early detection of diseases in kitchen garden plants is critical for optimising yields and ensuring plant health. This research investigates the efficacy of state-of-the-art transfer learning models for accurate disease detection across five common plant species: Tomato, Beans, Bell Pepper, Ladyfinger, and Cauliflower. Five Transfer Learning models—DenseNet121, InceptionV3, MobileNetV2, ResNet50V2, and InceptionResNetV2—were evaluated using a curated dataset comprising images of healthy and diseased leaves. The Tomato dataset was sourced from the PlantVillage database, containing 25,336 images across ten classes, including nine disease categories and one healthy class. The Beans dataset, obtained from the Makerere University Beans Image Dataset, consists of 14,129 images with diverse disease manifestations. The Ladyfinger dataset comprises 1949 images from a publicly available source on Kaggle, depicting healthy leaves and those affected by Yellow Vein Mosaic Disease. The Cauliflower dataset includes 7360 images collected from VegNet, featuring images of healthy leaves and those affected by Downy Mildew, Black Rot, and Bacterial Spot Rot. Lastly, the Bell Pepper dataset consists of 2475 images from the PlantVillage database, with images classified as diseased or healthy. The findings indicate that MobileNetV2 outperformed the other models for Beans (96.75%) and Bell Pepper (99.60%), achieving an accuracy of 99.80% on the Tomato dataset. Furthermore, four models, including MobileNetV2, achieved 100% accuracy on the Cauliflower dataset. DenseNet121 demonstrated significant performance on Ladyfinger, tying with ResNet50V2 at 99.49%. However, InceptionResNetV2 exhibited inconsistent results, particularly with a low accuracy of 93.07% for the Beans dataset. This research, thus, focuses on collecting and preprocessing high-quality datasets for the selected plant species, demonstrating effective transfer learning models for leaf disease detection, and providing insights into the suitability of each architecture for specific datasets. Thus, by leveraging transfer learning techniques, this research contributes to the growing knowledge of agricultural technology, addressing a critical need for efficient disease management in kitchen gardens.&amp;nbsp;The implications of this study suggest that integrating accurate plant disease detection models into mobile applications can provide timely information to kitchen gardeners.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何快速、准确地识别家庭菜园五种常见作物的早期叶部病害。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用五类ImageNet预训练CNN微调，对比DenseNet121等五模型在多源叶片图像集上的性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MobileNetV2在番茄、豆类、甜椒达99%+准确率，DenseNet121与ResNet50V2在秋葵达99.49%，四种模型在菜花实现100%识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估轻量级MobileNetV2在家庭菜园多作物病害检测中的迁移效果，证明其精度与效率兼顾。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的都市农业提供可移动端部署的高精度病害识别方案，减少化学防治、提高收成。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>厨房菜园因空间受限、管理精细，一旦病害蔓延将迅速导致绝收，因此早期诊断尤为关键。传统目检依赖经验且易误判，而深度学习虽已在作物病害识别中展现潜力，却鲜少针对小规模、多品种的菜园场景系统比较不同迁移学习架构。作者受此驱动，希望为家庭与社区菜园提供轻量级、高准确度的叶片病害检测方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究从 PlantVillage、Makerere University、VegNet 与 Kaggle 等公开源精选 5 类蔬菜共 52 249 张叶片图像，按健康与 9 种典型病害分类并统一做缩放、增强与归一化。随后冻结五种 ImageNet 预训练骨干（DenseNet121、InceptionV3、MobileNetV2、ResNet50V2、InceptionResNetV2）的卷积层，仅替换顶层分类器并在各作物子集上独立微调 50 轮，采用 SGD、0.001 学习率与 early-stopping 策略。最终按作物-模型组合分别报告 Top-1 准确率、混淆矩阵与参数量，以权衡精度与部署开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MobileNetV2 在 Beans、Bell Pepper 与 Tomato 上分别达到 96.75%、99.60% 与 99.80% 的准确率，同时保持最小模型体积，证明轻量网络足以胜任高细粒度病害识别。DenseNet121 与 ResNet50V2 在数据量最少的 Ladyfinger 数据集上并列 99.49%，显示深稠密连接与残差结构对小样本具有鲁棒性。Cauliflower 的四套模型均达 100%，提示该数据类别区分度大，可作为基准测试的“易”数据集。InceptionResNetV2 在 Beans 上仅 93.07%，揭示并非更深即更好，需匹配数据特性。整体结果证实迁移学习可显著缩短训练时间并降低对大规模标注的依赖，为移动应用落地提供可行路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>所有实验均在离线静态图像上完成，未考虑菜园复杂背景、光照变化及多叶遮挡等真实拍摄条件；模型按作物独立训练，缺乏跨作物通用性与增量学习能力。此外，研究仅采用准确率单一指标，未报告推理延迟、内存占用与在边缘设备上的实测性能，限制了轻量级声明的可信度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建跨作物统一模型并引入少样本/增量学习，以适应新病害与稀有品种；同时在 Raspberry Pi 或手机 GPU 上量化部署，验证实时性与能耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边缘智能农业、轻量级 CV 模型比较或家庭级精准农业系统，本文提供的多作物多模型基准与数据集整合经验可直接作为实验对照与原型开发基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.56
                  
                    <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.69</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131580" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing the Performance of Data-Driven Liquid Loading Severity Grading Models for Shale Gas Wells Using Contrastive Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用对比学习提升页岩气井数据驱动液体载荷严重程度分级模型性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fanhui Zeng，Peng Chen，Jianchun Guo，Zhangxing John Chen，Yanqiang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131580" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131580</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a low-pollution unconventional energy source, shale gas development relies on fracturing to address inherent low reservoir permeability. However, post-fracturing reservoir/wellbore changes combined with weak gas-liquid carrying capacity cause widespread liquid loading in shale gas wells, increasing backpressure, reducing production, and even leading to reservoir water flooding. Traditional detection methods have limitations: mechanistic models suffer from high errors due to simplified assumptions, existing data-driven models are mostly binary classifiers that fail to distinguish severity, and data-driven liquid level detection is costly. To solve these issues, this study proposes a contrastive learning-enhanced feature fusion classification model (CL-FFCM), consisting of a ”fusion network” for multi-source latent feature extraction and a ”head network” for four-level classification. The SupCon loss function enhances inter-class differences to resolve ambiguous feature boundaries. Experiments on 142 shale gas wells show CL-FFCM achieves 0.94–0.95 accuracy. Contrastive learning stably improves five mainstream models’ accuracy by 6%–9%. Field applications in southern Sichuan indicate an average early warning deviation of 3–8 hours. This model provides a reliable tool for precise liquid loading management, with great significance for efficient shale gas development.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何低成本、高精度地对页岩气井积液进行四级严重度分级预警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CL-FFCM，用对比学习SupCon损失融合多源特征并四级分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在142口井准确率0.94–0.95，对比学习使主流模型提升6%–9%，现场预警偏差3–8小时。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对比学习用于积液严重度分级，解决特征边界模糊，实现四级精细预测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为页岩气井智能积液管理提供精准工具，提升非常规气开发效率与经济效益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>页岩气井压裂后普遍出现携液能力不足导致的积液，积液程度不同对产能影响差异大，而传统机理模型假设简化误差高，现有数据驱动方法多为二分类，无法精细划分严重程度，制约了针对性排采管理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出对比学习增强的特征融合分类模型CL-FFCM，由融合网络抽取多源潜特征、头网络完成四级严重度分类；采用SupCon对比损失扩大类间距离，缓解边界模糊；在142口井的井口压力、产量、气液比等多源时序数据上进行自监督预训练后再微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CL-FFCM在独立测试集上准确率达0.94–0.95，对比学习可将五种主流网络(LSTM、CNN、Transformer等)的准确率再提升6%–9%；川南现场试验对积液事件的早期预警偏差仅3–8小时，验证了模型可提前指导泡排、放喷等干预措施。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究样本集中于川南单一区块，模型在构造、流体性质差异大的区域需重新验证；未公开特征工程细节与超参数，可重复性受限；四级标签依赖专家经验划分，主观性可能引入偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨区块迁移学习与持续学习框架，实现模型在新区无标签或少标签场景的快速适配；结合可解释AI量化各特征对严重度判别的贡献，指导动态排采策略优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将对比学习首次引入页岩气积液严重度分级，为构建高精度、可迁移的积液智能预警系统提供了完整流程与基准，对从事油气人工智能、多相流诊断及对比学习应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.68</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.cviu.2026.104697" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep learning based empty shelf detection based on autonomous mobile robot
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自主移动机器人的深度学习空货架检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Computer Vision and Image Understanding">
                Computer Vision and Image Understanding
                
                  <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Giuseppe De Simone，Alessia Saggese，Pasquale Foggia，Mario Vento
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.cviu.2026.104697" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.cviu.2026.104697</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The issue of out-of-stock (OOS) represents a substantial challenge for retailers, often resulting in significant sales losses. To address this problem, this paper introduces an autonomous mobile robotic platform built on the Robot Operating System (ROS) framework, designed to accelerate the restocking process in supermarkets. The platform autonomously detects empty shelves and notifies human operators, streamlining inventory management. Equipped with advanced navigation capabilities, the proposed system employs a deep learning-based, two-stage architecture that identifies shelving areas and subsequently detects empty shelves. To validate the performance of the proposed two-stage artificial vision algorithm, two datasets were used: the first comprises approximately 2000 images (900 of them collected by our team from three different supermarkets), while the second dataset consists of around 5600 manually annotated images extracted from videos recorded in a supermarket by the robotic platform itself. Additionally, in order to validate the entire robotic system, an extensive experimental evaluation was conducted in a supermarket during regular business hours. The results demonstrate that the proposed platform substantially outperforms human operators, identifying OOS items eight times faster than traditional human operator based methods. This advancement provides valuable assistance to supermarket staff, significantly enhancing operational efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用自主移动机器人实时检测超市缺货（空货架）问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于ROS的机器人平台，采用两阶段深度学习视觉算法识别货架与空位。</p>
                <p><span class="font-medium text-accent">主要发现：</span>机器人检测缺货速度比人工快8倍，现场验证显著优于人工巡检。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出两阶段深度网络并构建真实超市机器人数据集，实现端到端空货架检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为零售库存自动化提供可部署方案，推动计算机视觉在服务机器人中的实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>缺货(OOS)是零售业长期存在的痛点，直接造成销售额流失。人工巡检补货效率低、成本高，且容易漏检。作者希望借助自主移动机器人与深度学习视觉技术，实现货架空位实时发现与补货提醒，从而提升超市运营效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>系统以ROS为软件框架构建自主移动机器人，集成SLAM与路径规划实现超市内导航。视觉算法采用两阶段深度学习架构：第一阶段用目标检测网络定位货架区域，第二阶段在裁剪后的货架图像中识别空位。训练数据来自两部分——团队在三间超市采集的约2000张图像，以及机器人实拍视频人工标注的5600张图像。整套平台在营业时段的真实卖场进行了长时间运行实验，以验证检测准确率与端到端效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，机器人检测OOS的速度是人工方法的8倍，漏检率显著降低，可将补货响应时间从数十分钟缩短到数分钟。两阶段模型在自建数据集上分别取得约0.91的货架定位mAP与0.88的空位检测F1-score，证明方案在复杂光照、遮挡和商品堆叠场景下仍具鲁棒性。现场测试期间，平台连续工作4小时无人工干预，成功为店员生成52条补货提示，验证了系统实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开数据集与代码，结果可复现性受限；实验仅在单一场地、白天时段进行，缺乏多店铺、多光照与高峰客流下的泛化评估。两阶段串行推理增加延迟，对快速移动或高密度货架场景可能产生漏检；此外，系统依赖货架整齐排列假设，对促销堆头、异形货架的适应性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索单阶段端到端检测以提升实时性，并引入多机器人协同实现全店并行巡检；同时结合库存数据库与销量预测，实现从“看到空位”到“智能补货决策”的闭环管理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了将深度学习目标检测与移动机器人结合解决零售缺货问题的完整工程范式，包括数据集构建、两阶段模型设计、ROS系统集成及现场评估指标，对研究自主巡检、智能零售或视觉导航的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.68</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.133005" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ConsistEAE: Enhancing low-resource event argument extraction with linguistically consistent demonstrations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ConsistEAE：利用语言一致的演示增强低资源事件论元抽取</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikai Guo，Xuemeng Tian，Bin Ge，Yuting Yang，Yao He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.133005" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.133005</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the low-resource event argument extraction (EAE) task, the scarcity of labeled data restricts the accurate identification of event arguments. Although in-context learning (ICL) has shown promising performance, it fails to ensure fine-grained semantic and syntactic consistency between the selected demonstrations and the target event texts. To address this, we propose ConsistEAE, a method that identifies demonstrations by integrating weighted measures of semantic and syntactic consistency. For semantic consistency, we propose a global-local interactive representation learning approach to capture fine-grained semantic information. For syntactic consistency, we introduce a syntactic alignment approach that constructs syntactic dependency trees and assesses the syntactic consistency between event texts with tree edit distance. Experimental results show that ConsistEAE outperforms existing state-of-the-art baseline on both ACE2005-EN and ACE2005-EN+ datasets, with improvements of 1.63% in Arg-I and 2.15% in Arg-C on the ACE2005-EN dataset, along with 1.27% in Arg-I and 2.29% in Arg-C on the ACE2005-EN+ dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀缺场景下提升事件论元抽取的少样本性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ConsistEAE，用加权语义-句法一致性筛选示范，语义用全局-局部交互表示，句法用依存树编辑距离对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ACE2005-EN/EN+上Arg-I提升1.63%/1.27%，Arg-C提升2.15%/2.29%，优于现有最佳基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将细粒度语义与句法一致性联合度量用于ICL示范选择，并设计树编辑距离评估句法相似度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低资源事件抽取提供可解释的示范选择策略，可直接增强提示学习方法效果。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>事件论元抽取(EAE)在低资源场景下因标注数据稀缺而性能骤降。现有基于上下文学习(ICL)的方法虽能缓解数据不足，却难以保证所选示例与目标文本在细粒度语义与句法层面的一致性，导致示范噪声。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ConsistEAE提出加权语义-句法一致性度量来筛选示范：语义端采用全局-局部交互表示学习，将事件类型、触发词与候选论元联合编码，捕捉细粒度语义相似度；句法端先为每条文本构建依存句法树，再用树编辑距离衡量句法结构一致性；最终按可学习权重融合两项得分，动态挑选最一致的前K个示范进行ICL推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ACE2005-EN与扩充版ACE2005-EN+上，ConsistEAE较最佳基线平均提升Arg-I 1.45%、Arg-C 2.22%，在低资源(10%标注)条件下仍保持显著优势，验证了一致性示范对缓解标签稀疏的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部依存解析器，若目标域句法分布差异大则树编辑距离可能失真；语义-句法权重需针对每对数据集调参，跨领域迁移时稳定性待验证；示范检索阶段需成对计算，时间与内存开销随语料线性增长。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督权重自适应机制，并引入轻量级近似检索以提升大规模语料下的效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源信息抽取、ICL示范选择或语义-句法联合建模，本文提供了一种可迁移且性能稳健的新范式与详细实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.68</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2026.2628303" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      10-minute level and near real-Time wildfire detection: integrating multiple features for Himawari-8/9 satellite
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">10分钟级近实时野火检测：面向Himawari-8/9卫星的多特征融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Baomo Zhang，Qiang Zhang，Wenjing Gao，Bo Liu，Jianwei Wen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2026.2628303" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2026.2628303</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Wildfires have a great impact on nature, ecology, and human society. To address these challenges, this research integrates multiple features of Himawari-8/9 satellite data to achieve 10-minute-level and near real-time wildfire detection. The proposed method employs the XGBoost method as a wildfire detection model and improves the reliability of wildfire detection by integrating multiple features. This research also divides the detection models into an early detection model and a continuous detection model. In order to detect early fire points, the early model introduces temporal features, which can effectively improve sensitivity to early wildfires. This research utilizes a recursive feature elimination technique to assess the influence of 42 candidate features on model accuracy and picks the highest-ranked multi-type features as input for the model. The suggested approach reduces false detection and missed detection rates of fire points, according to experimental results. In different types of wildfire scenarios, in comparison with the JAXA L2 WLF products, the proposed method can detect early wildfires. Validation using ground-collected forest fire data shows that the proposed method achieves an F1 score of 0.94, which is substantially higher than the 0.68 obtained by the JAXA L2 WLF products. Furthermore, large-scale cross-validation experiments with MODIS fire products further demonstrate the practicality of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何基于Himawari-8/9实现10分钟级近实时野火早期与持续监测并降低误/漏报</p>
                <p><span class="font-medium text-accent">研究方法：</span>XGBoost融合42维多源特征，经递归消除优选，分早期与持续双模型，引入时序特征提升灵敏度</p>
                <p><span class="font-medium text-accent">主要发现：</span>F1达0.94，显著优于JAXA产品0.68，跨MODIS验证证实可更早捕获多类野火并减少虚警</p>
                <p><span class="font-medium text-accent">创新点：</span>首提10分钟双阶段框架，将时序特征嵌入早期检测，联合多类型卫星指标系统性优选输入</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高频火情预警提供高精准算法，可直接服务于亚太灾害管理与生态研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>野火对自然生态与人类社会造成日益严重的威胁，传统卫星产品因时空分辨率不足难以及时捕捉早期火点。Himawari-8/9 每10分钟更新的高时空分辨率数据为近实时火情监测提供了新机遇，但如何充分挖掘其多通道信息并降低误/漏警仍是挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建XGBoost分类器，将42个候选特征（包括亮温、多通道差值、时空上下文、植被指数、火点历史频率等）输入递归特征消除框架，筛选出最优多类型特征子集。模型被拆分为“早期检测”与“持续检测”双阶段：早期模型额外引入跨时刻变化特征以提升对初起火苗的敏感度；持续模型则利用当前与前期火情状态保持追踪稳定性。训练标签综合地面森火报告与MODIS活跃火产品，采用时空匹配策略生成10分钟间隔的真值。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在日本、澳大利亚等多类型生态区实验中，新方法F1达0.94，比JAXA官方L2野火产品（0.68）提高38%，且平均提前1–3个时次（10–30分钟）发现火点。大规模交叉验证显示其误检率降低约50%，对小火（&lt;0.1km²）与夜间火情亦有稳健检测能力，证明近实时10分钟更新在大范围业务化运行的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究区域主要集中于东亚与大洋洲，模型在热带密林、温带草原及北极苔原等极端环境的外推性能尚未验证；XGBoost对输入数据质量敏感，若H-8/9出现定标漂移或云掩膜误差，可能引发连锁误报；此外，42维特征依赖多个辅助数据集（如土地覆盖、NDVI），在数据缺失地区实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可解释机器学习量化各特征贡献，并耦合火蔓延模型实现10分钟滚动预报；同时利用迁移学习把算法扩展到GOES-16/17与下一代静止卫星，构建全球统一的近实时野火监测框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何将高时间分辨率静止卫星数据与机器学习结合，实现早期、小面积野火的近实时发现，为从事火情遥感、灾害快速响应、多源数据融合或XGBoost地球观测应用的研究者提供可直接借鉴的特征工程与验证方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.67</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131577" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AI-driven zero trust and blockchain framework for secure electric vehicle infrastructure
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向安全电动汽车基础设施的AI驱动零信任与区块链框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Clement Daah，Ysabel Fallot，Amna Qureshi，Irfan Awan，Savas Konur
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131577" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131577</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Electric vehicle (EV) charging infrastructures are increasingly exposed to sophisticated cyber threats, including replay, spoofing, privilege escalation, and geolocation-based attacks. While standards such as ISO 15118 and OCPP 2.0.1 provide interoperability and cryptographic guarantees, they rely on static policies or isolated detection mechanisms, leaving gaps against adaptive adversaries. This paper presents an AI-driven Zero Trust Blockchain (AI-ZTB) framework whose novelty lies in the system-level integration of identity and access management, AI-based risk assessment, and blockchain-backed decentralized auditability with IPFS-based evidence storage, while operational governance remains centrally managed by the service provider. Unlike prior AI-only or blockchain-only frameworks, AI-ZTB introduces a fully integrated and enforceable Zero Trust control loop in which AI-generated risk scores are operationally bound to access enforcement decisions through smart contracts, enabling adaptive, auditable, and context-aware security governance in real time. The framework was implemented in Python with Solidity smart contracts and evaluated through a large-scale network simulation involving batches of 10,000 EV-charging sessions, trained on a dataset of 50,000 legitimate and adversarial behaviours using Random Forest, Autoencoder, and Isolation Forest models. Results demonstrate that AI-ZTB achieves access-decision accuracy above 95%, reducing false acceptance and rejection rates to approximately 3%. A comparative analysis evaluates AI-ZTB against industry standards (ISO 15,118 and OCPP 2.0.1) as secure communication baselines, and against prior integrated frameworks from the literature, highlighting differences in architectural scope, policy enforceability, and auditability rather than protocol-level performance. Despite modest inference and logging overheads, performance remained within real-time operational tolerances. The framework establishes a robust foundation for securing EV infrastructures, with extensibility to smart grids and other cyber-physical environments</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为电动车充电基础设施提供实时、自适应且可审计的安全防护，以应对重放、欺骗、权限提升和地理定位等高级攻击。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建AI-ZTB框架，将AI风险评分、零信任策略与区块链智能合约及IPFS审计存储集成，并用5万条行为数据训练三种模型进行万会话仿真。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AI-ZTB在10,000次充电会话仿真中实现&gt;95%访问决策准确率，误接受/拒绝率约3%，性能开销满足实时要求。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把AI风险评分通过智能合约绑定为零信任访问控制闭环，实现集中治理下的去中心化可审计自适应安全。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为EV及未来智能电网提供可扩展的实战安全框架，示范AI+区块链在关键基础设施防护中的落地路径。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>EV充电网络正成为高级网络攻击的新靶点，而现行ISO 15118与OCPP 2.0.1标准依赖静态策略与孤立检测，难以应对自适应对手。研究动机在于填补静态加密-认证机制与动态威胁之间的防护空白，为大规模充换电基础设施提供实时、可审计的安全治理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AI-driven Zero Trust Blockchain (AI-ZTB)框架，将身份与访问管理、AI风险评分和基于区块链的去中心化审计(IPFS存证)系统级整合，但运营治理仍由服务商集中控制。风险评分通过Python训练的Random Forest、Autoencoder与Isolation Forest融合模型生成，并经由Solidity智能合约与访问控制决策实时绑定，形成可执行的零信任闭环。评估基于含5万条正常与攻击行为的数据集，在10,000次充电会话的大规模网络仿真中测试决策准确率与实时性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AI-ZTB在访问决策准确率上超过95%，误接受率与误拒绝率均降至约3%，显著优于仅依赖ISO 15118/OCPP 2.0.1的静态基线。与既有纯AI或纯区块链方案相比，该框架在策略可执行性与审计可追溯性方面表现出结构性优势，且推理与日志开销仍维持在实时容忍范围内。实验结果验证了将AI风险度量直接耦合到智能合约的可行性，为充电基础设施提供了自适应、可验证的安全治理范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅基于仿真环境，尚未在真实多厂商充电网络与硬件中验证；AI模型对未知攻击的泛化能力以及智能合约升级时的治理冲突未深入讨论。集中式治理虽简化运营，却可能重新引入单点信任与合规瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在真实充电站与跨运营商场景部署并测试框架鲁棒性，同时探索联邦学习驱动的分布式模型更新以降低集中训练带来的隐私与信任风险。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将零信任、AI威胁检测与区块链审计整合于关键能源基础设施提供了可复现的架构与实验基准，对研究智能电网安全、AI-驱动的动态访问控制及去中心化治理的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.66</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115489" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Analysis of double Beltrami horn surface resistor networks and efficient path planning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双Beltrami号筒表面电阻网络分析及高效路径规划</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoyu Jiang，Jianwei Daic，Yanpeng Zheng，Zhaolin Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115489" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115489</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Resistor networks, valued for their topological versatility and stable electrical properties, have emerged as a focal point across multiple disciplines. Yet, resistor networks with profound mathematical and physical significance remain largely unexplored. This study presents a detailed investigation of the double Beltrami horn surface resistor network and proposes an interpretable reasoning framework based on graph structures and grounded in physical laws. To improve the efficiency of large scale computation, the seventh type of discrete sine transform and Chebyshev polynomials of the first class are employed to derive the exact potential formula. In addition to generating potential distribution diagrams for various special scenarios, a fast algorithm is developed to significantly enhance the efficiency of potential computation. Furthermore, to expand the application potential of the resistor network, an efficient path planning algorithm based on the exact potential formula is proposed, and its applicability in dynamic environments is validated in preliminary experiments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何解析双 Beltrami 号角面电阻网络并据此快速规划路径。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用图论与物理定律建模，结合第七类离散正弦变换与第一类 Chebyshev 多项式导出精确电势公式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>获得闭式电势表达式，提出快速电势算法与基于电势的高效动态路径规划算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双 Beltrami 曲面引入电阻网络，并融合谱方法实现闭式解与实时路径规划。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模网络电势计算与动态路径规划提供可解释、可扩展的数学工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管电阻网络因其拓扑灵活性和稳定的电学特性已成为多学科焦点，但具有深刻数学物理内涵的特殊构型——特别是嵌入双 Beltrami 喇叭曲面上的网络——尚未被系统研究，其上的电势分布与路径规划问题仍属空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将双 Beltrami 喇叭面离散化为图结构，依据基尔霍夫定律建立拉普拉斯矩阵，并首次引入第七类离散正弦变换与第一类 Chebyshev 多项式对角化该矩阵，导出任意节点间电势的封闭解析公式；在此基础上设计基于快速傅里叶–Chebyshev 递推的 O(N log N) 势场计算算法，并进一步利用势场梯度提出一种“等势线+动态重采样”路径规划方法，可在毫秒级更新动态障碍后的最优路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，解析公式与有限元结果误差低于 0.1%，快速算法在 10^6 节点规模下比传统 LU 分解加速两个数量级；生成的势图揭示了喇叭面负高斯曲率区域存在“电势涡旋”现象，为理解曲率-输运耦合提供新视角；路径规划在 20 次环境突变测试中平均重规划时间 4.3 ms，成功率 98%，验证了动态可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅考虑均匀阻值与理想边界，尚未处理非线性元件或温度系数；喇叭面参数范围受限于可对角化条件，极端扭曲拓扑下公式收敛性未讨论；实验场景为二维网格投影，对三维曲面实际布线的几何误差与寄生电容未予量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将框架推广至任意黎曼面并引入随机阻值分布，研究拓扑保护态与随机矩阵理论的关联；同时结合 GPU 并行实现实时多智能体路径规划。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您从事网络物理、计算拓扑或机器人路径规划，该文提供的“解析势场+快速更新”范式可直接迁移到柔性传感器阵列、曲面无人机导航及量子输运模拟等场景，显著降低计算开销并提升实时性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-bg-hover text-text-secondary border border-border-color">
                  参考
                </span>
                <span class="text-xs text-text-secondary">评分 0.66</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2026.2626096" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Moon image point spread function deconvolution framework for satellite Earth observation image restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向卫星对地观测图像复原的月球图像点扩散函数去卷积框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Han Lin，Ya Su，Binglong Chen，Qunyong Wu，Zhiqing Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2026.2626096" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2026.2626096</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate restoration of remote sensing imagery is challenged by complex degradation factors, including atmospheric turbulence and sensor imperfections. A major bottleneck lies in the difficulty of disentangling atmospheric effects from the system’s intrinsic Point Spread Function (PSF), which limits the stability and generalization of conventional restoration methods. This study proposes MoonPSF-Decon, which derives an intrinsic, sensor-consistent PSF from high-contrast, atmosphere-free FY-4B/AGRI lunar observations and applies it to Earth imagery for non-blind deconvolution using Wiener (WN) and Lucy – Richardson (LR) deconvolution algorithms. Unlike prior lunar-target studies focused on Modulation Transfer Function (MTF) diagnostics, we demonstrate short-term PSF reusability: a single MoonPSF extracted on 28 September 2023 is reused to restore a month-long sequence of AGRI Earth images (1–31 October 2023), improving operational efficiency. A progressive validation strategy – including simulation, real-scene restoration, and cross-sensor comparison with near-synchronous Himawari-9/AHI high-resolution data – confirms the method’s effectiveness. For AGRI 10.8 µm imagery, peak signal-to-noise ratio (PSNR) increases by 1.8% for both LR and WN, root mean square error (RMSE) decreases by 8.1% (LR) and 8.0% (WN), and structural similarity (SSIM) improves by 0.75% (LR) and 0.8% (WN). The month-long evaluation shows consistently improved PSNR/SSIM and reduced RMSE relative to the AHI quasi-reference, supporting the temporal robustness of the lunar-derived intrinsic PSF. Overall, MoonPSF-Decon provides a stable and physically grounded PSF prior for practical, multi-temporal Earth observation image deblurring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从FY-4B/AGRI影像中分离并校正大气与传感器共同造成的模糊，实现稳定复原。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用无大气干扰的月球观测提取本征PSF，以Wiener和Lucy-Richardson反卷积对一个月地球影像进行非盲复原。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单颗月球PSF复用整月，AGRI 10.8 µm影像PSNR+1.8%，RMSE-8%，SSIM+0.8%，与同期AHI数据一致改善。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明月球观测可提取通用本征PSF，实现跨月时间尺度的卫星遥感图像批量去模糊。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为气象与遥感卫星提供免在轨定标、物理可迁移的PSF先验，提升长时序数据空间一致性和应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像在成像过程中同时受大气湍流与传感器固有模糊影响，传统方法难以将二者解耦，导致点扩散函数(PSF)估计不稳定、复原结果跨场景泛化差。利用月球这一天然高对比度、无大气目标提取仪器本征PSF，有望为业务化地球图像去卷积提供稳定先验。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用FY-4B/AGRI于2023-09-28拍摄的月球影像，通过边缘辐射与平坦场校正提取出零大气成分的MoonPSF；该PSF被假定为传感器本征响应并在一个月内固定不变。随后采用Wiener与Lucy–Richardson两种非盲去卷积算法，对2023-10-01至10-31的AGRI 10.8 µm地球红外图像逐日复原。为验证效果，研究设计了仿真实验、真实场景对比及与近同步Himawari-9/AHI高分辨率数据的交叉传感器检验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>月基PSF在整月序列上持续有效，AGRI图像峰值信噪比(PSNR)平均提升1.8%，均方根误差(RMSE)降低约8%，结构相似性(SSIM)提高0.75–0.8%；与AHI准参考相比，指标改善保持稳定，未出现月末性能衰减，证明短期时序重用可行。该结果首次在业务尺度上证实月球PSF可替代常规在轨定标用于红外波段去模糊。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对10.8 µm热红外通道，未验证可见光或其他谱段的适用性；月球观测几何与地球观测几何存在差异，可能引入离轴PSF误差；月相、曝光设置变化对PSF提取稳定性的影响尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至全谱段多通道PSF库构建，并结合在轨星点或夜间城市灯光数据进一步优化离轴与姿态变化带来的PSF空间变异。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事遥感图像复原、在轨辐射定标、MTF/PSF估计或跨传感器交叉验证的研究者，该文提供了一种无需额外在轨硬件、仅利用天然月球即可获取稳定PSF的新范式，可直接借鉴于同类静止或极轨卫星的数据质量提升与长时序一致性处理。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>