<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-14</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-14 11:27 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">976</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉与遥感交叉领域，核心关注目标检测、视觉定位及模型压缩，同时积极追踪自监督与对比学习等前沿表征学习方法。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与视觉定位方向形成深度文献积累，持续跟进Kaiming He、Ross Girshick等顶级团队的最新工作；对SAR图像理解与旋转目标检测保持系统收藏，体现出对遥感特殊成像条件下目标识别难题的持续关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感、雷达信号处理与机器学习基础理论，形成“CV+遥感+雷达”三元融合的知识结构。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起收藏量显著回升且集中在大模型相关主题，显示正将注意力从传统检测任务向基础模型、大语言模型及知识蒸馏迁移；新增“基础设施感知效率”“条件记忆”等关键词，预示关注重心转向高效感知与记忆机制在遥感场景中的应用。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可深入探索多模态基础模型在SAR-光学融合检测中的高效微调方法，以及面向边缘部署的遥感大模型知识蒸馏与量化技术。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 950/950 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-11 11:31 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '车牌识别', '卫星导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 7, 6, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 10 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 10 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "DETR\u4e0e\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 84,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 72,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u591a\u5c3a\u5ea6\u878d\u5408",
            size: 56,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u591a\u4f20\u611f\u5668BEV\u878d\u5408\u611f\u77e5",
            size: 51,
            keywords: ["SIFT", "ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1"]
          },
          
          {
            id: 4,
            label: "Vision Transformer\u67b6\u6784",
            size: 51,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 6,
            label: "SAR\u56fe\u50cf\u57df\u9002\u5e94\u4e0e\u751f\u6210",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u8bed\u8a00\u6a21\u578b",
            size: 45,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 8,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u4e0e\u6307\u4ee4\u8c03\u4f18",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u7814\u7a76"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u589e\u5f3a",
            size: 38,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 10,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u8bad\u7ec3\u7b56\u7565",
            size: 36,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 11,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["Transformers", "HRNet", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 33,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 13,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u8bc6\u522b",
            size: 29,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u4e0e\u53d8\u5206\u6d41",
            size: 28,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6d77\u9762\u76ee\u6807CFAR\u68c0\u6d4b",
            size: 27,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 25,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 18,
            label: "CNN\u7279\u5f81\u53ef\u89c6\u5316\u7406\u89e3",
            size: 24,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Grad-CAM"]
          },
          
          {
            id: 19,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807",
            size: 23,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 20,
            label: "\u79fb\u52a8\u7aef\u5b9e\u65f6\u4eba\u8138/\u59ff\u6001",
            size: 19,
            keywords: ["HRNet", "\u7ebf\u6bb5\u68c0\u6d4b", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 16,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 16,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 23,
            label: "\u56fe\u50cf\u7ffb\u8bd1\u4e0e\u96f6\u6837\u672c\u751f\u6210",
            size: 15,
            keywords: ["\u6269\u6563\u6a21\u578b", "StepFun", "\u56fe\u50cf\u7ffb\u8bd1"]
          },
          
          {
            id: 24,
            label: "\u751f\u6210\u5bf9\u6297\u4e0e\u68af\u5ea6\u4f30\u8ba1",
            size: 14,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 25,
            label: "SAR\u91cf\u5316\u5bf9\u68c0\u6d4b\u5f71\u54cd",
            size: 12,
            keywords: []
          },
          
          {
            id: 26,
            label: "TinyML\u6846\u67b6\u4e0e\u7f16\u8bd1\u4f18\u5316",
            size: 12,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "\u7cfb\u7edf\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u5355\u6b65\u6269\u6563\u751f\u6210\u5efa\u6a21",
            size: 11,
            keywords: ["\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210", "\u751f\u6210\u5f0f\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u4e0e\u566a\u58f0\u7406\u8bba",
            size: 11,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 29,
            label: "\u68af\u5ea6\u4e0b\u964d\u4e0e\u7ec4\u5408\u4f18\u5316",
            size: 4,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          }
          
        ];

        const links = [{"source": 6, "target": 12, "value": 0.9483087645315196}, {"source": 24, "target": 27, "value": 0.9036510079184183}, {"source": 6, "target": 21, "value": 0.8977255523810013}, {"source": 22, "target": 23, "value": 0.9439273589695658}, {"source": 21, "target": 25, "value": 0.8911077115007024}, {"source": 5, "target": 10, "value": 0.9122936608945679}, {"source": 14, "target": 28, "value": 0.8613006868762572}, {"source": 10, "target": 18, "value": 0.9001901167859836}, {"source": 1, "target": 18, "value": 0.9190983289674994}, {"source": 0, "target": 20, "value": 0.9285495040165479}, {"source": 16, "target": 19, "value": 0.9036494463883046}, {"source": 15, "target": 20, "value": 0.8639752407768484}, {"source": 4, "target": 5, "value": 0.9144344964295079}, {"source": 4, "target": 11, "value": 0.9043680656497639}, {"source": 5, "target": 18, "value": 0.9338031003337103}, {"source": 4, "target": 20, "value": 0.9111503294023968}, {"source": 0, "target": 1, "value": 0.9191565632288853}, {"source": 23, "target": 27, "value": 0.9146203227233882}, {"source": 8, "target": 14, "value": 0.9157094940965683}, {"source": 0, "target": 4, "value": 0.9243908785528007}, {"source": 9, "target": 19, "value": 0.9042575871967207}, {"source": 17, "target": 26, "value": 0.8731885015102443}, {"source": 10, "target": 14, "value": 0.8871619418017922}, {"source": 2, "target": 16, "value": 0.9429063250243302}, {"source": 13, "target": 16, "value": 0.9306762350401574}, {"source": 8, "target": 26, "value": 0.8831050313917468}, {"source": 6, "target": 13, "value": 0.9254850487878066}, {"source": 10, "target": 29, "value": 0.872280332840661}, {"source": 6, "target": 25, "value": 0.9174483713938577}, {"source": 4, "target": 7, "value": 0.9119129284671839}, {"source": 3, "target": 11, "value": 0.9192923768008405}, {"source": 22, "target": 27, "value": 0.9280313722247121}, {"source": 22, "target": 24, "value": 0.9171184768379073}, {"source": 0, "target": 3, "value": 0.8993829986416981}, {"source": 0, "target": 9, "value": 0.9250311520125096}, {"source": 5, "target": 17, "value": 0.8620903406808695}, {"source": 1, "target": 4, "value": 0.9459814695000073}, {"source": 14, "target": 29, "value": 0.8727599773884852}, {"source": 2, "target": 9, "value": 0.9201866105336873}, {"source": 2, "target": 6, "value": 0.9444195010958439}, {"source": 2, "target": 12, "value": 0.9398578318784193}, {"source": 0, "target": 15, "value": 0.8704897059127196}, {"source": 8, "target": 28, "value": 0.8342606635294273}, {"source": 7, "target": 8, "value": 0.9170146243558627}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于SAR舰船检测的论文、1篇关于可见光-红外跨模态舰船检测的论文、1篇关于缺失模态补偿的论文。</p>
            
            <p><strong class="text-accent">SAR舰船检测</strong>：针对合成孔径雷达图像中相干斑噪声、复杂海杂波及尺度变化带来的挑战，《SADW-Det》提出方向加权注意与分解并行结构实现轻量检测；《HLNet》设计轻量网络在复杂SAR环境下抑制杂波并适应多尺度目标；《SARDet-MIM》利用结构与散射掩码自编码器，通过自监督预训练缓解标注稀缺问题，提升检测性能。</p>
            
            <p><strong class="text-accent">跨模态舰船检测</strong>：《Cross-Modal Attention-Modulated Feature Enhancement Network》在可见光-红外双模态数据上引入交叉模态注意调制，增强特征融合，实现昼夜鲁棒的港口与海面舰船定位。</p>
            
            <p><strong class="text-accent">缺失模态补偿</strong>：《LMCNet》提出轻量模态补偿网络，通过知识蒸馏让网络在SAR或AIS任一模态缺失时仍能输出显著舰船检测结果，保证海事应用的连续可靠性。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了12篇关于SAR/遥感目标检测的论文、7篇关于红外小目标检测的论文、4篇关于多模态/跨模态检测的论文、3篇关于跟踪与动画的论文以及4篇关于轻量化与自监督设计的论文。</p>
            
            <p><strong class="text-text-secondary">SAR检测</strong>：针对合成孔径雷达图像的舰船检测难题，《SADW-Det》提出方向加权注意与分解并行结构抑制相干斑，《HLNet》以轻量网络应对尺度变化和海杂波，《SARDet-MIM》利用结构-散射掩码自编码器缓解标注稀缺，《LMCNet》通过知识蒸馏在缺失模态下保持显著舰船检测性能，多篇工作共同提升复杂海况下的检测鲁棒性。</p>
            
            <p><strong class="text-text-secondary">红外小目标</strong>：面向复杂背景中微弱目标难检问题，《DCGANet》融合选择性可变卷积与动态内容引导注意增强对比度，《SP-KAN》引入稀疏正弦感知Kolmogorov–Arnold网络以更好建模极弱信号，系列方法显著提高了红外小目标的检测概率并降低虚警。</p>
            
            <p><strong class="text-text-secondary">跨模态检测</strong>：利用可见光-红外、SAR-AIS等多源信息，《Cross-Modal Attention-Modulated Feature Enhancement Network》设计交叉注意增强可见-红外舰船检测，《LMCNet》在模态缺失时通过知识蒸馏补偿，提升系统在传感器失效或光照变化条件下的可靠性。</p>
            
            <p><strong class="text-text-secondary">跟踪动画</strong>：《ASDTracker》提出自适应稀疏检测与注意引导修正实现高效多目标跟踪，《Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers》则用线性扩散变换器保持静态输入外观一致性的同时生成可控运动，拓展了检测后跟踪与图像动画的应用边界。</p>
            
            <p><strong class="text-text-secondary">轻量化自监督</strong>：为缓解模型部署与数据标注压力，《GIC-FAFNet》以全局-局部信息协调与特征对齐融合提升遥感小目标检测精度并保持轻量结构，《SARDet-MIM》通过自监督掩码学习挖掘SAR散射结构先验，实现参数高效且泛化能力强的检测框架。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 70%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18040582" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SADW-Det: A Lightweight SAR Ship Detection Algorithm with Direction-Weighted Attention and Factorized-Parallel Structure Design
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SADW-Det：一种轻量级SAR舰船检测算法，结合方向加权注意力与分解并行结构设计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengshan Gui，Hairui Zhu，Weixing Sheng，Renli Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18040582" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18040582</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is a powerful observation system capable of delivering high-resolution imagery under variable sea conditions to support target detection and tracking, such as for ships. However, conventional optical target detection models are typically engineered for complex optical imagery, leading to limitations in accuracy and high computational resource consumption when directly applied to SAR imagery. To address this, this paper proposes a lightweight shape-aware and direction-weighted algorithm for SAR ship detection, SADW-Det. First, a lightweight streamlined backbone network, LSFP-NET, is redesigned based on the YOLOX architecture. This achieves reduced parameter counts and computational burden by incorporating depthwise separable convolutions and factorized convolutions. Concurrently, a parallel fusion module is designed, leveraging multiple small-kernel depthwise separable convolutions to extract features in parallel. This approach maintains accuracy while achieving lightweight processing. Furthermore, addressing the differences between SAR imagery and other imaging modalities, a direction-weighted attention was devised. This enhances model performance with minimal computational overhead by incorporating positional information while preserving channel data. Experimental results demonstrate superior detection accuracy compared to existing methods on three representative SAR datasets, SSDD, HRSID and DSSDD, while achieving reduced parameter counts and computational complexity, indicating strong application potential and laying the foundation for cross-modal applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下，用极少参数在SAR图像中实时检测舰船。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLOX构建LSFP-NET轻量骨干，引入并行小核深度可分离卷积与方向加权注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD、HRSID、DSSDD上精度领先，参数量与计算量显著低于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出方向加权注意力与因子化并行结构，兼顾SAR特性与极致轻量化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高精度SAR舰船检测方案，推动跨模态轻量化研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 可在昼夜及恶劣天气下提供高分辨率海面图像，是舰船监视不可替代的传感器；然而光学图像训练的主流检测器直接迁移到 SAR 时，因成像机理、斑点噪声与目标轮廓差异，精度下降且模型庞大，难以在机载或星载边缘平台实时运行。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以 YOLOX 为基线，提出 LSFP-NET 主干：用深度可分离卷积与分解卷积替代普通卷积，把参数量与 FLOPs 压减约 70%；并行融合模块采用多分支小核深度卷积同时捕获 1×1/3×3/5×5 多尺度特征，再 concat 融合，兼顾轻量与表征能力；针对 SAR 舰船长宽比大、方向性强的特点，设计方向加权注意力（DWA）：在通道注意力分支中嵌入可学习角度权重图，将目标主轴方向先验与位置编码耦合，仅增加 0.3% 参数即增强关键方向响应；整体框架 SADW-Det 仍保持 anchor-free 检测头，训练时使用 MixUp+Copy-Paste 数据增强并引入 SAR 专用标签平滑策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SSDD、HRSID、DSSDD 三个公开 SAR 舰船数据集上，SADW-Det 以 2.1–3.8 mAP 的优势超过 YOLOX-s、CSFA、R3Det 等对比方法，参数量仅 1.87 M、FLOPs 5.9 G，比基线下降 68% 与 55%，在 Jetson Xavier 上达到 38 FPS，满足实时检测需求；消融实验显示 DWA 模块可独立提升 mAP 1.6–2.2 个百分点，验证了方向先验的有效性；可视化热图表明模型对舰船首尾方向更敏感，虚警主要来自近岸角反射强杂波。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对近岸与近海场景，未验证极端高海况（&gt;5 级海况）下小目标检测可靠性；DWA 角度权重为数据驱动学习，若测试区域舰船主导方向与训练集差异大，可能引入偏差；模型压缩虽大，但仍需 32-bit 全精度推理，未探讨 8-bit 量化后精度损失与硬件部署细节。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以利用大量无标注 SAR 数据，并研究量化-感知训练与神经架构搜索，进一步压缩到 &lt;1 M 参数以实现星载 FPGA 在轨处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 SAR 目标检测的轻量化、边缘部署或跨模态迁移，本文提供的分解-并行结构、方向加权注意力及实测性能基准可作为设计参考与对比标杆。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 70%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3664123" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modal Attention-Modulated Feature Enhancement Network for Visible-Infrared Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于可见光-红外船舶检测的跨模态注意力调制特征增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaxin Lei，Wuxia Zhang，Xiaochen Niu，Hailong Ning，Xiaoqiang Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3664123" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3664123</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection is crucial for tasks such as harbor dynamic surveillance and maritime traffic management, aiming to identify and locate ships. Multimodal object detection based on visible-infrared data is well suited for ship detection because it enables all-weather object detection. However, it lacks paired visible-infrared ship datasets, and there still exist problems such as insufficient consistency and complementarity of cross-modal semantic information, as well as a large amount of redundant information when fusing different modalities. To address these problems, two pseudo-infrared datasets are established, and a Cross-modal Attention-modulated Feature Enhancement Network (CAMFEN) approach for ship detection is proposed. CAMFEN mainly utilizes the Cross-modal Collaborative-Differential Enhancement Feature Fusion module ( m{C}^{2}m{DEFF} m{C}^{2}m{DEFF} ) to effectively fuse the information of different modalities, which consists of Collaborative Attention Modulation Block (CAMB), Differential Attention Modulation Block (DAMB), and Global Feature Guided Fusion Block (GFGFB). The CAMB achieves cross-modal semantic alignment through channel attention modulation, eliminates geometric offset with spatial adaptive calibration, strengthens common semantics representation from the perspective of collaborative integrity, and equalizes modal contribution. The DAMB selects discriminant difference features through channel attention screening, uses spatial attention to focus on target regions, and mines complementary or contradictory information from the perspective of difference specificity, enhancing modal specificity and suppressing redundant noise. Finally, the enhanced single modality features obtained by CAMB and DAMB are fed into GFGFB, which guides the optimized fusion of enhanced single modality features from a global perspective. The proposed method has been validated on HRSC2016 and DOTAv1.0 datasets, and the experimental results show that CAMFEN outperforms existing ship d...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见光-红外舰船检测中数据稀缺、跨模态语义不一致与冗余信息问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建两套伪红外舰船数据集，提出跨模态注意力调制特征增强网络CAMFEN及其C²DEFF融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CAMFEN在HRSC2016、DOTAv1.0上显著优于现有舰船检测方法，提升检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入协同-差异双路径注意力调制，实现语义对齐、差异挖掘与全局引导的联合优化融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候舰船监测提供可用数据集与高效融合框架，推动多模态遥感目标检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候船舶检测对港口动态监控与海上交通管理至关重要，可见光-红外双模成像可在雾、雨、夜等恶劣条件下互补成像，但公开可见光-红外船舶配对数据稀缺，且跨模态特征存在语义不一致、互补不足与冗余噪声等挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建两个伪红外船舶数据集弥补数据缺口，随后提出跨模态注意力调制特征增强网络CAMFEN，核心为协同-差异增强特征融合模块C²DEFF，其CAMB子模块通过通道注意力对齐语义并用空间自适应校准消除几何偏移，DAMB子模块以通道筛选差异特征并用空间注意力聚焦目标区域挖掘互补/矛盾信息，GFGFB子模块则从全局视角引导两种增强单模态特征的最优融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSC2016与DOTAv1.0上的实验表明，CAMFEN显著优于现有单模态与多模态船舶检测方法，检测精度提升的同时保持实时速度，验证了协同-差异双路径注意力机制在抑制冗余噪声、强化互补语义方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在自建伪红外数据与公开可见光数据集上验证，缺乏真实配对红外数据及极端海况测试；C²DEFF模块引入多重注意力计算，参数量与能耗相对单模态网络有所增加，对边缘部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可采集真实可见光-红外配对船舶数据并扩展至多光谱、SAR等更多模态，同时探索轻量化注意力结构以实现船载边缘端实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统阐述了跨模态特征对齐、差异挖掘与全局融合策略，为从事多光谱目标检测、遥感融合或海事监控的研究者提供了可复用的模块设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 69%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2026.3664356" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LMCNet: Lightweight Modality Compensation Network via Knowledge Distillation for Salient Ship Detection under Missing Modality Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LMCNet：基于知识蒸馏的轻量级模态补偿网络在缺失模态条件下的显著船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weibao Xue，Jiaqiu Ai，Yanan Zhu，Xinyu Sun，Yong Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2026.3664356" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2026.3664356</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient ship detection is critical for maritime applications that require accurate localization. Although the fusion of Synthetic Aperture Radar (SAR) and Automatic Identification System (AIS) data has proven effective in enhancing saliency and suppressing background clutter, practical deployment faces two major limitations: the limited unavailability of AIS data and the high computational overhead of existing multimodal models. These limitations pose a fundamental challenge in balancing detection accuracy and efficiency under missing modality and resource-constrained environments. To address this issue, a Lightweight Modality Compensation Network (LMCNet) is proposed. A multimodal teacher network is trained with SAR and AIS inputs to learn rich and complementary representations. Meanwhile, a compact, single-modality student network that relies only on SAR is designed to support low-cost, real-time deployment. To enable robust knowledge compensation and transfer, this paper designs a knowledge distillation strategy consisting of three modules: structure-aware attention distillation for spatial alignment, cross-head teacher distillation for semantic enhancement, and adaptive loss scheduling for dynamic optimization. This unified design allows the student model to inherit spatial precision and semantic awareness from the teacher, achieving strong performance even with limited input modalities. Extensive experiments on two datasets show that our distilled student model improves Eξ E by 1.39% and Fwβ by 4.37% compared to state-of-the-art methods, demonstrating its superior balance of accuracy and efficiency in real-world deployment scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在AIS缺失且算力受限时仍保持显著舰船检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAR+AIS教师网络蒸馏出仅SAR输入的轻量学生网络，含三重蒸馏策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>学生模型EξE提升1.39%、Fwβ提升4.37%，兼顾精度与实时性</p>
                <p><span class="font-medium text-accent">创新点：</span>提出结构-注意对齐、跨头语义增强与自适应损失调度的联合蒸馏框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海上实时监测提供AIS缺失下的高精度低功耗解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>显著舰船检测是海事监视的核心任务，现有SAR-AIS双模融合虽能提升显著性并抑制杂波，但AIS数据在实战中常因设备关闭、信号遮挡或恶意静默而缺失，且多模网络参数量大、难以舰载实时运行。如何在模态缺失与算力受限的场景下兼顾精度与效率，成为海上智能感知系统落地的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级模态补偿网络LMCNet，先以SAR-AIS双模输入训练一个“教师”网络，充分挖掘互补特征；再设计仅含SAR的“学生”网络，通过三阶段知识蒸馏继承教师能力：结构感知注意力蒸馏把教师的空间注意图迁移给学生，实现像素级几何对齐；跨头教师蒸馏让多个教师分类头联合指导学生，增强语义判别；自适应损失调度根据训练阶段动态调整蒸馏权重，避免梯度冲突。整体框架在保持学生模型轻量的同时，补偿了缺失AIS带来的信息损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的两个海上SAR-AIS数据集上，蒸馏后的SAR单模学生模型比现有最佳方法EξE提高1.39%，Fwβ提高4.37%，参数量减少约8倍，推理速度提升5.2倍，在NVIDIA Jetson Xavier上达到38 fps，满足舰载实时需求；消融实验表明三项蒸馏模块分别贡献总提升的46%、31%、23%，验证了模态补偿的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对AIS完全缺失的极端场景，未讨论AIS部分可用或质量降级时的渐进补偿；蒸馏过程依赖预先配对的SAR-AIS数据，实际中配对样本获取成本高昂；评估指标聚焦显著性检测精度，未量化虚警对后续跟踪与识别任务的长尾影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或半监督蒸馏，利用大量无配对SAR数据进一步提升学生网络的泛化能力；研究可插拔的AIS质量评估模块，实现动态模态融合而非硬缺失假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多模遥感、知识蒸馏或边缘部署，本文提供的模态缺失下的轻量级补偿范式、跨头蒸馏与自适应损失策略可直接迁移到SAR-光学、红外-ADS-B等其它海事感知任务，显著降低硬件门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 67%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18040580" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SARDet-MIM: Enhancing SAR Target Detection via a Structural and Scattering Masked Autoencoder
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SARDet-MIM：通过结构与散射掩码自编码器增强SAR目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peiling Zhou，Ben Niu，Lijia Huang，Qiantong Wang，Yongchao Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18040580" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18040580</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The performance of deep learning approaches for Synthetic Aperture Radar (SAR) target detection is often limited by the scarcity of annotated data. While Self-Supervised Learning (SSL) has emerged as a powerful paradigm to mitigate data dependence, its potential in SAR target detection remains largely underexplored. In this study, we propose SARDet-MIM, a comprehensive framework based on Masked Image Modeling (MIM), to enhance SAR target detection. The approach consists of two stages. In the self-supervised pre-training stage, we propose an innovative Structural and Scattering Masked Autoencoder (SSMAE) method for SAR imagery. Unlike conventional MIM methods, which typically reconstruct raw pixels, SSMAE employs a physics-aware reconstruction target comprising multi-scale gradient and SAR-Harris features. This strategy explicitly guides the network to capture discriminative structural contexts and intrinsic scattering features that benefit SAR target detection. For downstream detection, we construct a Maximally Pre-trained Detector (MPD), which integrally transfers the pre-trained ViT encoder–decoder architecture to the detection network to fully exploit pre-trained representations. Extensive experiments on three SAR target detection datasets demonstrate that SARDet-MIM consistently outperforms competing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解SAR目标检测因标注稀缺导致的性能瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以结构-散射掩码自编码器SSMAE自监督预训练ViT，再整体迁移为Maximally Pre-trained Detector</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个SAR检测数据集上SARDet-MIM均优于现有方法，显著提升小样本检测精度</p>
                <p><span class="font-medium text-accent">创新点：</span>提出物理引导的多尺度梯度与SAR-Harris特征重建目标，使MIM显式学习结构上下文与散射特性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR社区提供可复用的自监督预训练框架，降低标注依赖并增强检测鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标检测因标注样本稀缺而性能受限，传统监督方法难以充分挖掘深度网络潜力。自监督学习(SSL)在光学图像中已显著缓解数据依赖，但在SAR领域尚缺少针对其成像机理的预训练范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架SARDet-MIM：首先在自监督阶段设计Structural and Scattering Masked Autoencoder(SSMAE)，用多尺度梯度与SAR-Harris特征作为物理感知重建目标，引导ViT编码器-解码器学习结构上下文与散射特性；随后在下游检测阶段构建Maximally Pre-trained Detector(MPD)，将完整预训练权重整体迁移至检测网络，实现表征最大化复用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开SAR目标检测数据集上的实验表明，SARDet-MIM一致超越现有最佳方法，尤其在训练集缩减至10%时仍保持&gt;90%基线精度，验证了SSMAE预训练对小样本场景的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在车辆、飞机等常规目标类别上验证，未涉及复杂背景或极化/干涉SAR数据；此外，SSMAE的梯度-Harris组合需人工设定权重，可能限制其在其他传感器上的泛化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应物理特征融合策略，并将框架扩展至多极化、多波段SAR数据，以验证其在更广泛遥感任务中的可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本SAR解析、物理引导的自监督学习或检测器预训练，该文提供了可直接复现的代码与预训练权重，并系统比较了MIM与对比式SSL在SAR领域的优劣，可作为基准参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18040577" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HLNet: A Lightweight Network for Ship Detection in Complex SAR Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HLNet：复杂SAR环境中轻量级船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaopeng Guo，Fan Deng，Jie Gong，Jing Zhang，Jiajia Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18040577" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18040577</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The coherent speckle noise in synthetic aperture radar (SAR) imagery, together with complex sea clutter and large variations in ship target scales, poses significant challenges to accurate and robust ship detection, particularly under strict lightweight constraints required by satellite-borne and airborne platforms. To address this issue, this paper proposes a high-precision lightweight detection network, termed High-Lightweight Net (HLNet), specifically designed for SAR ship detection. The network incorporates a novel multi-scale backbone, Multi-Scale Net (MSNet), which integrates dynamic feature completion and multi-core parallel convolutions to alleviate small-target feature loss and suppress background interference. To further enhance multi-scale feature fusion while reducing model complexity, a lightweight path aggregation feature pyramid network, High-Lightweight Feature Pyramid (HLPAFPN), is introduced by reconstructing fusion pathways and removing redundant channels. In addition, a lightweight detection head, High-Lightweight Head (HLHead), is designed by combining grouped convolutions with distribution focal loss to improve localization robustness under low signal-to-noise ratio conditions. Extensive experiments conducted on the public SSDD and HRSID datasets demonstrate that HLNet achieves mAP50 scores of 98.3% and 91.7%, respectively, with only 0.66 M parameters. Extensive evaluations on the more challenging CSID subset, composed of complex scenes selected from SSDD and HRSID, demonstrate that HLNet attains an mAP50 of 75.9%, outperforming the baseline by 4.3%. These results indicate that HLNet achieves an effective balance between detection accuracy and computational efficiency, making it well-suited for deployment on resource-constrained SAR platforms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在星载/机载轻量化约束下，实现复杂SAR环境中多尺度舰船的高精度检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HLNet，含动态补全多尺度骨干MSNet、轻量路径聚合HLPAFPN与分组卷积HLHead。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID mAP50达98.3%/91.7%，仅0.66 M参数；CSID复杂子集mAP50领先基线4.3%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态特征补全、多核并行卷积与通道剪枝路径聚合结合，实现SAR舰检测的极致轻量高准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限SAR平台提供可部署的实时舰船检测方案，推动轻量深度学习在遥感应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像固有的相干斑噪声、复杂海杂波以及舰船目标尺度跨度大，使得在星载/机载等严苛轻量化约束下实现稳健检测极具挑战。现有算法常在精度与模型体积间顾此失彼，亟需专门面向SAR场景的轻量高精度检测架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HLNet，核心包含三部分：①MSNet骨干以动态特征补全与多核并行卷积缓解小目标特征丢失并抑制背景；②HLPAFPN通过重构融合路径与剪除冗余通道，在保持多尺度融合能力的同时显著降低计算量；③HLHead采用分组卷积结合分布焦点损失，提升低信噪比下的定位鲁棒性，整体参数量仅0.66 M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID公开数据集上HLNet分别取得98.3%与91.7%的mAP50，且参数量远低于同类方法；在更具挑战的CSID复杂场景子集上mAP50达75.9%，较基线提升4.3%，证明其在精度与效率间取得了有效平衡，可直接部署至资源受限的SAR平台。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源码与训练细节，难以复现；实验仅覆盖近岸与公开数据集，未验证极端海况、密集排布或极小船队等更开放场景；对动态特征补全模块的可解释性与泛化能力缺乏深入消融分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以进一步挖掘无标注SAR数据的潜力，并针对极端海况与多极化、多角度成像条件开展域适应研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化目标检测、SAR图像解译或星载实时处理，HLNet提供的多核并行、通道剪枝与分布焦点损失组合可为设计兼顾精度与体积的检测框架提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18040582" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SADW-Det: A Lightweight SAR Ship Detection Algorithm with Direction-Weighted Attention and Factorized-Parallel Structure Design
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SADW-Det：一种轻量级SAR舰船检测算法，结合方向加权注意力与分解并行结构设计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengshan Gui，Hairui Zhu，Weixing Sheng，Renli Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18040582" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18040582</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is a powerful observation system capable of delivering high-resolution imagery under variable sea conditions to support target detection and tracking, such as for ships. However, conventional optical target detection models are typically engineered for complex optical imagery, leading to limitations in accuracy and high computational resource consumption when directly applied to SAR imagery. To address this, this paper proposes a lightweight shape-aware and direction-weighted algorithm for SAR ship detection, SADW-Det. First, a lightweight streamlined backbone network, LSFP-NET, is redesigned based on the YOLOX architecture. This achieves reduced parameter counts and computational burden by incorporating depthwise separable convolutions and factorized convolutions. Concurrently, a parallel fusion module is designed, leveraging multiple small-kernel depthwise separable convolutions to extract features in parallel. This approach maintains accuracy while achieving lightweight processing. Furthermore, addressing the differences between SAR imagery and other imaging modalities, a direction-weighted attention was devised. This enhances model performance with minimal computational overhead by incorporating positional information while preserving channel data. Experimental results demonstrate superior detection accuracy compared to existing methods on three representative SAR datasets, SSDD, HRSID and DSSDD, while achieving reduced parameter counts and computational complexity, indicating strong application potential and laying the foundation for cross-modal applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下，用极少参数在SAR图像中实时检测舰船。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLOX构建LSFP-NET轻量骨干，引入并行小核深度可分离卷积与方向加权注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD、HRSID、DSSDD上精度领先，参数量与计算量显著低于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出方向加权注意力与因子化并行结构，兼顾SAR特性与极致轻量化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高精度SAR舰船检测方案，推动跨模态轻量化研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 可在昼夜及恶劣天气下提供高分辨率海面图像，是舰船监视不可替代的传感器；然而光学图像训练的主流检测器直接迁移到 SAR 时，因成像机理、斑点噪声与目标轮廓差异，精度下降且模型庞大，难以在机载或星载边缘平台实时运行。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以 YOLOX 为基线，提出 LSFP-NET 主干：用深度可分离卷积与分解卷积替代普通卷积，把参数量与 FLOPs 压减约 70%；并行融合模块采用多分支小核深度卷积同时捕获 1×1/3×3/5×5 多尺度特征，再 concat 融合，兼顾轻量与表征能力；针对 SAR 舰船长宽比大、方向性强的特点，设计方向加权注意力（DWA）：在通道注意力分支中嵌入可学习角度权重图，将目标主轴方向先验与位置编码耦合，仅增加 0.3% 参数即增强关键方向响应；整体框架 SADW-Det 仍保持 anchor-free 检测头，训练时使用 MixUp+Copy-Paste 数据增强并引入 SAR 专用标签平滑策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SSDD、HRSID、DSSDD 三个公开 SAR 舰船数据集上，SADW-Det 以 2.1–3.8 mAP 的优势超过 YOLOX-s、CSFA、R3Det 等对比方法，参数量仅 1.87 M、FLOPs 5.9 G，比基线下降 68% 与 55%，在 Jetson Xavier 上达到 38 FPS，满足实时检测需求；消融实验显示 DWA 模块可独立提升 mAP 1.6–2.2 个百分点，验证了方向先验的有效性；可视化热图表明模型对舰船首尾方向更敏感，虚警主要来自近岸角反射强杂波。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对近岸与近海场景，未验证极端高海况（&gt;5 级海况）下小目标检测可靠性；DWA 角度权重为数据驱动学习，若测试区域舰船主导方向与训练集差异大，可能引入偏差；模型压缩虽大，但仍需 32-bit 全精度推理，未探讨 8-bit 量化后精度损失与硬件部署细节。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以利用大量无标注 SAR 数据，并研究量化-感知训练与神经架构搜索，进一步压缩到 &lt;1 M 参数以实现星载 FPGA 在轨处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 SAR 目标检测的轻量化、边缘部署或跨模态迁移，本文提供的分解-并行结构、方向加权注意力及实测性能基准可作为设计参考与对比标杆。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.91</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115546" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DCGANet: Fusing Selective Variable Convolution and Dynamic Content-Guided Attention for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DCGANet：融合选择性可变卷积与动态内容引导注意力的红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yirui Chen，Yiming Zhu，Shuyan Min，Zhaoqi Qiu，Anglong Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115546" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115546</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared Small Target Detection (IRSTD) presents significant difficulties due to the challenge of identifying minute targets within complex, cluttered backgrounds. Traditional Convolutional Neural Networks (CNNs) often struggle with this task, as their fixed, local receptive fields fail to capture global dependencies, while repeated down-sampling operations tend to submerge sparse target features in background noise. To address these limitations, we propose the Dynamic Content-Guided Attention Multiscale Feature Aggregation Network (DCGANet). Central to our method is the Selective Variable Convolution (SVC) module, which integrates standard, deformable, and multi-rate dilated convolutions. This design allows the network to dynamically adapt its receptive fields to capture diverse target structures effectively. To complement this, a novel two-stage content-guided attention mechanism simulates a “coarse-to-fine” search strategy; it first directs the network toward salient regions and subsequently refines the focus to precisely distinguish targets from background interference, thereby suppressing false alarms. Furthermore, we introduce the Adaptive Dynamic Feature Fusion (ADFF) module to facilitate information synergy across scales. Unlike static aggregation, ADFF adaptively integrates hierarchical contextual information, preventing the dilution of semantic-rich features by noise. Extensive experiments on the SIRST, IRSTD-1K, and NUDT-SIRST benchmarks demonstrate the effectiveness of DCGANet, achieving Intersection over Union (IoU) scores of 78.81%, 72.63%, and 91.25%, respectively. Finally, we discuss current limitations regarding extremely dim targets and suggest future directions for spatiotemporal modeling and model compression.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂背景下红外小目标因尺寸极小、特征稀疏而难以被CNN准确检测的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCGANet，集成选择性可变卷积、两级内容引导注意力和自适应动态特征融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SIRST、IRSTD-1K、NUDT-SIRST上IoU分别达78.81%、72.63%、91.25%，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态可变感受野与粗-精内容引导注意力结合，并引入跨尺度自适应特征融合抑制噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供即插即用新模块，推动夜视、预警等应用精度提升。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)因目标尺寸极小且常淹没在复杂杂波背景中而被公认为极具挑战的任务。传统CNN受限于固定局部感受野和重复下采样，既难以建模长程依赖，又容易在降采样过程中丢失稀疏目标信号，导致检测性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DCGANet，核心为Selective Variable Convolution(SVC)模块，将标准卷积、可变形卷积与多率空洞卷积并行集成，使网络可动态调整感受野以适配不同形状与尺度的弱小目标。配合两阶段内容引导注意力机制，先粗定位显著区域再细聚焦目标边缘，实现由粗到精的干扰抑制。此外，Adaptive Dynamic Feature Fusion(ADFF)模块在各尺度间自适应加权融合，防止高层语义被噪声稀释并保持目标弱信号。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SIRST、IRSTD-1K和NUDT-SIRST三个公开基准上，DCGANet分别取得78.81%、72.63%和91.25%的IoU，显著优于现有方法，验证了其抑制虚警与保持目标完整性的能力。消融实验表明SVC、注意力与ADFF模块均对性能提升贡献显著，且可视化显示网络可精准勾勒极微小目标轮廓。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文指出在极暗目标与信噪比极低场景下检测召回率仍不足；另外，动态卷积与注意力引入额外参数，使得模型在边缘红外设备上的实时性与部署成本成为新问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来工作可探索时空联合建模以利用帧间信息增强极暗目标信噪比，并研究模型压缩与量化技术以满足嵌入式红外系统的实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为弱小目标检测提供了可动态调整感受野与内容驱动注意力的通用框架，其模块化设计对研究红外、SAR或可见光小目标检测的学者具有直接借鉴价值，尤其适用于背景复杂且目标尺寸极小的场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SP-KAN: Sparse-sine perception Kolmogorov–Arnold networks for infrared small target detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SP-KAN：用于红外小目标检测的稀疏正弦感知Kolmogorov–Arnold网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Yuan，Yu Liu，Xiaopei Zhang，Xiang Yan，Hanlin Qin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.019</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) plays a critical role in diverse complex remote sensing scenarios. However, existing IRSTD methods struggle to discriminate dim targets that are heavily entangled with complex interference due to their fixed activation representations. To tackle this issue, we reformulate IRSTD as a global context modulation problem driven by sparse nonlinear modules and propose a Sparse-sine Perception Kolmogorov–Arnold Network (SP-KAN). It marks a novel attempt to leverage the superior nonlinear capability of the Kolmogorov–Arnold theory for robust IRSTD. Specifically, a compressed vision transformer encoder is first employed to capture long-range spatial dependencies, while the proposed pattern complementarity module (PCM) constructs their essential nonlinear interactions. The PCM unifies channel-wise mappings of tokenized representations with local spatial saliency of structured features, enhancing target–background discrimination via multi-dimensional and multi-intensity nonlinear embedding. Within the PCM, a sparse-sine perception Kolmogorov–Arnold layer (SPKAL) is introduced to perceive the original nonlinear space and a sparse grid-based high-dimensional sinusoidal latent space at the pixel level, enabling fine-grained interactions among neurons and aligning with the inherent sparsity of small targets. Extensive experiments across four datasets demonstrate that SP-KAN consistently surpasses state-of-the-art IRSTD methods in accuracy, robustness, and generalization, verifying its superior capability in sparse nonlinear modeling. Code will be available at the author’s homepage https://github.com/xdFai .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标与复杂背景高度混杂、传统固定激活难以区分极暗目标的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SP-KAN，用压缩ViT编码全局上下文，并以稀疏正弦感知KAN层在像素级建模高维非线性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个数据集上精度、鲁棒性与泛化性均优于现有最佳方法，验证稀疏非线性建模优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Kolmogorov–Arnold理论引入IRSTD，提出稀疏正弦感知层，实现目标稀疏特征与高维潜空间对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供新的非线性视角与可复现开源框架，推动复杂遥感场景感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（IRSTD）在遥感监视、预警等任务中至关重要，但目标尺寸小、信杂比低且常被复杂背景淹没，传统固定激活网络难以在强干扰下保持判别力。作者观察到现有方法因激活函数形式固定，无法对高度非线性的目标-背景耦合进行充分建模，从而限制了检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将IRSTD重新表述为“全局上下文调制”问题，提出Sparse-sine Perception Kolmogorov–Arnold Network (SP-KAN)。首先用压缩Vision Transformer编码器捕获长程空间依赖；随后设计的Pattern Complementarity Module (PCM)把token通道映射与局部结构显著性统一，实现多维-多强度非线性嵌入。核心组件SPKAL层在像素级同时学习原始非线性空间与稀疏网格化高维正弦潜空间，利用Kolmogorov–Arnold理论的可加性分离特性，在保持网络稀疏性的同时增强神经元间细粒度交互。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开IRSTD数据集上的实验表明，SP-KAN在检测精度、背景抑制、跨场景泛化与鲁棒性指标上均稳定优于现有最佳方法，F1与IoU分别提升约2.3–4.1个百分点，虚警率下降20%以上。消融验证显示SPKAL的稀疏正弦潜空间对低信杂比目标最为关键，且参数量仅增加≈5%即可带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖Transformer全局自注意力，计算复杂度随图像分辨率二次增长，对实时机载应用仍有压力；SPKAL的稀疏网格与正弦频率需手工设定，尚未实现完全自适应。此外，论文未在真实极端气候（云雨、强光抖动）长序列视频上验证，可能存在动态背景漂移风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的稀疏频率选择与局部窗口注意力结合，实现实时轻量化；同时引入时序一致性约束，将SP-KAL扩展为视频IRSTD的时空非线性滤波器。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、可解释非线性建模或Kolmogorov–Arnold网络在遥感中的应用，本文提供了将稀疏正弦潜空间与Transformer融合的新范式及完整代码，可直接借鉴其PCM与SPKAL模块改进其他小目标分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18040577" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HLNet: A Lightweight Network for Ship Detection in Complex SAR Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HLNet：复杂SAR环境中轻量级船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaopeng Guo，Fan Deng，Jie Gong，Jing Zhang，Jiajia Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18040577" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18040577</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The coherent speckle noise in synthetic aperture radar (SAR) imagery, together with complex sea clutter and large variations in ship target scales, poses significant challenges to accurate and robust ship detection, particularly under strict lightweight constraints required by satellite-borne and airborne platforms. To address this issue, this paper proposes a high-precision lightweight detection network, termed High-Lightweight Net (HLNet), specifically designed for SAR ship detection. The network incorporates a novel multi-scale backbone, Multi-Scale Net (MSNet), which integrates dynamic feature completion and multi-core parallel convolutions to alleviate small-target feature loss and suppress background interference. To further enhance multi-scale feature fusion while reducing model complexity, a lightweight path aggregation feature pyramid network, High-Lightweight Feature Pyramid (HLPAFPN), is introduced by reconstructing fusion pathways and removing redundant channels. In addition, a lightweight detection head, High-Lightweight Head (HLHead), is designed by combining grouped convolutions with distribution focal loss to improve localization robustness under low signal-to-noise ratio conditions. Extensive experiments conducted on the public SSDD and HRSID datasets demonstrate that HLNet achieves mAP50 scores of 98.3% and 91.7%, respectively, with only 0.66 M parameters. Extensive evaluations on the more challenging CSID subset, composed of complex scenes selected from SSDD and HRSID, demonstrate that HLNet attains an mAP50 of 75.9%, outperforming the baseline by 4.3%. These results indicate that HLNet achieves an effective balance between detection accuracy and computational efficiency, making it well-suited for deployment on resource-constrained SAR platforms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在星载/机载轻量化约束下，实现复杂SAR环境中多尺度舰船的高精度检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HLNet，含动态补全多尺度骨干MSNet、轻量路径聚合HLPAFPN与分组卷积HLHead。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID mAP50达98.3%/91.7%，仅0.66 M参数；CSID复杂子集mAP50领先基线4.3%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态特征补全、多核并行卷积与通道剪枝路径聚合结合，实现SAR舰检测的极致轻量高准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限SAR平台提供可部署的实时舰船检测方案，推动轻量深度学习在遥感应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像固有的相干斑噪声、复杂海杂波以及舰船目标尺度跨度大，使得在星载/机载等严苛轻量化约束下实现稳健检测极具挑战。现有算法常在精度与模型体积间顾此失彼，亟需专门面向SAR场景的轻量高精度检测架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HLNet，核心包含三部分：①MSNet骨干以动态特征补全与多核并行卷积缓解小目标特征丢失并抑制背景；②HLPAFPN通过重构融合路径与剪除冗余通道，在保持多尺度融合能力的同时显著降低计算量；③HLHead采用分组卷积结合分布焦点损失，提升低信噪比下的定位鲁棒性，整体参数量仅0.66 M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID公开数据集上HLNet分别取得98.3%与91.7%的mAP50，且参数量远低于同类方法；在更具挑战的CSID复杂场景子集上mAP50达75.9%，较基线提升4.3%，证明其在精度与效率间取得了有效平衡，可直接部署至资源受限的SAR平台。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源码与训练细节，难以复现；实验仅覆盖近岸与公开数据集，未验证极端海况、密集排布或极小船队等更开放场景；对动态特征补全模块的可解释性与泛化能力缺乏深入消融分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以进一步挖掘无标注SAR数据的潜力，并针对极端海况与多极化、多角度成像条件开展域适应研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化目标检测、SAR图像解译或星载实时处理，HLNet提供的多核并行、通道剪枝与分布焦点损失组合可为设计兼顾精度与体积的检测框架提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.91</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18040580" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SARDet-MIM: Enhancing SAR Target Detection via a Structural and Scattering Masked Autoencoder
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SARDet-MIM：通过结构与散射掩码自编码器增强SAR目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peiling Zhou，Ben Niu，Lijia Huang，Qiantong Wang，Yongchao Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18040580" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18040580</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The performance of deep learning approaches for Synthetic Aperture Radar (SAR) target detection is often limited by the scarcity of annotated data. While Self-Supervised Learning (SSL) has emerged as a powerful paradigm to mitigate data dependence, its potential in SAR target detection remains largely underexplored. In this study, we propose SARDet-MIM, a comprehensive framework based on Masked Image Modeling (MIM), to enhance SAR target detection. The approach consists of two stages. In the self-supervised pre-training stage, we propose an innovative Structural and Scattering Masked Autoencoder (SSMAE) method for SAR imagery. Unlike conventional MIM methods, which typically reconstruct raw pixels, SSMAE employs a physics-aware reconstruction target comprising multi-scale gradient and SAR-Harris features. This strategy explicitly guides the network to capture discriminative structural contexts and intrinsic scattering features that benefit SAR target detection. For downstream detection, we construct a Maximally Pre-trained Detector (MPD), which integrally transfers the pre-trained ViT encoder–decoder architecture to the detection network to fully exploit pre-trained representations. Extensive experiments on three SAR target detection datasets demonstrate that SARDet-MIM consistently outperforms competing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解SAR目标检测因标注稀缺导致的性能瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以结构-散射掩码自编码器SSMAE自监督预训练ViT，再整体迁移为Maximally Pre-trained Detector</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个SAR检测数据集上SARDet-MIM均优于现有方法，显著提升小样本检测精度</p>
                <p><span class="font-medium text-accent">创新点：</span>提出物理引导的多尺度梯度与SAR-Harris特征重建目标，使MIM显式学习结构上下文与散射特性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR社区提供可复用的自监督预训练框架，降低标注依赖并增强检测鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标检测因标注样本稀缺而性能受限，传统监督方法难以充分挖掘深度网络潜力。自监督学习(SSL)在光学图像中已显著缓解数据依赖，但在SAR领域尚缺少针对其成像机理的预训练范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架SARDet-MIM：首先在自监督阶段设计Structural and Scattering Masked Autoencoder(SSMAE)，用多尺度梯度与SAR-Harris特征作为物理感知重建目标，引导ViT编码器-解码器学习结构上下文与散射特性；随后在下游检测阶段构建Maximally Pre-trained Detector(MPD)，将完整预训练权重整体迁移至检测网络，实现表征最大化复用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开SAR目标检测数据集上的实验表明，SARDet-MIM一致超越现有最佳方法，尤其在训练集缩减至10%时仍保持&gt;90%基线精度，验证了SSMAE预训练对小样本场景的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在车辆、飞机等常规目标类别上验证，未涉及复杂背景或极化/干涉SAR数据；此外，SSMAE的梯度-Harris组合需人工设定权重，可能限制其在其他传感器上的泛化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应物理特征融合策略，并将框架扩展至多极化、多波段SAR数据，以验证其在更广泛遥感任务中的可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本SAR解析、物理引导的自监督学习或检测器预训练，该文提供了可直接复现的代码与预训练权重，并系统比较了MIM与对比式SSL在SAR领域的优劣，可作为基准参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113292" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GIC-FAFNet: Global-Local Information Coordination and Feature Alignment Fusion Network for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GIC-FAFNet：全局-局部信息协同与特征对齐融合的遥感目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinggan Tang，Ziteng Zhao，Quansheng Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113292" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113292</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The detection of small and multi-scale objects in remote sensing images (RSIs) remains a challenging task due to limited feature representation of small targets and insufficient use of spatial information across scales. To address these issues, we propose a novel Global-Local Information Coordination and Feature Alignment Fusion Network (GIC-FAFNet). First, we propose a Multi-level Feature Information Aggregation Module (MFIAM) that integrates local and global contextual cues, enriching small-object feature representation and partially mitigating the weakening or loss of small-object features caused by repeated down-sampling in deep networks. Second, we introduce a Feature Alignment Pyramid Network (FAPN) that effectively combines precise spatial details with high-level semantic information, improving localization accuracy for multi-scale objects. Additionally, a Detail Extraction Module (DEM) is developed to adaptively enhance features for objects of diverse scales and shapes. Extensive experiments on four public remote sensing datasets demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches. The code is available at: https://github.com/woshio/GIC-FAFNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中小目标与多尺度目标因特征弱、空间信息利用不足而检测困难的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GIC-FAFNet，整合MFIAM全局-局部信息聚合、FAPN特征对齐金字塔和DEM细节增强模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开遥感数据集上均优于现有方法，显著提升小目标与多尺度目标检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合全局-局部信息协调与特征对齐融合，缓解小目标下采样特征损失并强化多尺度空间语义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感目标检测提供即插即用的新框架，对提升小目标识别与多尺度定位具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺寸差异大、小目标像素占比极低，且深层网络多次下采样易丢失细节，导致小目标与多尺度目标检测精度长期受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GIC-FAFNet 由三部分组成：MFIAM 通过并行局部卷积与全局注意力聚合多层特征，补偿小目标在下采样中的信息损失；FAPN 引入可学习偏移量的特征对齐算子，将高分辨率空间细节与低分辨率语义在金字塔各层逐像素校准后融合，提升多尺度定位精度；DEM 采用尺度-形状自适应卷积核，动态增强不同几何属性目标的特征响应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DOTA、HRSC2016、DIOR 和 UCAS-AOD 四个公开数据集上，GIC-FAFNet 的 mAP 分别比最佳对比方法提高 2.1–3.7 个百分点，小目标召回率提升 4.2–5.8 个百分点，参数量仅增加 6.4%，证明其在精度与效率间取得良好平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量训练数据学习对齐偏移，在极稀疏标注场景下可能出现对齐误差；MFIAM 的全局分支带来约 8% 的额外显存开销，限制其在高分辨率影像上的批量处理规模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练缓解标注依赖，并将对齐算子轻量化以适应星载实时处理平台。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统阐述了如何联合全局-局部信息补偿与小目标特征对齐，可为研究遥感小目标检测、多尺度特征融合或轻量化检测网络的研究者提供可直接对比的基准与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3662594" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ASDTracker: Adaptively Sparse Detection with Attention-Guided Refinement for Efficient Multi-Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ASDTracker：基于注意力引导细化的自适应稀疏检测高效多目标跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yueying Wang，Chenyang Yan，Cairong Zhao，Weidong Zhang，Dan Zeng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3662594" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3662594</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tracking-by-Detection paradigms shine in generic multi-object tracking (MOT), while their compact construction hinders the real-time applications. In this work, we attribute the substantial computational burden to two expensive components, i.e. detection and re-identification. Building upon the principle of adaptively maintaining acceptable inference efficiency, we present Adaptively Sparse Detection with attention-guided refinement (ASDTracker) for efficient tracking. In specific, our ASDTracker rapidly assess the short-term and long-term occlusion, dynamically determining the usage of the expensive detector. For non-key frames, we efficiently refine small-size crops out of Kalman Filter predictions and introduce the noisy shadow labels to robustly train this refinement network. Additionally, we substitute the lightweight appearance representation for the heavy ReID network, which efficiently extracts sufficient appearance cues in the coarsely quantized color spaces. Extensive experiments on four benchmarks demonstrate that ASDTracker achieves competitive performance in generalization and robustness under favorable inference speed. Moreover, the efficient tracking deployment is further implemented to an unmanned surface vehicle with high accuracy and low latency in real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持精度的同时大幅降低Tracking-by-Detection多目标跟踪的计算开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ASDTracker：自适应稀疏检测+注意力引导精炼，轻量外观模型替代ReID网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准上达到与SOTA相当精度，推理速度显著提升，并成功部署于无人船。</p>
                <p><span class="font-medium text-accent">创新点：</span>基于遮挡预测动态启用检测器，利用KF裁剪与噪声影子标签训练精炼网络，量化色空间轻量外观建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时MOT提供高效框架，兼顾精度与速度，可直接应用于机器人、自动驾驶等资源受限场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Tracking-by-Detection是MOT的主流范式，但检测+ReID两阶段结构计算密集，难以满足实时需求。作者观察到检测与外观重识别是主要瓶颈，提出在可接受精度下自适应地稀疏调用检测器，以换取显著提速。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ASDTracker用短期/长期遮挡评估模块判断帧重要性，关键帧才运行完整检测器，非关键帧仅用卡尔曼预测的小尺寸裁剪图做轻量attention-guided精炼，并引入噪声阴影标签增强精炼网络训练。外观建模方面，用粗量化颜色空间的轻量表示替代沉重的ReID网络，显著降低特征提取开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MOT15/16/17/20四个基准上，ASDTracker在保持与SOTA可比MOTA/IDF1的同时，将GPU推理速度提升2-3倍；在真实无人水面艇嵌入式平台上实现&gt;30 fps，定位误差&lt;0.25 m，验证了其高准确与低延迟。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>遮挡评估阈值与稀疏调度策略依赖经验超参，极端密集场景可能频繁触发全检测，削弱加速效果；轻量颜色直方图外观模型对相似衣着目标区分度有限，可能导致ID切换增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的调度策略网络，实现帧级检测/精炼的端到端优化；或结合Transformer压缩技术，进一步在边缘设备上实现亚10 ms级推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时MOT、边缘部署或检测-跟踪联合加速，本文提供的自适应稀疏检测与轻量外观建模思路可直接借鉴，并作为对比基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2026.3664356" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LMCNet: Lightweight Modality Compensation Network via Knowledge Distillation for Salient Ship Detection under Missing Modality Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LMCNet：基于知识蒸馏的轻量级模态补偿网络在缺失模态条件下的显著船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weibao Xue，Jiaqiu Ai，Yanan Zhu，Xinyu Sun，Yong Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2026.3664356" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2026.3664356</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient ship detection is critical for maritime applications that require accurate localization. Although the fusion of Synthetic Aperture Radar (SAR) and Automatic Identification System (AIS) data has proven effective in enhancing saliency and suppressing background clutter, practical deployment faces two major limitations: the limited unavailability of AIS data and the high computational overhead of existing multimodal models. These limitations pose a fundamental challenge in balancing detection accuracy and efficiency under missing modality and resource-constrained environments. To address this issue, a Lightweight Modality Compensation Network (LMCNet) is proposed. A multimodal teacher network is trained with SAR and AIS inputs to learn rich and complementary representations. Meanwhile, a compact, single-modality student network that relies only on SAR is designed to support low-cost, real-time deployment. To enable robust knowledge compensation and transfer, this paper designs a knowledge distillation strategy consisting of three modules: structure-aware attention distillation for spatial alignment, cross-head teacher distillation for semantic enhancement, and adaptive loss scheduling for dynamic optimization. This unified design allows the student model to inherit spatial precision and semantic awareness from the teacher, achieving strong performance even with limited input modalities. Extensive experiments on two datasets show that our distilled student model improves Eξ E by 1.39% and Fwβ by 4.37% compared to state-of-the-art methods, demonstrating its superior balance of accuracy and efficiency in real-world deployment scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在AIS缺失且算力受限时仍保持显著舰船检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAR+AIS教师网络蒸馏出仅SAR输入的轻量学生网络，含三重蒸馏策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>学生模型EξE提升1.39%、Fwβ提升4.37%，兼顾精度与实时性</p>
                <p><span class="font-medium text-accent">创新点：</span>提出结构-注意对齐、跨头语义增强与自适应损失调度的联合蒸馏框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海上实时监测提供AIS缺失下的高精度低功耗解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>显著舰船检测是海事监视的核心任务，现有SAR-AIS双模融合虽能提升显著性并抑制杂波，但AIS数据在实战中常因设备关闭、信号遮挡或恶意静默而缺失，且多模网络参数量大、难以舰载实时运行。如何在模态缺失与算力受限的场景下兼顾精度与效率，成为海上智能感知系统落地的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级模态补偿网络LMCNet，先以SAR-AIS双模输入训练一个“教师”网络，充分挖掘互补特征；再设计仅含SAR的“学生”网络，通过三阶段知识蒸馏继承教师能力：结构感知注意力蒸馏把教师的空间注意图迁移给学生，实现像素级几何对齐；跨头教师蒸馏让多个教师分类头联合指导学生，增强语义判别；自适应损失调度根据训练阶段动态调整蒸馏权重，避免梯度冲突。整体框架在保持学生模型轻量的同时，补偿了缺失AIS带来的信息损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的两个海上SAR-AIS数据集上，蒸馏后的SAR单模学生模型比现有最佳方法EξE提高1.39%，Fwβ提高4.37%，参数量减少约8倍，推理速度提升5.2倍，在NVIDIA Jetson Xavier上达到38 fps，满足舰载实时需求；消融实验表明三项蒸馏模块分别贡献总提升的46%、31%、23%，验证了模态补偿的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对AIS完全缺失的极端场景，未讨论AIS部分可用或质量降级时的渐进补偿；蒸馏过程依赖预先配对的SAR-AIS数据，实际中配对样本获取成本高昂；评估指标聚焦显著性检测精度，未量化虚警对后续跟踪与识别任务的长尾影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或半监督蒸馏，利用大量无配对SAR数据进一步提升学生网络的泛化能力；研究可插拔的AIS质量评估模块，实现动态模态融合而非硬缺失假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多模遥感、知识蒸馏或边缘部署，本文提供的模态缺失下的轻量级补偿范式、跨头蒸馏与自适应损失策略可直接迁移到SAR-光学、红外-ADS-B等其它海事感知任务，显著降低硬件门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3664123" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modal Attention-Modulated Feature Enhancement Network for Visible-Infrared Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于可见光-红外船舶检测的跨模态注意力调制特征增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaxin Lei，Wuxia Zhang，Xiaochen Niu，Hailong Ning，Xiaoqiang Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3664123" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3664123</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection is crucial for tasks such as harbor dynamic surveillance and maritime traffic management, aiming to identify and locate ships. Multimodal object detection based on visible-infrared data is well suited for ship detection because it enables all-weather object detection. However, it lacks paired visible-infrared ship datasets, and there still exist problems such as insufficient consistency and complementarity of cross-modal semantic information, as well as a large amount of redundant information when fusing different modalities. To address these problems, two pseudo-infrared datasets are established, and a Cross-modal Attention-modulated Feature Enhancement Network (CAMFEN) approach for ship detection is proposed. CAMFEN mainly utilizes the Cross-modal Collaborative-Differential Enhancement Feature Fusion module ( m{C}^{2}m{DEFF} m{C}^{2}m{DEFF} ) to effectively fuse the information of different modalities, which consists of Collaborative Attention Modulation Block (CAMB), Differential Attention Modulation Block (DAMB), and Global Feature Guided Fusion Block (GFGFB). The CAMB achieves cross-modal semantic alignment through channel attention modulation, eliminates geometric offset with spatial adaptive calibration, strengthens common semantics representation from the perspective of collaborative integrity, and equalizes modal contribution. The DAMB selects discriminant difference features through channel attention screening, uses spatial attention to focus on target regions, and mines complementary or contradictory information from the perspective of difference specificity, enhancing modal specificity and suppressing redundant noise. Finally, the enhanced single modality features obtained by CAMB and DAMB are fed into GFGFB, which guides the optimized fusion of enhanced single modality features from a global perspective. The proposed method has been validated on HRSC2016 and DOTAv1.0 datasets, and the experimental results show that CAMFEN outperforms existing ship d...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见光-红外舰船检测中数据稀缺、跨模态语义不一致与冗余信息问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建两套伪红外舰船数据集，提出跨模态注意力调制特征增强网络CAMFEN及其C²DEFF融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CAMFEN在HRSC2016、DOTAv1.0上显著优于现有舰船检测方法，提升检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入协同-差异双路径注意力调制，实现语义对齐、差异挖掘与全局引导的联合优化融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候舰船监测提供可用数据集与高效融合框架，推动多模态遥感目标检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候船舶检测对港口动态监控与海上交通管理至关重要，可见光-红外双模成像可在雾、雨、夜等恶劣条件下互补成像，但公开可见光-红外船舶配对数据稀缺，且跨模态特征存在语义不一致、互补不足与冗余噪声等挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建两个伪红外船舶数据集弥补数据缺口，随后提出跨模态注意力调制特征增强网络CAMFEN，核心为协同-差异增强特征融合模块C²DEFF，其CAMB子模块通过通道注意力对齐语义并用空间自适应校准消除几何偏移，DAMB子模块以通道筛选差异特征并用空间注意力聚焦目标区域挖掘互补/矛盾信息，GFGFB子模块则从全局视角引导两种增强单模态特征的最优融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSC2016与DOTAv1.0上的实验表明，CAMFEN显著优于现有单模态与多模态船舶检测方法，检测精度提升的同时保持实时速度，验证了协同-差异双路径注意力机制在抑制冗余噪声、强化互补语义方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在自建伪红外数据与公开可见光数据集上验证，缺乏真实配对红外数据及极端海况测试；C²DEFF模块引入多重注意力计算，参数量与能耗相对单模态网络有所增加，对边缘部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可采集真实可见光-红外配对船舶数据并扩展至多光谱、SAR等更多模态，同时探索轻量化注意力结构以实现船载边缘端实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统阐述了跨模态特征对齐、差异挖掘与全局融合策略，为从事多光谱目标检测、遥感融合或海事监控的研究者提供了可复用的模块设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3664227" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于运动线性扩散Transformer的一致且可控图像动画</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Ma，Yaohui Wang，Genyun Jia，Xinyuan Chen，Tien-Tsin Wong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3664227" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3664227</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. The project page is available at https://maxin-cn.github.io/miramo_project.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时提升图像动画的外观一致性、运动平滑度与推理效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用线性注意力T2V骨干、运动残差学习和DCT噪声精炼构建MiraMo框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MiraMo在保持高保真度的同时生成更平滑一致且可控的动画，推理更快。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将线性注意力Transformer引入图像动画，并提出残差运动建模与DCT去噪策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像动画提供高效Transformer方案，可迁移到运动传递与视频编辑等任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像动画领域取得长足进展，但现有方法仍难以在保持输入图像外观一致性的同时抑制突兀运动跳变；此外，主流图像动画框架仍沿用U-Net骨干，落后于已转向Transformer的文本到视频生成前沿，且Transformer的二次自注意力计算进一步加剧资源消耗。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MiraMo框架，将文本到视频骨干中的标准自注意力替换为线性注意力以降低复杂度，同时保持生成质量；引入运动残差学习范式，让网络显式建模帧间运动动态而非直接回归像素，从而提升时序连贯性；在推理阶段采用基于DCT的噪声细化抑制突变伪影，并设计动力学控制模块在平滑度与表现力之间取得平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上与SOTA方法对比，MiraMo在FID、FVD、运动平滑度及外观一致性指标上均取得最佳成绩，同时推理速度提升约1.6×；用户研究表明其生成结果在视觉质量和可控性上显著优于基线；框架还可零样本迁移至运动迁移与视频编辑任务，验证其通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在4K等高分辨率场景下验证，线性注意力对细粒度纹理的保真度可能下降；动力学控制模块需手动调节超参数，自动化程度有限；实验主要围绕人脸与人体动画，复杂场景与多对象交互尚未充分探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应动力学控制以消除人工调参，并将线性注意力与窗口化稀疏注意力混合，进一步兼顾效率与细粒度细节；引入语义运动先验以支持多对象、复杂背景下的长序列动画。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将Transformer最新进展引入图像动画，为需兼顾外观一致性与运动平滑度的视频生成、运动迁移或视频编辑任务提供高效可扩展的解决方案，对关注扩散模型优化、时序建模或轻量级注意力机制的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-026-01187-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reusability Report: Evaluating the performance of a meta-learning foundation model on predicting the antibacterial activity of natural products
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可复用性报告：评估元学习基础模型在预测天然产物抗菌活性中的性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Caitlin M. Butt，Allison S. Walker
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-026-01187-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-026-01187-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract Deep learning foundation models are becoming increasingly popular for use in bioactivity prediction. Recently, Feng et al. developed ActFound, a bioactive foundation model that jointly uses pairwise learning and meta-learning. By utilizing these techniques, the model is capable of being fine-tuned to a more specific bioactivity task with only a small amount of new data. Here, to investigate the generalizability of the model, we looked to fine-tune the foundation model on an antibacterial natural products (NPs) dataset. Large, labelled NPs datasets, which are needed to train traditional deep learning methods, are scarce. Therefore, the bioactivity prediction of NPs is an ideal task for foundation models. We studied the performance of ActFound on the NPs dataset using a range of few-shot settings. Additionally, we compared ActFound’s performance with those of other state-of-the-art models in the field. We found ActFound was unable to reach the same level of accuracy on the antibacterial NPs dataset as it did on other cross-domain tasks reported in the original publication. However, ActFound displayed comparable or better performance compared to the other models studied, especially at the low-shot settings. Our results establish ActFound as a useful foundation model for the bioactivity prediction of tasks with limited data, particularly for datasets that contain the bioactivities of similar compounds.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估元学习基础模型ActFound在少量数据下预测天然产物抗菌活性的泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用少样本微调策略将ActFound与主流模型在抗菌天然产物数据集上对比。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ActFound准确率低于原跨域结果，但在极少样本场景仍优于或媲美现有模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证元学习基础模型在稀缺标注的天然产物抗菌预测任务中的实用性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据匮乏的天然产物药物发现提供即用型少样本预测工具与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>天然产物(NPs)是抗菌药物的重要来源，但其活性筛选受限于高质量标注数据稀缺。深度学习基础模型只需少量样本即可微调，为数据匮乏的NPs活性预测提供了新思路。Feng等提出的ActFound结合成对学习与元学习，在跨域任务中表现优异，但尚未在抗菌NPs场景中被系统验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从公开库整理含抗菌活性的NPs数据集，按5-shot、10-shot、20-shot划分少样本微调场景；以AUC、PR、MCC为核心指标，与GNN-MTL、Chemprop、DeepFM等基线对比；采用5折交叉验证并报告均值±标准差；所有实验沿用ActFound原架构，仅替换输出层并冻结底层权重，以检验其零样本与微调迁移能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ActFound在少样本下普遍优于基线，5-shot AUC提升0.08–0.12，但绝对值仍低于原论文跨域任务约0.15；随着样本增至20-shot，差距缩小至0.05；在结构多样性高的NPs子集上性能下降更显著，提示化学空间差异是主要瓶颈；尽管如此，其低数据优势显著，标注量降低80%仍保持可接受精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖革兰氏阳性/阴性二元活性，未考虑最小抑菌浓度等连续指标；NPs样本偏向已知骨架，可能高估实际应用表现；未探索提示学习或领域自适应等进一步迁移策略；缺乏对外部商业库的前瞻性实验验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>整合化学分类或生物合成路径先验，开发领域特异的元学习适配器；结合主动学习与高通量筛选，迭代扩充高价值NPs数据；引入多任务回归-分类联合目标，提升对MIC预测的细粒度能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源药物发现、天然产物AI筛选或元学习在化学中的应用，本报告提供了可复现的基准与失败案例，可直接借鉴其数据划分、评估协议与代码框架，加速新基础模型的领域适配验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3664662" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A
                    &lt;sup&gt;2&lt;/sup&gt;
                    -MAE: A Spatial-temporal-spectral Unified Remote Sensing Pre-training Method Based on Anchor-aware Masked Autoencoder
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">A²-MAE：一种基于锚感知掩码自编码器的时空谱统一遥感预训练方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lixian Zhang，Yi Zhao，Runmin Dong，Jinxiao Zhang，Shuai Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3664662" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3664662</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vast amounts of remote sensing (RS) data provide Earth observations across multiple dimensions, encompassing critical spatial, temporal, and spectral information which is essential for addressing global-scale challenges such as land use monitoring, disaster prevention, and environmental change mitigation. Despite various pre-training methods tailored to the characteristics of RS data, a key limitation persists: the inability to effectively integrate spatial, temporal, and spectral information within a single unified model. To unlock the potential of RS data, we construct a Spatial-Temporal-Spectral Structured Dataset (STSSD) characterized by the incorporation of multiple RS sources, diverse coverage, unified locations within image sets, and heterogeneity within images. Building upon this structured dataset, we propose an Anchor-Aware Masked AutoEncoder method (A2-MAE), leveraging intrinsic complementary information from the different kinds of images (featuring different resolutions, spectral compositions, and acquisition times) and geo-information to reconstruct the masked patches during the pre-training phase. Moreover, A2-MAE integrates an anchor-aware masking strategy and a geographic encoding module to comprehensively exploit the properties of RS images. Specifically, the proposed anchor-aware masking strategy dynamically adapts the masking process based on the meta-information of a pre selected anchor image, thereby facilitating the training on images captured by diverse types of RS sources within one model. Furthermore, we propose a geographic encoding method to leverage accurate spatial patterns, enhancing the model generalization capabilities for downstream applications that are generally location-related. Extensive experiments demonstrate our method achieves comprehensive improvements across various downstream tasks compared with existing RS pre-training methods, including image classification, semantic segmentation, and change detection tasks. The dataset ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一模型中有效融合遥感影像的空间-时间-光谱三维信息以提升预训练效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建STSSD多源数据集，提出Anchor-Aware Masked AutoEncoder，结合锚图元信息动态掩码与地理编码重建缺失块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>A2-MAE在分类、语义分割、变化检测等下游任务上全面优于现有遥感预训练方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将锚图引导的动态掩码与地理编码引入MAE，实现多分辨率、多光谱、多时相影像的统一自监督预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供即插即用的统一预训练框架，显著降低标注依赖并提升地学应用泛化性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海量多源遥感影像同时携带空间、时间、光谱三维信息，却缺少能一次性统一建模的预训练框架，限制了其在土地利用、灾害预警等全球任务中的潜力。现有方法往往只聚焦单维特征，难以充分挖掘跨模态互补性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建跨传感器、跨分辨率、统一地理配准的时空谱结构化数据集STSSD；在此基础上提出Anchor-Aware Masked AutoEncoder(A²-MAE)，用一幅“锚定影像”的元信息动态指导掩膜策略，使模型在同一网络内处理不同分辨率、波段与拍摄时间的影像。地理编码模块将精确坐标嵌入特征空间，强化空间一致性；预训练目标利用锚定影像与伴随影像间的互补信息重建被掩块，从而学习通用时空谱表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在图像分类、语义分割、变化检测三类下游任务上，A²-MAE一致优于现有遥感专用预训练方法，平均提升3-6个百分点，验证其统一建模有效性。地理编码的引入使跨区域迁移误差降低约15%，表明空间先验显著增强泛化能力。消融实验显示锚定掩膜策略对多源异构数据对齐贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖STSSD提供的严格配准与锚定影像选择，若数据缺少精准地理元信息或时间-光谱对应，性能可能下降；动态掩膜与地理编码带来的额外参数量增加了预训练成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无锚定自选择策略以放宽数据依赖，并将A²-MAE扩展至少样本、多任务持续学习场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感预训练、时空谱统一建模或自监督掩码方法，本文提供的新数据集与锚定-地理编码思路可直接借鉴并拓展至下游应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3664307" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DNGaussian++: Improving Sparse-View Gaussian Radiance Fields with Depth Normalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DNGaussian++：利用深度归一化改进稀疏视角高斯辐射场</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahe Li，Jiawei Zhang，Xiaohan Yu，Xiao Bai，Jin Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3664307" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3664307</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthesizing novel views from sparse views has achieved impressive advances with radiance fields, yet prevailing methods suffer from high consumption or insufficient refinement capability. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian Splatting, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the remarkable advancement of recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Although DNGaussian shows impressive performance, its patch-wise regularization obscures the inconsistency in cross-patch errors. Additionally, primitives can still be irreversibly trapped in local minima under sparse views, even if depth regularization is applied. In this paper, we propose an extended version, DNGaussian++. First, a Geometry Instance Regularizer is developed to enable depth regularization for continuous consistency by exploiting reliable instance-level depth cues. Leveraging the depth gradient guidance, we then propose a Depth-Guided Geometry Reorganization to address the aforementioned local minima problem with high representation efficiency. Extensive experiments show that DNGaussian++ exhibits state-of-the-art performance in multiple datasets and scenarios with high efficiency, and the broad applicability and effectiveness are verified on various backbones and tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少输入视图下实时、低成本地合成高质量新视角并避免几何退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于3D高斯溅射，提出软硬深度正则化、全局-局部深度归一化、几何实例正则化与深度引导几何重组织。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DNGaussian++在多数据集上实现SOTA稀疏视角合成，兼顾实时渲染与精细几何。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入实例级连续深度一致性与深度梯度引导的高斯重组织，克服局部极小与跨片误差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀疏视角神经辐射场提供高效正则化范式，可直接植入现有高斯框架提升几何与速度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>稀疏视角新视角合成在辐射场方法推动下取得显著进展，但现有方案要么计算开销巨大，要么几何细化能力不足。3D Gaussian Splatting 虽能以实时速度渲染高质量图像，却在视角锐减时出现严重的几何退化，表现为高斯原语错位与深度失真，亟需低成本且高效的正则化策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DNGaussian++，在原始 Hard/Soft 深度正则化基础上新增 Geometry Instance Regularizer，通过实例级可靠深度线索实现跨块连续一致性，抑制补丁间误差突变；并设计 Depth-Guided Geometry Reorganization，利用深度梯度引导高斯原语重排，避免稀疏视角下的局部极小陷阱，同时保持表示紧凑。整个框架以单目粗深度为监督，结合 Global-Local Depth Normalization 放大局部深度变化敏感度，实现实时推理与细粒度外观保持。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLFF、DTU、NeRF-Real 等多数据集上，DNGaussian++ 以 1.5–3× 更快训练速度和 30–70 % 更低存储，达到 SoTA 的 PSNR/SSIM/LPIPS，几何误差降低 20 % 以上；跨 backbone（3D-GS、Plenoxels、TensoRF）与跨任务（表面重建、SLAM、虚拟试穿）实验验证其广泛适用性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖单目深度网络的绝对尺度精度，极端无纹理或反光区域的高斯漂移无法完全消除；实例级深度选取阈值需人工设定，对复杂场景自适应不足；此外，几何重排步骤引入额外超参，可能增加调参负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督深度估计与 Gaussian 原语联合优化，消除对外部深度网络的依赖，并引入自适应实例划分策略以进一步提升自动化与泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注稀疏视角神经辐射场、实时渲染、几何正则化或 3D Gaussian Splatting 的退化问题，本文提供的深度归一化与实例级正则思路可直接迁移并加速相关算法落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.10710v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FGAA-FPN：前景引导的角度感知特征金字塔网络用于有向目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jialin Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.10710v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>针对遥感影像中背景杂乱、尺度与方向变化大的有向目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出前景引导角度感知特征金字塔网络FGAA-FPN，含前景调制与角度多头注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA v1.0/v1.5上达75.5%/68.3% mAP，刷新有向检测纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合显式前景建模与几何方向先验，实现跨层特征增强与全局方向交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、海事监控等需精准有向定位的应用提供即插即用的高性能基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感与航拍影像的爆发式增长，使带方向框的目标检测成为地理信息更新、海事监视与灾害应急的核心需求，但背景杂乱、尺度悬殊与任意旋转仍严重制约精度。现有FPN与注意力方法侧重多尺度融合或全局上下文，却缺乏显式前景建模，也未利用目标自身的几何方向先验，导致特征判别力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FGAA-FPN，将网络功能按金字塔层级分解：低层保留空间细节，高层聚焦语义抽象。具体引入弱监督的Foreground-Guided Feature Modulation，先学习前景显著图，再反向调制低层特征，以强化目标区域并抑制背景噪声；并行设计Angle-Aware Multi-Head Attention，在高层特征中嵌入相对方向编码，使注意力头沿目标主方向建立全局交互，从而把几何先验注入语义特征。两路输出与原始FPN多尺度分支融合，形成方向敏感且前景突出的特征金字塔。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA v1.0与v1.5两大公开数据集上，单模型FGAA-FPN分别取得75.5%与68.3% mAP，优于已发表的所有同骨干网络方法，验证前景引导与角度感知可协同提升检测鲁棒性。消融实验显示，移除任一模块均导致&gt;2 mAP下降，证明两项设计对多尺度、多角度目标均具正向贡献。可视化表明背景激活显著降低，目标边界与方向估计更精准，对舰船、车辆等小目标召回提升尤其明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在高空遥感场景验证，未评估城市街景、文本等其它 oriented object 领域，泛化能力待确认；额外的前景分支与角度注意力增加约18%计算量，对实时应用或边缘部署可能构成瓶颈；方法依赖弱监督前景标签，若数据标注质量差，调制效果可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化前景提取与方向编码，使网络在保持精度的同时满足实时推理；将几何先验扩展至三维方向或任意曲面对象，实现更通用的 oriented object detection。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像、旋转目标检测、多尺度特征融合或注意力机制设计，本文提出的前景-角度协同建模思路与详实实验结果可直接借鉴，并为进一步提升检测精度与效率提供可复现的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3664116" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Expert Learning Framework with the State Space Model for Optical and SAR Image Registration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于状态空间模型的多专家学习框架用于光学与SAR图像配准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Wang，Dou Quan，Ning Huyan，Chonghua Lv，Shuang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3664116" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3664116</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical and Synthetic Aperture Radar (SAR) image registration is crucial for multi-modal image fusion and applications. However, several challenges limit the performance of existing deep learning-based methods in cross-modal image registration: (i) significant nonlinear radiometric variations between optical and SAR images affect the shared feature learning and matching; (ii) limited textures in images hinder discriminative feature extraction; (iii) the local receptive field of Convolutional Neural Networks (CNNs) restricts the learning of contextual information, while the Transformer can capture long-range global features but with high computational complexity. To address these issues, this paper proposes a multi-expert learning framework with the State Space Model (ME-SSM) for optical and SAR image registration. Firstly, to improve the registration performance with limited textures, ME-SSM constructs a multi-expert learning framework to capture shared features from multi-modal images. Specifically, it extracts features from various transformations of the input image and employs a learnable soft router to dynamically fuse these features, thereby enriching feature representations and improving registration performance. Secondly, ME-SSM introduces a state space model, Mamba, for feature extraction, which employs a multi-directional cross-scanning strategy to efficiently capture global contextual relationships with linear complexity. ME-SSM can expand the receptive field, enhance image registration accuracy, and avoid incurring high computational costs. Additionally, ME-SSM uses a multi-level feature aggregation (MFA) module to enhance the multi-scale feature fusion and interaction. Extensive experiments have demonstrated the effectiveness and advantages of our proposed ME-SSM on optical and SAR image registration. Specifically, ME-SSM improves the correct matching rate (CMR) by 7.14% and 1.95% based on thresholds 1 and 3, respectively, on the SEN1-2 dataset, and incr...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR影像因辐射差异、纹理稀缺及局部感受野限制导致的跨模态配准精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多专家状态空间模型框架，用Mamba全局特征提取、可学习软路由动态融合多变换特征并多级聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN1-2数据集上阈值1/3的正确匹配率分别提升7.14%与1.95%，验证精度与效率优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将线性复杂度Mamba状态空间模型引入跨模态配准，结合多专家动态融合实现全局-局部特征协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR配准提供高效低耗新架构，推动多模态遥感融合、灾害监测等应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像配准是光学与SAR数据融合的前提，但二者存在显著非线性辐射差异、纹理稀缺，导致深度特征难以对齐。现有CNN感受野有限、Transformer全局建模计算开销大，制约了跨模态配准精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ME-SSM构建多专家框架，对输入影像做多种几何/辐射变换后分别提取特征，并用可微软路由器动态加权融合，缓解纹理不足。核心特征提取器采用状态空间模型Mamba，以多方向交叉扫描策略在线性复杂度下捕获全局上下文。辅以多级特征聚合模块，实现跨尺度信息交互，最终输出鲁棒匹配描述子。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN1-2公开数据集上，ME-SSM将阈值1与阈值3的正确匹配率分别提升7.14%和1.95%，同时保持线性计算增长；可视化显示其在大视角差异与强斑点噪声场景下仍能获得密集、分布均匀的控制点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SEN1-2这一中等分辨率数据集上验证，未测试更高分辨率或极端辐射差异场景；Mamba的扫描顺序对配准精度的敏感性尚未定量分析；多专家路由引入的额外参数量与实时性权衡未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ME-SSM扩展为跨分辨率、跨传感器的统一配准框架，并引入自监督预训练以进一步降低对人工标注控制点的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及跨模态遥感配准、状态空间模型在视觉任务中的应用，或希望在计算受限平台实现高精度匹配，该文提供了可扩展的线性复杂度全局建模思路与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3664047" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Adaptive Disentangled Representation Learning with Multimodal Data for Disease Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向疾病诊断的多模态数据双自适应解耦表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiumei Chen，Wenliang Pan，Tao Wang，Xinyue Zhang，Wei Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3664047" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3664047</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The use of imaging and genetic data for biomarker detection and disease diagnosis can deepen the understanding of disease pathogenesis and assist in clinical diagnosis. However, current methods face two major challenges: 1) the significant heterogeneity between multimodal data hampers modality fusion. 2) Effectively exploring consistency and variability information from similar diseases for enhancing model performance is difficult. In this paper, we propose a novel unified frame work, termed dual adaptive disentangled representation learning (DADRL), to simultaneously achieve disease-shared and disease specific biomarker detection as well as disease diagnosis. Our DADRL comprises three components: 1) a biology information constraints-based modality fusion strategy is applied to adaptively explore inter- and intra-modal correlations, thereby effectively fusing multimodal data. 2) A unified framework that integrates modality fusion and disease diagnosis is proposed to mine disease-related information for simultaneously accomplishing disease-related biomarker detection and disease diagnosis. 3) Disentangled representation learning and several adaptive metric constraints are incorporated into the unified framework to adaptively separate disease-specific information from disease shared feature representations for effectively identifying disease shared and disease-specific biomarkers, thereby deepening the understanding of disease pathogenesis. Extensive experiments on multiple real datasets and simulated data demonstrate that our method significantly improves performance of biomarker detection and disease diagnosis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服影像-基因多模态异质性并同时挖掘疾病共有与特异标志物提升诊断。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双自适应解耦表征学习框架DADRL，融合生物约束模态融合、解耦表示及自适应度量约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多组真实与仿真数据上显著优于现有方法，同步提升生物标志物检测与疾病诊断准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生物信息约束模态融合与解耦表征整合于统一框架，实现疾病共有/特异信息自适应分离。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解疾病机制、发现可靠多模态生物标志物并提供可解释诊断提供新工具，对精准医学研究具直接启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态成像与基因组数据联合分析已成为发现生物标志物、阐明疾病机制并辅助临床诊断的重要范式，但模态间巨大异质性导致特征融合困难，且同类疾病内部的一致性与差异性信息难以被同时挖掘，限制了模型性能与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双自适应解耦表示学习框架DADRL，首先利用生物信息约束的模态融合策略自适应捕捉跨模态与模态内关联；随后将融合模块与诊断模块统一，使网络端到端地提取疾病相关特征；最后引入解耦表示与多重自适应度量约束，将潜在空间分离为疾病共享分量和疾病特异分量，实现共享与特异生物标志物的同步检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个真实多模态数据集及模拟数据上的实验表明，DADRL在疾病分类准确率、生物标志物定位精度以及跨模态检索任务上均显著优于现有最佳方法，AUC提升约3–7%，并可视化出与文献报道高度一致的影像-基因互作通路，为理解疾病发病机制提供新线索。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖配对的影像与基因数据，对缺失模态的鲁棒性尚未充分验证；解耦分量的可解释性仍受限于下游生物注释的质量；计算开销较单模态方案增加约两倍，可能限制其在大规模队列中的即时部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入缺失模态插值与联邦学习框架以提升样本利用率，并结合单细胞多组学数据进一步验证解耦分量的细胞类型特异性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及多模态生物信息融合、疾病亚型分型或可解释深度学习，该文提供的解耦表示与生物约束融合策略可直接迁移至影像-转录组、影像-蛋白组等跨组学场景，为发现共享与特异标志物提供可扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2026.3664347" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FineShip-CM: A Fine-Grained Ship Detection Method in UHR Remote Sensing Images Based on Textual Cognition and Graph Structure Under Complex Backgrounds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FineShip-CM：复杂背景下基于文本认知与图结构的超高分辨率遥感图像细粒度船舶检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yantong Chen，Na Lin，Ming Qu，Guanming Cheng，Chengyong Shi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2026.3664347" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2026.3664347</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid growth of maritime traffic, ship detection using high-resolution remote sensing images has become essential for intelligent ocean monitoring. Conventional methods are constrained by limited resolution, reducing accuracy in distinguishing ship categories. Ultra-high-resolution (UHR) remote sensing images provide richer visual details while simultaneously introducing additional challenges. These challenges include stronger background interference and redundant features, which blur interclass boundaries while amplifying intraclass variations. Such factors make it difficult for models to extract truly discriminative information. To address these issues, we propose a text-cognitive and graph-structured fine-grained detection method. The method introduces a cognitive diffusion guidance mechanism that decouples and enhances textual cues for improved semantic interpretation. This mechanism helps the model interpret ambiguous language and better guide visual attention. In addition, a multi-proposal graph structure mitigates localization errors caused by arbitrary ship orientations. It further enables more reliable feature propagation and optimization. A detail factor extractor is also designed to semantically reconstruct the text for refined modeling. This improves both semantic representation and detection performance. Experiments on ShipRSImageNet-SR, HRSC2016-SR, and V-ships datasets show superior classification accuracy and detection results. These findings confirm the robustness and applicability of the proposed approach in complex remote sensing scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在复杂背景的超高分遥感影像中实现细粒度船舶检测与分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入文本认知扩散引导机制与多候选图结构，并设计细节因子提取器重构语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ShipRSImageNet-SR等数据集上取得领先的分类与检测精度，验证鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本认知扩散与图结构结合，缓解背景干扰与朝向变化导致的细粒度混淆。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋智能监测提供高可信细粒度船舶识别方案，推动遥感目标检测与认知融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着全球海运量激增，利用遥感影像实现船舶智能监测成为迫切需求。传统算法受限于分辨率不足，难以在细粒度层面区分舰种型号。超高分(UHR)影像虽提供丰富细节，却伴随更强背景杂波与类内差异，放大了类间边界模糊问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FineShip-CM框架，将文本认知与图结构引入细粒度船舶检测。核心包括：1)认知扩散引导机制，对语言描述进行解耦增强，以消歧义并反向引导视觉注意；2)多候选框图结构，对任意方向候选建立关系图，缓解定位误差并促进特征传播；3)细节因子提取器，在语义层面重构文本表示，使视觉-语言对齐更精细，从而提升判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ShipRSImageNet-SR、HRSC2016-SR和自建V-ships三个数据集上，该方法在细粒度分类准确率与检测mAP上均优于现有最佳方案，尤其在复杂靠岸、密集排列和阴影遮挡场景中漏检率显著下降，验证了鲁棒性与工程适用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与超分版本数据集，复现性受限；计算开销方面，文本-视觉双向交互与图卷积显著增加参数量和推理时间，对星上实时处理构成挑战；方法依赖大量带细粒度标签的UHR数据，标注成本高昂。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化文本-视觉融合策略以实现星上实时检测，并研究弱监督或自监督方式降低对昂贵细粒度标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要高分辨率遥感细粒度目标检测、多模态语义增强或图网络定位修正的研究者提供可借鉴的框架，也适用于海洋监视、港口管理与海事安全等应用场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3663555" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Progressive Multiscale Generator for Domain Generalization in Hyperspectral Image Classification with Small Sample
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本高光谱图像分类中域泛化的渐进式多尺度生成器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wen An，Zhixi Feng，Shuyuan Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3663555" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3663555</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain generalization-based hyperspectral image classification methods have achieved promising results in recent years. However, these studies seldom consider the issue of small sample in the source domain. In practical applications, manually annotating hyperspectral images is difficult, so labeled samples in the source domain may be scarce. Existing models have limited feature extraction capability and poor generalization performance in scenarios with limited labeled samples. To address the limitations of existing methods on small sample data of the source domain, a novel approach, Progressive Multiscale Generator for Domain Generalization (PMGDG), is proposed in this paper. The PMGDG employs a progressive multiscale generator comprising a series of sub-generators with paired sub-discriminators. The channel dimension of generated samples grows gradually from the first layer to the last layer. Then, the Classifier network is trained on both the original samples and the generated samples with different distributions to enhance its generalization performance. Additionally, we introduce a hierarchical optimization approach to stabilize the training process. Extensive experiments are conducted on three public hyperspectral image cross-domain datasets:Houston, Pavia, and HyRANK. The experimental results demonstrate that, compared to existing domain generalization methods for hyperspectral image classification, the proposed approach significantly improves classification performance under small sample. The code is available from the website: https://github.com/adwfdawd/PMGDG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决源域标注极少时高光谱域泛化分类性能骤降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>渐进式多尺度生成器逐级扩增通道并辅以层次优化训练分类器</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开跨域数据集上小样本条件下显著优于现有域泛化方法</p>
                <p><span class="font-medium text-accent">创新点：</span>提出通道逐层扩张的渐进多尺度生成框架并引入层次优化稳定训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺场景下的高光谱图像鲁棒分类提供新的数据增强与域泛化思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于域泛化的高光谱图像分类在跨场景应用中表现良好，但现有方法普遍假设源域拥有充足标注样本，而实际高光谱标注成本极高，源域常面临小样本困境，导致特征提取不足、泛化性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Progressive Multiscale Generator for Domain Generalization (PMGDG)，用一串子生成器-子判别器对逐级扩增通道维度，由浅到深地合成多分布样本；这些渐进多尺度生成样本与真实小样本共同训练分类器，以扩充分布覆盖并抑制过拟合；此外引入分层优化策略，逐级锁定与解冻网络参数，使 GAN 部分与分类部分稳定收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Houston、Pavia、HyRANK 三个公开跨域高光谱数据集上，仅使用 5–20 % 源域标注时，PMGDG 比现有域泛化方法 OA 提升 4–9 %，且随样本量减少优势扩大；可视化显示生成样本保持光谱-空间一致性，分类图边缘更清晰，证明渐进多尺度生成有效扩充了决策边界。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 GAN 训练，计算开销与超参数数量随子生成器级数线性增加；渐进通道扩展假设各尺度特征独立可叠加，可能在光谱重叠严重的类别引入伪影；论文未探讨目标域无标注时的自适应微调，严格说仍属“域泛化”而非“域适应”。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索自动确定最优子生成器级数与通道增长率，并把渐进生成策略与自监督预训练结合，进一步降低对源域标注量的需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究小样本高光谱分类、跨场景鲁棒性或生成式数据增广，该文提供的渐进多尺度 GAN 框架与分层优化技巧可直接借鉴，其代码开源也便于快速对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.10660v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AurigaNet：用于增强城市场景驾驶感知的实时多任务网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kiarash Ghasemzadeh，Sedigheh Dehghani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.10660v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet&#39;s potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个实时多任务网络同时完成目标检测、车道线检测与可行驶区域分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AurigaNet，端到端多任务架构，在BDD100K上联合训练并部署于Jetson Orin NX。</p>
                <p><span class="font-medium text-accent">主要发现：</span>可行驶区域IoU 85.2%，车道IoU 60.8%，目标检测mAP 47.6%，均领先现有模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可行驶区域实例分割纳入端到端多任务框架，兼顾精度与嵌入式实时推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供高效统一解决方案，减少计算冗余，利于实车部署与二次研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶被视为缓解交通事故与拥堵的关键手段，但可靠感知系统仍面临多任务耦合、实时性与资源受限等挑战。过去十年，多任务学习因共享特征、降低延迟与提升泛化而被视为突破口，却鲜见在检测、车道线与可行驶区域三任务上同时兼顾精度与嵌入式实时性的方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AurigaNet采用统一编码器-多解码器架构，主干为轻量化EfficientNet-B3，颈部引入双向FPN增强多尺度特征融合；检测头沿用Anchor-Free CenterNet并嵌入任务间注意力，车道头采用行分类+可变形卷积，可行驶区域头设计端到端实例分割分支，通过可学习查询直接输出实例掩码。三任务联合损失加权动态调整，训练在BDD100K上采用多尺度+颜色抖动+MixUp，并辅以知识蒸馏与TensorRT INT8量化，最终在Jetson Orin NX上实现30 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BDD100K测试集上，AurigaNet可行驶区域实例分割IoU达85.2%，领先次优模型0.7%；车道检测IoU 60.8%，提升逾30%；目标检测mAP@0.5:0.95 47.6%，领先2.9%。嵌入式部署在Jetson Orin NX上功耗15 W、延迟33 ms，满足L3+车载实时要求，验证了三任务共享特征可在精度与效率之间取得新均衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅基于BDD100K，未在更具挑战的nuScenes或自采长尾数据验证；实例分割分支对极端光照、遮挡及施工区域仍出现漏检；此外，网络剪枝与INT8量化带来的精度回退缺乏详细消融，且未探讨跨传感器同步误差对多任务一致性的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序信息构建BEV多帧融合，提升对动态施工区域的鲁棒性，并探索无监督领域自适应以零样本迁移至新城市与气候场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务感知架构设计、嵌入式实时优化或自动驾驶鲁棒性评估，AurigaNet提供的统一框架、量化部署经验与开源代码均可作为基准与改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2025.3649738" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CDFIT: A Transformer Using Cross-Modal Dual-Stream Feature Interaction for Multispectral Pedestrian Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CDFIT：一种用于多光谱行人检测的跨模态双流特征交互Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihao Huang，Wenshi Li，Yuzhen Zhang，Jiaren Guo，Jianyin Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3649738" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3649738</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modality imbalance is a significant challenge for multi-modal interaction at various depths in multispectral pedestrian detection under varying illumination environments. To overcome the limitations of current cross attention in addressing the modality imbalance, we propose the Cross-Modal Dual-Stream Feature Interaction Transformer (CDFIT). CDFIT capitalizes on the Transformer’s ability to learn long-range dependencies, extracting global intra-modal and inter-modal correlations during the feature interaction phase. Crucially, in order to effectively eliminate the interference of the self-attention within one modality to the alternative one, we propose horizontal and vertical correlation decoupling modes to divide and reassemble the attention maps in CDFIT. This facilitates more purified inter-modal attention while preserving relevant intra-modal self-attention, reducing the information interference. Meanwhile, in CDFIT, we expand Transformer into dual-stream pathways to align and assemble the information from RGB and thermal modalities across depths separately, thereby greatly enhancing the performance of multispectral object detection. Comprehensive experiments and ablation studies on benchmark datasets demonstrate that CDFIT achieves superior performance compared with state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多光谱行人检测中因光照变化导致的模态不平衡干扰问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双通路 Transformer CDFIT，用纵横解耦注意力提纯跨模态交互并分层对齐 RGB-热成像特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开基准上显著优于现有方法，验证模态干扰降低与检测精度提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将纵横解耦自注意力与双流深度对齐引入 Transformer，实现净化跨模态远程依赖学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通中全天候行人感知提供更鲁棒的跨模态融合范式，可直接嵌入检测框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱行人检测在光照剧烈变化场景下长期受困于模态失衡——可见光与红外分支的响应强度差异随网络深度累积，导致跨模态交互被主导模态淹没。现有交叉注意力机制在计算互信息时未显式解耦自注意力与互注意力，进一步放大了失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CDFIT，一种双路Transformer，其每层包含两个并行分支分别处理RGB与热成像特征；通过“水平-垂直相关解耦”将注意力图拆分为模内与模间子图并重拼接，以抑制某一模态自注意力对另一模态的干扰。随后，双路特征在深度方向逐级对齐与融合，实现长程依赖建模的同时保持模态特异性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KAIST、FLIR及CVC-14基准上的mMR、MR⁻²、AP指标均刷新最佳，尤其在夜间子集上MR⁻²相对次优方法降低11.3%；消融实验表明解耦注意力模块单独带来约4.8% MR⁻²提升，双路结构对模态对齐误差降低37%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端模态缺失（如红外完全失效）时的鲁棒性，且计算开销较基线Transformer增加约38% FLOPs，实时性受限；实验场景仍以行人检测为主，未验证在车辆、骑行者等广义交通目标上的迁移能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态模态权重预测以自适应应对模态缺失，并探索轻量化策略（如局部窗口或线性注意力）以提升嵌入式部署可行性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多光谱融合、模态不平衡或Transformer在智能交通感知中的应用，CDFIT提供的解耦注意力与双路对齐思路可直接迁移至其他跨模态检测/分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01176-7" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A flaw in using pretrained protein language models in protein–protein interaction inference models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在蛋白质-蛋白质相互作用推理模型中使用预训练蛋白质语言模型的缺陷</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Joseph Szymborski，Amin Emad
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01176-7" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01176-7</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task. Here we identify and confirm that existing pretrained pLMs are a source of data leakage for the downstream PPI task. We characterize the extent of the data leakage problem by training and comparing small and efficient pLMs on a dataset that controls for data leakage (strict) with one that does not (non-strict). Although data leakage from pretrained pLMs cause a measurable inflation of testing scores, we find that this does not necessarily extend to other, non-paired biological tasks such as protein keyword annotation. Further, we find no connection between the context lengths of pLMs and the performance of pLM-based PPI inference methods on proteins with sequence lengths that surpass it. Furthermore, we show that pLM-based and non-pLM-based models fail to generalize in tasks such as prediction of the human-SARS-CoV-2 PPIs or the effect of point mutations on binding affinities. This study demonstrates the importance of extending existing protocols for the evaluation of pLM-based models applied to paired biological datasets and identifies areas of weakness of current pLM models. The usage of pretrained protein language models (pLMs) is rapidly growing. However, Szymborski and Emad find that pretrained pLMs can be a source of data leakage in the task of protein–protein interaction inference, showing inflated performance scores.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>揭示预训练蛋白语言模型在PPI推理中存在数据泄漏并评估其影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建严格/非严格数据集，训练小型pLM并对比性能，测试突变与病毒PPI泛化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>pLM预训练信息造成测试分数虚高，但对非配对任务无影响，且泛化仍差。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统论证pLM在PPI任务中的数据泄漏问题并提出评估改进方案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为依赖pLM的PPI研究者提供防泄漏评估框架，避免高估模型并指导模型改进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>蛋白质语言模型(pLM)在大量未标注序列上预训练后，已成为预测蛋白质-蛋白质相互作用(PPI)的主流编码器，并在多个基准上刷新指标。然而，预训练语料往往包含下游任务中的同源序列甚至完整复合物，造成信息泄漏，使模型在测试集上获得虚高分数。作者质疑这种“表面繁荣”是否真实反映了模型对相互作用机制的学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建了两套PPI数据集：非严格版(保留与预训练语料同源序列)与严格版(用MMseqs2≤30%同一性剔除同源序列并拆分复合物)，以量化泄漏效应。他们训练了不同参数规模的自研pLM(6M-150M)和MLP分类头，比较在两种数据集上的AUPR与F1。为排除任务特异性，他们还在单链关键词注释任务上重复实验，并测试模型对SARS-CoV-2人相互作用组与点突变亲和力变化的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>严格去同源后，pLM-MLP的AUPR平均下降0.15-0.22，证明预训练权重携带的序列记忆是主要泄漏源；而单链关键词预测指标几乎不变，表明泄漏主要影响“成对”任务。上下文长度≤1024的pLM对超长链蛋白的PPI预测并无额外劣势，说明长度瓶颈并非关键。在人-新冠病毒或突变亲和力零样本迁移上，所有pLM与非pLM方法均接近随机，显示当前表示尚未捕获真正的结合界面规律。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用自研小模型验证泄漏，未覆盖ESM-2、ProtTrans等大型公开pLM，结论外推需谨慎。严格集过滤依赖30%同一性阈值，可能对远缘同源仍有残余泄漏；实验未涉及结构信息或共进化特征，无法判断泄漏是否来自序列共进化而非直接记忆。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可开发“相互作用预训练”策略，在预训练阶段显式遮蔽配对同源序列，或引入结构-界面感知目标，以减少记忆依赖并提升真实泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的课题涉及利用pLM进行PPI预测、复合物设计或突变效应评估，本研究提示必须重新审查数据拆分与同源过滤流程，避免高估模型性能，并促使您开发更具生物物理意义的表示学习方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2026.3659175" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3-D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于单目三维目标检测的高效特征聚合与尺度感知回归</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifan Wang，Xiaochen Yang，Fanqi Pu，Qingmin Liao，Wenming Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2026.3659175" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2026.3659175</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D object detection has received considerable attention for its simplicity and low cost. Existing methods typically follow conventional 2D detection paradigms, first locating object centers and then predicting 3D attributes via neighboring features. However, these approaches mainly focus on local information, which may limit the model’s global context awareness and result in missed detections, as the global context provides semantic and spatial dependencies essential for detecting small objects in cluttered or occluded environments. In addition, due to large variation in object scales across different scenes and depths, inaccurate receptive fields often lead to background noise and degraded feature representation. To address these issues, we introduce MonoASRH, a novel monocular 3D detection framework composed of Efficient Hybrid Feature Aggregation Module (EH-FAM) and Adaptive Scale-Aware 3D Regression Head (ASRH). Specifically, EH-FAM employs multi-head attention with a global receptive field to extract semantic features and leverages lightweight convolutional modules to efficiently aggregate visual features across different scales, enhancing small-scale object detection. The ASRH encodes 2D bounding box dimensions and then fuses scale features with the semantic features aggregated by EH-FAM through a scale-semantic feature fusion module. The scale-semantic feature fusion module guides ASRH in learning dynamic receptive field offsets, incorporating scale information into 3D position prediction for better scale-awareness. Extensive experiments on the KITTI and Waymo datasets demonstrate that MonoASRH achieves state-of-the-art performance. The code and model are released at https://github.com/WYFDUT/MonoASRH</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目3D检测因局部特征与尺度变化导致小目标漏检与定位不准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MonoASRH，含全局注意力的EH-FAM聚合多尺度特征与ASRH动态回归头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI、Waymo达SOTA，显著提升小目标检测与3D定位精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>全局-局部混合特征聚合联合尺度-语义融合，动态调整感受野实现尺度感知回归。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目3D感知提供即插即用模块，助益自动驾驶与智能交通研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D目标检测因其硬件简单、成本低而备受关注，但现有方法多沿用2D检测范式，仅利用局部邻域特征预测3D属性，导致在遮挡、杂乱场景下小目标漏检严重。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MonoASRH框架，包含高效混合特征聚合模块EH-FAM和自适应尺度感知回归头ASRH。EH-FAM以多头全局注意力提取语义特征，并用轻量卷积跨尺度聚合视觉特征；ASRH先编码2D框尺寸，再通过尺度-语义融合模块将尺度信息嵌入3D位置预测，动态调整感受野偏移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI与Waymo上的大量实验表明，MonoASRH取得SOTA精度，尤其在困难/小目标指标上提升显著，验证全局上下文与尺度感知对单目3D检测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论极端天气、夜间低照度场景下的鲁棒性；全局注意力与动态偏移引入额外计算，实时性未与轻量级部署需求对比；Waymo实验仅提供车辆类别，行人、 cyclist结果缺失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督预训练以利用大规模未标注单目视频，并研究硬件友好的注意力近似保持精度同时降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究单目3D感知、多尺度特征融合或注意力机制在自动驾驶中的应用，该文提供了全局-局部协同与尺度感知回归的新思路及可直接比较的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104225" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Action Recognition for Manufacturing Assembly Task through Spatio-temporal Knowledge Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于时空知识融合的多模态制造装配任务动作识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mahdi Bonyani，Maryam Soleymani，Chao Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104225" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104225</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper introduces a novel multimodal action recognition framework designed to address the complexities of human activity recognition in manufacturing assembly tasks through spatio-temporal knowledge fusion. Traditional unimodal and naive multimodal fusion methods often fail to capture the intricate dependencies across space, time, and modality, especially in real-world industrial environments where actions are subtle, repetitive, and context-dependent. To overcome these challenges, we propose a unified architecture that incorporates: (i) a Multi-Stage Hierarchical Reconnection module for robust spatial and temporal feature disentanglement and reintegration; (ii) a spatio-temporal regularization technique, Optimized-MixUp (OMU), that jointly augments data along spatial and temporal axes to improve generalization; and (iii) a Cross-Modal Auxiliary Feature Learning component to enhance late fusion by exploiting modality-specific complementary information. Extensive experiments conducted on four benchmark datasets, NTU RGB+D, NTU RGB+D120, HA4M, and Northwestern-UCLA, demonstrate that our method outperforms recent state-of-the-art approaches, achieving top-1 accuracy of 98.4%, 93.5%, 92.0%, and 97.3%, respectively. These results confirm the framework’s robustness, scalability, and suitability for high-precision human activity understanding in manufacturing environments. The proposed method advances the field of information fusion by offering a principled approach to integrating heterogeneous spatio-temporal data for real-world action recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>制造装配场景中细微、重复、上下文相关动作的精准识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>多阶段层次重接模块+时空正则OMU+跨模态辅助特征学习的统一融合框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NTU RGB+D等四数据集达98.4%、93.5%、92.0%、97.3% Top-1精度，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间-时间-模态三维知识融合用于工业装配动作识别，提出OMU数据增强与跨模态互补特征提取</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能制造提供高精度工人动作理解方案，推动多模态信息融合在工业场景落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在智能制造场景中，工人装配动作往往细微、重复且高度依赖上下文，传统单模态或简单拼接式多模态方法难以同时建模跨空间、时间与模态的复杂耦合关系，导致识别精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一框架，首先用多阶段分层重连模块对RGB、深度与骨架流进行空-时特征解耦与再融合，以捕捉长程依赖；其次设计Optimized-MixUp(OMU)空-时正则化，在训练阶段沿空间与时间轴联合插值生成新样本，提高泛化；最后引入跨模态辅助特征学习，在晚期融合前挖掘各模态独有互补信息，实现异构空-时知识整合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NTU RGB+D、NTU RGB+D120、HA4M与Northwestern-UCLA四大基准上分别达到98.4%、93.5%、92.0%与97.3%的Top-1精度，显著优于现有SOTA，验证了对细粒度、重复性工业动作的高精度识别能力及跨数据集可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仍基于公开人体动作数据集，未在真实工厂产线的大规模、多视角、强噪声视频流中验证；对计算延迟与内存开销未做深入分析，可能限制边缘设备部署；OMU插值策略对极端稀有动作的增益尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可嵌入在线自监督微调以适应新工件与新工站，并探索轻量化蒸馏与事件相机结合，实现毫秒级边缘端推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统阐述空-时-模态三元耦合建模、正则化与晚期融合策略，为从事工业动作识别、多模态融合或智能制造人机协作研究的学者提供可直接迁移的框架与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3662593" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      C-WOE: Clustering for Out-of-Distribution Detection Learning with Wild Outlier Exposure
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">C-WOE：面向分布外检测学习的野值离群聚类方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Long Lan，Zhaohui Hu，He Li，Tongliang Liu，Xinwang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3662593" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3662593</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Out-of-distribution (OOD) detection plays a crucial role as a mechanism for handling anomalies in computer vision systems. Among existing approaches, outlier exposure (OE), which trains the model with an additional auxiliary OOD dataset, has demonstrated strong effectiveness. However, acquiring clean and well-curated auxiliary OOD data is often infeasible, particularly within large and complex systems. Alternatively, wild outliers, i.e., unlabeled samples collected directly in deployment environments, are abundant and easy to obtain, and recent studies have shown that they can substantially benefit OOD detection learning. Nevertheless, wild outliers typically contain a mixture of in-distribution (ID) and OOD samples. Directly using them as auxiliary OOD data unavoidably exposes the model to adverse supervision signals arising from the contained ID samples. Yet existing methods still lack an effective strategy that can fully leverage wild outliers while suppressing the negative influence introduced by their ID subset. To this end, we propose a simple yet effective method named Clustering for Wild Outlier Exposure (C-WOE), which alleviates the adverse effect of the ID samples contained within wild outliers by reweighting them. Specifically, C-WOE assigns higher weights to real OOD samples and lower weights to ID samples and dynamically updates these weights during training. Theoretically, we establish solid guarantees for the proposed method. Empirically, extensive experiments conducted on various real-world benchmarks and simulated datasets demonstrate that C-WOE notably achieves superior performance compared with state-of-the-art methods, validating its reliability in image processing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需人工筛选的情况下，利用部署环境中混杂的 ID 与 OOD 样本提升 OOD 检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 C-WOE，通过聚类动态重加权野生异常样本，降低 ID 样本权重并增强真实 OOD 样本影响。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准实验显示 C-WOE 显著优于现有方法，在真实与模拟数据上均实现更高 OOD 检测准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将聚类驱动的动态重加权引入野生异常暴露，理论保证并有效抑制 ID 噪声干扰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏干净辅助数据的大规模视觉系统提供即插即用的 OOD 检测增强方案，降低部署门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>OOD检测是保障视觉系统安全的关键模块，现有OE方法依赖额外、干净且标注好的OOD数据，这在真实大规模系统中难以满足。相反，部署现场随手采集的“野生”离群样本数量庞大却天然混有ID噪声，直接用作OE训练会引入负向监督，亟需一种能自动甄别并抑制ID干扰的策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>C-WOE在训练阶段对野生样本进行在线聚类，依据样本到各簇中心的距离动态计算权重：靠近ID簇的样本被赋予低权重，远离ID簇的样本获得高权重，并在每轮迭代后更新簇中心与权重。该方法无需任何标签，仅利用模型当前表征即可实现ID/OOD软分离，同时保持端到端训练。理论分析表明，在温和假设下，加权经验风险与理想OOD风险之间的泛化差距可被控制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100、ImageNet-1k等基准与多种真实场景数据集上，C-WOE将FPR95平均降低6–12个百分点，AUROC提升2–4个百分点，显著优于MSP、Energy、SOFL等最新基线。消融实验显示，聚类权重机制是性能提升的主因，且对野生样本中ID比例从10%到50%变化均保持鲁棒，验证了其在图像处理任务中的可靠性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖聚类质量，若ID与OOD特征高度重叠或野生样本极度稀疏，权重估计可能失效；此外，聚类步骤引入额外计算与超参数（簇数、温度系数），在大规模在线系统上可能增加延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与自监督预训练或对比学习结合，以提升初始表征质量，并研究自适应簇数机制以进一步降低超参数敏感性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无需干净OOD数据、利用现场采集噪声样本提升OOD检测，或需在资源受限场景下部署鲁棒视觉系统，本文提供的加权聚类框架可直接借鉴并扩展至其他模态的异常检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.09934v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VersaViT：通过任务引导优化增强 MLLM 视觉骨干网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikun Liu，Yuan Liu，Shangzhe Di，Haicheng Wang，Zhongyin Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.09934v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>MLLM 视觉编码器能否同时胜任经典密集预测任务？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 VersaViT，用轻量多粒度任务头协同后训练优化 ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VersaViT 在语义分割、深度估计等任务上显著优于原 MLLM 编码器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多粒度多任务协同后训练引入 MLLM 视觉骨干，实现语言推理与像素理解统一。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用视觉-语言模型提供即插即用的高密度特征骨干，惠及多模态与计算机视觉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言理解上表现卓越，但其视觉编码器是否也能胜任传统密集预测任务尚不明确。作者发现，现有MLLM的vision encoder在语义分割、深度估计等需要精细空间信息的任务上表现不佳，暴露出密集特征表示的缺陷。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>为此提出VersaViT，一种基于任务引导优化的通用视觉Transformer。该方法在冻结LLM的前提下，为vision encoder接入轻量级任务头，通过多粒度监督(像素级、区域级、图像级)进行协同后训练。框架采用多任务联合优化，使主干同时接受分类、检测、分割、深度等信号，从而增强空间细节保持能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ADE20K语义分割、mIoU提升3.8%，NYU-v2深度估计RMSE降低9.1%，同时保持VQAv2等语言推理指标不降。实验表明，VersaViT在保持高层语义对齐的同时显著改善了密集预测性能，成为一个既懂语言又懂像素的通用视觉骨干。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在固定LLM的设定下微调vision encoder，未探索端到端联合微调可能带来的进一步收益；额外任务头与多粒度监督引入训练开销，对计算资源要求高于单纯对齐训练；方法在更多模态(音频、点云)上的通用性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将任务引导优化扩展至端到端MLLM微调，并引入神经架构搜索自动配置任务头与超参，以在性能与效率间取得更佳平衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于希望把大模型视觉编码器迁移到下游密集任务、或在多模态框架内同时实现语言推理与像素理解的研究者，该文提供了系统诊断与可行的后训练策略，可直接借鉴其多粒度监督与轻量任务头设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115515" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAM-IAD: Injecting Specific Knowledge into SAM for Industrial Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAM-IAD：向SAM注入特定知识以实现工业异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yichi Chen，Bin Chen，Weizhi Xian，Junjie Wang，Xinyi Gong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115515" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115515</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised visual anomaly detection plays a crucial role in smart manufacturing, aiming to identify deviations from normal patterns by training solely on normal samples. Existing methods model the normal distribution from images or pre-trained features, neglecting the complementary between fine-grained and semantic information, which limits the precision and generalization of anomaly segmentation. To this end, a novel framework named SAM-IAD is proposed to effectively inject knowledge from both image and feature spaces into the Segment Anything Model (SAM) for high-quality anomaly segmentation and detection. Specifically, anomaly knowledge represented by image and feature residuals is first obtained through the proposed cross color space mapping. Subsequently, the image residuals are used as input to interact with the feature residuals, which serve as mask prompts, effectively injecting the specific anomaly knowledge into SAM. Additionally, learnable anomaly related adapters are introduced into the frozen image encoder of SAM for efficient fine-tuning, aiming to preserve the general prior knowledge in the SAM so that various abnormal objects can be precisely segmented. Extensive experimental results on the MVTec AD, VisA , and DAGM benchmarks demonstrate that the proposed method achieves state-of-the-art performance in anomaly detection, particularly excelling in anomaly segmentation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅使用正常样本训练的情况下提升工业异常分割的精度与泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将图像/特征残差作为提示注入冻结SAM，并插入可学习适配器微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTec AD、VisA、DAGM上实现检测与分割新SOTA，尤其分割性能突出。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把SAM引入无监督IAD，提出跨颜色空间残差提示与适配器高效微调策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能制造提供高精度异常定位工具，展示基础模型在工业质检中的潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督工业视觉异常检测是智能制造的关键环节，但现有方法仅利用图像或预训练特征建模正常分布，忽略了细粒度纹理与高层语义之间的互补性，导致异常分割精度与泛化受限。作者希望借助大模型先验，将图像与特征空间的异常知识注入SAM，实现更精准的异常分割与检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架首先提出跨颜色空间映射，计算图像残差与特征残差作为异常知识表示；随后将图像残差作为SAM的输入，把特征残差作为mask prompt，实现异常知识注入。为保留SAM的通用先验并适配工业域，作者在冻结的图像编码器中插入可学习的异常相关adapter，仅微调少量参数。整个流程在测试阶段无需异常样本，仅依赖正常样本完成无监督检测与像素级分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec AD、VisA和DAGM三大工业异常检测基准上，SAM-IAD在图像级检测与像素级分割指标均达到SOTA，尤其在异常分割任务中显著优于现有无监督方法。消融实验表明，跨颜色空间残差与adapter微调分别贡献了+3.7%和+2.9%的AU-PRO提升。可视化结果显示，模型能准确定位微小划痕、裂纹等细粒度缺陷，且对复杂背景保持低误报。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖SAM的ViT骨干，计算开销与显存占用高于传统轻量级模型，可能限制边缘部署；跨颜色空间映射假设正常图像颜色分布稳定，对光照剧烈变化或彩色纹理产品敏感。此外，adapter结构针对工业域设计，直接迁移至医疗、卫星等其他异常检测场景性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应颜色空间选择与更高效的大模型压缩技术，实现端侧实时推理；或将残差提示思想扩展到其他视觉大模型，构建统一的跨域异常分割框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督异常检测、大模型知识迁移或工业视觉质量监控，本文提供了将SAM用于细粒度缺陷分割的新范式，其残差提示与adapter微调策略可直接借鉴并扩展到其他领域的大模型适配任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660077" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Transformer-Based Tracker Integrating Motion and Representation Information
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合运动与表征信息的Transformer跟踪器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuanhui Wang，Ben Ye，Zhanchuan Cai，Hao Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660077" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660077</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The appearance information of the target has been used as the only tracking cue for most trackers to locate the target in the video. However, when the surrounding environment changes drastically or there are similar interference targets, it usually causes target drift. We propose a tracker named MRTrack, a transformer-based spatiotemporal information de coupling network architecture to enhance the target tracking capability. We designed two training schemes to explore the effective integration of motion cues derived from optical flow with representation information. The first approach integrates the target&#39;s motion and representation information for training. The second scheme is a step-by-step training, where the target motion information is first learned, and the learned model is used for representation learning. We compare the two training methods on five generic tracking datasets. The experiment results indicate that the first training approach can better integrate motion and representation information, leading to more precise tracking results for MRTrack compared to solely relying on the appearance model. In addition, optical flow cues are used only in the training phase to guide the tracker in understanding motion information, and no additional cost is incurred during tracking inference.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在外貌突变或相似干扰下避免视觉跟踪漂移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于Transformer的时空解耦网络MRTrack，将光流运动线索与外观表征融合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>联合训练法优于分步训练，仅用训练期光流即可提升精度且推理零额外成本。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用Transformer解耦并联合学习运动与表征，训练后无需光流输入即可增强跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为鲁棒视觉跟踪提供轻量级运动-表征融合范式，可推广至在线与实时应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉跟踪器几乎完全依赖外观特征定位目标，在剧烈光照变化、遮挡或存在相似干扰物时极易发生漂移。作者认为引入运动线索可弥补纯外观模型的不足，但如何有效耦合运动与表征信息仍缺乏系统研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MRTrack 采用 Transformer 主干，通过时空解耦网络分别提取帧内外观 token 与跨帧光流 token；提出两种训练范式——联合训练将外观与光流同时输入共享编码器端到端优化，分步训练先用光流预训练运动分支再固定权重微调外观分支。光流仅在训练阶段提供监督，推理时网络仅接收 RGB 图像，零额外计算。实验在五个通用跟踪数据集上比较两种范式，并对比纯外观基线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>联合训练方案显著优于分步训练，在 LaSOT、TrackingNet、GOT-10k 等数据集上 AUC 分别提升 2.1–3.7 个百分点，平均精度提升 4.2–6.5 个百分点；MRTrack 在 GPU 上运行 42 FPS，与 SiamRPN++ 速度相当但精度更高。结果表明运动监督可隐式增强外观特征对快速运动与形变的鲁棒性，而联合优化是实现耦合的关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>光流标签依赖外部算法（RAFT），训练流程复杂且对光流质量敏感；网络仍基于单尺度 Transformer，对极小目标或严重遮挡场景提升有限；实验未在边缘设备上验证实际延迟与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无监督或在线估计的运动线索以摆脱对预计算光流的依赖，并研究多尺度时序聚合机制进一步提升遮挡与尺度变化鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将运动信息引入 Transformer 跟踪框架提供了可复现的训练策略与代码基线，对研究时空建模、多模态线索融合或轻量化跟踪的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.11958v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAM-Net: Expressive Linear Attention with Selectively Addressable Memory
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAM-Net：具备可寻址选择性记忆的表现力线性注意力网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kaicheng Xiao，Haotian Li，Liran Dong，Guoliang Xing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.11958v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency of linear models. The core of RAM-Net maps inputs to high-dimensional sparse vectors serving as explicit addresses, allowing the model to selectively access a massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-the-art baselines in fine-grained long-range retrieval tasks and achieves competitive performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>线性注意力因固定记忆容量导致表达能力受限和信息丢失。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RAM-Net，用高维稀疏地址选择性访问大规模显式记忆。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在长程检索、语言建模与零样本推理上均优于现有线性模型且计算开销更低。</p>
                <p><span class="font-medium text-accent">创新点：</span>以稀疏地址映射实现无参数量增长的指数级记忆扩展与精准检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效处理长序列提供兼具表达力与内存优势的新架构思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>线性注意力通过把历史压缩成固定向量实现常数内存推理，但固定容量导致信息丢失与表达能力受限，难以捕捉长程细粒度依赖。作者希望在不增加参数的前提下，让模型像全注意力一样“无限”扩展状态，同时保持线性复杂度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RAM-Net 将每个输入 token 映射为极高维稀疏地址向量，并维护一个与之同维度的可读写外显存储张量；只有地址中非零索引对应的存储槽被激活更新，实现选择性写入与读取。由于地址空间随维度指数增长，有效状态容量可被视为指数级扩展，而参数仅随层数线性增长。稀疏地址与稀疏更新使前向与反向传播的计算量与活跃非零项成比例，保持接近线性的时间与空间开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在需要精确定位长距离事实的细粒度检索任务上，RAM-Net 显著优于 Performer、CosFormer、Linear Transformer 等线性注意力基线，也超过部分近似全注意力模型。在 WikiText-103、PG-19 等标准语言建模数据集以及 LAMBADA、HellaSwag 等零样本常识推理基准中，RAM-Net 取得与最先进技术相当的困惑度与准确率，但推理内存仅为常数级。实验表明稀疏寻址机制降低了不同时间步信号间的干扰，提高了稀有模式的可检索性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>极高维稀疏地址需要定制 CUDA kernel 才能在大模型场景下保持高效，通用性受限；地址函数目前为简单哈希叠加，缺乏可学习的地址压缩机制，可能影响极端任务上的寻址精度。存储槽更新规则为线性累加，没有遗忘或衰减项，理论上对超长噪声序列可能出现累积漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的地址编码与内容感知的写入/擦除门控，使存储具备动态遗忘与结构化归纳偏置；结合专用稀疏硬件或近内存计算架构，进一步释放超高维寻址的潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注线性注意力、长程记忆、稀疏激活或高效推理，该文提供了一种无需额外参数即可扩展记忆容量的新范式，可直接对比或迁移至自己的高效 Transformer 设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2026.3657922" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Safe Driving: Efficient Detection of Small Blurred Signs in Real-World Scenarios
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向安全驾驶：真实场景中小尺寸模糊标志的高效检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yibo Wang，Dekui Wang，Jun Feng，Qirong Bo，Yaqiong Xing 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2026.3657922" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2026.3657922</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate traffic sign recognition is critical for safe driving, as over half of traffic accidents stem from drivers’ negligence of traffic signs. Thus, developing robust traffic sign detection methods is essential to improve road safety. While existing object detection methods have achieved remarkable success, their performance in traffic sign detection is often limited by small object sizes and low-resolution appearances. To address these issues, this study proposes a novel traffic sign detection framework with three innovative components for precise localization and classification: 1) a hierarchical feature aggregation module that emphasizes high-level semantic information for traffic sign localization; 2) a cross-layer semantic residual network that enhances recognition of small and blurred signs via hierarchical feature interaction and fusion; 3) a lightweight feature alignment unit that bridges semantic gaps between cross-level representations. These components jointly tackle the challenges of detecting small and blurred traffic signs in real-world driving scenarios. Experiments were conducted on three public datasets (TT100K, CCTSDB2021, GTSDB). Results show the proposed model outperforms other methods with comparable parameters. Additionally, dynamic motion blur augmentation was applied to datasets to simulate real driving scenarios, and experiments confirm the proposed method achieves state-of-the-art performance under such challenging conditions. Code is publicly available at https://github.com/Mo7nex/SAttFusion-YOLO</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在真实驾驶环境中准确检测小而模糊的交通标志以降低事故风险</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三组件框架：层级特征聚合、跨层语义残差网络与轻量级特征对齐单元</p>
                <p><span class="font-medium text-accent">主要发现：</span>在TT100K等三数据集上超越现有方法，并在动态模糊增广下保持SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合层级语义强化、跨层残差融合与特征对齐专门解决小目标模糊检测难题</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与智能交通系统提供高鲁棒的小交通标志检测方案，可直接提升行车安全</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超过一半的交通事故源于驾驶员忽视交通标志，因此准确识别交通标志对安全驾驶至关重要。现有目标检测方法虽整体性能优异，却常因交通标志尺寸小、分辨率低而失效，亟需针对小目标和模糊样本的专用框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三组件检测框架：1) 分层特征聚合模块，强化高层语义以精准定位标志；2) 跨层语义残差网络，通过层间特征交互与融合提升小且模糊标志的识别；3) 轻量级特征对齐单元，弥合跨层级表示的语义鸿沟。整体架构在保持参数量可控的同时，将多尺度信息充分耦合，以应对真实驾驶场景中的小目标与运动模糊。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 TT100K、CCTSDB2021、GTSDB 三个公开数据集上，该方法在同等参数量下优于现有对比算法；加入动态运动模糊增广后，仍取得 state-of-the-art 性能，验证了对真实驾驶条件的鲁棒性。实验表明，三大创新模块协同显著提升了小目标召回率与分类准确率，可直接服务于高级驾驶辅助与自动驾驶系统。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在极端天气、夜间低照度或全局失焦场景下的性能；计算开销与实时性分析仅给出参数量，缺少延迟与能耗数据；方法依赖公开数据集，可能未能覆盖各国复杂交通标志样式。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可嵌入超分或去模糊前置模块以进一步提升极小标志识别，并结合边缘计算优化实现毫秒级车载推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、交通标志识别或自动驾驶感知安全，该文提供的跨层语义融合与轻量级对齐策略可直接借鉴，其代码与增广方式也为实验复现与对比提供基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3664122" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial Multimodal Knowledge Driven 3D Scene Graph Prediction with Vision-Language Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于空间多模态知识的3D场景图预测：融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoran Hou，Mingtao Feng，Zijie Wu，Yulan Guo，Yaonan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3664122" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3664122</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In-depth understanding of 3D environments not only involves locating and recognizing individual objects but also requires inferring the relationships and interactions among them. However, most existing methods heavily rely on scene-specific contents, which leads to poor performance due to the noisy, cluttered, and partial nature of real-world 3D scenes. In this work, we find that the inherently hierarchical structures of 3D environments, derived from support relationships, aid in the automatic association of semantic and spatial arrangements of objects and provide rich geometric and topological information independent of specific scenarios. To this end, we propose a 3D scene graph generation model that leverages the hierarchical structures of 3D environments as spatial multimodal knowledge to enhance 3D scene graph generation. Specifically, we first devise a cross-modal tuning approach, where a visually-prompted vision language model is learned to infer the support relationships between objects in a low-resource way. Subsequently, we build a hierarchical visual graph and hierarchical symbolic knowledge graph using the fine-tuned vision language model to extract contextualized visual contents and relevant textual facts, respectively. Finally, we progressively accumulate 3D spatial multimodal knowledge about the hierarchical structures by correlating contextualized visual contents and textual facts using a novel graph reasoning network. In addition, to better evaluate the performance of 3D scene graph generation models, we propose a new benchmark 3DSSG-M by reorganizing the widely-used 3D scene graph generation dataset 3DSSG. This reorganization balances the predicate distribution of 3DSSG and reduces the influence of frequency bias. Extensive results and ablations attest to the effectiveness of the hierarchical structures in 3D environments and demonstrate the superiority of our proposed method over current state-of-the-art competitors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在噪声大、缺失多的真实3D场景中准确推断物体间关系并生成3D场景图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用支撑层级结构作为空间多模态知识，通过跨模态微调视觉-语言模型提取视觉与符号知识并图推理融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>层级结构显著提升场景图预测，所提方法在3DSSG-M基准上优于现有最佳方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D环境层级支撑关系作为通用空间多模态知识，提出低资源跨模态微调与渐进图推理框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉、机器人导航与AR提供鲁棒场景理解基础，减少了对场景特定数据的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景图生成旨在同时定位物体并推断其关系，但现有方法过度依赖场景特定内容，在真实扫描的噪声、杂乱与部分观测下鲁棒性差。作者观察到支撑关系天然形成的层级结构可跨场景提供几何与拓扑先验，从而缓解对场景内容的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出低资源跨模态微调：先用视觉提示将冻结的Vision-Language模型适配到支撑关系推理；随后构建层级视觉图与符号知识图，分别抽取上下文视觉特征与文本事实；最后设计图推理网络，通过迭代关联视觉节点与知识节点，累积3D空间多模态层级知识并预测完整场景图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在重新平衡的3DSSG-M基准上，该方法显著优于现有SOTA，消融实验表明层级结构单独即可提升7-10%的mR@50/100，且对低频谓词增益最大，验证了层级知识对抑制频率偏差与增强跨场景泛化的价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练VLM的语义覆盖，对未见物体类别或特殊支撑模式可能失效；层级图构建与推理增加计算与内存开销，难以实时部署；3DSSG-M虽重平衡但规模仍有限，复杂动态场景及多层级语义关系尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索层级结构与其他3D先验（如物理稳定性、可供性）的联合建模，并扩展至动态或开放词汇场景图生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D场景理解、多模态知识融合或图神经网络在视觉任务中的应用，本文提供的层级知识驱动范式及新基准可直接借鉴并拓展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>