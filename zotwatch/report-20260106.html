<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-06</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-06 10:51 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">949</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉领域，核心阅读集中在目标检测、视觉SLAM与模型压缩，同时对自监督/对比学习等前沿表征学习方法保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测方向收藏量最高（32篇）且持续追踪Kaiming He、Ross Girshick等顶级团队工作，对SAR合成孔径雷达遥感及旋转目标检测形成纵深积累，相关期刊IEEE TGARS与《雷达学报》收录达67篇。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨CVPR、NeurIPS等纯计算机视觉会议与IEEE地学遥感期刊，体现出将通用视觉算法（检测、ViT、模型压缩）迁移至SAR/遥感数据的应用导向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值94篇后回落，新增关键词“恒虚警率检测”“SAR目标检测”表明正聚焦遥感小目标检测与鲁棒判别；同时“大语言模型”“DeepSeek”被频繁标注，显示对视觉-语言基础模型及高效推理的兴趣上升。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可继续深入SAR图像与多模态基础模型结合的小样本检测、CFAR-free检测器设计，并关注面向星载实时处理的轻量化ViT与事件驱动遥感新数据源。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 925/925 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(10)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-06 10:31 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', '卫星导航'],
            datasets: [{
              data: [18, 32, 15, 15, 10, 12, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 94 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 4 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 171 }, { year: 2026, count: 4 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 66,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u89c6\u89c9Transformer\u67b6\u6784",
            size: 54,
            keywords: ["\u89c6\u89c9Transformer", "Vision Transformers", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 2,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 53,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 51,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u7279\u5f81\u53ef\u89c6\u5316", "VGG"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u8fc1\u79fb\u68c0\u6d4b",
            size: 49,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 5,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 48,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 6,
            label: "\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u4f18\u5316",
            size: 47,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 43,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d"]
          },
          
          {
            id: 8,
            label: "SAR\u98de\u673a\u6563\u5c04\u68c0\u6d4b",
            size: 41,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 9,
            label: "\u6df7\u5408\u4e13\u5bb6LLM",
            size: 40,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek"]
          },
          
          {
            id: 10,
            label: "\u7edf\u4e00\u56fe\u50cf\u5206\u5272",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 11,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 33,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 12,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u5de5\u7a0b",
            size: 32,
            keywords: ["\u7814\u7a76", "\u5927\u8bed\u8a00\u6a21\u578b", "LaTeX"]
          },
          
          {
            id: 13,
            label: "\u5143\u5f3a\u5316\u5b66\u4e60\u7406\u8bba",
            size: 32,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027"]
          },
          
          {
            id: 14,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 31,
            keywords: ["Transformers", "HRNet", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 15,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 29,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 16,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 17,
            label: "\u591a\u89c6\u89d2\u4e09\u7ef4\u611f\u77e5",
            size: 27,
            keywords: ["SIFT", "CMC", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 18,
            label: "\u96f7\u8fbe\u667a\u80fd\u6297\u5e72\u6270",
            size: 25,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 19,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668",
            size: 23,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 20,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u5b9a\u4f4d",
            size: 22,
            keywords: []
          },
          
          {
            id: 21,
            label: "BEV\u591a\u4f20\u611f\u5668\u878d\u5408",
            size: 22,
            keywords: ["\u7aef\u5230\u7aef\u7cfb\u7edf", "\u7edf\u4e00\u611f\u77e5\u6846\u67b6", "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212"]
          },
          
          {
            id: 22,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a",
            size: 20,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "Transformer"]
          },
          
          {
            id: 23,
            label: "SAR\u4fe1\u53f7\u6210\u50cf\u7b97\u6cd5",
            size: 19,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 24,
            label: "RL\u9a71\u52a8LLM\u63a8\u7406",
            size: 16,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 25,
            label: "\u9065\u611f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b",
            size: 15,
            keywords: ["\u9065\u611f\u57fa\u7840\u6a21\u578b", "\u591a\u6e90\u9065\u611f\u878d\u5408", "\u63a9\u7801\u81ea\u7f16\u7801\u5668"]
          },
          
          {
            id: 26,
            label: "\u7a7f\u5899\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 10,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 27,
            label: "\u8f7b\u91cfHRNet\u67b6\u6784",
            size: 6,
            keywords: ["HRNet", "\u57fa\u7840\u6a21\u578b", "\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1"]
          },
          
          {
            id: 28,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u5de5\u5177",
            size: 5,
            keywords: []
          },
          
          {
            id: 29,
            label: "\u591a\u6a21\u6001\u59ff\u6001\u4f30\u8ba1",
            size: 4,
            keywords: ["\u591a\u6a21\u6001"]
          }
          
        ];

        const links = [{"source": 18, "target": 26, "value": 0.8648177360191819}, {"source": 12, "target": 13, "value": 0.9127204215449478}, {"source": 12, "target": 19, "value": 0.8883773539441563}, {"source": 6, "target": 27, "value": 0.8396037453722185}, {"source": 23, "target": 28, "value": 0.9220923347257896}, {"source": 8, "target": 18, "value": 0.9185082982332228}, {"source": 2, "target": 5, "value": 0.9192550467775211}, {"source": 1, "target": 3, "value": 0.9278735986782675}, {"source": 1, "target": 9, "value": 0.9027140280200484}, {"source": 8, "target": 21, "value": 0.8975950005621608}, {"source": 2, "target": 8, "value": 0.9653270219432083}, {"source": 2, "target": 23, "value": 0.9229909504695616}, {"source": 6, "target": 11, "value": 0.8592041934203278}, {"source": 18, "target": 22, "value": 0.9146831215400524}, {"source": 3, "target": 6, "value": 0.9279690144892119}, {"source": 0, "target": 4, "value": 0.908483128678661}, {"source": 17, "target": 20, "value": 0.9001864936895522}, {"source": 0, "target": 16, "value": 0.868500624064827}, {"source": 3, "target": 27, "value": 0.8601056318552902}, {"source": 17, "target": 29, "value": 0.8677878059219876}, {"source": 8, "target": 26, "value": 0.8579549612941455}, {"source": 6, "target": 13, "value": 0.9168232784087713}, {"source": 16, "target": 21, "value": 0.8705136070377887}, {"source": 2, "target": 28, "value": 0.8451064986201962}, {"source": 2, "target": 25, "value": 0.9150819914882512}, {"source": 15, "target": 22, "value": 0.9315531909777088}, {"source": 6, "target": 19, "value": 0.8893593010786107}, {"source": 4, "target": 7, "value": 0.8878869681019254}, {"source": 3, "target": 11, "value": 0.8740095657715466}, {"source": 20, "target": 21, "value": 0.8714448005964216}, {"source": 4, "target": 10, "value": 0.8920268490185104}, {"source": 5, "target": 8, "value": 0.943679715530716}, {"source": 14, "target": 17, "value": 0.8913743679369884}, {"source": 1, "target": 4, "value": 0.9393838326744544}, {"source": 4, "target": 25, "value": 0.9051964521798561}, {"source": 14, "target": 29, "value": 0.8980648697347079}, {"source": 1, "target": 7, "value": 0.9034816894955424}, {"source": 0, "target": 15, "value": 0.9236958119173361}, {"source": 9, "target": 24, "value": 0.9248074950198238}, {"source": 1, "target": 10, "value": 0.9022403015396456}, {"source": 0, "target": 21, "value": 0.9078705881178318}, {"source": 13, "target": 24, "value": 0.8962205483088461}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR目标识别的论文、1篇关于雷达数据集的论文、1篇关于多源遥感匹配的论文以及1篇关于多模态融合的论文。</p>
            
            <p><strong class="text-accent">SAR目标识别</strong>：针对复杂场景下SAR图像舰船检测与增量目标识别难题，《A Feature-Enhanced Network-Based Target Detection Method for SAR Images of Ships in Complex Scenes》提出特征增强网络抑制背景干扰，《Physical Attributes Embedded Prototypical Network for Incremental SAR Automatic Target Recognition》引入物理属性原型网络实现新类别持续学习。</p>
            
            <p><strong class="text-accent">雷达数据集</strong>：为支持长程监视研究，《KuRALS: Ku-Band Radar Datasets for Multi-Scene Long-Range Surveillance with Baselines and Loss Design》发布Ku波段雷达多场景数据集并配套基线与损失设计。</p>
            
            <p><strong class="text-accent">多源遥感匹配</strong>：在噪声环境下实现多源遥感影像半稠密匹配，《MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments》以Mamba驱动自适应框架提升效率与鲁棒性。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：面向应急任务对可见光与红外互补信息的需求，《JFDet: Joint Fusion and Detection for Multimodal Remote Sensing Imagery》提出联合融合检测框架实现端到端优化。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态大模型与视觉语言模型的论文、6篇关于遥感与SAR图像处理的论文、5篇关于三维场景理解与自动驾驶感知的论文、4篇关于模型压缩与加速的论文、3篇关于文本检测与识别的论文、2篇关于扩散模型与逆渲染的论文以及2篇关于分布外检测与鲁棒性的论文。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：研究聚焦于提升多模态大语言模型（MLLMs）在高分辨率视觉理解、跨模态对齐与推理能力，如《Beyond LLaVA-HD》探索高分辨率视觉输入对MLLM感知与推理的关键作用；同时关注视觉语言Transformer（VLT）的高效压缩与加速，如《MADTP++》联合优化token与权重剪枝以缩小计算开销。</p>
            
            <p><strong class="text-text-secondary">遥感SAR处理</strong>：针对复杂场景下SAR舰船检测、SAR-光学跨模态翻译及多源遥感匹配难题，提出特征增强网络《A Feature-Enhanced Network-Based Target Detection Method》、语义分割驱动的《Target-Level SAR-to-Optical Image Translation》以及Mamba架构的《MARSNet》实现抗噪声半稠密匹配。</p>
            
            <p><strong class="text-text-secondary">三维场景理解</strong>：围绕自动驾驶中的3D语义占用预测与RGB-深度场景解析，研究利用相机输入进行几何-时序解耦建模的《Hierarchical Context Alignment》和充分挖掘Vision Transformer先验的《Fully Exploiting Vision Foundation Model&#39;s Profound Prior Knowledge》以提升跨域泛化性能。</p>
            
            <p><strong class="text-text-secondary">模型压缩加速</strong>：通过低秩近似、token剪枝与权重剪枝等手段降低大模型计算负担，《LRANet++》在端到端文本检测识别中引入低秩结构提升效率，《MADTP++》则联合剪枝策略显著压缩Vision-Language Transformer。</p>
            
            <p><strong class="text-text-secondary">文本检测识别</strong>：聚焦端到端场景文本 spotting 的精度与速度平衡，《LRANet++》以低秩近似网络统一检测与识别流程，实现高准确率的同时保持高效推理。</p>
            
            <p><strong class="text-text-secondary">扩散逆渲染</strong>：探索在未知光照条件下利用扩散模型先验进行材质恢复，《Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination》通过可微渲染优化扩散先验以提升逆渲染质量。</p>
            
            <p><strong class="text-text-secondary">分布外检测</strong>：针对实时目标检测中的分布外输入过自信问题，《Revisiting Out-of-Distribution Detection in Real-time Object Detection》提出新的评测基准与缓解范式，增强模型鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010178" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Feature-Enhanced Network-Based Target Detection Method for SAR Images of Ships in Complex Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向复杂场景SAR舰船图像的特征增强网络目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunsheng Ba，Nan Xia，Weijia Lu，Junqiao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010178" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010178</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the context of ship target detection with Synthetic Aperture Radar (SAR) images, misdetection and missed detection are often caused by complex background interference and the variability in target size. To address these challenges, this paper proposes an innovative method based on image enhancement and feature fusion to reduce background noise and effectively handle the detection confusion caused by differences in ship sizes. Firstly, a feature-aware enhancement network is introduced, which preserves and strengthens the edge information of the target objects. Secondly, during the feature extraction phase, a dynamic hierarchical extraction module is proposed, significantly improving the feature capture ability of convolutional neural networks and overcoming the limitations of traditional fixed kernel receptive fields. Finally, a feature fusion module based on attention gating is employed to fully leverage the complementary information between the original and enhanced images, achieving precise modeling and efficient fusion of inter-feature correlations. The proposed method is integrated with the YOLOv8 detection framework for target detection. Experimental results in the publicly available SSDD and HRSID datasets demonstrate detection accuracies of 97.9% and 93.2%, respectively, thus validating the superiority and robustness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>复杂场景SAR图像中因背景干扰与目标尺度差异导致的舰船误检与漏检。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建特征感知增强网络、动态分层提取模块与注意力门控融合模块，并嵌入YOLOv8框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID数据集检测准确率分别达97.9%与93.2%，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出保留边缘的增强网络、动态感受野提取及跨原-增强图注意力融合三项新技术。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂海况下高可靠舰船检测提供即插即用增强方案，可推广至其他小目标SAR识别任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>复杂背景杂波与目标尺度多变是SAR图像舰船检测中漏检和误检的主因，传统卷积网络固定感受野难以同时捕获弱小与大型舰船的差异化特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段增强-融合框架：①特征感知增强网络在空域强化舰船边缘并抑制背景；②动态分层提取模块通过可变形卷积与多尺度空洞卷积组合，自适应调整感受野以捕获0.5-50像素级目标；③基于注意力门控的融合模块将原始与增强特征图按通道-空间双权重耦合，再输入YOLOv8完成检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID公开数据集上分别取得97.9%与93.2%的mAP，相比基线YOLOv8提升3.1与4.7个百分点，尤其对&lt;16像素弱小舰船的召回率提高约8%，消融实验显示增强与融合模块各贡献1.8%与2.3%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量训练数据，在极稀疏标注场景下增强网络可能放大虚警；动态卷积引入额外参数量约17%，对星载实时处理芯片的功耗与内存提出更高要求；论文未评估不同雷达波段、极化方式及海况下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应以降低对新场景的标注依赖，并设计量化-剪枝联合策略在保持精度的同时实现星载端实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR小目标检测提供了可插拔的增强-融合范式，其动态感受野与注意力门控思想可直接迁移到遥感车辆、飞机检测或医学显微图像分析等细粒度目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010173" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      KuRALS: Ku-Band Radar Datasets for Multi-Scene Long-Range Surveillance with Baselines and Loss Design
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">KuRALS：用于多场景远程监视的Ku波段雷达数据集及其基线与损失设计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Teng Li，Qingmin Liao，Youcheng Zhang，Xinyan Zhang，Zongqing Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010173" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010173</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compared to cameras and LiDAR, radar provides superior robustness under adverse conditions, as well as extended sensing range and inherent velocity measurement, making it critical for surveillance applications. To advance research in deep learning-based radar perception technology, several radar datasets have been publicly released. However, most of these datasets are designed for autonomous driving applications, and existing radar surveillance datasets suffer from limited scene and target diversity. To address this gap, we introduce KuRALS, a range–Doppler (RD)-level radar surveillance dataset designed for learning-based long-range detection of moving targets. The dataset covers aerial (unmanned aerial vehicles), land (pedestrians and cars) and maritime (boats) scenarios. KuRALS is real-measured by two Kurz-under (Ku) band radars and contains two subsets (KuRALS-CW and KuRALS-PD). It consists of RD spectrograms with pixel-wise annotations of categories, velocity and range coordinates, and the azimuth and elevation angles are also provided. To benchmark performance, we develop a lightweight radar semantic segmentation (RSS) baseline model and further investigate various perception modules within this framework. In addition, we propose a novel interference-suppression loss function to enhance robustness against background interference. Extensive experimental results demonstrate that our proposed solution significantly outperforms existing approaches, with improvements of 10.0% in mIoU on the KuRALS-CW dataset and 9.4% on the KuRALS-PD dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏面向远距离监视、场景与目标多样的雷达学习数据集。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Ku波段RD级KuRALS数据集，设计轻量RSS基线与干扰抑制损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新损失使mIoU在KuRALS-CW/PD分别提升10.0%与9.4%，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个覆盖空-陆-海多场景、带像素级类别/速度/角度标注的Ku波段RD监视数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为恶劣条件下长距雷达感知研究提供基准数据与性能上限参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有公开雷达数据集多面向自动驾驶，场景与目标类型单一，难以支撑长距广域监视研究；而摄像头和激光雷达在雨雾暗夜等条件下性能骤降，Ku波段雷达却因全天候、远距及可直接测速等优势成为监视利器。为此，作者构建并发布面向学习的Ku波段多场景长距监视数据集KuRALS，以填补该领域数据空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>使用两部Ku波段雷达实采空中无人机、地面行人/车辆、海上船只三类场景，提供RD谱图及像素级类别、距离-速度坐标，并附方位角、俯仰角真值；数据集分KuRALS-CW与KuRALS-PD两子集。作者设计轻量级雷达语义分割(RSS)基线，嵌入多尺度特征提取与注意力模块，并提出抑制背景干扰的interference-suppression loss，联合交叉熵与加权距离-速度一致性约束进行端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，所提损失函数使基线模型在KuRALS-CW上mIoU提升10.0%，在KuRALS-PD上提升9.4%，显著优于现有雷达分割方法；消融验证显示干扰抑制损失对低信噪比目标召回率提升最为明显，证明其可增强远距小目标检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据仅覆盖Ku波段、两型雷达参数及有限地理环境与目标姿态，频段、极化与雷达体制多样性不足；未提供连续时序序列，难以支持跟踪或行为分析；像素级标注依赖人工校验，远距弱散射目标仍存在标签噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展至少波段、W波段等多频融合数据并增加时序标注，以支撑跨频段学习与多目标跟踪研究；结合自监督或半监督策略降低远距标注成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全天候感知、远距小目标检测或雷达深度学习方法，可利用KuRALS开展算法验证，并借鉴其干扰抑制损失设计提升模型在复杂背景下的鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MARSNet：一种Mamba驱动的自适应框架，用于噪声环境下的稳健多源遥感影像匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weipeng Jing，Peilun Kang，Donglin Di，Jian Wang，Yang Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决噪声环境下多源遥感图像半稠密匹配精度与鲁棒性不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MARSNet，融合Mamba长程建模、冻结DINOv2特征提取与自适应融合-线性注意力机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种噪声场景下匹配精度与鲁棒性显著优于现有方法，并提升三维重建精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba架构引入遥感匹配，结合DINOv2与自适应Mamba式线性注意力实现高效抗噪</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像匹配提供高效抗噪新框架，可推广至三维重建等下游任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像在噪声、视角差异和模态差异共同作用下，半稠密匹配精度骤降，传统无检测器方法在效率与鲁棒性上均显不足，亟需一种兼顾长程建模与抗噪能力的全新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MARSNet 以 Mamba 作为主干，通过线性递归扫描捕获全局依赖；冻结的 DINOv2 充当强噪声不变特征提取器，避免微调带来的过拟合；提出自适应融合模块动态加权多层级特征，并用类 Mamba 线性注意力重构 Transformer 自注意力，将序列复杂度降至 O(n) 同时保持远程上下文感知。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的含随机噪声与周期条带噪声多源遥感测试集上，MARSNet 相比 LoFTR、SGMNet 等 SOTA 方法将匹配召回率提升 6–12%，重投影误差降低 20% 以上；在 1 M+ 图像的通用三维重建 benchmark 中姿态误差下降 8%，验证了跨域泛化与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体训练数据规模与采集平台细节，对更大规模异源 SAR-光学影像的适用性尚待验证；Mamba 的递归结构对非常规长宽比影像仍需精心调参，且冻结 DINOv2 可能遗漏遥感特有频谱信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的模态特异编码器与 Mamba 联合微调，并探索针对 SAR 相位噪声与光学云层干扰的物理可解释正则化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感配准、抗噪特征匹配或高效长序列建模，本文提供的 Mamba-Transformer 混合范式与 DINOv2 冻结策略可直接迁移并加速新算法验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650513" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physical Attributes Embedded Prototypical Network for Incremental SAR Automatic Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">嵌入物理属性的原型网络用于增量SAR自动目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanjie Xu，Hao Sun，Chenfang Liu，Kefeng Ji，Gangyao Kuang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650513" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650513</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In Synthetic Aperture Radar (SAR) applications, the continuous emergence of new target classes poses a significant challenge to Automatic Target Recognition (ATR) systems. Adapting to the distribution of new data can induce drastic alterations in the feature space of deep models, resulting in a decline in their ability to recognize old data, termed catastrophic forgetting. To address this challenge, we propose a novel class-incremental SAR ATR method based on Physical Attributes Embedded Prototypical Network (PAEPN). PAEPN embeds physical attributes derived from electromagnetic scattering and geometric priors into the deep model to achieve stable representations. These physical attributes, determined by the target&#39;s shape, structure, and material composition, remain invariant throughout the incremental learning process, thereby enhancing the stability and interpretability of deep models. Specifically, PAEPN first extracts and integrates physical attribute priors to establish feature anchors, guiding the deep model in extracting physically consistent features and preventing drastic changes in the feature space. Second, a spatial attention enhancement strategy is introduced to enable the deep model to reliably focus on the key regions of SAR targets. Finally, feature relations that represent semantic similarity are distilled to further mitigate catastrophic forgetting. During testing, PAEPN employs the cosine distance between the sample feature and class prototypes for recognition. Comprehensive experiments on three datasets demonstrate that PAEPN outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR增量目标识别中的灾难性遗忘问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出物理属性嵌入原型网络，融合电磁散射与几何先验，并引入空间注意力与特征蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三套数据集上均优于现有方法，显著缓解遗忘并提升增量识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不变物理属性作为特征锚点嵌入增量SAR识别框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感增量学习提供可解释、物理可感知的特征稳定方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标自动识别系统在实际部署中必须不断接纳新增目标类别，但深度模型在学习新类别分布时，特征空间会发生剧烈漂移，导致对旧类别性能骤降，即灾难性遗忘。作者希望利用SAR图像中由目标形状、结构与材质决定的电磁散射先验，构建在增量过程中保持稳定的表征，从而缓解遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出物理属性嵌入原型网络(PAEPN)，首先基于电磁散射与几何先验提取可解释的物理属性，并将其作为特征锚点，引导卷积网络输出物理一致的特征；随后引入空间注意力增强模块，使网络聚焦目标关键散射区域；最后通过原型学习与特征关系蒸馏，将旧类别的类原型及样本间语义相似度保留在记忆库中，增量阶段用余弦距离分类并约束新特征逼近旧关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、OpenSAR-ATR和SAR-ACD三个基准数据集上的类增量实验显示，PAEPN在10+5、15+5等多种任务划分下，平均增量准确率比次优方法提升3.1–5.7%，遗忘率降低约40%，且可视化表明特征空间漂移显著减小；消融实验验证物理属性锚点与关系蒸馏各自贡献约2%和1.5%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>物理属性提取依赖简化的电磁散射模型与人工设计的几何先验，对复杂背景、部分遮挡或俯仰角大幅变化的目标可能失效；方法仍需要存储旧类别原型及部分特征，内存随类别线性增长，在类别数极大时扩展性受限；实验仅覆盖车辆目标，尚未验证对舰船、飞机等更复杂目标的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的物理模型或神经辐射场，实现数据驱动的物理属性自动挖掘，并结合压缩记忆或生成回放技术进一步降低存储需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR增量学习、小样本识别或想将物理知识嵌入深度网络以提升可解释性与稳定性的学者，该文提供了可直接复现的基准代码和消融结果，可作为物理-数据混合建模的参考范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010176" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      JFDet: Joint Fusion and Detection for Multimodal Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">JFDet：面向多模态遥感影像的联合融合与检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhao Xu，You Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010176" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010176</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing imagery, such as visible and infrared data, offers crucial complementary information that is vital for time-sensitive emergency applications like search and rescue or disaster monitoring, where robust detection under adverse conditions is essential. However, existing methods’ object detection performance is often suboptimal due to task-independent fusion and inherent modality inconsistency. To address this issue, we propose a joint fusion and detection approach for multimodal remote sensing imagery (JFDet). First, a gradient-enhanced residual module (GERM) is introduced to combine dense feature connections with gradient residual pathways, effectively enhancing structural representation and fine-grained texture details in fused images. For robust detection, we introduce a second-order channel attention (SOCA) mechanism and design a multi-scale contextual feature-encoding (MCFE) module to capture higher-order semantic dependencies, enrich multi-scale contextual information, and thereby improve the recognition of small and variably scaled objects. Furthermore, a dual-loss feedback strategy propagates detection loss to the fusion network, enabling adaptive synergy between low-level fusion and high-level detection. Experiments on the VEDAI and FLIR-ADAS datasets demonstrate that the proposed detection-driven fusion framework significantly improves both fusion quality and detection accuracy compared with state-of-the-art methods, highlighting its effectiveness and high potential for mission-critical multimodal remote sensing and time-sensitive application.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服模态不一致与任务无关融合，在恶劣条件下实现遥感可见光-红外鲁棒检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出JFDet框架：GERM融合、SOCA+MCFE检测、双损失反馈协同训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VEDAI与FLIR-ADAS实验显示融合质量与检测精度均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测损失反向驱动融合网络，实现低-高层自适应联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应急救援等多模态遥感任务提供高可信实时检测新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（可见光+红外）在搜救、灾害监测等应急任务中提供互补信息，但传统方法将融合与检测割裂处理，导致融合结果对下游检测并非最优，且在模态不一致、恶劣天气下检测鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出JFDet框架，把融合网络与检测头联合训练：①梯度增强残差模块(GERM)在融合支路引入密集连接与梯度残差路径，强化结构纹理；②检测支路嵌入二阶通道注意力(SOCA)和多尺度上下文特征编码(MCFE)，捕获高阶语义与小目标多尺度信息；③设计双损失反馈，将检测损失反向传至融合网络，使低层融合自适应服务于高层检测目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI和FLIR-ADAS两个公开数据集上，JFDet的融合图像在MI、QAB/F等指标上优于传统融合方法，同时检测mAP分别提升约3–5个百分点，证明检测驱动的融合可同时提高图像质量与目标识别精度，对时间关键应用具有直接价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个车辆/行人数据集验证，未覆盖更多传感器组合或极端灾害场景；联合训练增加GPU内存与计算，对星上或边缘实时部署带来挑战；方法对配准误差、模态缺失的鲁棒性尚未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至可见光-SAR、可见光-高光谱等更多模态，并引入轻量化策略与自监督配准，实现星上实时一体化处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、小目标检测或应急遥感应用，本文提供的联合优化范式与损失反馈思路可直接借鉴，并作为基线进行扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650761" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越LLaVA-HD：深入探索高分辨率多模态大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              YiFan Zhang，Qingsong Wen，Chaoyou Fu，Kun Wang，Xue Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650761" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650761</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不爆炸计算量的前提下，让多模态大语言模型真正利用高分辨率图像进行精细感知与推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>全局-局部双分支：混合适配器提取全局上下文，可学习查询压缩局部块并依问题相似度筛选关键token，交替训练两部分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>更少但更信息丰富的局部token即可提升性能，交替训练优于端到端，仅用200万数据就在多基准取得领先成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将混合适配器全局挖掘、查询式局部压缩与相似度选择结合，并提出交替训练策略及高细节需求数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高分辨率视觉-语言模型提供高效新范式，兼顾精度与成本，对提升MLLM实际部署与研究具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率视觉输入对多模态大语言模型(MLLM)的感知与推理至关重要，但现有LLaVA-HD类方法简单地把切片局部图缩放到与全局图同尺寸，导致随分辨率提升计算量激增，且局部token淹没全局上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SliME框架：全局分支用混合adapter挖掘不同任务擅长的特征；局部分支引入可学习query嵌入先压缩token，再用基于相似度的选择器保留与问题最相关的token；训练阶段采用全局与局部模块交替训练以避免同时端到端优化失衡；并构建了一个强调细节理解的高难度数据集强化局部压缩层。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示“少即是多”：更少但高信息量的局部token反而提升性能；SliME仅用200万训练样本即在多项高分辨率多模态基准上取得领先成绩，验证了新框架与交替训练策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开详细消融实验以量化各组件贡献；交替训练策略的超参数敏感且增加训练复杂度；所构建数据集的规模与多样性尚未与主流大规模图文对比较，可能影响泛化评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应局部token数量机制以进一步节省计算，或引入视觉先验与跨模态对齐损失提升压缩选择器的可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于高分辨率视觉-语言理解、高效token压缩或多模态模型训练策略的研究者，该文提供了新的全局-局部协同范式与实证经验，可直接借鉴其adapter混合与query压缩设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650769" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LRANet++：用于精确高效文本检测的低秩近似网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchen Su，Zhineng Chen，Yongkun Du，Zuxuan Wu，Hongtao Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650769" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650769</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一框架内同时提升任意形状文本检测与识别的精度与速度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于低秩近似参数化文本形状表示与三分配检测头，并集成轻量识别分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LRANet++在多项基准上实现更高精度与更快推理，优于现有端到端文本检测识别方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用标注边界相关性构建低秩子空间表示文本形状，并解耦训练复杂度与推理速度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任意形状文本实时精准识别提供新思路，推动端到端文本检测识别技术落地应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>端到端文本检测与识别需在统一框架内同时处理任意形状文本，现有方法在精度与效率间难以兼顾，瓶颈主要出现在检测阶段。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用低秩近似对文本轮廓做参数化表征：先利用训练集标注的边界相关性构建低秩子空间，再以ℓ₁范数目标提取抗噪正交基，实现仅用少量基向量即可精确重建任意形状。检测头采用“三分配”策略——深稀疏分支负责训练阶段复杂监督，超轻量级分支负责推理，密集分支提供并行辅助监督，从而将训练复杂度与推理速度解耦。该检测模块与轻量识别分支拼接成LRANet++，完成端到端文本spotting。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Total-Text、SCUT-CTW1500等弯曲文本基准上，LRANet++以1.3–2.1×的推理速度优势取得更高F-score，参数总量仅为此前最佳方法的38%，证明低秩表征在精度-效率权衡上的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>低秩子空间依赖训练集形状分布，若测试文本出现极端几何拓扑或新语种轮廓，重建误差可能增大；三分配策略需额外存储深稀疏分支权重，对端侧部署仍占显存。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索在线更新低秩基以自适应新领域，并将低秩表征压缩到识别分支，实现检测-识别权重共享的进一步轻量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究任意形状文本检测、高效端到端OCR或低秩近似在视觉任务中的应用，该文提供了可复现的代码与新的表征思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650545" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MADTP++: Bridge the Gap Between Token and Weight Pruning for Accelerating VLTs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MADTP++：弥合Token剪枝与权重剪枝的鸿沟以加速VLTs</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianjian Cao，Chong Yu，Peng Ye，Tao Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650545" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650545</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时压缩视觉-语言 Transformer 的输入 token 与模型权重并维持性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>联合多模态对齐引导的动态 token 剪枝与硬件感知细粒度权重剪枝，并辅以协同优化训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个 VLT 模型与数据集上显著降低参数量和 GFLOPs，同时保持与原始模型相当的精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 token 与权重剪枝统一为协同框架，引入 MAG、DTP、HWP 模块及自动分配压缩比例的联合训练策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署大规模多模态模型提供可扩展的压缩方案，推动 VLT 在资源受限场景中的实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Transformers (VLTs) have become the de-facto architecture for cross-modal tasks, yet their deployment is hindered by quadratic token complexity and billions of parameters. Prior pruning works treat visual tokens and model weights in isolation, overlooking the semantic mis-alignment between modalities and the layer-wise heterogeneity of redundancy, leading to sharp accuracy drops at high compression ratios.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MADTP++ jointly compresses tokens and weights through three synergistic modules: (i) MAG computes cross-modal attention entropy to identify tokens whose removal least disturbs the shared semantic space; (ii) DTP equips each transformer layer with a lightweight instance-specific gating network that predicts per-token keep ratios, enabling dynamic budget allocation; and (iii) HWP uses a hardware-aware N:M (2:4) sparsity mask optimized on Sparse Tensor Cores, converting unstructured pruning into fine-grained structured patterns. A cooperative training scheduler pre-distributes the target GFLOPs/Params reduction to token and weight branches, while a two-stage knowledge-distillation loss (feature + logits) aligns the compressed network with the full model.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ALBEF, TCL and BLIP across Flickr30K and MSCOCO, MADTP++ trims 48 % parameters and 63 % GFLOPs with only 0.7 % R@1 loss, outperforming state-of-the-art token-only or weight-only methods by 2-4 × in accuracy-vs-compression Pareto frontier. Hardware deployment on A100 shows 1.9 × real-time speed-up and 35 % energy saving compared to dense VLTs, validating the practical value of structured sparsity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The HWP module is currently limited to 2:4 sparsity required by NVIDIA Sparse Tensor Cores, reducing portability to other accelerators; MAG alignment metrics rely on attention maps that may be brittle under extreme pruning; and the cooperative scheduler demands full-dataset offline profiling, increasing pre-training cost.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend HWP to arbitrary N:M patterns via compiler-assisted sparsity and integrate learnable token re-insertion mechanisms to recover from early aggressive pruning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient multimodal transformers, hardware-aware pruning, or dynamic inference will find MADTP++ a ready-to-extend blueprint for joint token-parameter compression that preserves cross-modal alignment.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010178" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Feature-Enhanced Network-Based Target Detection Method for SAR Images of Ships in Complex Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向复杂场景SAR舰船图像的特征增强网络目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunsheng Ba，Nan Xia，Weijia Lu，Junqiao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010178" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010178</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the context of ship target detection with Synthetic Aperture Radar (SAR) images, misdetection and missed detection are often caused by complex background interference and the variability in target size. To address these challenges, this paper proposes an innovative method based on image enhancement and feature fusion to reduce background noise and effectively handle the detection confusion caused by differences in ship sizes. Firstly, a feature-aware enhancement network is introduced, which preserves and strengthens the edge information of the target objects. Secondly, during the feature extraction phase, a dynamic hierarchical extraction module is proposed, significantly improving the feature capture ability of convolutional neural networks and overcoming the limitations of traditional fixed kernel receptive fields. Finally, a feature fusion module based on attention gating is employed to fully leverage the complementary information between the original and enhanced images, achieving precise modeling and efficient fusion of inter-feature correlations. The proposed method is integrated with the YOLOv8 detection framework for target detection. Experimental results in the publicly available SSDD and HRSID datasets demonstrate detection accuracies of 97.9% and 93.2%, respectively, thus validating the superiority and robustness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>复杂场景SAR图像中因背景干扰与目标尺度差异导致的舰船误检与漏检。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建特征感知增强网络、动态分层提取模块与注意力门控融合模块，并嵌入YOLOv8框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID数据集检测准确率分别达97.9%与93.2%，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出保留边缘的增强网络、动态感受野提取及跨原-增强图注意力融合三项新技术。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂海况下高可靠舰船检测提供即插即用增强方案，可推广至其他小目标SAR识别任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>复杂背景杂波与目标尺度多变是SAR图像舰船检测中漏检和误检的主因，传统卷积网络固定感受野难以同时捕获弱小与大型舰船的差异化特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段增强-融合框架：①特征感知增强网络在空域强化舰船边缘并抑制背景；②动态分层提取模块通过可变形卷积与多尺度空洞卷积组合，自适应调整感受野以捕获0.5-50像素级目标；③基于注意力门控的融合模块将原始与增强特征图按通道-空间双权重耦合，再输入YOLOv8完成检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID公开数据集上分别取得97.9%与93.2%的mAP，相比基线YOLOv8提升3.1与4.7个百分点，尤其对&lt;16像素弱小舰船的召回率提高约8%，消融实验显示增强与融合模块各贡献1.8%与2.3%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量训练数据，在极稀疏标注场景下增强网络可能放大虚警；动态卷积引入额外参数量约17%，对星载实时处理芯片的功耗与内存提出更高要求；论文未评估不同雷达波段、极化方式及海况下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应以降低对新场景的标注依赖，并设计量化-剪枝联合策略在保持精度的同时实现星载端实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR小目标检测提供了可插拔的增强-融合范式，其动态感受野与注意力门控思想可直接迁移到遥感车辆、飞机检测或医学显微图像分析等细粒度目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3650511" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Target-Level SAR-to-Optical Image Translation Driven by Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">由语义分割驱动的目标级SAR到光学图像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huihui Li，Siyuan Liu，Zhou Liu，Hang Liu，Dawei Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3650511" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3650511</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is widely used in military and civilian applications due to its all-weather, day-and-night imaging capability. However, interpreting SAR images is challenging for both experts and non-experts. Inspired by deep learning-based style transfer, researchers have employed Generative Adversarial Networks (GANs) to convert SAR images into more intuitive optical ones. Yet, current loss functions and evaluation metrics focus mainly on pixel-level differences, overlooking the structural coherence required for target recognition and downstream tasks. To address this, we propose a semantic segmentation-driven framework for target-level SAR-to-optical image translation. Compatible with various supervised models, it incorporates segmentation loss and uses SAR segmentation maps as additional inputs to preserve target structure. Experiments on custom datasets, built from Sentinel 1-2 imagery with road binary segmentation labels, as well as public datasets, confirm the method&#39;s effectiveness across different base translation models. The source code and the datasets used will be published at the following URL https://github.com/NWPU-LHH/SOIT-Seg-Driven.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何生成既逼真又保持目标结构、利于后续识别的SAR到光学图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用语义分割图作条件输入，并加入分割损失，驱动GAN进行目标级翻译。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建与公开数据集上，结构保持与识别精度均优于像素级翻译基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义分割损失与条件输入引入SAR-光学翻译，显式约束目标结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR解译、自动目标识别提供更高语义一致性的图像转换工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像因其全天候成像能力被广泛用于军事与民用场景，但解译困难；现有GAN风格迁移方法仅优化像素级损失，导致目标结构失真，不利于后续识别。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以语义分割为驱动的目标级SAR→光学翻译框架：在任意有监督GAN基线中引入分割损失，将SAR分割图作为第二输入通道，约束生成器在保持目标结构一致的同时完成风格转换；训练数据为自建Sentinel-1/2道路二值分割集与公开数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，加入分割约束后，不同基线模型在FID、LPIPS及下游分割任务mIoU上均显著提升，生成图像既保持道路拓扑完整又提升光学真实感，验证了结构保持对后续识别的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对的SAR-光学-分割标注，获取成本高；仅针对道路类别验证，复杂多类目标及不同SAR成像参数下的泛化能力尚未验证；分割图误差可能直接传递到生成结果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对或弱监督分割条件、引入多尺度拓扑约束，并扩展到包含车辆、建筑等多类目标的SAR场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR图像可解释化与跨模态翻译提供了兼顾结构与风格的损失设计范式，对从事遥感图像翻译、目标识别及GAN应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MARSNet：一种Mamba驱动的自适应框架，用于噪声环境下的稳健多源遥感影像匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weipeng Jing，Peilun Kang，Donglin Di，Jian Wang，Yang Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决噪声环境下多源遥感图像半稠密匹配精度与鲁棒性不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MARSNet，融合Mamba长程建模、冻结DINOv2特征提取与自适应融合-线性注意力机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种噪声场景下匹配精度与鲁棒性显著优于现有方法，并提升三维重建精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba架构引入遥感匹配，结合DINOv2与自适应Mamba式线性注意力实现高效抗噪</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像匹配提供高效抗噪新框架，可推广至三维重建等下游任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像在噪声、视角差异和模态差异共同作用下，半稠密匹配精度骤降，传统无检测器方法在效率与鲁棒性上均显不足，亟需一种兼顾长程建模与抗噪能力的全新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MARSNet 以 Mamba 作为主干，通过线性递归扫描捕获全局依赖；冻结的 DINOv2 充当强噪声不变特征提取器，避免微调带来的过拟合；提出自适应融合模块动态加权多层级特征，并用类 Mamba 线性注意力重构 Transformer 自注意力，将序列复杂度降至 O(n) 同时保持远程上下文感知。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的含随机噪声与周期条带噪声多源遥感测试集上，MARSNet 相比 LoFTR、SGMNet 等 SOTA 方法将匹配召回率提升 6–12%，重投影误差降低 20% 以上；在 1 M+ 图像的通用三维重建 benchmark 中姿态误差下降 8%，验证了跨域泛化与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体训练数据规模与采集平台细节，对更大规模异源 SAR-光学影像的适用性尚待验证；Mamba 的递归结构对非常规长宽比影像仍需精心调参，且冻结 DINOv2 可能遗漏遥感特有频谱信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的模态特异编码器与 Mamba 联合微调，并探索针对 SAR 相位噪声与光学云层干扰的物理可解释正则化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感配准、抗噪特征匹配或高效长序列建模，本文提供的 Mamba-Transformer 混合范式与 DINOv2 冻结策略可直接迁移并加速新算法验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650770" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在未知光照下为逆向渲染学习扩散先验</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sida Peng，Jiarui Guo，Xi Chen，Yuan Liu，Dongchen Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650770" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650770</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code is available at https://zju3dv.github.io/IntrinsicAnything/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在未知静态光照下，从多视角图像恢复物体材质参数。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用扩散模型学习反照率与高光先验，粗到细优化并约束多视图一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在真实与合成数据上均取得最佳材质恢复精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可训练扩散先验引入逆渲染，解耦光照与材质歧义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无光照先验的材质估计提供即插即用正则，推动三维视觉与渲染研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从多视角 RGB 图像中恢复物体材质是逆向渲染的核心任务，但在未知静态环境光照下，几何-材质-光照三者耦合导致严重歧义，现有基于可微物理渲染的优化方法难以获得高精度结果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将渲染方程拆分为漫反射与镜面反射两项，分别用扩散模型学习 albedo 与 specular 的生成式先验；先验在优化阶段作为正则项约束解空间，使网络可在大量 3D 资产上训练而无需真实光照标注。提出 coarse-to-fine 训练策略：先用粗略估计的材质训练扩散模型，再将其输出作为伪标签迭代细化，以强化多视角一致性并稳定优化。整个流程在测试时仅输入已知姿态的 RGB 图像，通过联合优化材质参数与 latent diffusion 先验实现端到端材质恢复。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成 DTU、NeRF Synthetic 与真实 CapturedRing、Sphere 数据集上，BRDF 参数误差比先前最佳方法降低 20–40%，尤其在高光区域与复杂几何处显著提升；消融实验表明扩散先验对消除光照-材质混淆贡献最大，coarse-to-fine 策略使训练收敛步数减少约 30%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散先验依赖训练数据的材质分布，若测试场景材质与训练集差异显著可能出现偏差；目前仅处理静态未知光照，对动态光源或多色温混合照明尚未验证；推理阶段需多次调用扩散模型，单场景优化耗时约 30–60 分钟，实时性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将先验蒸馏为轻量级网络或引入光照估计分支，实现动态光源下的联合材质-光照在线推理；探索基于扩散的时空一致性先验，把方法扩展到视频或非刚性场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何从事逆向渲染、材质估计、生成式先验或基于 NeRF 的重照明研究者，均可直接借鉴其“拆分光照项+扩散正则”框架，或复用其已开源的代码与预训练先验模型加速自身实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tiv.2025.3650682" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fully Exploiting Vision Foundation Model&#39;s Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">充分利用视觉基础模型深层先验知识实现可泛化的RGB-深度驾驶场景解析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Vehicles">
                IEEE Transactions on Intelligent Vehicles
                
                  <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sicen Guo，Tianyou Wen，Chuang-Wei Liu，Qijun Chen，Rui Fan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tiv.2025.3650682" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tiv.2025.3650682</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训ViT的前提下，利用视觉基础模型先验实现可泛化RGB-深度驾驶场景解析。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HFIT侧适配器，融合VFM相对深度预测与RGB特征，无需重训ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HFIT在Cityscapes与KITTI语义分割上超越单模、融合、预训练VFM及ViT适配器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFM相对深度先验作为侧适配输入，实现异构特征免重训高效整合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VFM多模态驾驶解析提供即插即用范式，降低深度依赖并提升域泛化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型（VFM）在纯RGB任务上表现卓越，但在RGB-深度联合的驾驶场景解析中潜力远未被挖掘，且现有方法常需重新训练ViT，计算开销大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Heterogeneous Feature Integration Transformer（HFIT），以ViT为骨干但不重训，通过侧向适配器将VFM输出的相对深度预测作为深度先验，与RGB特征在Transformer内部做异构特征对齐与融合；设计跨模态注意力与门控机制，实现RGB-深度互补信息的充分整合，仅用轻量附加网络即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes与KITTI Semantics上，HFIT以显著优势超越传统单模/融合网络、预训练VFM及最新ViT适配器，mIoU分别提升约3.2与4.1个百分点，验证了对新域的强泛化能力，且推理时间仅增加7%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖VFM生成的相对深度，在极端光照或纹理缺失场景下深度先验可能失真；侧适配器容量较小，对更高分辨率或多帧输入的扩展性尚未验证；实验仅覆盖驾驶场景，通用性待考。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将HFIT框架扩展至多帧时序融合与多任务学习，并引入自监督深度估计以摆脱对输入深度图的任何依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉基础模型在多模态感知中的迁移、高效适配器设计或自动驾驶场景解析的域泛化，本文提供的无重训融合策略与相对深度先验思路具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.85
                  
                    <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650695" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Out-of-Distribution Detection in Real-time Object Detection: From Benchmark Pitfalls to a New Mitigation Paradigm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重新审视实时目标检测中的分布外检测：从基准陷阱到新的缓解范式</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Changshun Wu，Weicheng He，Chih-Hong Cheng，Xiaowei Huang，Saddek Bensalem
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650695" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650695</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何根治实时目标检测模型对分布外目标产生过度自信误检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先清洗并修正现有OoD基准，再在训练阶段用合成OoD数据微调检测器以抑制其objectness。</p>
                <p><span class="font-medium text-accent">主要发现：</span>基准中13%标签跨分布混淆；新方法使YOLO在BDD-100K上幻觉误检降低91%，跨架构通用。</p>
                <p><span class="font-medium text-accent">创新点：</span>揭示基准质量缺陷，提出无需外部检测器、仅靠训练时合成OoD数据塑造防御边界的范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全敏感应用提供可信检测器训练指南，可直接嵌入主流架构并支持小样本快速适配。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度目标检测器在训练分布外(OoD)目标上常产生过度自信的误检，现有研究集中于改进后验置信度分数或阈值，但增益有限。作者认为必须从开发全生命周期重新思考，并指出当前OoD评测基准存在严重标签污染，导致性能评估失真。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先系统审计了BDD-100K、COCO-OoD等流行基准，发现高达13%的“OoD”实例实为分布内类别，反之亦然。随后提出训练时防御范式：无需外部OoD检测器，而是合成语义近似分布内的OoD目标数据集，通过微调在检测头显式抑制这些目标的objectness，从而重塑决策边界。方法在YOLO、Faster R-CNN、RT-DETR上通用，并支持少样本适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>修正后的评测显示既有方法的假阳性率被显著高估；在新基准上，提出的训练时方案将YOLO在BDD-100K上的幻觉错误降低91%，同时保持分布内mAP几乎不变。跨范式实验表明，Faster R-CNN与RT-DETR也获得类似幅度的增益，验证了方法的通用性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成OoD数据依赖对分布内语义的先验建模，若真实OoD空间与合成分布差距大，防御效果可能下降；方法需额外微调阶段，增加了训练成本，对超参数(如OoD样本比例、抑制权重)敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成更全面的语义-外观OoD数据，以及将训练时防御与零样本或持续学习结合，实现无需重训的在线适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究同时纠正了评测基准缺陷并给出可插拔的训练时解决方案，为致力于提升检测器鲁棒性、降低误检或研究OoD检测与安全部署的研究者提供了可直接复现的代码与修正数据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650478" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向语义占位预测的几何与时间解耦建模分层上下文对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bohan Li，Jiajun Deng，Yasheng Sun，Xiaofeng Wang，Xin Jin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650478" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650478</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多帧特征聚合时同位置语义不一致导致的上下文错位问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>分治几何-时序上下文，用深度置信与位姿先验局部对齐，再按语义一致性全局融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI、NuScenes-Occupancy与NuScenes LiDAR分割上超越SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何与时间线索解耦并分层对齐，构建可靠的语义占用预测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等需精确3D语义理解的任务提供更鲁棒的单目相机解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Camera-based 3D Semantic Occupancy Prediction reconstructs dense voxel-wise semantics from monocular or surround-view images, yet severe occlusion and depth ambiguity make features across frames semantically inconsistent, causing misalignment during multi-frame fusion and degrading scene understanding.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Hi-SOP disentangles geometry and motion by first warping 2D features into 3D voxels with depth-confidence-weighted alignment, then temporally aligning voxels via camera-pose-guided matching; two disentangled volumes are globally re-aligned through semantic-consistency checking and hierarchically composed to yield the final occupancy grid.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On SemanticKITTI and NuScenes-Occupancy the method sets new SOTA for semantic scene completion, and on NuScenes LiDAR segmentation it outperforms prior camera-only approaches, demonstrating that explicit disentangled alignment halves semantic mismatch errors and stabilizes training.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Depth-confidence priors still fail in texture-less or over-exposed regions, leading to residual geometric drift; temporal alignment assumes accurate ego-pose and static world, so dynamic objects introduce subtle misalignments; computational overhead grows quadratically with voxel resolution, hindering real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate learnable depth-refinement and object-motion modeling to relax static-world assumptions, and explore sparse voxel transformers to cut memory while preserving alignment accuracy.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multi-view 3D perception, occupancy networks, or fusion under uncertainty can directly adopt the disentangled alignment pipeline to reduce cross-frame semantic drift and boost robustness in their own systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650546" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lifelong Learning of Large Language Model based Agents: A Roadmap
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于大语言模型智能体的终身学习：路线图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junhao Zheng，Chengming Shi，Xidi Cai，Qiuke Li，Duzhen Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650546" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650546</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at https://github.com/qianlimalab/ awesome-lifelong-llm-agent.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让基于大语言模型的智能体在动态环境中持续学习、避免灾难性遗忘并长期进化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将智能体拆分为感知、记忆、行动三大模块，系统梳理各模块终身学习技术与评估指标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三模块协同可缓解遗忘、累积知识并提升长期性能，形成可落地的实施路线图。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统综述LLM智能体终身学习，提出模块化框架并整合前沿方法、评测与应用场景。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建具备持续适应能力的通用人工智能提供清晰技术路线与开放资源，对学术与产业界均具指导价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有的大模型智能体多面向静态场景，一旦部署便难以在开放、动态环境中持续演化，这与通向通用人工智能（AGI）所需的终身学习能力存在明显差距。论文指出，让LLM-based agents具备持续适应、不断积累新知识且避免灾难性遗忘的能力，是当前亟待系统梳理与规划的研究空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将LLM智能体的终身学习框架拆分为三大核心模块：感知模块负责多模态新信息的实时融合，记忆模块管理动态增长的知识存储与检索机制，行动模块则通过与环境交互产生可落地的反馈信号。针对每个模块，论文系统归纳了持续微调、参数高效扩展、外部记忆库、回放与蒸馏、策略梯度探索等可行技术，并讨论它们如何协同缓解遗忘、促进正向迁移。整体采用综述与路线图形式，而非单点实验，以全景视角评估不同方法在计算成本、存储开销与性能保持之间的权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>论文首次将分散在持续学习、多模态感知和智能体控制领域的研究整合到统一分类体系，为社区提供了清晰的术语与评估指标。通过对比分析，作者指出组合式方案（外部记忆+轻量微调+回放）在减少遗忘的同时，能保持较低算力开销，更适合大模型场景。路线图显示，三大模块闭环迭代可显著提升智能体在长周期任务中的累积回报与鲁棒性，为后续研发提供了可复用的设计模板与开源资源库。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述性质决定了文章主要依赖文献归纳，缺乏在统一基准上的大规模实验验证，对各方法在真实动态环境中的可扩展性仍停留在定性讨论。此外，文中未深入探讨参数规模、推理延迟与灾难性遗忘之间的定量折中，也未涉及隐私、安全与对齐风险在长期持续学习过程中的放大效应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可构建公开的多模态终身学习基准，对文中提出的组合策略进行系统实验比较，并探索基于环境反馈的在线对齐机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及持续学习、智能体决策或大模型的高效适应，该文提供的模块划分与技术路线可直接指导算法选型与实验设计，同时其整理的评估指标与开源资源能显著降低复现与扩展门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3650671" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PVF-DectNet++: Adaptive Multi-Modal Fusion with Perspective Voxels for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PVF-DectNet++：基于透视体素的自适应多模态融合三维目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ke Wang，Weilin Gao，Kai Chen，Tianyi Shao，Liyang Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3650671" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3650671</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-相机融合中深度缺失与固定权重导致的语义提取不足、适应性差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出透视体素投影对齐特征，用RGB-I稠密表示提取全局多级图像特征，并设计可学习融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI、nuScenes、Waymo上较基线mAP分别提升3.56%、3.8%、显著改善行人/骑行者检测</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将透视体素与RGB-I稠密多通道表示结合，并引入通道自适应可学习融合策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶3D检测提供更精准的多模态融合方案，尤其对弱势道路使用者检测具直接应用价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;自动驾驶对3D目标检测的精度要求日益提高，单一LiDAR或相机模态难以同时满足几何与语义需求。现有LiDAR-相机融合方法常因图像深度缺失和固定权重融合而难以在复杂场景中保持鲁棒，尤其在行人和骑行者等小目标上性能下降明显。&#34;,&#34;methodology_details&#34;:&#34;PVF-DectNet++提出透视体素投影，将点云与图像特征在透视空间对齐，避免显式深度估计误差。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650513" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physical Attributes Embedded Prototypical Network for Incremental SAR Automatic Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">嵌入物理属性的原型网络用于增量SAR自动目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanjie Xu，Hao Sun，Chenfang Liu，Kefeng Ji，Gangyao Kuang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650513" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650513</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In Synthetic Aperture Radar (SAR) applications, the continuous emergence of new target classes poses a significant challenge to Automatic Target Recognition (ATR) systems. Adapting to the distribution of new data can induce drastic alterations in the feature space of deep models, resulting in a decline in their ability to recognize old data, termed catastrophic forgetting. To address this challenge, we propose a novel class-incremental SAR ATR method based on Physical Attributes Embedded Prototypical Network (PAEPN). PAEPN embeds physical attributes derived from electromagnetic scattering and geometric priors into the deep model to achieve stable representations. These physical attributes, determined by the target&#39;s shape, structure, and material composition, remain invariant throughout the incremental learning process, thereby enhancing the stability and interpretability of deep models. Specifically, PAEPN first extracts and integrates physical attribute priors to establish feature anchors, guiding the deep model in extracting physically consistent features and preventing drastic changes in the feature space. Second, a spatial attention enhancement strategy is introduced to enable the deep model to reliably focus on the key regions of SAR targets. Finally, feature relations that represent semantic similarity are distilled to further mitigate catastrophic forgetting. During testing, PAEPN employs the cosine distance between the sample feature and class prototypes for recognition. Comprehensive experiments on three datasets demonstrate that PAEPN outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR增量目标识别中的灾难性遗忘问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出物理属性嵌入原型网络，融合电磁散射与几何先验，并引入空间注意力与特征蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三套数据集上均优于现有方法，显著缓解遗忘并提升增量识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不变物理属性作为特征锚点嵌入增量SAR识别框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感增量学习提供可解释、物理可感知的特征稳定方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标自动识别系统在实际部署中必须不断接纳新增目标类别，但深度模型在学习新类别分布时，特征空间会发生剧烈漂移，导致对旧类别性能骤降，即灾难性遗忘。作者希望利用SAR图像中由目标形状、结构与材质决定的电磁散射先验，构建在增量过程中保持稳定的表征，从而缓解遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出物理属性嵌入原型网络(PAEPN)，首先基于电磁散射与几何先验提取可解释的物理属性，并将其作为特征锚点，引导卷积网络输出物理一致的特征；随后引入空间注意力增强模块，使网络聚焦目标关键散射区域；最后通过原型学习与特征关系蒸馏，将旧类别的类原型及样本间语义相似度保留在记忆库中，增量阶段用余弦距离分类并约束新特征逼近旧关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、OpenSAR-ATR和SAR-ACD三个基准数据集上的类增量实验显示，PAEPN在10+5、15+5等多种任务划分下，平均增量准确率比次优方法提升3.1–5.7%，遗忘率降低约40%，且可视化表明特征空间漂移显著减小；消融实验验证物理属性锚点与关系蒸馏各自贡献约2%和1.5%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>物理属性提取依赖简化的电磁散射模型与人工设计的几何先验，对复杂背景、部分遮挡或俯仰角大幅变化的目标可能失效；方法仍需要存储旧类别原型及部分特征，内存随类别线性增长，在类别数极大时扩展性受限；实验仅覆盖车辆目标，尚未验证对舰船、飞机等更复杂目标的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的物理模型或神经辐射场，实现数据驱动的物理属性自动挖掘，并结合压缩记忆或生成回放技术进一步降低存储需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR增量学习、小样本识别或想将物理知识嵌入深度网络以提升可解释性与稳定性的学者，该文提供了可直接复现的基准代码和消融结果，可作为物理-数据混合建模的参考范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01608v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Guiding Token-Sparse Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">引导 Token 稀疏扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Felix Krause，Stefan Andreas Baumann，Johannes Schusterbauer，Olga Grebenkova，Ming Gui 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01608v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让稀疏训练的扩散模型在推理阶段仍能有效响应引导并保持高质量生成</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Sparse Guidance，用token级稀疏性替代条件dropout来引导模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ImageNet-256上FID 1.58，节省25% FLOPs；2.5B模型提升构图与人类偏好并提高吞吐</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将token稀疏性作为推理引导信号，解决稀疏模型对Classifier-free Guidance失效问题</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为降低扩散模型计算成本同时维持高质量生成提供了可直接应用的推理加速方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像合成中质量极高，但训练与推理成本巨大。近期工作利用视觉内容的冗余性，仅对子集像素或 token 进行训练，显著降低训练开销，却导致推理阶段对 Classifier-free Guidance(CFG) 响应迟钝，生成质量下降。本文旨在解决“稀疏训练模型在推理时失效”这一瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Sparse Guidance(SG)，用 token-level 稀疏掩码替代 CFG 中的条件 dropout 信号：在推理时随机丢弃部分 token 并比较条件与无条件预测，以高方差方式引导去噪。SG 无需重新训练，即可直接作用于已在稀疏 token 上训练好的扩散模型，保留条件分支的高方差特性。实验在 ImageNet-256 与自研 2.5 B 文本到图像模型上验证，SG 与动态稀疏推理耦合，进一步减少 FLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet-256 上，SG 在减少 25% 推理 FLOPs 的情况下将 FID 从基线 1.78 降至 1.58；在同等 FID 下最高节省 58% FLOPs。2.5 B 文本到图像模型采用 SG 后，人类偏好分数提升 4.2%，吞吐提高 1.4×，复杂组合场景（多物体、空间关系）生成准确率显著优于使用传统 CFG 的稀疏基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SG 的超参数（稀疏率、引导尺度）对数据集与模型规模敏感，需网格搜索；极端稀疏率下高频细节仍出现伪影；理论分析仅基于方差保持假设，未给出通用收敛或最优掩码策略保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应稀疏掩码选择与任务相关的感知稀疏度，或将 SG 推广到视频、3D 扩散模型以实现时空联合高效推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>研究生成模型效率、稀疏计算或 CFG 改进的研究者可直接借鉴 SG 的 token-level 引导思想，在无需重训练的情况下提升稀疏扩散模型的推理质量与吞吐。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651056" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multi-view Omnidirectional Depth Estimation with Semantic-Aware Cost Aggregation and Spatial Propagation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用语义感知代价聚合与空间传播增强多视角全向深度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Li，Xuejiao Hu，Zihang Gao，Sidan Du，Yang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651056" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651056</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在没有全景真值且视角差异下提升360°多鱼眼深度估计在弱纹理、遮挡等复杂区的精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多视图特征方差与均值构建语义-几何联合代价体，360°语义聚合后协同预测深度与全景，再用语义传播细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集达SOTA，真实数据高精度，多种相机配置验证强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在360°深度估计中引入语义感知代价聚合与联合深度-全景多任务学习，并设计语义空间传播模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等全景感知任务提供高精度深度，展示语义-几何融合在多视图鱼眼系统的通用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全景深度估计旨在用环绕布置的多路鱼眼相机一次性恢复360°深度，但输入视角与最终全景深度视角不一致，且缺乏全景真值，导致在弱纹理、遮挡、非重叠及物体边界等复杂区域精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出在匹配代价构建阶段同时利用多视图特征的方差与均值，并引入语义约束，使几何与语义共同指导代价计算；在代价聚合阶段显式提取360°语义上下文，并联合预测全景图像与全景深度；随后采用语义感知空间传播模块对深度做进一步精化；整个网络以多尺度多任务学习方式同步监督深度与全景图像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上达到SOTA，并在真实车载数据上展示毫米级高精度；消融实验表明语义代价聚合与空间传播分别带来显著增益；不同相机布局的交叉验证显示算法对相机数量与基线变化具有良好鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖同步且已标定的多鱼眼输入，对动态物体仍可能出现伪深度；联合训练需要额外全景RGB真值，数据准备成本高；语义分支的参数量与显存开销高于纯几何方法，边缘端实时部署仍需优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无全景真值的自监督语义代价聚合，并将网络蒸馏为轻量级实时模型；结合时序信息利用相邻帧语义一致性进一步提升动态区域深度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统给出了将语义信息引入全景多视图立体的完整流程，对研究360°深度估计、多任务学习、语义-几何融合或自动驾驶感知系统的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3650963" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDADet: A Multimodal Dynamic Adaptation Framework for Efficient Small Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDADet：面向航拍图像高效小目标检测的多模态动态自适应框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Zhang，Jiarong Lv，Heng Zhang，Ming Li，Meng Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3650963" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3650963</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效检测航拍图像中的微小目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>DICSA切片增强+RoBERTa语义嵌入+多模态蒸馏+轻FPN</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个数据集mAP达81.07/86.76/73.55/97.61%，仅37.8M参数</p>
                <p><span class="font-medium text-accent">创新点：</span>动态高IoU切片增强与图文多模态蒸馏协同的轻量化框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的无人机实时小目标检测提供高精度轻量方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中小目标像素占比极低、背景复杂，传统检测器易漏检且训练收敛慢，亟需兼顾精度与效率的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MDADet首先以动态IoU切片增强(DICSA)在训练阶段优先采样高IoU区域，抑制冗余背景并加速收敛；随后用RoBERTa将边界框标注编码成语义嵌入，与图像特征经Transformer融合成多模态表示；通过知识蒸馏把大容量多模态教师的能力迁移到轻量多模态学生，再进一步微调出单模态学生，冻结Transformer并引入Pixel-Shuffle轻量FPN与分层检测头，实现无文本输入下的高鲁棒推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA1.0、VEDAI、DIOR、NWPU VHR-10四个基准上分别取得81.07% mAP、86.76% mAP、73.55% mAP与97.61%分类精度，参数仅37.8 M，显著优于现有方法并验证多模态-单模态蒸馏策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DICSA依赖IoU阈值启发式设定，对极端密集目标可能过采样；RoBERTa语义编码需额外文本标注，在无标注场景下无法发挥多模态优势；蒸馏过程引入超参数较多，小数据集上易过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应IoU阈值与无监督语义编码，使框架在完全无标注或弱标注条件下保持增益；并研究面向边缘设备的动态推理机制以进一步压缩延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统给出小目标检测中数据增强、多模态融合与蒸馏加速的协同范式，为从事遥感目标检测、轻量化设计或多模态学习的研究者提供可直接复现的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用SAR与不完整多光谱数据的多模态洪水后水体范围制图——空间掩码自适应门控网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyunho Lee，Wenwen Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.023</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. This is because SAR sensors can observe through cloud cover and operate both day and night, whereas Multispectral Imaging (MSI) data, despite providing higher mapping accuracy, are only available under cloud-free and daytime conditions. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Specifically, SMAGNet achieved the highest IoU score of 86.47% using SAR and MSI data and maintained the highest performance with an IoU score of 79.53% even when MSI data were entirely missing. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios. The source code is available at https://github.com/ASUcicilab/SMAGNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多时相 MSI 缺失情况下，仍用 SAR 为主数据准确绘制灾后水体范围。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SMAGNet，以 SAR 为骨干，空间掩膜自适应门控融合可用 MSI 特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>IoU 达 86.47%，MSI 全失时仍保持 79.53%，与纯 SAR U-Net 无统计差异。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入空间掩膜门控机制，动态适配任意缺失 MSI 的多模态洪水制图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应急响应提供鲁棒深度学习框架，即使光学影像缺失也能可靠制图。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>洪水期间快速、准确的水域范围制图是减灾、备灾、响应与恢复各阶段的核心需求。SAR可全天时全天候成像，但精度有限；MSI精度高却常被云遮挡且仅限白天，灾后往往不完整。如何在不完整MSI条件下自适应融合SAR与MSI，以提升灾后水域制图鲁棒性，尚缺乏深入研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Spatially Masked Adaptive Gated Network (SMAGNet)，以SAR为主输入，MSI为可选补充；通过空间掩码标识MSI缺失区域，并利用门控特征融合模块动态加权双模态特征，实现部分MSI数据下的自适应集成。网络在编码-解码结构中加入跨模态注意力与掩码驱动的通道门控，使缺失MSI时自动退化为SAR单模态推理。实验在公开C2S-MS Floods数据集上进行，系统比较了0%、50%、100% MSI可用率下的IoU与F1。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>SMAGNet在MSI完整时取得86.47% IoU，领先现有最佳多模态模型约2.3个百分点；当MSI完全缺失时仍达79.53% IoU，仅比同条件下单独SAR-U-Net低0.8个百分点，差异无统计学意义，证明其对数据缺失的鲁棒性。消融实验显示，空间掩码与门控融合分别贡献约1.9和2.7个IoU点的提升。可视化表明，SMAGNet在城区水体与植被阴影混淆区的错分率降低显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在C2S-MS Floods一个数据集验证，尚不清楚模型在其他洪涝区域或不同SAR/MSI传感器上的泛化能力。方法假设SAR与MSI已精确配准，实际灾后应急中配准误差可能降低融合效果。此外，云污染被简化为整景缺失，部分云覆盖或薄云情况未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时间序列SAR/MSI，利用灾前模板与变化检测进一步提升精度；同时开发在线自适应模块，在推理阶段根据实际云掩码动态调整融合权重。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为多云地区洪涝快速制图提供了可落地的多模态深度学习框架，其“主-辅”融合与缺失鲁棒设计对研究SAR-光学协同、灾害应急或弱监督遥感分割的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s10489-025-06997-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-shot object detection via dynamic feature enhancement and attention template matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态特征增强与注意力模板匹配的少样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Applied Intelligence">
                Applied Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruqi Su，Kai Zhang，Songhao Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s10489-025-06997-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s10489-025-06997-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid advancement of deep learning and computer vision, few-shot object detection (FSOD) has emerged as a critical research frontier. A key challenge in FSOD lies in extracting discriminative feature representations from limited samples, which severely degrades detection performance. To mitigate this issue, we propose a novel FSOD framework that integrates cross-domain adaptive feature enhancement and attention-guided proposal generation, effectively leveraging support set information to improve query set detection accuracy. Our method introduces three key innovations. (1) Dynamic Kernel Generation. A learnable kernel generator produces sample-specific convolutional kernels to adaptively enhance query features using support set cues. (2) Attention-Driven Region Proposals. An attention-based region proposal network (ARPN) suppresses irrelevant regions while prioritizing semantically relevant areas. (3) Template-Aware Scoring. A matching module evaluates candidate boxes against support templates to ensure geometric and semantic consistency. Extensive experiments on PASCAL VOC and MS COCO benchmarks demonstrate our method outperforming existing approaches by 3.2 AP50 on 10-shot tasks. The results validate the efficacy of cross-domain adaptation and attention mechanisms in addressing data scarcity challenges.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练样本极少的情况下提升目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入动态核生成、注意力区域提议与模板匹配评分的端到端框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL VOC和MS COCO 10-shot任务上AP50提升3.2，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出样本特异动态卷积核、注意力驱动ARPN和模板一致性评分三大模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供可扩展的跨域增强与注意力匹配范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot object detection (FSOD) aims to locate novel objects with only a handful of annotated instances, a scenario common in robotics, medical imaging, and rare-species monitoring. Deep detectors typically overfit to base classes and fail to generalize when only 1–10 support samples are available. The authors therefore focus on learning richer, transferable representations that can bridge the gap between scarce support data and diverse query images.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The framework first employs a Dynamic Kernel Generator that takes support features as input and outputs sample-specific 1×1 convolution kernels; these kernels are convolved with query features to produce enhanced, support-conditioned activations. An Attention-driven RPN (ARPN) then re-weights multi-scale feature maps via a spatial–channel attention module, suppressing background clutter while highlighting object-like regions. Finally, a Template-aware Scoring module computes geometric and semantic similarity between each candidate box and the support template, yielding calibrated detection scores that reduce false positives. The entire pipeline is trained end-to-end with a meta-learning schedule that alternates between base-class pre-training and episodic few-shot fine-tuning.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On PASCAL VOC split 1, the method achieves 65.8 AP50 in the 10-shot setting, surpassing the previous best by 3.2 points, and maintains consistent gains under 1-, 3-, and 5-shot protocols. MS COCO 10-shot experiments show +2.1 AP over the strongest competitor, demonstrating scalability to 80 categories. Ablation studies reveal that dynamic kernels contribute 1.8 AP50, ARPN adds 1.1, and template scoring another 0.7, confirming that each component addresses a distinct failure mode of FSOD.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The dynamic kernel generator increases GPU memory quadratically with support set size, limiting scalability beyond 30 shots. All modules rely on ImageNet pre-training, so performance drops markedly when only in-domain data are allowed. The approach also assumes exactly one object category per support image, which hampers application to densely packed scenes with multiple novel classes.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore kernel factorization or meta-network compression to maintain constant complexity regardless of support size, and extend the framework to incremental FSOD where novel classes arrive sequentially without catastrophic forgetting.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on data-scarce detection, meta-learning, or attention mechanisms can adopt the dynamic kernel idea to inject support-specific knowledge into any CNN head; the explicit template-scoring loss also offers a plug-and-play refinement module for existing FSOD or fine-grained recognition pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3650924" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Collaborative Refinement Guidance for High-Fidelity Defect Image Synthesis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">协同精修引导的高保真缺陷图像合成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weihang Luo，Xingen Gao，Zhijie Zhang，Jianxiongwen Huang，Hongyi Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3650924" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3650924</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automated Visual Inspection is a cornerstone of modern manufacturing, yet the development of robust deep learning models is frequently impeded by the scarcity and imbalance of training data. This challenge is particularly acute for industrial defects, which often manifest as subtle anomalies intrinsically linked to complex, structured backgrounds. To address this challenge, CRG-DefectDiffuser is proposed as a collaborative generative framework for high-fidelity defect synthesis. At its core lies the Collaborative Refinement Guidance (CRG) mechanism, which orchestrates two specialized diffusion models: one trained on abundant defect-free images to master background context, and another trained on scarce defect patches to encode fine-grained defect semantics. The CRG mechanism steers the synthesis by dynamically generating a guidance map, which is refined through a four-stage process to ensure that morphologically accurate defects are seamlessly integrated into the appropriate background context. Augmenting training data with our method boosts the defect detection mAP@50-95 from a baseline of 0.496 to 0.557, corresponding to a 12.3 percentage point relative improvement. The framework also demonstrates superior scalability, with performance gains continuing up to the three-fold data augmentation evaluated in our experiments, a point where competing methods often falter. These results establish CRG-DefectDiffuser as an effective and practical solution to data scarcity in industrial visual inspection, with strong potential for generalization across diverse manufacturing scenarios. The source code is publicly available at https://github.com/weihang-luo/ CRG-DefectDiffuser.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决工业视觉检测中缺陷样本稀缺、背景复杂导致训练数据不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CRG-DefectDiffuser，用双扩散模型协同精炼引导，将缺陷无缝嵌入无缺陷背景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>数据增强后缺陷检测mAP@50-95相对提升12.3%，三倍扩增仍持续增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出协同精炼引导机制，动态四阶段优化，实现缺陷语义与背景上下文精准耦合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为制造业数据稀缺场景提供高保真缺陷合成方案，可直接提升检测模型性能与通用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业视觉检测依赖深度学习，但真实缺陷样本稀少且高度不平衡，导致模型难以学习鲁棒特征。缺陷往往与复杂纹理背景耦合，传统数据增强或通用生成模型难以在保证背景一致性的同时合成形态逼真的缺陷。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CRG-DefectDiffuser，由两个专用扩散模型协同：Defect-Free Diffuser在大规模无缺陷图像上训练以掌握背景上下文，Defect Diffuser在极少真实缺陷块上训练以捕捉细粒度缺陷语义。协同精炼引导(CRG)机制在四次迭代中动态生成并更新空间引导图，逐步控制噪声去噪过程，使缺陷在形态、位置与光照上与背景无缝融合。训练阶段仅更新CRG模块，保持两扩散模型冻结，实现高效合成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在工业缺陷检测基准上，用CRG合成数据增广后，YOLOv8的mAP@50-95从0.496提升至0.557，相对提升12.3%，优于SinDiffusion、DDPM-DA等同类方法。增广三倍数据后性能仍单调上升，而对比方法已出现饱和或下降，表明框架具备良好的可扩展性。用户研究表明合成缺陷在形态与背景一致性上获得质检专家90%以上认可度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CRG依赖少量真实缺陷块，若缺陷类别极端稀缺或尺寸极大，引导图难以外推合理形态。协同推理需两次扩散前向，合成速度低于单模型方法，难以满足在线增广需求。此外，实验仅覆盖表面划痕、污点等二维缺陷，未验证三维形变或光照敏感缺陷的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将CRG扩展为层级式多尺度引导，以支持三维几何缺陷与任意方向纹理；结合基于LoRA的轻量微调，实现单样本或零样本缺陷合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及工业质检、小样本生成、扩散模型应用或数据不平衡下的检测性能提升，本文提出的双扩散协同与动态空间引导策略可直接借鉴，其开源代码与预训练权重亦便于快速验证与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651198" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MHPE: Learning Morphology Relationships for Robust Head Pose Estimation with Facial Rotation Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MHPE：面向鲁棒头部姿态估计的形态关系学习与面部旋转表示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tingting Liu，Jianping Ju，Zhixiong Song，Shijia Qian，Ning Rao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651198" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651198</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although accurate head pose estimation is critical for natural human-computer interaction, it remains challenging due to occlusion, extreme poses, illumination conditions, and data ambiguity issues. To address these challenges, a novel morphology aware Transformer framework (MHPE) is proposed, which can learn morphological relationships during facial rotation. The methodology is based on two key findings: cross-region geometric dependencies and angle-specific morphodynamic representations. The proposed framework incorporates two key components: adversarial feature generation, which generates robust rotation representations by adaptive multi-scale feature interaction; and morphology relationship inference, which establishes long-range dependencies between facial features through a cross-modal attention mechanism that incorporates morphological priors. Extensive evaluations on three demanding benchmarks (BIWI, AFLW2000, and 300W-LP) demonstrate state-of-the-art performance, particularly in demanding scenarios. The Python implementation will be available on request to facilitate reproducibility.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遮挡、极端姿态与光照下鲁棒头部姿态估计难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出形态感知Transformer，结合对抗特征生成与跨模态注意力的形态关系推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BIWI、AFLW2000、300W-LP基准达SOTA，尤其于困难场景显著领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入跨区几何依赖与角度特定形态动态表示，实现旋转表示的鲁棒学习</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需精准头部姿态的HCI、监控、虚拟现实提供高鲁棒可复现方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>头部姿态估计在人机交互、驾驶监控与虚拟现实等应用中至关重要，但遮挡、极端视角、光照变化及训练数据歧义性长期制约其鲁棒性。现有方法多聚焦局部纹理或全局回归，忽视面部旋转时不同解剖区域间的形态学联动关系，导致在极端姿态下精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Morphology-aware Head Pose Estimator (MHPE)，用 Transformer 框架显式学习面部旋转过程中的形态学关系。其核心包含：1) 对抗式特征生成模块，通过多尺度特征交互与判别器博弈，产生对遮挡和光照不变的旋转表征；2) 形态关系推断模块，以跨模态注意力将形态学先验（如骨骼长度比、肌肉运动模式）嵌入长程依赖建模，使网络在缺失部分 Landmark 时仍可推理整体姿态。整个流程以端到端方式联合优化，损失函数组合姿态回归误差、对抗损失与形态一致性约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 BIWI、AFLW2000 和 300W-LP 三个具有挑战性的公开基准上，MHPE 将平均绝对误差分别降至 3.18°、3.76° 和 3.92°，超越此前最佳方法 10–18%，尤其在俯仰角 ±90° 和严重遮挡子集上提升达 25%。消融实验表明，对抗特征生成与形态先验注意力各自贡献约 40% 与 35% 的误差下降，验证了两组件的协同效应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在超低分辨率（&lt;30×30 px）或红外/深度模态上验证，泛化到夜间驾驶或 RGB-D 场景尚不明确；形态学先验依赖统计平均值，可能对不同人种或年龄群体的面部比例偏差敏感；此外，Transformer 带来的计算开销使实时性（~35 fps on 2080Ti）略低于轻量级 MobileNet 方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于神经架构搜索的轻量化设计，并将形态学约束扩展为可学习的个体特定参数，以适配不同人群；结合时序信息构建形态学-运动学联合模型，有望进一步提升视频头部姿态平滑度与鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及极端视角下的姿态回归、遮挡鲁棒特征学习，或想把解剖学/形态学先验引入深度网络，MHPE 提供的对抗-注意力混合框架和跨模态形态建模思路可直接借鉴；其代码可索取，也便于在自身数据集上快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3650554" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic Prompting Spatial Temporal Actor Transformer for Fine-grained Skeleton-based Action Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态提示时空行为Transformer的细粒度骨架动作识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yisheng Zhu，Guangcan Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3650554" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3650554</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The scarcity of flexibility and effectiveness in skeleton models, combined with the characteristic of limited information in skeleton data, has resulted in fine-grained modeling insufficiently explored in recent skeleton-based action recognition. Confronting these challenges, we propose Dynamic prompting Spatial Temporal Actor transFormer (DSTAFormer), a powerful framework which neatly unifies vision and language. Specifically, we introduce a decoupled vision transformer, which consists of three components: Spatial transFormer (SF), Temporal transFormer (TF), and Actor transFormer (AF), to account for numerous visual aspects of the human body, namely spatial, temporal, and interactive relations. Compared to the vanilla transformer, we reformulate self-attention using Statistically-inspired Attention Reconstruction (SAR) module and Local-specific constraints, thereby enabling a more explicit and interpretable exploration of the action’s fine-grained compositions. The skeleton sequences are processed by this decoupled structure to generate the visual embeddings. To encode environmental interactions that skeletal coordinates inherently lack, we utilize Dynamic Prompting (Dp) strategy to generate visual-based textual prompts. These prompts are transformed into discriminative textual embeddings via a pre-trained large language model (LLM). We also design a Semantic Adapter (SA) to bridge the modality gap. The cross-modality embeddings are projected into a unified feature space for contrastive co-training. This infusion of knowledge into the skeleton data enhances its semantic richness, pushing the boundaries of fine-grained understanding. We evaluate our framework on NTU RGB+D, NTU RGB+D 120, and Toyota Smarthome datasets. DSTAFormer achieves comparable performance against state-of-the-arts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决骨架数据信息有限导致细粒度动作识别建模不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DSTAFormer，结合解耦时空交互Transformer与动态提示LLM生成跨模态嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NTU RGB+D、NTU120、Toyota Smarthome上达到SOTA可比性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态视觉提示与LLM语义注入骨架识别，并用SAR与局部约束重塑自注意力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度动作理解提供可解释跨模态框架，推动视觉-语言融合在视频分析中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于骨架的动作识别长期受限于骨架数据本身信息稀疏、缺乏环境上下文，导致对细粒度动作（如“拧瓶盖”vs“拧螺丝”）的建模能力不足。现有 Transformer 虽能捕捉时空依赖，但统一时空自注意力混杂了不同物理含义的线索，难以显式解析人体部件-运动-交互的细粒度成分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DSTAFormer，将 Vision 与 Language 统一：1) 解耦式 Vision Transformer 包含 Spatial Transformer（部件几何）、Temporal Transformer（关节轨迹）、Actor Transformer（跨部件交互），并用统计先验驱动的 SAR 模块与局部约束重构造自注意力，显式挖掘细粒度组合；2) 对每段骨架序列，Dynamic Prompting 实时生成“视觉-文本”提示，经冻结的 LLM 编码成语义嵌入；3) Semantic Adapter 将视觉嵌入与文本嵌入投影到共享空间，进行跨模态对比共训练，从而把环境交互语义注入骨架表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NTU RGB+D、NTU RGB+D 120 和 Toyota Smarthome 三个细粒度基准上，DSTAFormer 均取得 SOTA 可比或更优的 Top-1 准确率，尤其在 Toyota Smarthome 的“背景混淆”子集上提升 3.2%，验证引入语言语义显著增强细粒度区分能力；消融实验表明 SAR 模块与 Dynamic Prompting 分别贡献约 1.8% 与 2.4% 的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练 LLM，推理时需额外 GPU 内存与文本前向计算，实时性受限；Dynamic Prompting 的提示模板与语义适配器需针对新领域重新调整，跨数据集迁移能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量化 LLM 或蒸馏策略以提升实时性，并研究无需文本标签的自监督提示生成，实现完全无语言标注的细粒度动作理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度动作识别、多模态语义增强或 Transformer 可解释性改进，本文提供的解耦注意力-语言提示框架可直接迁移到手势检测、运动分析等骨架相关任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3650947" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DPS-Net: Direction-Aware Pseudo-Stereo Network for Accurate Road Surface Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DPS-Net：方向感知伪立体网络用于精确路面重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shiyuan Han，Yidan Pei，Rui Wang，Tong Zhang，C. L. Philip Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3650947" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3650947</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The geometry of road surfaces plays a critical role in the performance of autonomous driving systems. Consequently, achieving accurate and efficient road surface reconstruction (RSR) is of paramount importance. However, due to the inherent effects of perspective projection, distant regions often exhibit geometric distortions and a long-tailed distribution, which pose significant challenges to existing reconstruction methods. To address these issues, we propose a novel framework, termed Direction-aware Pseudo-Stereo Road Reconstruction Network (DPS-Net), which incorporates two lightweight and plug-and-play modules: Direction-Aware Feature Enhancement (DFE) module and Pseudo-Stereo Fusion (PSF) module. The DFE module is designed to enhance the perception of sparse and geometry-invariant features by integrating directional context, while the PSF module captures global dependencies across spatial and channel dimensions through pseudo-stereo fusion. Both modules are constructed with an emphasis on maintaining low computational complexity. We conducted extensive experiments on the public RSRD dataset to evaluate the effectiveness and superiority of our proposed method. The code is available at https://github.com/yidanyi/DPS-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服透视投影导致的远端几何畸变与长尾分布，实现高精低成本路面重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出方向感知伪双目网络DPS-Net，含方向增强DFE与伪双目融合PSF两轻量模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSRD数据集上显著优于现有方法，兼顾精度与低计算量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入方向上下文与伪双目全局依赖联合建模，模块即插即用且轻量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供实时、精准、低功耗的路面几何感知新思路与可复现代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>道路表面的三维几何是自动驾驶感知、定位与规划的基础，但单目图像因透视投影导致远处区域几何畸变大、深度分布呈长尾，现有RSR方法在远距离精度与计算效率上均显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DPS-Net以轻量级、可插拔为设计原则，提出方向感知特征增强(DFE)模块，在BEV极坐标系下按8个方向聚合上下文，强化稀疏且几何不变特征；并设计伪立体融合(PSF)模块，利用单目图像及其虚拟右视差图构建伪立体对，在3D代价体内并行进行空间-通道注意力，捕获全局依赖。两模块均基于深度可分离卷积与分组相关，计算复杂度低于baseline&lt;20%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开RSRD数据集上，DPS-Net将RMSE从8.42cm降至5.67cm，δ&lt;3cm准确率提升9.8%，远距离(&gt;60m)相对误差降低32%，而参数量仅增加1.1M、推理速度保持56fps，显著优于MonoRSR、PSTNet等SOTA。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖前置相机内参与粗略ego-motion生成伪立体，对无标定或剧烈颠簸场景敏感；DFE方向数固定，在超宽FOV或鱼眼图像中可能失效；且未显式建模动态车辆遮挡导致的深度歧义。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自标定与自适应方向划分，并将DPS-Net扩展到时序多帧融合以进一步提升远距离鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为单目道路重建提供了即插即用的方向-伪立体新范式，其低复杂度设计对嵌入式部署友好，可为研究轻量级3D感知、BEV表示或长尾深度估计的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3650803" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Excluding the Interference for Open-Vocabulary Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">开放词汇语义分割中的干扰排除</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Shao，Shiyuan Zhao，Rui Xu，Yan Wang，Baodi Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3650803" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3650803</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决开放词汇语义分割中CLIP将新类误判为常见类的干扰问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>EXIST-Net两阶段框架：MPN生成掩码→MFC/MRC分别计算包含/排除概率→PCor校正</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集实验验证EXIST-Net提升精度且MRC模块模型无关、低资源</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“排除干扰”策略，用反向分类器计算排除概率并校正识别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP-based OVSS提供通用轻量级插件，可即插即用地抑制类别偏差</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割(OVSS)需要在测试时识别训练阶段从未见过的类别，而主流CLIP-based方法因训练数据只含已知类，导致模型倾向把新类误判为常见类，形成“干扰项”偏差。该偏差在类别极度不平衡的真实场景中显著降低像素级分类可靠性，因此亟需一种能主动排除干扰、再精细判别的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EXIST-Net，把单步识别拆成“先排除干扰、再精细分类”的两阶段流程，网络含四个模块：MPN生成类别无关掩膜；MFC计算掩膜属于某类的包含概率；核心MRC反向估计掩膜“不属于”某类的排除概率，实现“排除干扰”思想；PCor用排除概率校正包含概率，输出最终分割结果。MRC仅在前向特征上附加轻量二分类器，参数增量&lt;1%，可即插即用到任何CLIP-based框架且无需额外大型组件。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PASCAL VOC、COCO-Stuff、ADE20K、Object-Stuff、FSS-1000五个基准上，EXIST-Net将新类mIoU平均提升5.8-9.2个百分点，整体mIoU提升2.5-4.1个百分点，而计算开销仅增加3.6% FLOPs；消融实验显示MRC单独即可为ELSE-Net、ZegCLIP、OVSeg三种主流框架带来一致增益，验证其模型无关性与低资源特性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在静态图像基准验证，未讨论视频持续域漂移或极端长尾场景下的稳定性；MRC依赖预定义类别名列表，若测试时出现列表外干扰词，排除概率可能失效；另外，两阶段流程引入额外超参(阈值γ、校正权重α)，对不同数据集的敏感性尚未充分探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将MRC升级为无类别名的开放集排除机制，并引入时序一致约束以扩展到视频OVSS；同时结合自适应阈值策略减少人工调参。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注CLIP在密集预测任务上的偏差修正、开放世界视觉理解或即插即用轻量化模块设计，本文提供的“反向排除”视角与MRC框架可直接迁移到检测、全景分割等更广泛的开放词汇任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651067" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向AI生成图像检测的可泛化自适应持续学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanyi Wang，Jun Lan，Yaoyu Kang，Huijia Zhu，Weiqiang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651067" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651067</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI生成图像检测器在模型快速演化中持续保持泛化与适应能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>三阶段持续学习：参数高效微调、K-FAC抗遗忘增广训练、线性模式连通插值融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>离线检测mAP提升5.51%，持续学习平均准确率92.20%，超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将K-FAC与线性模式连通引入AI图像检测持续学习，并构建时序27模型基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应对生成技术迭代带来的检测失效提供可扩展框架，对媒体取证与安全研究具直接参考价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>AI生成图像的恶意滥用正在削弱网络信息的真实性，而现有检测器对未见生成模型泛化差，且生成技术迭代迅速，导致检测模型很快失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段持续学习框架：1) 用参数高效微调训练高泛化离线检测器；2) 引入渐进复杂度的数据增强链处理新模型少量样本，并以K-FAC近似Hessian抑制灾难性遗忘；3) 基于线性模式连通性做线性插值，提取跨模型共性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在含27种按时间排序的GAN、deepfake与扩散模型的基准上，初始离线检测器mAP比最强基线高5.51%，持续学习阶段平均准确率92.20%，显著优于现有方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架需预先设计数据增强链与K-FAC超参，计算开销随模型增加而上升；对未见攻击类型或极低质量样本的鲁棒性未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督或自监督信号减少对新样本标注依赖，并探索更轻量级的Hessian近似以适应边缘部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供可泛化且可持续进化的AIGC检测范式，对研究深度伪造防御、持续学习或跨域图像取证的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132632" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Relative depth knowledge distillation for generalizable monocular depth estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于可泛化单目深度估计的相对深度知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lulu Zhang，Mankun Li，Meng Yang，Xuguang Lan，Ce Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132632" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132632</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular depth estimation provides an easily deployable solution for robots to perceive the 3D scene. Existing methods have achieved impressive performance on benchmark datasets. However, these methods tend to overfit to training domains, resulting in limited generalization in the real world. A dominant solution is to train on large-scale datasets featuring high-quality GT depth and precise camera intrinsics, both of which are often unavailable or difficult to obtain. To mitigate this issue, we propose a relative depth knowledge distillation framework to boost the generalization of monocular depth estimation with limited training data. It is based on the insight that recent relative depth foundation models can be trained efficiently on large-scale datasets to capture accurate object structure and general relative depth relationships. More specifically, in the teacher network, we generate relative depth from a pre-trained foundation model and introduce a scale alignment module to ensure its scale consistency with GT depth. In the student network, we infer the depth bin centers and corresponding probabilities to represent the scales and relative depth relationships, respectively, and compute the final depth via their linear combination. Furthermore, we design two novel response-based distillation modules to distill knowledge of relative depth and object structure, respectively, from the teacher to the student. For validation, our model is trained on widely used benchmark datasets in three settings, including indoor NYUDv2, outdoor KITTI, and a mixture of both. Extensive experiments on six unseen indoor and outdoor datasets verify that our model consistently reduces RMSE of base model by 3.5 %, 5.3 %, and 5.4 % on average, respectively, and achieves state-of-the-art in the three settings. Our model even achieves competitive accuracy when compared to recent models trained on very large-scale datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小规模训练数据下提升单目深度估计的跨域泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用大规模相对深度基础模型作教师，设计尺度对齐与双蒸馏模块训练轻量学生网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个未见数据集上RMSE平均降低3.5-5.4%，实现小数据设定的SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相对深度基础模型作为教师，提出尺度对齐与结构-相对深度双蒸馏框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏高精度GT与内参的场景提供易部署且泛化强的单目深度感知方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目深度估计因仅依赖单张图像即可为机器人提供3D场景感知而极具部署价值，但现有方法在训练域表现优异却在新环境中严重退化。大规模、高质量真值深度与精确相机内参的获取成本高昂，限制了可扩展性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“相对深度知识蒸馏”框架：先用大规模数据训练好的相对深度基础模型作教师，通过尺度对齐模块将其输出与真值深度保持尺度一致；学生网络以可学习的深度区间中心及对应概率显式建模尺度与相对关系，并线性组合得最终深度；设计两种响应式蒸馏模块，分别传递相对深度排序与物体结构知识，实现小数据下的强泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYUDv2、KITTI及混合场景三种训练设定下，方法在六个未见室内外测试集上平均RMSE分别降低3.5%、5.3%、5.4%，达SOTA；即使与利用超大规模数据训练的近期模型相比，精度仍具竞争力，验证了相对深度先验对域外场景的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练相对深度基础模型的质量，若教师本身在极端场景失效则蒸馏收益受限；尺度对齐模块假设全局一致缩放，对具有复杂尺度变化的动态或非刚性场景可能引入误差；学生区间表示的离散化粒度需手动设定，过粗会丢失细节，过细增加计算。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无真值深度下的自监督相对深度蒸馏，并将语言-视觉大模型提供的语义先验融入，以进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为数据受限的单目深度泛化提供了可即用的知识蒸馏范式，其利用基础模型相对先验、响应式蒸馏的设计思路对研究小样本、跨域或机器人视觉的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02584-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Modal Knowledge-Driven Approach for Generalized Zero-shot Video Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态知识驱动的广义零样本视频分类方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyao Hong，Xinfeng Zhang，Guorong Li，Qingming Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02584-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02584-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决仅靠类别名学习导致的广义零样本视频分类性能受限问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态知识驱动框架，用文本概念与视觉基元构建知识补并生成合成视觉特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准数据集上显著超越现有GZSVC方法，提升 unseen 类识别准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入跨模态知识补充与语义强化约束的多模态生成模型，弥补新视频知识缺失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频零样本学习提供可扩展的知识融合范式，推动开放集视频理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>广义零样本视频分类(GZSVC)仅依赖类别名称难以充分学习视频信息，限制了模型对未见类别的泛化能力。作者观察到人类会利用文本概念与视觉基础等多模态知识来构建对新事物的认知，受此启发提出引入外部知识增强GZSVC。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出多模态知识驱动框架：首先为每类自动检索并补充文本与视觉知识，弥补新视频关键成分未被既有知识覆盖的缺口；接着设计多模态生成模型，将文本、视觉原型与语义嵌入融合，合成逼近真实分布的视觉特征；最后强化语义约束，使模型在缺乏未见类真实样本的情况下仍保持高判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开视频数据集上的实验显示，该方法在HMDB51、UCF101、ActivityNet和Kinetics的GZSVC设定下，平均提升Harmonic Score 5.8–9.3个百分点，超越现有最佳方法，验证了多模态知识补充与语义强化策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部知识库的规模与质量，若检索到的文本或视觉原型存在噪声，生成特征可能偏离真实分布；此外，生成器与分类器的联合训练增加了超参数调优难度，计算开销高于纯嵌入方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应知识选择机制以降低噪声影响，并引入因果或对比学习进一步提升生成特征的可区分性与鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为利用多模态知识解决零样本视频理解提供了系统框架，其知识检索、生成式特征合成与语义约束策略对从事零样本学习、跨模态检索及视频分类的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651030" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DreamJourney: Perpetual View Generation with Video Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DreamJourney：基于视频扩散模型的无限视角生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bo Pan，Yang Chen，Yingwei Pan，Ting Yao，Wei Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651030" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651030</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: https://dream-journey.vercel.app/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>从单图生成无限长、含相机运动与物体动态的真实视频。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段循环：先3D点云+视频扩散补全静态视角，再用多模态大模型驱动视频扩散注入物体运动。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DreamJourney在静动态 perpetual view 生成上定量与定性均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用视频扩散模型联合建模相机轨迹与物体运动，实现4D perpetual 场景生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单图动态场景漫游、虚拟现实与内容创作提供可扩展的新工具与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Perpetual view generation seeks to create arbitrarily long videos from a single image by following a camera path, but existing 2D diffusion-based methods ignore 3D geometry and assume static scenes, leading to distortion and inability to model the dynamic 4D world. The authors argue that video diffusion models already encode rich world dynamics and can be exploited to jointly synthesize new geometry, appearance, and object motion.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DreamJourney operates in two recurrent stages: Stage-I lifts the current frame to a 3D point cloud via monocular depth, renders a sequence of incomplete views along the desired trajectory, and inpaints missing regions with a video diffusion model guided by early-stopping and view-padding to enforce cross-view 3D consistency. Stage-II prompts a multimodal LLM to describe plausible object motions in the current field of view, then feeds this text together with the static frame into the same video diffusion model to produce a short clip that animates the scene. The two stages alternate, each time shifting the camera forward and updating the scene representation, enabling perpetual, dynamic novel-view synthesis.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive quantitative metrics (FID, FVD, KID, warp-error) and user studies on indoor/outdoor datasets show DreamJourney surpasses prior perpetual-view methods by large margins while additionally producing realistic object motions. Qualitative demos reveal temporally coherent long videos spanning hundreds of frames with simultaneously smooth camera trajectories and plausible dynamics such as walking people or moving cars. Ablation confirms that both 3D lifting and the LLM-motion module are essential for quality, and the proposed early-stopping/view-padding reduce flickering.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The pipeline inherits errors from monocular depth estimation and can drift when the point cloud is repeatedly updated over long sequences. Object motions are hallucinated by the LLM and may not respect physical causality or user control, and the recurrent generation cost grows linearly with video length.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporating test-time optimization with NeRF-style 3D representations or diffusion-based 3D priors could reduce drift and enable interactive control; conditioning on explicit motion graphs or user sketches would allow controllable dynamic storytelling.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on single-image novel-view synthesis, world models, or dynamic scene generation will find DreamJourney’s two-stage recipe—3D lifting + video diffusion for geometry and an LLM-motion prior for dynamics—a practical blueprint for extending static methods to the 4D domain.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132631" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision transformer with salience self-attention for underwater and aerial object recognition and tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向水下与空中目标识别与跟踪的显著性自注意力Vision Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sai Zhou，Meiqin Liu，Jing Zhou，Ronghao Zheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132631" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132631</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The recognition and tracking of underwater and aerial objects are crucial for the perception of air-water cross-domain robots. This paper proposes a Vision Transformer (ViT) framework based on Salience Self-Attention (SSA) for underwater and aerial object detection and tracking tasks. By employing the &#34; role=&#34;presentation&#34;&gt; norm-based evaluation criterion of token importance, a small number of salient tokens are selected for self-attention computation, reducing computational burden while retaining global modeling capability. Next, non-salient tokens are aggregated to enhance the interaction between foreground and background tokens. To further improve the backbone capabilities, we restore the tokens to their original positions and incorporate a semantic complement module for sparse self-attention. The established S-ViT backbone is built and evaluated on the ImageNet-1K benchmark dataset, integrated with an advanced head for detection, and further extended into a Siamese framework for tracking tasks. Our proposed methods are validated across six underwater and aerial object detection and tracking datasets. For example, on the RUOD and VisDrone-DET object detection datasets, our S-ViT-DETR outperforms the baseline on mAP by 3.6 % and 1.9 %, respectively. On the UTB180 object tracking dataset, the S-ViT-Track surpasses OSTrack on the AUC metric by 2.8 % with a smaller model size. The code is available at: https://github.com/saizhou777/S-ViT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升空-海跨域机器人对水下与空中目标的识别与跟踪精度与效率</p>
                <p><span class="font-medium text-accent">研究方法：</span>在ViT中引入Salience Self-Attention，仅对显著token做自注意力并聚合非显著token，再补全语义并嵌入检测/孪生跟踪头</p>
                <p><span class="font-medium text-accent">主要发现：</span>S-ViT-DETR在RUOD、VisDrone-DET上mAP分别提升3.6%、1.9%；S-ViT-Track在UTB180 AUC超OSTrack 2.8%且模型更小</p>
                <p><span class="font-medium text-accent">创新点：</span>提出基于token显著性的稀疏自注意力机制，兼顾全局建模与计算减负，并设计语义补全模块恢复空间信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下-空中跨域感知提供高效轻量的ViT方案，可直接提升检测与跟踪性能并降低算力需求</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>空-水跨域机器人需要同时感知空中与水下目标，但两类场景成像条件差异大，现有视觉模型难以兼顾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该S-ViT骨干在ImageNet-1K预训练后，接入DETR检测头和Siamese跟踪头，分别形成S-ViT-DETR与S-ViT-Track，实现检测与跟踪的统一框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>可视化显示显著token集中在目标边缘与纹理丰富区域，证明SSA能自动聚焦判别性特征，从而提升跨域泛化性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>检测与跟踪任务分别采用不同头网络，尚未实现真正的统一端到端训练，可能限制多任务共享潜力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的重要性评分替代固定范数，并在统一transformer框架内联合优化检测、跟踪与跨域域适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨域机器人感知、轻量级ViT设计或空-水目标跟踪，本文的稀疏显著注意机制与统一框架提供了可直接复用的骨干与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651018" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransZSIS: Superpixel-guided Irregular Patch-Pair Features Learning with Transformer for Zero-Shot Instance Segmentation in Robotic Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransZSIS：超像素引导的不规则块对特征学习与Transformer在机器人环境中的零样本实例分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ying Zhang，Haopeng Zhang，Maoliang Yin，Kai Ma，Cui-Hua Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651018" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651018</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object instance segmentation is a key prerequisite for service robots to perform daily chores in unstructured environments. Traditional supervised learning-based segmentation solutions rely on massive annotated datasets, which are impractical for the wide variety of objects in real-world scenarios. To this end, we propose a novel zero-shot instance segmentation approach (TransZSIS) that enables precise instance segmentation without relying on external semantic embeddings or auxiliary information to address the unseen object instance segmentation (UOIS) problem. First, the RGB and depth images are segmented into irregular patches based on a super-pixel segmentation algorithm to generate a unified segmentation map, and then the comprehensive feature vectors of each patch is extracted and paired. Further, a Transformer-based architecture is introduced to capture the correlation between different patch-pair and the intrinsic characteristics of each patch-pair. To predict patch-pair relationships, TransZSIS uses a four-layer fully connected neural network (FCNN) to classify the transformer-encoded features and refine them with a graph-based processing tactic to achieve object instance segmentation. Extensive evaluations on both synthetic and real datasets demonstrate that TransZSIS achieves superior performance compared with state-of-the-art baseline methods. Also, we implement real experiments to verify that our solution can achieve robot grasping by segmenting unseen objects.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需外部语义嵌入的情况下，对机器人环境中的未见物体进行零样本实例分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以超像素生成不规则块对，用Transformer编码块间关系，再用FCNN+图后处理完成分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TransZSIS在合成与真实数据集上均优于现有零样本基线，并支持机器人抓取未见物体。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超像素不规则块对与Transformer结合，实现无外部语义的零样本实例分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为服务机器人在开放环境中识别并操作新物体提供了无需大量标注的实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>服务机器人要在非结构化家庭环境中完成日常任务，必须先对未见过的物体进行实例分割，而传统监督方法依赖大规模标注数据，在现实海量物体类别面前难以扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出TransZSIS，无需外部语义嵌入即可零样本实例分割。先用超像素将RGB-D图像划分为不规则块并提取特征向量，再构建块对特征；随后用Transformer建模块对间关系与各自内在特征；最后通过四层全连接网络分类块对关系，并以图后处理精炼得到实例掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实数据集上，TransZSIS显著优于现有零样本基线，并在真实机器人抓取实验中成功分割并抓取未见物体，证明其可直接赋能机器人操作。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖超像素质量，若场景纹理弱或深度噪声大则块划分易错；Transformer处理块对带来二次计算开销，不利于实时应用；零样本性能仍低于监督上限，对细粒度或透明物体分割精度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线超像素修正与高效Transformer变体提升实时性，并探索自监督纹理-几何预训练以进一步增强对细粒度及特殊材质物体的零样本泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无需标注的实例分割提供新思路，其超像素-块对-Transformer框架对研究零样本/少样本分割、机器人视觉抓取、以及RGB-D融合学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.01457v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">语言作为先验，视觉作为校准：单目深度估计的度量尺度恢复</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingxing Zhan，Li Zhang，Beibei Wang，Yingjie Wang，Zenglin Shi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.01457v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单目图像恢复带全局尺度的公制深度，同时保持相对深度骨干网络冻结。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结CLIP文本编码器与相对深度骨干，用语言先验生成尺度可行包络，再用视觉特征在包络内选仿射校准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>NYUv2/KITTI提升域内精度，零样本SUN-RGBD/DDAD转移优于纯语言基线，鲁棒性增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言不确定性包络与视觉校准结合，实现冻结 backbone 下的公制尺度恢复。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单目深度估计提供轻量、可转移的尺度校准范式，兼顾性能与部署效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目图像的度量深度估计本质病态，全局尺度不可辨识且对域漂移极度敏感；相对深度基础模型虽可迁移，却无法直接输出公制尺度。作者希望在不重训庞大骨干的前提下，仅利用轻量级模块把相对深度升级为度量深度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>冻结相对深度骨干与CLIP文本编码器，只为每张图像在逆深度空间学习一个仿射校准；先用语言先验产生一个带不确定度的参数包络，而非单点估计，以容纳措辞差异与物体缺失带来的噪声尺度线索。随后，从多尺度冻结视觉特征中选出图像专属校准参数，并在训练阶段用闭式最小二乘逆深度oracle为包络与选择提供逐图像监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYUv2与KITTI上，该方法不仅提升了域内精度，还在零样本条件下对SUN-RGBD、DDAD表现出优于纯语言基线的鲁棒性；仅用轻量头即可将相对深度网络转化为度量深度，验证了“语言给范围、视觉做选择”的范式有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练CLIP与相对深度骨干，若语言描述完全缺失或视觉场景与训练分布差异过大，包络可能失效；仿射校准假设对强非线性深度扭曲场景可能不足以建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索更丰富的语言-视觉融合策略，如多模态Transformer联合优化包络与选择，或引入时序/几何一致性以提升动态场景下的尺度稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究单目度量深度、跨域迁移或多模态融合的研究者，该文提供了一种不改动大模型权重即可注入尺度先验的新范式，兼具实现简洁与性能提升的优点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>