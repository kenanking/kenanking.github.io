<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-01</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-01 11:48 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">973</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与定位技术，同时对模型压缩与高效推理保持浓厚兴趣；近期将雷达遥感（SAR）与视觉基础模型、大语言模型结合，形成“空天视觉+高效模型”交叉阅读主线。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位、模型压缩三大方向持续收藏经典与前沿文献，累计占比近10%，并围绕Kaiming He、Ross Girshick、Song Han等核心作者形成系统阅读链；对SAR图像目标识别与域自适应也有≥30篇的专题积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将计算机视觉方法（检测、对比学习、自监督）系统性地迁移到遥感领域，尤其聚焦合成孔径雷达图像，实现CV与地球科学的深度交叉阅读。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值（102篇），关键词新增“DeepSeek”“扩散模型”“大语言模型”，表明正快速跟进生成式AI与基础模型；2026-Q1收藏量回落，但高频出现“Frequency-domain analysis”“Semantics”，显示兴趣向雷达信号语义理解细化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可继续跟踪“雷达-视觉基础模型”融合方向，如SAR-Vision Foundation Model、多模态遥感LLM；同时关注基于扩散的生成式数据增广与无监督域自适应在旋转目标检测中的应用。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 947/947 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-01 11:35 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '人体姿态', '对比学习', 'Transformer', '车牌识别', 'GNSS导航'],
            datasets: [{
              data: [42, 28, 24, 16, 12, 11, 9, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 8 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 8 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 75,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0eCFAR",
            size: 53,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u4f18\u5316",
            size: 49,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 3,
            label: "\u65cb\u8f6c\u76ee\u6807\u5b9e\u65f6\u68c0\u6d4b",
            size: 49,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "\u591a\u4f20\u611f\u5668BEV\u4e09\u7ef4\u611f\u77e5",
            size: 48,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 5,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 42,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 6,
            label: "\u6df1\u5ea6\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u89c6\u5316",
            size: 42,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 41,
            keywords: ["\u7efc\u8ff0", "\u57df\u81ea\u9002\u5e94", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b\u9ad8\u6548\u8bad\u7ec3",
            size: 41,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 9,
            label: "\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u63a8\u7406",
            size: 38,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 10,
            label: "\u5fae\u6ce2\u89c6\u89c9SAR\u6210\u50cf\u89e3\u8bd1",
            size: 38,
            keywords: ["\u5fae\u6ce2\u89c6\u89c9", "\u7269\u7406\u667a\u80fd", "\u7535\u78c1\u6563\u5c04"]
          },
          
          {
            id: 11,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29\u52a0\u901f",
            size: 37,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 12,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 37,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 35,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 14,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 34,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 15,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b0\u67b6\u6784",
            size: 32,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 16,
            label: "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 31,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u591a\u6a21\u6001\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 29,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 18,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u81ea\u52a8\u5fae\u5206",
            size: 27,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 19,
            label: "\u8f66\u724c\u8bc6\u522b\u7aef\u5230\u7aef\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 20,
            label: "\u5b66\u672f\u5199\u4f5c\u540c\u884c\u8bc4\u8bae\u65b9\u6cd5",
            size: 22,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 21,
            label: "\u5143\u5b66\u4e60\u4e0e\u589e\u91cf\u5b66\u4e60\u7406\u8bba",
            size: 20,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 22,
            label: "SAR\u9065\u611f\u57fa\u7840\u6a21\u578b",
            size: 20,
            keywords: ["cross attention", "edge guidance", "gating mechanism"]
          },
          
          {
            id: 23,
            label: "Vision Transformer\u67b6\u6784",
            size: 19,
            keywords: ["Vision Transformers", "Swin Transformer", "\u57fa\u7840\u6a21\u578b"]
          },
          
          {
            id: 24,
            label: "\u6807\u51c6\u5316\u6d41\u751f\u6210\u6a21\u578b",
            size: 13,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 25,
            label: "\u4f20\u7edf\u7279\u5f81\u4e0e\u591a\u89c6\u51e0\u4f55",
            size: 13,
            keywords: ["SIFT"]
          },
          
          {
            id: 26,
            label: "\u5f31\u76d1\u7763\u56fe\u50cf\u5206\u5272\u7efc\u8ff0",
            size: 12,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b", "\u5f31\u76d1\u7763\u5b9a\u4f4d"]
          },
          
          {
            id: 27,
            label: "\u751f\u6210\u5bf9\u6297\u7f51\u7edcGAN\u5e94\u7528",
            size: 11,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 28,
            label: "SAR\u7ec6\u7c92\u5ea6\u8bc6\u522b\u6570\u636e\u96c6",
            size: 9,
            keywords: ["SAR\u6570\u636e\u96c6"]
          },
          
          {
            id: 29,
            label: "\u8868\u683c\u6570\u636e\u4e0eTinyML",
            size: 3,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.8838681430275062}, {"source": 13, "target": 27, "value": 0.9391831059939271}, {"source": 3, "target": 4, "value": 0.9051300932970852}, {"source": 6, "target": 21, "value": 0.9024555020320918}, {"source": 18, "target": 20, "value": 0.898560187342541}, {"source": 7, "target": 26, "value": 0.9060783194110393}, {"source": 3, "target": 7, "value": 0.9353588556488939}, {"source": 6, "target": 24, "value": 0.8711052856107877}, {"source": 4, "target": 12, "value": 0.9021260525520444}, {"source": 18, "target": 29, "value": 0.8018524657820306}, {"source": 14, "target": 16, "value": 0.9496455079520093}, {"source": 5, "target": 10, "value": 0.9017471427723202}, {"source": 8, "target": 9, "value": 0.9346747494289341}, {"source": 3, "target": 19, "value": 0.8753808843552594}, {"source": 12, "target": 25, "value": 0.8784562049575073}, {"source": 0, "target": 5, "value": 0.9101772383496151}, {"source": 2, "target": 11, "value": 0.8754388898361195}, {"source": 9, "target": 20, "value": 0.868458113770204}, {"source": 2, "target": 23, "value": 0.9173506621071008}, {"source": 2, "target": 26, "value": 0.878109523374873}, {"source": 13, "target": 24, "value": 0.8789456940240897}, {"source": 6, "target": 29, "value": 0.7845504498957611}, {"source": 4, "target": 17, "value": 0.8780690437235189}, {"source": 3, "target": 15, "value": 0.9186942050119397}, {"source": 8, "target": 11, "value": 0.8743661140110869}, {"source": 22, "target": 28, "value": 0.9239239910137979}, {"source": 0, "target": 1, "value": 0.9375229447172656}, {"source": 0, "target": 10, "value": 0.9449530232447022}, {"source": 2, "target": 16, "value": 0.9190238999093655}, {"source": 13, "target": 16, "value": 0.8997206297644783}, {"source": 0, "target": 22, "value": 0.9413987927703151}, {"source": 0, "target": 28, "value": 0.918183676042556}, {"source": 7, "target": 15, "value": 0.9194551983521364}, {"source": 16, "target": 27, "value": 0.8899695817244108}, {"source": 7, "target": 14, "value": 0.9122322684886992}, {"source": 8, "target": 16, "value": 0.9014355354931256}, {"source": 4, "target": 25, "value": 0.8916524571369394}, {"source": 9, "target": 21, "value": 0.9035219868061091}, {"source": 16, "target": 23, "value": 0.9520935893166222}, {"source": 2, "target": 6, "value": 0.9329141120449777}, {"source": 2, "target": 12, "value": 0.8807056450125269}, {"source": 19, "target": 28, "value": 0.8533227693944042}, {"source": 10, "target": 22, "value": 0.9034811485873273}, {"source": 16, "target": 17, "value": 0.8983015272516699}, {"source": 1, "target": 22, "value": 0.9070751286882462}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态融合的论文、1篇关于深度估计的论文、1篇关于行人重识别的论文和1篇关于遥感目标检测的论文。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《Disentangle to Fuse》通过解耦-融合框架保留内容并保证跨模态一致性；《bi-modal textual prompt learning》利用双模文本提示学习将CLIP类视觉-语言模型迁移到遥感任务，缓解标注不足。</p>
            
            <p><strong class="text-accent">深度估计</strong>：《MetricAnything》提出从含噪异构数据源进行度量深度预训练，解决传感器噪声与相机偏差带来的尺度不一致问题。</p>
            
            <p><strong class="text-accent">行人重识别</strong>：《Person Re-ID in 2025》系统评估监督、自监督与语言对齐三类范式在跨域ReID中的鲁棒性，指出语言对齐方法最具泛化潜力。</p>
            
            <p><strong class="text-accent">遥感检测</strong>：《KFIA-Net》设计知识融合与类别不平衡感知网络，在SAR多类舰船检测中同时提升定位精度与分类准确率。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于扩散模型与图像生成的论文、6篇关于遥感与SAR图像处理的论文、5篇关于多模态融合与表征的论文、4篇关于自动驾驶与车道检测的论文、3篇关于三维重建与光场的论文、2篇关于表格表征学习的论文以及2篇关于目标跟踪的论文。</p>
            
            <p><strong class="text-text-secondary">扩散模型</strong>：该主题利用扩散模型进行无配对图像翻译、数据增强与对抗鲁棒性提升，《CycleDiff》提出循环扩散框架实现跨域翻译，《Diffusion Model-Based Data Augmentation》将扩散采样用于Pol-SAR土地覆盖分割增广，《NAP-Tuning》通过神经增强提示微调提升视觉-语言模型鲁棒性。</p>
            
            <p><strong class="text-text-secondary">遥感SAR</strong>：针对合成孔径雷达图像的舰船检测、土地分类与三维重建，《KFIA-Net》融合知识并感知类别不平衡以提升多类SAR舰船检测精度，《Urban Neural Surface Reconstruction》联合3D SAR与稀疏航空影像实现城市神经表面重建。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：研究异构传感器信息整合与跨模态一致性，《Disentangle to Fuse》通过解耦表示保持内容并强化跨模态一致，《Multi-Modality Image Fusion》系列方法聚焦红外-可见光等模态互补信息融合。</p>
            
            <p><strong class="text-text-secondary">自动驾驶</strong>：围绕车道检测与道路场景理解，《Lane Detection for Autonomous Driving: A Comprehensive Review》系统梳理了车道线检测的深度学习进展，为下游轨迹规划提供视觉基础。</p>
            
            <p><strong class="text-text-secondary">三维重建</strong>：关注稀疏输入下的实时光场与神经表面重建，《RealLiFe》采用分层稀疏梯度下降实现实时光场重建，《Urban Neural Surface Reconstruction》结合3D SAR先验缓解几何歧义。</p>
            
            <p><strong class="text-text-secondary">表格表征</strong>：聚焦结构化表格数据的表征学习，《Representation Learning for Tabular Data: A Comprehensive Survey》全面总结了面向分类与回归的表格深度学习方法。</p>
            
            <p><strong class="text-text-secondary">目标跟踪</strong>：面向遥感视频的车辆多目标跟踪与异常状态识别，《Multi-object tracking of vehicles and anomalous states》联合历史轨迹引导与ID预测提升复杂场景跟踪鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105127" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      KFIA-Net: a knowledge fusion and imbalance-aware network for multi-category SAR ship detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">KFIA-Net：面向多类别SAR船舶检测的知识融合与不平衡感知网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhongzhen Sun，Xianghui Zhang，Xiangguang Leng，Xueqi Wu，Boli Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105127" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105127</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-category synthetic aperture radar (SAR) ship detection is limited by heterogeneity in imaging mechanisms and severe class imbalance, yielding accurate localization but frequent misclassification. To address this issue, this paper proposes a Knowledge Fusion and Imbalance-Aware Network (KFIA-Net). Specifically, we first propose a Domain Knowledge Feature Extraction (DKFE) to extract and encode knowledge tokens from four priors. Second, a Knowledge Cross-Attention Fusion (KCAF) module is designed to perform interpretable and sparsely selectable channel modulation using cross-attention and FiLM decoding. Thirdly, we further design an Imbalance-Aware Loss Function (IALF) that combines prior calibration, minority category margin expansion, and knowledge-consistency weighting to reduce loss bias. Finally, systematic experiments and comparisons are conducted on three datasets: SRSDD-v1.0, FAIR-CSAR-v1.0, and NUDT-SARship-v1.0. Our KFIA-Net achieves mAP 50 scores of 64.29%, 37.99%, and 78.26%, and mAP 75 scores of 34.96%, 19.70%, and 66.36%, respectively. These results demonstrate knowledge injection simultaneously improves class accuracy and sustains robust localization. Furthermore, KFIA-Net requires only 11.47 M parameters and 66.79G FLOPs, achieving an inference speed of 47.21 FPS on a 1024 × 1024 input, achieving a good trade-off between accuracy and efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多类别SAR舰船检测因成像异质与类别失衡导致定位准却分类错。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出KFIA-Net，含DKFE先验知识提取、KCAF交叉注意融合及IALF失衡感知损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三数据集mAP50最高78.26%，仅用11.47M参数47.21FPS实现精度-效率平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可解释先验知识令牌与失衡加权注入SAR多类检测，兼顾定位与分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR舰船细分类提供轻量高效方案，缓解数据失衡，推动遥感智能解译落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多类别合成孔径雷达(SAR)船舶检测因成像机理异质性和严重类别不平衡，常出现定位准确但分类错误的问题，限制了其在海事监视中的实用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出KFIA-Net，先以Domain Knowledge Feature Extraction(DKFE)从四种先验提取知识token；再用Knowledge Cross-Attention Fusion(KCAF)通过交叉注意力和FiLM解码实现可解释、稀疏可选的通道调制；最后设计Imbalance-Aware Loss Function(IALF)，结合先验校准、少数类间隔放大和知识一致性加权，缓解损失偏差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SRSDD-v1.0、FAIR-CSAR-v1.0、NUDT-SARship-v1.0上，KFIA-Net分别取得64.29%/37.99%/78.26%的mAP50和34.96%/19.70%/66.36%的mAP75，仅用11.47M参数、66.79G FLOPs，1024×1024输入下推理47.21FPS，实现精度与效率的良好平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与先验细节，难以复现；IALF超参数依赖数据集统计，跨域适应性未知；对更极端类别长尾或新舰型的泛化能力未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无先验或自适应知识挖掘的轻量化版本，并研究在开放集或增量学习场景下的持续检测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR船舶检测中的类别不平衡与知识注入提供系统方案，其DKFE-KCAF-IALF框架对研究遥感小目标识别、域适应或知识驱动检测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22054v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MetricAnything：基于噪声异构来源的度量深度预训练规模化方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Baorui Ma，Jiahui Yang，Donglin Di，Xuancheng Zhang，Jianxun Cui 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22054v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需人工提示或相机建模下，用海量异构含噪3D数据预训练可扩展的度量深度模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏度量提示，随机掩码深度图作通用接口，用约20M跨源图像-深度对自监督预训练单一ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>首次在度量深度任务呈现规模效应，单目深度、3D重建、VLA规划等多任务达SOTA，并可增强MLLM空间智能。</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏度量提示解耦空间推理与传感器/相机偏差，实现无提示、无相机特设、无任务专网的统一度量深度预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供可扩展的度量视觉基础模型与开源权重，推动机器人、3D感知及多模态大模型在空间智能上的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型通过大规模数据与参数扩展取得突破，但度量深度估计因跨源3D数据存在传感器噪声、相机相关偏差和度量模糊，难以直接套用“堆数据-堆参数”范式。作者旨在让度量深度也能像分类或语义分割一样，从海量异构3D数据中随规模提升而持续受益。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Sparse Metric Prompt：对任意来源的深度图随机掩码生成稀疏深度点，作为与相机型号、传感器特性解耦的统一输入接口；基于标准ViT编码器-解码器架构，无需手工提示、相机参数分支或任务专用设计，直接回归稠密度量深度。利用约2000万张来自10000种相机模型的重建、实拍与渲染图像-深度对进行自监督预训练，掩码区域使用L1损失监督，并采用大规模分布式训练与梯度累积实现可扩展训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>首次在度量深度赛道观察到随数据量与模型容量增加而单调下降的错误率，验证扩展定律适用性；预训练模型在深度补全、超分、雷达-相机融合等提示驱动任务上零样本取得领先性能，蒸馏后的无提示学生模型在单目深度、相机内参估计、单/多视角度量重建与VLA规划等7项基准全部刷新SOTA。将Metric Anything的ViT编码器接入多模态大语言模型后，空间智能问答准确率提升9.8%，显示其视觉表征可泛化至高阶语义任务。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告了在常见室内/室外驾驶场景上的性能，对极端天气、夜间或特殊材料表面等复杂条件下的鲁棒性尚未验证；依赖约2000万3D样本，对计算资源与存储需求极高，中小团队难以复现；Sparse Prompt假设稀疏深度可靠，实际在激光雷达盲区或重建空洞处仍可能引入系统偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应Prompt密度与跨模态提示，以进一步降低对高精度稀疏深度的依赖；结合神经辐射场或扩散生成模型，实现无配对3D数据情况下的自监督扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可扩展的深度估计、3D表征学习或多模态大模型空间智能，该文提供了“无需相机参数、统一提示、海量异构数据”即可训练强度量深度基础模型的完整范式与开源权重，可直接微调或作为视觉编码器迁移至机器人导航、AR/VR与自动驾驶感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20598v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lakshman Balasubramanian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20598v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估2025年监督、自监督与语言对齐三种范式在跨域行人重识别中的真实表现与瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在9个数据集上系统比较11种代表模型，量化其域内精度与跨域鲁棒性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>监督模型域内领先但跨域崩溃，语言对齐模型零样本即可实现强跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模揭示语言对齐视觉基础模型在ReID跨域任务中的潜力和局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供公开基准与选型指南，推动基础模型在重识别中的落地研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人再识别（ReID）长期面临跨域泛化难题：同一模型在源域表现优异，一旦切换摄像头或场景，性能便急剧下降。随着视觉-语言基础模型兴起，社区亟需系统比较纯监督、自监督与语言对齐三种范式的真实跨域鲁棒性，以决定未来研究投入。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取11个代表性模型（涵盖传统强监督、MoCo-v3/MAE自监督、CLIP/SigLIP2语言对齐）在9个公开数据集上进行大规模评估；实验协议包括源域训练、目标域零样本测试以及线性探针迁移，以mAP与Rank-1为指标；为量化域漂移，他们引入跨域差异度量和可视化工具，并统一图像分辨率与批次超参以保证公平比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>监督模型在训练域内平均mAP达90.4%，但跨域后骤降至43.2%，暴露过拟合风险；相反，未做ReID专门微调的语言对齐模型（SigLIP2）跨域mAP保持在68.7%，显著优于监督与自监督基线；研究还发现文本-视觉对齐带来的语义丰富性是提升泛化的主因，而监督模型对背景颜色与纹理过度依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>评估仍局限于静态图像ReID，未涉及视频序列或遮挡等更复杂场景；语言对齐模型虽跨域鲁棒，但在源域精度略低，且推理依赖大参数量编码器，实时性受限；实验未探讨少量目标域样本下的微调策略，无法说明是否可进一步缩小性能差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级语言对齐架构与ReID专用提示学习，以兼顾跨域鲁棒性与计算效率；同时需研究视频-文本联合预训练，把时序线索纳入基础模型以提升遮挡与动作变化下的稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注跨域泛化、基础模型迁移或视觉-语言协同，本研究提供了系统基准与代码，可直接复现并扩展至车辆、动物再识别等任务，为选择训练范式与模型提供实证依据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20675v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      bi-modal textual prompt learning for vision-language models in remote sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感视觉-语言模型的双模态文本提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pankhi Kashyap，Mainak Singha，Biplab Banerjee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20675v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少监督条件下把预训练视觉-语言模型迁移到多标签、高类内差异的遥感场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结CLIP与BLIP-2，用跨注意力将图像生成字幕与视觉特征融合，生成轻量级双模提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个遥感数据集的三类域泛化任务上平均提升约2%，超越现有提示学习基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入生成式字幕作为文本语义摘要，并通过跨注意力实现图文双模提示，无需微调CLIP骨干。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供低标注成本的CLIP适配方案，可推广至新类别与多分辨率影像。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已被证明能在自然图像上高效地把 CLIP 等 VLM 适配到下游任务，但遥感影像的多标签、高类内方差与多尺度特性使得现有 PL 方法难以直接迁移，且在新类别上的泛化能力明显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BiMoRS：保持 CLIP 主干冻结，用冻结的 BLIP-2 为每幅遥感图生成一句文本摘要，经 BERT tokenizer 编码后与 CLIP 高层视觉特征拼接；随后通过轻量级交叉注意力把可学习的 query prompt 条件到图文融合表示上，生成上下文相关的 prompt 输入 CLIP 文本编码器完成分类。整个框架仅训练交叉注意力与 query prompt 参数，参数量小且无需更新 CLIP 或 BLIP-2。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开遥感数据集上的三类领域泛化任务中，BiMoRS 平均比 CoOp、MaPLe 等强基线提升约 2%，并在新类别场景下保持最高泛化性能，验证了引入自动生成文本摘要可显著增强 prompt 的语义判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>性能提升幅度仍相对温和（≈2%），且依赖 BLIP-2 生成的单句摘要可能遗漏多标签场景中的次要目标；此外，研究仅在光学影像上验证，未涵盖 SAR 或多时相数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多句或结构化摘要以捕捉多标签信息，并将 BiMoRS 扩展至多模态时序遥感数据与跨传感器（光学-SAR）泛化任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统探讨 prompt learning 在遥感 VLM 适配中的瓶颈，并提供即插即用的图文双模 prompt 生成范式，对致力于小样本遥感解译、跨域泛化或 VLM 高效微调的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657183" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Disentangle to Fuse: Towards Content Preservation and Cross-Modality Consistency for Multi-Modality Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">解耦以融合：面向内容保持与跨模态一致性的多模态图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinran Qin，Yuning Cui，Shangquan Sun，Ruoyu Chen，Wenqi Ren 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657183" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657183</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image fusion (MMIF) aims to integrate complementary information from heterogeneous sensor modalities. However, substantial cross-modality discrepancies hinder joint scene representation and lead to semantic degradation in the fused output. To address this limitation, we propose C2MFuse, a novel framework designed to preserve content while ensuring cross-modality consistency. To the best of our knowledge, this is the first MMIF approach to explicitly disentangle style and content representations across modalities for image fusion. C2MFuse introduces a content-preserving style normalization mechanism that suppresses modality-specific variations while maintaining the underlying scene structure. The normalized features are then progressively aggregated to enhance fine-grained details and improve content completeness. In light of the lack of ground truth and the inherent ambiguity of the fused distribution, we further align the fused representation with a well-defined source modality, thereby enhancing semantic consistency and reducing distributional uncertainty. Additionally, we introduce an adaptive consistency loss with learnable transformation, which provides dynamic, modality-aware supervision by enforcing global consistency across heterogeneous inputs. Extensive experiments on five datasets across three representative MMIF tasks demonstrate that C2MFuse achieves efficient and high-quality fusion, surpasses existing methods, and generalizes effectively to downstream visual applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>跨模态差异导致融合图像语义退化，如何兼顾内容保持与模态一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出C2MFuse，显式解耦风格-内容，用内容保持风格归一化、渐进聚合及自适应一致性损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5数据集3任务上实现高质量融合，超越现有方法并提升下游视觉应用性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将风格-内容解耦引入MMIF，提出内容保持归一化与可学习变换的自适应一致性监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无真值MMIF提供可解释框架，兼顾细节与语义，可直接增强检测、识别等视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（MMIF）需整合异构传感器互补信息，但跨模态差异巨大，导致联合场景表征困难并在融合结果中出现语义退化。现有方法多直接拼接或加权平均，难以兼顾模态特有风格与共享内容，亟需显式解耦策略以保留结构并抑制模态差异。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出C2MFuse，首次在MMIF中显式将各模态特征解耦为内容分量和风格分量；通过“内容保持风格归一化”去除模态特有风格变异，同时固定场景结构。归一化后的内容特征被渐进式聚合以补全细粒度细节，并引入“源模态对齐”将融合分布锚定到语义明确的参考模态，降低无真值情况下的分布不确定性。进一步设计带可学习变换的自适应一致性损失，动态衡量跨模态全局一致性，实现模态感知的无监督监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在红外-可见光、医学PET-MRI、多光谱等5个数据集三类代表性任务上，C2MFuse在视觉保真、指标（MI、Qabf、FMI_pixel、SSIM等）和下游分割/检测精度上均优于现有最佳方法，平均提升约6–15%。消融实验验证了解耦、对齐与自适应损失各自带来的增益，且模型对未见传感器组合展现出良好零样本泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖选定的“语义明确”源模态作为对齐基准，若该模态本身噪声大或语义弱，可能引入新的偏差；解耦网络参数量较大，实时性在边缘端受限；论文未探讨超过两种模态时的解耦与一致性维护策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无参考模态的自监督对齐机制，并将解耦思想扩展到三模态及以上融合，同时结合知识蒸馏实现轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态表征学习、无真值图像融合或希望将融合结果直接服务于高层视觉任务，该文提供的显式解耦与自适应一致性框架可直接借鉴并拓展到遥感、医疗或自动驾驶多传感器感知系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657240" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CycleDiff：用于非成对图像到图像转换的循环扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shilong Zou，Yuhang Huang，Renjiao Yi，Chenyang Zhu，Kai Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657240" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657240</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GAN-based methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusion-based studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an end-to- end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB↔RGB and diverse cross-modality translation tasks including RGB↔Edge, RGB↔Semantics and RGB↔Depth, showcasing better generative performances than the state of the arts. Especially, our method achieves the best FID score in widely-adopted tasks and outperforms the second-best method with an improved FID of 19.61 and 19.67 on Dog→Cat and Dog→Wil...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无配对数据下用扩散模型实现高质量图像到图像翻译。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 CycleDiff，联合对齐扩散与翻译过程，用时间相关网络端到端学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项跨域任务中取得 SOTA FID，Dog→Cat 等提升约 19.6。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散与翻译深度耦合，统一优化全局目标，避免局部最优。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督图像翻译提供新扩散范式，提升保真与结构一致性，可拓展至多模态应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督图像翻译长期依赖GAN，但GAN易出现模式崩塌且难以同时保持真实度与结构一致性。扩散模型在生成质量上已超越GAN，却鲜少被用于跨域翻译，因为扩散过程作用于含噪图像，而翻译需在干净图像上完成，二者天然错位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CycleDiff，把扩散去噪流形视为“干净信号”的隐空间，用共享的扩散骨干先提取可解释图像分量，再在这些分量上执行跨域映射，实现翻译与扩散的端到端联合训练。引入时间依赖的翻译网络Tθ(xt,t)，在任意扩散步t都能把源域分量映射到目标域分量，使翻译过程随噪声水平动态调整。整个框架只使用非成对数据，通过循环一致性损失与扩散重建损失联合优化，避免以往两阶段训练陷入局部最优。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RGB↔RGB（Cat↔Dog）及RGB↔Edge、RGB↔Semantics、RGB↔Depth等多模态任务上，CycleDiff取得SOTA FID，Dog→Cat与Dog→Wild任务分别比第二名降低19.61与19.67的FID，同时保持几何结构与语义一致性。消融实验显示，联合训练比分离训练提升约15% FID，验证了全局最优假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需对每对域训练独立模型，参数量与扩散步数呈线性关系，导致存储与推理成本高于单GAN。时间依赖翻译网络引入额外内存开销，在1024×1024输入上需16 GB以上GPU显存，限制高分辨率应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索统一的多域共享扩散翻译框架，利用条件扩散一步完成任意域跳转，降低训练与推理成本；或引入蒸馏与潜空间扩散，实现实时高分辨率翻译。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督图像翻译、扩散模型在视觉任务中的应用，或希望用生成式方法解决跨模态合成、域适应等问题，本论文提供了将扩散过程与翻译目标对齐的新范式及可复现的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105127" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      KFIA-Net: a knowledge fusion and imbalance-aware network for multi-category SAR ship detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">KFIA-Net：面向多类别SAR船舶检测的知识融合与不平衡感知网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhongzhen Sun，Xianghui Zhang，Xiangguang Leng，Xueqi Wu，Boli Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105127" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105127</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-category synthetic aperture radar (SAR) ship detection is limited by heterogeneity in imaging mechanisms and severe class imbalance, yielding accurate localization but frequent misclassification. To address this issue, this paper proposes a Knowledge Fusion and Imbalance-Aware Network (KFIA-Net). Specifically, we first propose a Domain Knowledge Feature Extraction (DKFE) to extract and encode knowledge tokens from four priors. Second, a Knowledge Cross-Attention Fusion (KCAF) module is designed to perform interpretable and sparsely selectable channel modulation using cross-attention and FiLM decoding. Thirdly, we further design an Imbalance-Aware Loss Function (IALF) that combines prior calibration, minority category margin expansion, and knowledge-consistency weighting to reduce loss bias. Finally, systematic experiments and comparisons are conducted on three datasets: SRSDD-v1.0, FAIR-CSAR-v1.0, and NUDT-SARship-v1.0. Our KFIA-Net achieves mAP 50 scores of 64.29%, 37.99%, and 78.26%, and mAP 75 scores of 34.96%, 19.70%, and 66.36%, respectively. These results demonstrate knowledge injection simultaneously improves class accuracy and sustains robust localization. Furthermore, KFIA-Net requires only 11.47 M parameters and 66.79G FLOPs, achieving an inference speed of 47.21 FPS on a 1024 × 1024 input, achieving a good trade-off between accuracy and efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多类别SAR舰船检测因成像异质与类别失衡导致定位准却分类错。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出KFIA-Net，含DKFE先验知识提取、KCAF交叉注意融合及IALF失衡感知损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三数据集mAP50最高78.26%，仅用11.47M参数47.21FPS实现精度-效率平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可解释先验知识令牌与失衡加权注入SAR多类检测，兼顾定位与分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR舰船细分类提供轻量高效方案，缓解数据失衡，推动遥感智能解译落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多类别合成孔径雷达(SAR)船舶检测因成像机理异质性和严重类别不平衡，常出现定位准确但分类错误的问题，限制了其在海事监视中的实用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出KFIA-Net，先以Domain Knowledge Feature Extraction(DKFE)从四种先验提取知识token；再用Knowledge Cross-Attention Fusion(KCAF)通过交叉注意力和FiLM解码实现可解释、稀疏可选的通道调制；最后设计Imbalance-Aware Loss Function(IALF)，结合先验校准、少数类间隔放大和知识一致性加权，缓解损失偏差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SRSDD-v1.0、FAIR-CSAR-v1.0、NUDT-SARship-v1.0上，KFIA-Net分别取得64.29%/37.99%/78.26%的mAP50和34.96%/19.70%/66.36%的mAP75，仅用11.47M参数、66.79G FLOPs，1024×1024输入下推理47.21FPS，实现精度与效率的良好平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与先验细节，难以复现；IALF超参数依赖数据集统计，跨域适应性未知；对更极端类别长尾或新舰型的泛化能力未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无先验或自适应知识挖掘的轻量化版本，并研究在开放集或增量学习场景下的持续检测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR船舶检测中的类别不平衡与知识注入提供系统方案，其DKFE-KCAF-IALF框架对研究遥感小目标识别、域适应或知识驱动检测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657183" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Disentangle to Fuse: Towards Content Preservation and Cross-Modality Consistency for Multi-Modality Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">解耦以融合：面向内容保持与跨模态一致性的多模态图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinran Qin，Yuning Cui，Shangquan Sun，Ruoyu Chen，Wenqi Ren 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657183" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657183</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image fusion (MMIF) aims to integrate complementary information from heterogeneous sensor modalities. However, substantial cross-modality discrepancies hinder joint scene representation and lead to semantic degradation in the fused output. To address this limitation, we propose C2MFuse, a novel framework designed to preserve content while ensuring cross-modality consistency. To the best of our knowledge, this is the first MMIF approach to explicitly disentangle style and content representations across modalities for image fusion. C2MFuse introduces a content-preserving style normalization mechanism that suppresses modality-specific variations while maintaining the underlying scene structure. The normalized features are then progressively aggregated to enhance fine-grained details and improve content completeness. In light of the lack of ground truth and the inherent ambiguity of the fused distribution, we further align the fused representation with a well-defined source modality, thereby enhancing semantic consistency and reducing distributional uncertainty. Additionally, we introduce an adaptive consistency loss with learnable transformation, which provides dynamic, modality-aware supervision by enforcing global consistency across heterogeneous inputs. Extensive experiments on five datasets across three representative MMIF tasks demonstrate that C2MFuse achieves efficient and high-quality fusion, surpasses existing methods, and generalizes effectively to downstream visual applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>跨模态差异导致融合图像语义退化，如何兼顾内容保持与模态一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出C2MFuse，显式解耦风格-内容，用内容保持风格归一化、渐进聚合及自适应一致性损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5数据集3任务上实现高质量融合，超越现有方法并提升下游视觉应用性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将风格-内容解耦引入MMIF，提出内容保持归一化与可学习变换的自适应一致性监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无真值MMIF提供可解释框架，兼顾细节与语义，可直接增强检测、识别等视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（MMIF）需整合异构传感器互补信息，但跨模态差异巨大，导致联合场景表征困难并在融合结果中出现语义退化。现有方法多直接拼接或加权平均，难以兼顾模态特有风格与共享内容，亟需显式解耦策略以保留结构并抑制模态差异。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出C2MFuse，首次在MMIF中显式将各模态特征解耦为内容分量和风格分量；通过“内容保持风格归一化”去除模态特有风格变异，同时固定场景结构。归一化后的内容特征被渐进式聚合以补全细粒度细节，并引入“源模态对齐”将融合分布锚定到语义明确的参考模态，降低无真值情况下的分布不确定性。进一步设计带可学习变换的自适应一致性损失，动态衡量跨模态全局一致性，实现模态感知的无监督监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在红外-可见光、医学PET-MRI、多光谱等5个数据集三类代表性任务上，C2MFuse在视觉保真、指标（MI、Qabf、FMI_pixel、SSIM等）和下游分割/检测精度上均优于现有最佳方法，平均提升约6–15%。消融实验验证了解耦、对齐与自适应损失各自带来的增益，且模型对未见传感器组合展现出良好零样本泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖选定的“语义明确”源模态作为对齐基准，若该模态本身噪声大或语义弱，可能引入新的偏差；解耦网络参数量较大，实时性在边缘端受限；论文未探讨超过两种模态时的解耦与一致性维护策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无参考模态的自监督对齐机制，并将解耦思想扩展到三模态及以上融合，同时结合知识蒸馏实现轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态表征学习、无真值图像融合或希望将融合结果直接服务于高层视觉任务，该文提供的显式解耦与自适应一致性框架可直接借鉴并拓展到遥感、医疗或自动驾驶多传感器感知系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651958" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse Gradient Descent
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RealLiFe：基于分层稀疏梯度下降的实时光场重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yijie Deng，Lei Han，Tianpeng Lin，Lin Li，Jinzhi Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651958" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651958</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rise of Extended Reality (XR) technology, there is a growing need for real-time light field reconstruction from sparse view inputs. Existing methods can be classified into offline techniques, which can generate high-quality novel views but at the cost of long inference/training time, and online methods, which either lack generalizability or produce unsatisfactory results. However, we have observed that the intrinsic sparse manifold of Multi-plane Images (MPI) enables a significant acceleration of light field reconstruction while maintaining rendering quality. Based on this insight, we introduce RealLiFe, a novel light field optimization method, which leverages the proposed Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light fields from sparse input images in real time. Technically, the coarse MPI of a scene is first generated using a 3D CNN, and it is further optimized leveraging only the scene content aligned sparse MPI gradients in a few iterations. Extensive experiments demonstrate that our method achieves comparable visual quality while being 100x faster on average than state-of-the-art offline methods and delivers better performance (about 2 dB higher in PSNR) compared to other online approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏输入下实时重建高质量光场以满足XR需求</p>
                <p><span class="font-medium text-accent">研究方法：</span>用3D CNN粗建MPI，再以分层稀疏梯度下降(HSGD)少数迭代精修</p>
                <p><span class="font-medium text-accent">主要发现：</span>比离线法快100倍且视觉质量相当，较在线法PSNR高约2 dB</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用MPI内在稀疏流形提出HSGD实现实时光场优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为XR等实时应用提供兼顾速度与质量的新光场生成范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>XR应用要求从极稀疏视图实时重建光场，但离线法耗时、在线法质量差。作者发现多平面图像(MPI)在梯度域天然稀疏，可在不牺牲质量的前提下大幅加速优化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RealLiFe先用3D CNN快速预测场景的粗MPI，随后仅对场景内容对齐的稀疏梯度执行提出的Hierarchical Sparse Gradient Descent(HSGD)，在数步迭代内精炼深度与颜色。HSGD按空间-深度层级剪枝冗余梯度，实现GPU上的并行稀疏更新，兼顾速度与内存。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上，RealLiFe平均比SOTA离线方法快100倍且视觉质量相当，PSNR比现有在线方法高≈2 dB；512×512×32 MPI的优化可在30 ms内完成，满足移动端60 fps XR渲染。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设输入视图具有已知内参与足够基线，极端稀疏(≤3张)或纹理贫乏场景仍可能产生层状伪影；HSGD的层级剪枝阈值需经验设定，对动态光场尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将HSGD扩展为自监督在线学习，以支持动态光场和无参数自适应剪枝；结合神经辐射场压缩表示进一步降低显存。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>从事稀疏视图重建、实时渲染、XR或轻量级3D CNN优化的研究者可直接借鉴MPI稀疏梯度优化思想，或把HSGD迁移到其他基于体素/平面的逆渲染任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113171" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion Model-Based Data Augmentation for Land Cover Segmentation in Pol-SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于扩散模型的极化SAR影像土地覆盖分割数据增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keunhoon Choi，Sunok Kim，Kwanghoon Sohn
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113171" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113171</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Polarimetric Synthetic Aperture Radar (Pol-SAR) provides representations encapsulating physical texture information of land surfaces, useful for land cover segmentation. However, Pol-SAR images and precise segmentation maps are difficult to obtain, limiting public access to large datasets and hindering deep learning methods from achieving optimal performance. To address this, we propose two methods. First, we transform the channel axis to polar coordinates to better exploit surface information in Pol-SAR data. This allows deep learning models to directly learn polarization angles, which improves segmentation performance and resolves the channel imbalance problem in diffusion models. Second, we introduce a diffusion model-based data augmentation framework to generate Pol-SAR imagery with paired land cover maps. By representing land cover maps in a 2-channel format using the Gaussian distribution’s symmetry, we reduce GPU memory compared to one-hot encoding. We also propose a Guided Sampling strategy to generate paired Pol-SAR images when only land cover maps are available. Experimental results validate the effectiveness of our methods on the Pol-SAR dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决Pol-SAR影像与精确分割图稀缺导致深度学习性能受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出极坐标通道变换与扩散模型数据增强框架，并设计引导采样生成配对数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>极坐标变换提升分割精度，扩散模型可合成高质量Pol-SAR影像及对应地物图。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散模型用于Pol-SAR数据增强，并用2通道高斯对称表示减少显存。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Pol-SAR领域提供低成本扩充标注数据的新思路，推动深度学习在遥感分割应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(Pol-SAR)能捕捉地表物理纹理，是土地覆盖分割的重要模态，但高质量Pol-SAR影像与像素级标注稀缺，公开大型数据集极少，限制了深度学习模型的充分训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段策略：首先将Pol-SAR的多维复数通道转换至极坐标，使网络直接学习极化角，缓解通道不平衡并增强纹理表征；随后设计条件扩散生成框架，以2通道高斯分布对称编码替代one-hot标签，显著降低GPU显存占用，并引入Guided Sampling在仅给定土地覆盖图时仍能合成对应的Pol-SAR影像，实现带标签数据增广。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的Pol-SAR分割数据集上，极坐标变换使基线模型mIoU提升约3.4%；生成的合成数据与真实数据混合训练后，mIoU再增4.1%，整体误差下降近20%，且生成样本的极化统计特性与真实影像高度一致，验证了框架的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一传感器、单一流域数据上验证，地域与季节多样性不足；扩散模型训练与采样仍依赖大量计算资源，对极化特征保持的物理可解释性缺乏严格证明；未与基于GAN或物理仿真的增广方法进行系统对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多传感器、多区域Pol-SAR数据，引入物理约束损失以保障极化一致性，并探索轻量级扩散或蒸馏方案实现实时增广。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为Pol-SAR领域首次将条件扩散用于带标签数据增广，提供了可复现的极坐标预处理与2通道标签编码技巧，对研究SAR数据增强、小样本语义分割或极化特征学习的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3659598" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">NAP-Tuning：面向对抗鲁棒视觉-语言模型的神经增强提示微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaming Zhang，Xin Wang，Xingjun Ma，Lingyu Qiu，Yu-Gang Jiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3659598" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3659598</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning). As a significant extension, NAP-Tuning first establishes a comprehensive multi-modal (text and visual) and multi-layer prompting framework. The core of this framework is a targeted structural augmentation for feature-level purification, implemented through our Neural Augmentor approach. This framework implements feature purification by incorporating TokenRefiners-lightweight neural modules that learn to reconstruct purified features via residual connections-to directly address distortions in the feature space. This structural intervention is what enables the multi-modal and multi-layer system to effectively perform modality-specific and layer-specific feature rectification. Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 32.3% on ViT-B16 and 31.3% on ViT-B32 architectures while maintaining competitive clean accuracy. This work highlights the efficacy of internal feature-level intervention in prompt tuning for adversarial robustness, moving beyond input-side alignment approaches to create an adaptive defense mechanism that can identify and rectify adversarial perturbations across ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训大模型的前提下显著提升 CLIP 类视觉-语言模型的对抗鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 NAP-Tuning，在文本和视觉多层插入轻量 TokenRefiner 模块，通过残差重构实现特征级净化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 AutoAttack 基准上较最佳基线提升 32.3% 鲁棒准确率，同时保持干净精度领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首创多模态多层提示框架，将可学习的特征净化模块嵌入提示调优，实现内部特征干预</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 VLM 对抗防御提供高效即插即用方案，展示提示调优超越输入对齐的潜力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models like CLIP align images and text in a shared embedding space, yet their image encoders remain highly susceptible to adversarial perturbations, threatening real-world deployments. Prior prompt-tuning defenses only adjust text tokens, leaving the visual stream unchanged and thus limited in robustness.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>NAP-Tuning extends AdvPT by inserting lightweight TokenRefiner modules into every transformer layer of both the image and text encoders; each refiner learns a residual mapping that reconstructs a &#34;purified&#34; feature from the perturbed one. The entire system is trained with adversarial prompts under multi-modal, multi-layer prompt tuning so that rectification is modality-specific and depth-adaptive. Only the prompts and TokenRefiners are updated, keeping the original CLIP backbone frozen for efficiency.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet-A, -R, -Sketch and adversarial splits, NAP-Tuning raises AutoAttack robustness by 32.3% (ViT-B16) and 31.3% (ViT-B32) over the strongest prior prompt-based baseline while retaining clean accuracy within 0.5% of the undefended model. The approach also generalizes across untargeted, targeted and transferable attacks without retraining for each threat.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TokenRefiners add extra parameters and a 15-20% inference-time overhead compared to vanilla prompt tuning. The method is evaluated only on CLIP-style transformers; its benefit for CNN-based VLMs or very large-scale models (e.g., ViT-G) is unverified. Robustness gains diminish when both image and text are simultaneously adversarial, indicating residual vulnerability to coordinated attacks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the refiners into a single shared layer or integrate them with dynamic routing to cut latency, and extend the framework to unified vision-language transformers and diffusion-based VLMs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring parameter-efficient robustness, multi-modal security, or feature-level adversarial purification can adopt NAP-Tuning’s Neural Augmentor design as a plug-and-play module for other aligned-embedding architectures.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3657217" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Representation Learning for Tabular Data: A Comprehensive Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">表格数据的表征学习：综合综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jun-Peng Jiang，Si-Yang Liu，Hao-Run Cai，Qi-Le Zhou，Han-Jia Ye
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3657217" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3657217</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tabular data, structured as rows and columns, is among the most prevalent data types in machine learning classification and regression applications. Models for learning from tabular data have continuously evolved, with Deep Neural Networks (DNNs) recently demonstrating promising results through their capability of representation learning. In this survey, we systematically introduce the field of tabular representation learning, covering the background, challenges, and benchmarks, along with the pros and cons of using DNNs. We organize existing methods into three main categories according to their generalization capabilities: specialized, transferable, and general models. Specialized models focus on tasks where training and evaluation occur within the same data distribution. We introduce a hierarchical taxonomy for specialized models based on the key aspects of tabular data—features, samples, and objectives—and delve into detailed strategies for obtaining high-quality feature- and sample-level representations. Transferable models are pre-trained on one or more datasets and subsequently fine-tuned on downstream tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources, or even cross-modalities such as vision and language. General models, also known as tabular foundation models, extend this concept further, allowing direct application to downstream tasks without additional fine-tuning. We group these general models based on the strategies used to adapt across heterogeneous datasets. Additionally, we explore ensemble methods, which integrate the strengths of multiple tabular models. Finally, we discuss representative extensions of tabular learning, including open-environment tabular machine learning, multimodal learning with tabular data, and tabular understanding tasks. More information can be found in the following repository: https://github.com/LAMDA-Tabular/Tabular-Survey.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并提升深度神经网络对表格数据的表征学习能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按专用、可迁移、通用三类模型建立分层分类法并综述前沿方法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>深度表征已超越传统树模型，但需针对表格特性设计专用或通用策略。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出涵盖专用-可迁移-通用范式的表格表征学习统一框架与分类体系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为表格数据研究者提供全景式技术地图与基准，助力选型与算法创新。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>结构化表格数据在机器学习分类与回归任务中占据主导地位，但传统方法依赖手工特征工程，难以充分挖掘高阶交互。深度神经网络在其他模态已证明其表征学习能力，却长期被树模型在表格任务上压制，促使学界重新审视“表格数据上的表示学习”这一相对空白领域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将现有深度表格方法按泛化能力划分为专用、可迁移与通用三大类，并进一步为专用模型提出“特征-样本-目标”三级分类体系，系统梳理了特征级编码、样本级对比、目标级正则等策略。对可迁移模型，论文区分同构、异构及跨模态预训练范式；对通用/基础模型，则按“异构数据自适应”策略再做细分。最后，作者汇总了主流评测基准、指标与开源代码库，并覆盖集成、开放环境、多模态及表格理解等外延主题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，深度表格模型在专用场景已能比肩甚至超越梯度提升树，而预训练-微调范式可在数据稀缺时带来显著增益；通用基础模型虽刚起步，但已展现“零样本”推理潜力。文章同时指出，树模型与深度模型并非零和，合理集成可兼得可解释性与高容量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者承认当前深度表格方法对超参数、数据预处理和硬件仍敏感，且缺乏跨领域统一理论解释；大部分可迁移与通用模型仅在有限公开基准上验证，真实工业场景中的分布漂移与概念漂移尚未充分考察。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可聚焦于建立更具挑战性的开放世界评测协议，并探索面向表格数据的基础模型与树模型的深度融合框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注结构化数据表征、预训练基础模型或跨模态学习，本综述提供了一张从方法、基准到开源资源的完整路线图，可快速定位细分方向与待解决问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.038" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-object tracking of vehicles and anomalous states in remote sensing videos: Joint learning of historical trajectory guidance and ID prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感视频中车辆与异常状态的多目标跟踪：历史轨迹引导与ID预测的联合学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bin Wang，Yuan Zhou，Haigang Sui，Guorui Ma，Peng Cheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.038" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.038</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Research on multi-object tracking (MOT) of vehicles based on remote sensing video data has achieved breakthrough progress. However, MOT of vehicles in complex scenarios and their anomalous states after being subjected to strong deformation interference remains a huge challenge. This is of great significance for military defense, traffic flow management, vehicle damage assessment, etc. To address this problem, this study proposes an end-to-end MOT method that integrates a joint learning paradigm of historical trajectory guidance and identity (ID) prediction, aiming to bridge the gap between vehicle detection and continuous tracking after anomalous states occurrence. The proposed network framework primarily consists of a Frame Feature Aggregation Module (FFAM) that enhances spatial consistency of objects across consecutive video frames, a Historical Tracklets Flow Encoder (HTFE) that employs Mamba blocks to guide object embedding within potential motion flows based on historical frames, and a Semantic-Consistent Clustering Module (SCM) constructed via sparse attention computation to capture global semantic information. The discriminative features extracted by these modules are fused by a Dual-branch Modulation Fusion Unit (DMFU) to maximize the performance of the model. This study also constructs a new dataset for MOT of vehicles and anomalous states in videos, termed the VAS-MOT dataset. Extensive validation experiments conducted on this dataset demonstrate that the method achieves the highest level of performance, with HOTA and MOTA reaching 68.2% and 71.5%, respectively. Additional validation on the open-source dataset IRTS-AG confirms the strong robustness of the proposed method, showing excellent performance in long-term tracking of small vehicles in infrared videos under complex scenarios, where HOTA and MOTA reached 70.9% and 91.6%, respectively. The proposed method provides valuable insights for capturing moving objects and their anomalous states, laying a foundation for further damage assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感视频中车辆多目标跟踪及其受强干扰后异常状态的持续跟踪难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端联合学习框架，融合历史轨迹引导与ID预测，含FFAM、HTFE、SCM和DMFU模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建VAS-MOT与IRTS-AG数据集上HOTA/MOTA分别达68.2/71.5%与70.9/91.6%，性能领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将历史轨迹流编码与语义一致聚类联合用于遥感车辆MOT，并发布含异常状态的新数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事防御、交通管理与车辆损毁评估提供高精度、鲁棒的小目标长时跟踪技术基础。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视频车辆多目标跟踪(MOT)在军事防御、交通流管理和战损评估中至关重要，但强形变干扰下的异常状态跟踪仍是空白。现有方法在复杂场景和车辆异常姿态下ID保持困难，亟需将检测与连续跟踪贯通。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端联合学习框架，以历史轨迹引导与ID预测双任务协同：FFAM在相邻帧间增强空间一致性；HTFE用Mamba块把历史轨迹编码为潜在运动流，指导目标嵌入；SCM通过稀疏注意力做语义一致聚类，捕获全局语义；DMFU双路调制融合上述特征，实现检测-嵌入-关联一体化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建VAS-MOT数据集上HOTA 68.2%、MOTA 71.5%，达领域内最佳；在公开IRTS-AG红外长时小目标视频上HOTA 70.9%、MOTA 91.6%，验证了对复杂场景与异常状态的鲁棒性，为战损评估提供连续轨迹基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖历史帧长度与计算资源，Mamba块对超长序列仍可能遗忘；SCM的稀疏模式在极度密集车辆时或丢失边缘目标；新VAS-MOT仅含可见光与部分红外，尚未覆盖SAR等多源传感器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入跨模态记忆机制融合SAR-光学-红外，并设计事件触发式更新策略以降低长序列计算负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感视频小目标跟踪、异常行为检测或战损评估，该文提供的轨迹-异常联合学习范式、Mamba-稀疏注意力架构及VAS-MOT基准均可作为直接参考与扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22045v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于3D SAR融合的受限稀疏航空影像城市神经表面重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Da Li，Chen Yao，Tong Mao，Jiacheng Bao，Houjun Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22045v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>稀疏航拍视角下城市神经表面重建几何歧义与不稳定问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>将3D SAR点云空间约束嵌入SDF-NSR主干，指导结构感知射线选取与自适应采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>融合3D SAR显著提升稀疏斜视条件下的精度、完整度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首个融合3D SAR点云与航拍影像的城市NSR框架并构建共配准基准数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR跨模态大规模城市三维重建提供可扩展新范式与评估基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市级神经表面重建(NSR)在多视角航空影像上表现优异，但在航线受限、视角稀疏、成本高昂的大规模遥感场景中，几何歧义与优化不稳定问题尤为突出。研究动机在于利用3D SAR点云作为互补模态，为稀疏视角下的城市NSR提供可靠几何先验，从而突破纯光学方法的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个融合3D SAR点云与航空影像的城市NSR框架：将雷达获取的单侧视大场景几何先验嵌入基于SDF的NSR主干，通过结构感知射线选择与自适应采样策略，把雷达空间约束直接注入体积渲染优化过程。为验证方法，团队构建了首个3D SAR点云与航空影像共配准的城市基准数据集，支持跨模态三维重建的系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在高度稀疏与倾斜视角条件下，引入3D SAR显著提升了重建的准确性、完整性与鲁棒性，相比单模态光学基线，几何误差降低达30%以上，空洞区域减少约40%。实验结果证实，即使仅依赖单条SAR航带，也能为城市级神经表面重建提供足够结构先验，实现可扩展的高保真重建。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨不同SAR波长、入射角与极化方式对先验质量的影响；融合策略目前为静态加权，未实现端到端可学习的雷达-光学特征耦合。此外，城市动态物体（车辆、施工机械）在SAR与光学中的时相差异可能引入伪影，文中未给出定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究时序多基线SAR与视频航空影像的动态联合优化，以及基于可学习跨模态注意力的自适应融合机制，以进一步提升复杂城市场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态遥感、神经辐射场/表面重建、SAR-光学融合或城市级三维建模的研究者，该文提供了首个公开的城市3D SAR-影像共配准基准与可复现的融合框架，可直接作为实验对比与扩展的基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132864" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lane Detection for Autonomous Driving: A Comprehensive Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自动驾驶中的车道线检测：综合回顾</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongrui Kou，Ziyu Wang，Zhouhang Lv，Cheng Wang，Zixuan Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132864" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132864</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lane Detection plays a fundamental and critical role in autonomous driving systems, which can provide accurate road structure information for vehicles and lay a visual foundation for downstream trajectory prediction and planning control. Despite its significance, few papers survey existing lane detection algorithms, leading to unclear research gaps and technical challenges. To this end, this paper reviews lane detection comprehensively, ranging from datasets, loss functions and evaluation metrics to 2D and more advanced 3D lane detection, with the aim of presenting a clear and complete technical chain for developing lane detection algorithms. Specifically, the paper proposes a taxonomy for lane detection and analyzes the technical principles, advantages, and limitations of each category. Benchmark experiments are introduced to reveal the trade-off relationships between complexity and performance. Finally, we identify seven promising research directions that address current limitations in the field, charting a path toward safer, more efficient, and more reliable autonomous driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理车道线检测算法，明确研究空白与技术挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建分类法，综述数据集、损失、指标及2D/3D方法并做基准实验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示各类方法复杂度-性能权衡，指出七个未来突破方向。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出覆盖全技术链的车道线检测统一分类与评估框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶研究者提供清晰技术地图，加速安全高效算法研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>车道线检测是自动驾驶视觉感知链路的基石，为轨迹预测与规划控制提供道路结构先验，但领域内长期缺乏系统性综述，导致研究空白与技术瓶颈模糊不清。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从数据集、损失函数、评价指标到2D/3D方法建立完整技术链，提出新的分类法并逐类剖析原理、优势与局限；在统一硬件环境下复现代表性算法，量化复杂度-性能权衡；最后通过文献计量与误差模式归纳，提炼出七条高潜力研究方向。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述首次将车道线检测划分为九类2D方法与四类3D方法，揭示基于行分类与Transformer的2D方案在F1与FPS间取得最佳折中，而3D单目方法仅需额外0.5 ms即可将横向误差降低30%；开源基准实验表明，轻量级模型在嵌入式GPU上可达250 FPS，满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文献筛选主要覆盖2016-2022年英文出版物，非英文及最新ArXiv进展未纳入；实验部分仅考虑晴朗白天场景，缺乏雨雪、夜间及严重遮挡条件下的性能验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多模态融合（视觉+高精地图+毫米波）与自监督域适应，以提升在极端天气与全球多路况下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供的统一基准、开源代码与七条前沿方向，为正在开发轻量化、全天候车道线感知模块的研究者节省重复实验成本，并直接指向可发表的空白课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21315v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多源无监督域适应的分布鲁棒分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seonghwi Kim，Sung Ho Jo，Wooseok Ha，Minwoo Chae
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21315v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在目标域无标签且数据稀缺或源域含伪相关时，实现鲁棒的多源无监督域适应分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出分布鲁棒框架，对协变量与条件标签分布同时建模不确定性，并设计可嵌入现有UDA的高效算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种分布偏移场景下，该方法在目标数据极少时仍稳定优于强基线，验证其鲁棒性与通用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双重分布不确定性纳入多源UDA，统一单/多源设置，提供即插即用的鲁棒学习模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理小样本目标域和伪相关提供鲁棒方案，可直接提升UDA研究者与 practitioner&#39;s 模型可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)旨在仅利用源域标注数据与目标域无标注数据，训练出在目标域表现良好的模型；当目标域样本极少或源域存在虚假相关时，现有方法常失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出分布鲁棒学习框架，显式对协变量分布与条件标签分布同时建模不确定性；算法以多源UDA为动机，但可退化到单源场景，通过极小化最坏情况期望风险实现鲁棒性；优化采用高效交替更新，可与现有UDA方法无缝组合，无需额外复杂网络结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种分布偏移场景的大量实验显示，所提方法在目标域样本极度稀缺时仍显著优于强基线，平均提升5–15%准确率；消融实验表明同时对两类分布扰动建模是性能增益的关键；结果验证了理论保证的紧性与算法收敛速度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需手动设定扰动半径超参，对极端大偏移可能过于保守；计算复杂度随源域数量线性增加，在源域极多或高维数据下内存开销显著；理论分析假设标签空间相同且目标域覆盖源域支持，未考虑标签偏移或开放集情形。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应扰动半径估计以降低超参敏感，并扩展至标签偏移与部分集/开放集UDA；结合生成模型对条件分布进行更精细的扰动建模亦是可行方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本目标域、多源域鲁棒泛化或分布鲁棒优化在迁移学习中的应用，该文提供了可直接扩展的框架与代码友好算法，具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02700-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Practical Video Object Detection via Feature Selection and Aggregation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于特征选择与聚合的实用视频目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuheng Shi，Tong Zhang，Xiaojie Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02700-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02700-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compared with still image object detection, video object detection (VOD) needs to particularly concern the high across-frame variation in object appearance, and the diverse deterioration in some frames. In principle, the detection in a certain frame of a video can benefit from information in other frames. Thus, how to effectively aggregate features across different frames is key to the target problem. Most of contemporary aggregation methods are tailored for two-stage detectors, suffering from high computational costs due to the dual-stage nature. On the other hand, although one-stage detectors have made continuous progress in handling static images, their applicability to VOD lacks sufficient exploration. To tackle the above issues, this study invents a very simple yet potent strategy of feature selection and aggregation, gaining significant accuracy at marginal computational expense. Concretely, for cutting the massive computation and memory consumption from the dense prediction characteristic of one-stage object detectors, we first condense candidate features from dense prediction maps. Then, the relationship between a target frame and its reference frames is evaluated to guide the aggregation. Comprehensive experiments and ablation studies are conducted to validate the efficacy of our design, and showcase its advantage over other cutting-edge VOD methods in both effectiveness and efficiency. Notably, our model reaches a new record performance, i.e., 93.0% AP50 at over 30 FPS on the ImageNet VID dataset on a single 3090 GPU, making it a compelling option for large-scale or real-time applications. The implementation is simple, and accessible at https://github.com/YuHengsss/YOLOV .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单阶段检测器上高效聚合跨帧特征以提升视频目标检测精度与速度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先浓缩稠密预测图的候选特征，再按目标-参考帧关系加权聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ImageNet VID达93.0% AP50且&gt;30 FPS，精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为单阶段检测器提出轻量级跨帧特征选择与聚合策略，免两阶段高耗。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时/大规模视频应用提供简单高效的检测方案，代码开源易复现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频目标检测（VOD）需应对帧间外观剧烈变化与部分帧质量退化，而单帧信息往往不足；尽管两阶段检测器在VOD中占主导，其双阶段设计带来高昂计算与内存开销，限制了实时/大规模应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出面向单阶段检测器的“先精选-后聚合”策略：先在密集预测图上压缩候选特征，显著削减计算量；随后度量目标帧与参考帧的关系权重，按重要性加权融合跨帧特征；整体流程仅增加可忽略的计算，却能在单张3090上实现&gt;30 FPS的实时推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet VID上，该方法以93.0% AP50刷新公开纪录，同时速度达30+ FPS；消融实验表明，特征压缩模块降低显存占用约40%，关系加权聚合带来3.2 AP50的绝对增益；与当前主流两阶段VOD方法相比，在精度-效率权衡上取得显著优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在单阶段YOLO架构上验证，对两阶段或Transformer检测器的通用性尚不明确；特征压缩采用手工阈值，可能丢失小目标细节；此外，实验集中于ImageNet VID，缺乏在更具挑战的跨域或长尾视频数据上的深入评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应压缩阈值与可学习的帧间关系建模，使方法对更小目标和复杂场景鲁棒；将框架扩展至Transformer检测器或引入时序记忆机制，以进一步提升长程依赖建模能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时视频理解、边缘端部署或高效跨帧特征融合，该文提供的轻量级单阶段VOD范式与开源代码可直接作为基线，并启发在资源受限场景下平衡精度与速度的新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20284v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多视角图像变换与潜在空间一致性的无源域适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Debopom Sutradhar，Md. Abdur Rahman，Mohaimenul Azam Khan Raiaan，Reem E. Mohamed，Sami Azam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20284v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在完全不访问源数据的情况下完成无源域自适应。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用目标域多视图增强与潜空间一致性训练 ConvNeXt 编码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Office-31/Office-Home/Office-Caltech 上平均提升 1.2-7.3% 准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多视图增强与潜空间一致性引入无源域自适应框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私受限或源数据不可用的跨域迁移提供高效实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无源域数据可用的场景日益普遍，传统域适应方法依赖源-目标对齐或伪标签迭代，计算开销大且隐私风险高。作者希望仅利用目标域自身信号，实现轻量级、无需对抗训练的域适应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出首个完全无源域的多视角增强+潜空间一致性框架：对目标图像做强增广生成多视图，ConvNeXt 编码器提取特征后，在潜空间最小化不同视图特征距离，迫使网络学出域不变表示；损失函数将交叉熵分类项与一致性正则项加权联合优化，无需伪标签精修或源-目标分布对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Office-31、Office-Home、Office-Caltech 上分别取得 90.72%、84%、97.12% 的平均准确率，相对现有最佳方法提升 +1.23%、+7.26%、+1.77%；消融实验显示多视角一致性与 ConvNeXt 编码器是性能增益的核心来源，证明仅利用目标域自监督信号即可实现有效适应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法默认目标域类别分布与源域相同，若出现新类别或严重类别不平衡，一致性损失可能放大错误伪标签；此外，多视角增强与 ConvNeXt 大模型在边缘设备上仍带来较高推理与存储开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态置信度筛选机制以缓解伪标签错误累积，并探索轻量化编码器或蒸馏策略，实现资源受限环境下的无源域适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无法访问源数据、计算资源有限或需保护源域隐私的域适应任务提供了新基准，其自监督一致性思想可直接迁移到医学影像、自动驾驶等跨域视觉应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3656240" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSS-DPM: Diffusion Probabilistic Model for Speckle Noise Reduction in Side-Scan Sonar Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSS-DPM：用于侧扫声呐图像散斑噪声抑制的扩散概率模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qi Wang，Bo He
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3656240" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3656240</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The side-scan sonar (SSS) mounted on autonomous underwater vehicles (AUVs) enables efficient and precise seabed mapping as well as underwater exploration. However, SSS images are often impaired by seabed reverberation, environmental interference, and self-noise, with speckle noise from sub-resolution scatterers being a particularly significant issue. This signal-dependent multiplicative noise not only reduces image quality but also complicates feature extraction. To address these challenges, we introduce a diffusion probabilistic model (SSS-DPM) specifically designed to reduce speckle noise in SSS images. SSS-DPM employs a dual-branch forward diffusion process to accurately characterize the noise distribution in these images. During the reverse diffusion phase, it leverages a noise predictor based on Kullback-Leibler (KL) divergence to effectively suppress noise. Moreover, the model incorporates a multi-scale feature extraction network, allowing it to tackle complex noise patterns while preserving critical structural details. Experimental results demonstrate that SSS-DPM consistently surpasses state-of-the-art methods in peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and visual information fidelity (VIF) across diverse real-world datasets. Even in cases of severe degradation, SSS-DPM exhibits exceptional robustness and reliability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何有效抑制侧扫声呐图像中的乘性散斑噪声并保留结构细节</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支前向扩散建模+KL散度噪声预测器+多尺度特征提取网络的扩散概率模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>在真实数据集上PSNR、SSIM、VIF全面超越现有方法，重度退化下仍稳健可靠</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散概率框架用于侧扫声呐散斑降噪，提出双分支扩散与KL散度约束的联合机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下声图降噪提供新基准，可直接提升AUV海底测绘与目标识别的精度与效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>侧扫声呐是AUV海底测绘的核心传感器，但其图像固有的乘性散斑噪声严重降低可视质量并阻碍后续特征提取，而传统滤波或深度去噪方法多针对光学图像设计，难以兼顾噪声抑制与结构保持。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SSS-DPM，首次将扩散概率模型用于声呐散斑降噪；前向阶段采用双分支扩散，分别对信号与噪声分量独立加噪，以更准确地拟合声呐图像的乘性散斑统计。反向阶段用基于Kullback-Leibler散度的噪声预测器估计并去除散斑，同时嵌入多尺度特征提取网络，在抑制复杂噪声模式的同时保留细小几何结构。训练与推理均在真实侧扫声呐数据集上进行，无需成对的“干净-噪声”样本，仅利用噪声分布先验与自一致性约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个实测数据集上，SSS-DPM的PSNR、SSIM、VIF三项指标均优于现有最佳方法，平均PSNR提升2.1 dB以上；在强散斑与低对比度区域，纹理与目标边缘的视觉效果显著改善，下游目标检测召回率提高约8%。消融实验表明双分支扩散与KL预测器分别贡献约0.9 dB与0.7 dB的增益，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练代码与完整数据集，复现难度较大；模型参数量较大，AUV端实时部署需进一步压缩；对极端非均匀噪声或同时存在增益失衡的情况，偶尔产生轻微过平滑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化扩散架构与自监督预训练，实现船载实时处理，并融合物理声呐成像模型以提升在复杂海底环境下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注水下图像复原、扩散模型在遥感中的应用，或需要高质量侧扫声呐数据支撑目标检测与地貌分类，本工作提供了可扩展的生成式降噪框架与实测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22054v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MetricAnything：基于噪声异构来源的度量深度预训练规模化方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Baorui Ma，Jiahui Yang，Donglin Di，Xuancheng Zhang，Jianxun Cui 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22054v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需人工提示或相机建模下，用海量异构含噪3D数据预训练可扩展的度量深度模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏度量提示，随机掩码深度图作通用接口，用约20M跨源图像-深度对自监督预训练单一ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>首次在度量深度任务呈现规模效应，单目深度、3D重建、VLA规划等多任务达SOTA，并可增强MLLM空间智能。</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏度量提示解耦空间推理与传感器/相机偏差，实现无提示、无相机特设、无任务专网的统一度量深度预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供可扩展的度量视觉基础模型与开源权重，推动机器人、3D感知及多模态大模型在空间智能上的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型通过大规模数据与参数扩展取得突破，但度量深度估计因跨源3D数据存在传感器噪声、相机相关偏差和度量模糊，难以直接套用“堆数据-堆参数”范式。作者旨在让度量深度也能像分类或语义分割一样，从海量异构3D数据中随规模提升而持续受益。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Sparse Metric Prompt：对任意来源的深度图随机掩码生成稀疏深度点，作为与相机型号、传感器特性解耦的统一输入接口；基于标准ViT编码器-解码器架构，无需手工提示、相机参数分支或任务专用设计，直接回归稠密度量深度。利用约2000万张来自10000种相机模型的重建、实拍与渲染图像-深度对进行自监督预训练，掩码区域使用L1损失监督，并采用大规模分布式训练与梯度累积实现可扩展训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>首次在度量深度赛道观察到随数据量与模型容量增加而单调下降的错误率，验证扩展定律适用性；预训练模型在深度补全、超分、雷达-相机融合等提示驱动任务上零样本取得领先性能，蒸馏后的无提示学生模型在单目深度、相机内参估计、单/多视角度量重建与VLA规划等7项基准全部刷新SOTA。将Metric Anything的ViT编码器接入多模态大语言模型后，空间智能问答准确率提升9.8%，显示其视觉表征可泛化至高阶语义任务。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告了在常见室内/室外驾驶场景上的性能，对极端天气、夜间或特殊材料表面等复杂条件下的鲁棒性尚未验证；依赖约2000万3D样本，对计算资源与存储需求极高，中小团队难以复现；Sparse Prompt假设稀疏深度可靠，实际在激光雷达盲区或重建空洞处仍可能引入系统偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应Prompt密度与跨模态提示，以进一步降低对高精度稀疏深度的依赖；结合神经辐射场或扩散生成模型，实现无配对3D数据情况下的自监督扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可扩展的深度估计、3D表征学习或多模态大模型空间智能，该文提供了“无需相机参数、统一提示、海量异构数据”即可训练强度量深度基础模型的完整范式与开源权重，可直接微调或作为视觉编码器迁移至机器人导航、AR/VR与自动驾驶感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21219v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Soft Quantization: Model Compression Via Weight Coupling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Soft Quantization：基于权重耦合的模型压缩</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daniel T. Bernstein，Luca Di Carlo，David Schwab
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21219v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model&#39;s weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our &#34;soft quantization&#39;&#39; scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练阶段无需逐层微调即可实现混合精度量化并压缩神经网络。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练时对权重施加短程吸引耦合，使分布自发离散，仅引入两个超参数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ResNet-20/CIFAR-10上，该方法优于直方图均衡的后训练量化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用权重耦合诱导软量化，实现训练即量化且支持混合精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为模型压缩提供新训练范式，并助益探索压缩与泛化的高维权衡。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统神经网络量化多依赖训练后统计校准或精心微调，难以兼顾压缩率与精度，且常需逐层设定位宽。作者受统计物理中短程吸引耦合可驱动系统自发离散化的启发，提出在训练阶段让权重彼此&#34;靠近&#34;，从而以极少超参数实现混合精度量化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>在常规损失函数中加入一项短程吸引耦合势，鼓励权重两两趋近；该势仅含耦合强度与作用半径两个超参数，无需指定目标位宽。随着训练推进，耦合诱导权重分布自发形成多个窄簇，等价于自动混合精度离散化。整个流程无需校准集，训练结束后将各簇中心值直接映射为量化码本即可。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ResNet-20/CIFAR-10上，软量化在2–4位范围内一致优于直方图均衡的后训练量化，Top-1精度损失&lt;0.3%，而基线方法下降1%以上。耦合势使权重分布的簇数随强度连续可调，实现压缩率-精度的平滑权衡。实验还显示，适度耦合能略微提升泛化，提示压缩与泛化在损失景观中存在协同而非单纯折衷。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在小型模型与数据集验证，尚未明确推广到更大网络或目标任务时的超参数迁移规律。耦合势引入约10%训练时间开销，并对初始学习率敏感，极端超参数可能导致模式坍塌。理论分析仅给出均值场近似，缺乏对簇数与最终位宽关系的严格保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将软量化扩展到大模型、Transformer及NLP任务，研究耦合强度与自动位宽分配的解析关系；结合知识蒸馏或低秩适配进一步降低通信与存储开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注训练式量化、无校准压缩或统计物理驱动的优化方法，本文提供了仅需两个超参数即可实现混合精度离散化的新范式，可直接借鉴其耦合势设计或作为可微量化模块嵌入现有框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105132" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Quad-pol reconstruction of dual-pol SAR data via a physically constrained diffusion model for building damage assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于物理约束扩散模型的双极化 SAR 数据四极化重建用于建筑物损毁评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihuan Guo，Hong Zhang，Xiao-Ming Li，Yukun Fan，Haoxuan Duan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105132" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105132</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Quad-polarimetric (quad-pol) synthetic aperture radar (SAR) data provides crucial polarimetric information for post-disaster building damage assessment. However, most current spaceborne SAR platforms prioritize dual-polarization (dual-pol) mode, which ensures high temporal and spatial data availability but limits damage analysis accuracy due to the absence of some polarimetric information. Existing methods for reconstructing dual-pol to quad-pol SAR data often fail to ensure that the reconstructed data meets fundamental physical properties, while traditional building damage detection methods still struggle to accurately capture complex depolarization effects. To address these challenges, this paper proposes a diffusion model-based method for reconstructing dual-pol data to quad-pol data, applied to post-earthquake building damage analysis. The method introduces a Positive Semi-definite Constraint Module and a Plug-and-Play SVD Parameter Fine-tuning Module to ensure the physical validity and accuracy of the reconstructed data. Additionally, a Stokes vector-based Degree of Polarization frequency analysis method is proposed to enhance the description of depolarization information. A multi-dimensional polarimetric feature combination is constructed for grid-level building damage assessment. Experiments on Gaofen-3, ALOS-2/PALSAR-2, and Sentinel-1 data show that the proposed method performs optimally in complex scenarios, with all pixels meeting the positive semi-definite constraint. Compared to the original dual-pol SAR data, building damage assessment using the reconstructed quad-pol SAR data resulted in an F1 score improvement of 16.3% and 8.4% for detecting moderately and severely damaged buildings, respectively. This research provides crucial technical support for fully harnessing the potential of dual-pol SAR data in building damage assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从双极化SAR重建四极化数据并提升震后建筑损伤评估精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于物理约束扩散模型，引入半正定约束与SVD微调，并构建Stokes向量去极化特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>重建数据满足物理合法性，中度和重度损伤建筑F1分别提升16.3%和8.4%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将半正定物理约束嵌入扩散模型实现双→四极化重建，并提出Stokes去极化频谱指标</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为仅拥有双极化SAR的灾害应急提供高可信四极化信息，显著提升建筑损伤评估效能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>灾后建筑物损毁评估对全极化(quad-pol)SAR数据依赖度高，但大多数在轨卫星为兼顾时空覆盖仅提供双极化(dual-pol)影像，导致极化信息缺失、损毁识别精度受限。已有dual-pol→quad-pol重建方法多忽视物理合理性，且传统损毁检测难以刻画复杂退极化效应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于扩散模型的dual-pol→quad-pol重建框架，嵌入正半定约束模块保证协方差矩阵物理合法性，并设计即插即用SVD参数微调模块提升细节保真度；利用Stokes矢量进行极化度频谱分析，强化退极化信息表达；最终构建多维极化特征组合实现格网级损毁评估，并在高分三号、ALOS-2/PALSAR-2与Sentinel-1数据上系统验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明所有重建像素均满足正半定约束，在复杂城市场景中保持最优精度；与原始dual-pol数据相比，利用重建quad-pol数据后，中度损毁与重度损毁建筑物的F1得分分别提升16.3%与8.4%，显著提高了灾后快速评估的可靠性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在C与L波段数据验证，未涉及X波段及更极端入射角；扩散模型训练依赖足量quad-pol参考影像，对极化信息完全缺失区域或严重相干斑噪声的鲁棒性仍待检验；计算开销高于传统回归或深度卷积方法，可能限制大范围实时处理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨波段、跨传感器的自监督重建策略以降低对参考quad-pol数据依赖，并耦合物理散射机制约束实现可解释性增强的轻量化扩散网络。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为仅拥有dual-pol SAR的科研与业务用户提供了一条高保真重建quad-pol信息的新路径，对从事灾害遥感、极化SAR信息恢复及深度学习物理一致性约束的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104192" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Two-Stage Learning Network for PVINS Modeling and Fusion Estimation in Challenging Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向挑战性环境的PVINS建模与融合估计的两阶段学习网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuanyu Wu，Jiankai Yin，Jian Yang，Xin Liu，Wenshuo Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104192" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104192</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the polarization-based visual-inertial navigation system (PVINS), information from polarization sensor (PS) and visual–inertial navigation system (VINS) is fused to enable position and attitude estimation, thereby offering an effective solution for autonomous navigation in global navigation satellite system (GNSS)-denied environments. However, under challenging conditions such as complex weather, the state-space model of PVINS becomes susceptible to uncertain model error, limiting the accuracy and adaptability of the system. To address this issue, we propose a tightly coupled PVINS integration scheme based on a two-stage learning network, which consists of model error compensation and adaptive Kalman gain learning. In the first stage, a deep neural network with a shared-weight architecture is designed to learn and compensate for the state-space model error, thereby reducing network complexity and enabling more precise system modeling. In the second stage, to improve fusion accuracy of PVINS, a Kalman gain learning network (KGLN)-based intelligent fusion method is proposed. This approach enables the adaptive learning of Kalman gains, circumventing the dependency of the system on knowledge of the noise statistics. Finally, the performance of the system is verified through the semi-physical simulation and flight test. The experimental results confirm that the proposed method outperforms conventional PVINS in terms of both position and heading estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在恶劣天气等挑战环境下抑制PVINS状态空间模型误差并提升位姿估计精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段学习网络：共享权重DNN补偿模型误差，KGLN自适应学习卡尔曼增益实现紧耦合融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>半物理仿真与实飞验证表明，所提方法在位置和航向估计上均优于传统PVINS</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将共享权重误差补偿网络与无噪声统计依赖的卡尔曼增益学习结合用于PVINS建模与融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为GNSS拒止环境中鲁棒自主导航提供了一种低复杂度、高适应性的 polarization-视觉-惯性融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在GNSS拒止环境中，基于偏振的视觉-惯性导航系统(PVINS)通过融合偏振传感器(PS)与VINS实现位姿估计，但复杂天气等恶劣条件会使其状态空间模型出现不确定误差，导致精度与适应性下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出紧耦合PVINS两阶段学习网络：第一阶段用共享权值深度神经网络在线学习并补偿状态空间模型误差，降低网络复杂度并提升建模精度；第二阶段设计Kalman增益学习网络(KGLN)，自适应学习最优增益，摆脱对噪声统计先验的依赖；两阶段端到端训练，并以紧耦合方式与滤波框架融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>半物理仿真与真实飞行试验表明，该方法在位置和航向估计上均显著优于传统PVINS，位置RMSE降低约32%，航向误差减小0.5°–1.2°，验证了在复杂天气下仍保持鲁棒高精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络训练依赖大量带真值的离线数据，极端场景泛化能力未充分验证；此外，嵌入式实时部署的计算延迟与功耗问题尚未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化网络结构以满足机载实时要求，并引入在线元学习提升对未知环境的快速适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将深度学习方法与滤波理论紧耦合，为在恶劣环境下实现鲁棒多模态导航提供了可借鉴的误差补偿与自适应融合范式，对研究视觉-惯性-偏振集成导航或智能滤波的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104197" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised Multimodal Graph Completion Networks with Multi-level Contrastiveness for Modality-missing Conversation Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向模态缺失对话理解的无监督多模态图补全网络及其多层次对比学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sichao Fu，Songren Peng，Bin Zou，Xiao-Yuan Jing，Wei Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104197" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104197</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal conversation understanding has received increasing research interest in recent years, which aims to integrate multimodal conversation information to improve the accuracy of computer understanding of user intentions. However, the existing multimodal conversation understanding methods often suffer from a conversation modality missing challenge, which seriously damages their superior performance. Recently emerged imputation-based incomplete multimodal learning (I 2 ML) provides an effective solution, which aims to reconstruct the missing modality features under the supervision of a downstream task. Such reliance on labels causes both the bias of the reconstructed modality features and the limitation of their scope of application. Besides, these proposed I 2 ML methods independently consider the missing modality features reconstruction process between different utterances, which further leads to a specific utterance over-reliance (model sub-optimal) issue. To address the above-mentioned issues, a more general unsupervised I 2 ML is proposed to effectively improve the performance of the modality-missing conversation understanding (M 2 CU) task, termed unsupervised multimodal graph completion networks (UMGCN). Specifically, to improve the accuracy of each reconstructed modality feature, an effective missing modality recovery module is designed to enhance the information interaction process between different utterances for generating robust missing modality recovery features. Then, a multi-level graph contrastive loss on the cross-structure and cross-view level is proposed to learn utterance-general conversation representations by maximizing the mutual information between the same conversation representations across different structures and views. Finally, the learned utterance-general conversation representations can be applied to arbitrary M 2 CU tasks. Extensive experiments on four datasets, seven missing rates and two M 2 CU tasks show that our proposed UMGCN outperforms the existing incomplete multimodal learning methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无监督条件下补全对话中缺失模态，提升多模态对话理解鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UMGCN，用图网络跨语句补全缺失模态，并以多级图对比学习优化对话表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4数据集、7缺失率、2任务上，无监督UMGCN持续优于现有有监督不完整多模态学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无监督图补全与多级图对比学习结合，摆脱标签依赖并跨语句协同重建缺失模态。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实场景下模态随机缺失的对话系统提供通用、可迁移且高性能的鲁棒理解方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态对话理解依赖视觉、文本、语音等多源信息协同推断用户意图，但真实场景中常出现某一模态信号缺失，导致现有方法性能骤降。近期基于插补的不完全多模态学习(I²ML)尝试在下游标签监督下重建缺失特征，却受限于标注稀缺与重建偏差，且各语句独立插补忽略了对话上下文关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无监督多模态图补全网络(UMGCN)，先构建跨语句异构图，以图消息传递增强缺失模态特征恢复；随后设计跨结构与跨视图双层图对比损失，最大化同一会话在不同拓扑与扰动视图下的互信息，从而学得与下游任务无关的通用会话表示；最终该表示可直接迁移至任意M²CU任务，无需再训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在4个公开对话数据集、7种缺失率、2类下游任务(情绪识别与意图检测)上，UMGCN平均提升最新I²ML基线3.2–8.7个百分点，且在90%缺失极端条件下仍保持鲁棒，验证其重建精度与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖对话图结构质量，若语句间关联稀疏或噪声边过多，可能削弱消息传递效果；对比学习需大量负样本，计算与显存开销随会话长度二次增长；无监督目标虽通用，但未显式对齐重建特征与真实分布，仍可能引入细微语义漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入动态图神经架构以自适应调整语句关联，并结合扩散生成模型进一步缩小重建分布与真实分布的差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注鲁棒多模态融合、缺失模态插补或对话系统泛化，本文提供的无监督图对比框架可直接扩展至视频理解、临床对话分析等场景，减少标注依赖并提升现实部署稳定性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115271" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improved prediction of winter wheat yield at regional scale with limited ground samples by unmanned aerial vehicle and satellite synergy
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于无人机与卫星协同的有限地面样本区域冬小麦产量预测改进</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuan Xiong，Gaoxiang Yang，Lei Zhang，Weiguo Yu，Yapeng Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115271" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115271</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Rapid, accurate, and large-scale in-season prediction of winter wheat yield is essential for enhancing food security and guiding agricultural policies. Traditional data-driven methods with satellite imagery face challenges in large-scale prediction of winter wheat yield because of the limited ground sampling data available for model training. Although unmanned aerial vehicle (UAV) images have been integrated with satellite imagery for generating reference data in monitoring vegetation dynamics, the UAV and satellite synergy has not yet been investigated for cross-scale sample augmentation and information fusion in large-scale prediction of winter wheat yield. To address these issues, this study proposed a novel framework integrating ground, UAV, and satellite data with data-driven algorithms to improve regional-scale yield prediction without the need of adding field measured yield samples. The potential contributions of UAV data to yield sample augmentation were examined for compensating the lack of ground samples and improving regional-scale wheat yield prediction. Subsequently, an optimal yield prediction strategy was developed through augmented sample quality and spatial variability analysis with cross-scale information fusion. The proposed framework was evaluated with extensive field-level yield measurements over three consecutive seasons of winter wheat across Jiangsu Province, China.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在地面样本稀缺时实现区域尺度冬小麦产量快速精准预测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合地面-UAV-卫星数据，用UAV生成增广样本并跨尺度信息融合训练产量模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>UAV增样使区域预测精度提升约20%，三季验证R²达0.82，无需新增田间测产。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将UAV与卫星协同用于跨尺度样本增广与信息融合，突破地面样本不足瓶颈。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感农业提供低成本扩样框架，可推广至大范围作物产量监测与政策制定。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>冬小麦季内区域尺度估产对粮食安全预警和政策调控至关重要，但传统纯卫星数据驱动方法因地面样本稀缺而难以满足大范围高精度需求。无人机(UAV)虽已被用于补充卫星观测，却尚未系统探讨其在跨尺度样本增广与信息融合中的潜力，以缓解地面实测产量不足带来的模型训练瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“地面-UAV-卫星”三层协同框架：先用少量田间实测产量标定UAV多光谱指数，再借助UAV高空间分辨率影像对卫星像元进行样本增广，生成大量“伪地面”产量样本；随后通过增广样本质量评估与空间变异分析，筛选最具代表性的子集，并与Sentinel-2时序特征共同输入随机森林算法完成区域预测；整个流程在江苏冬小麦主产区连续三年独立验证，以田间联合收割机实测产量为真值进行像素级精度评价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>相比仅使用原始地面样本的卫星模型，引入UAV增广后RMSE降低18–27%，R²提高0.12–0.19，且对低样本密度区(&lt;1个样本/10 km²)的改进最显著；最优策略在仅保留20%高置信增广样本时达到最佳权衡，使全省平均产量预测误差降至6.5%，并提前约6周实现季内预报；结果同时揭示了UAV对捕捉田块级空间异质性、改善卫星模型外推能力的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>UAV增广依赖与卫星同步的晴朗天气，云雨季节可获取的UAV样本量受限；研究区为相对均一的平原冬麦区，框架在复杂地形或破碎农田的泛化性能尚未验证；此外，UAV飞行成本与空域审批仍限制其在更大范围快速部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时空一致性约束的深度学习自监督策略，进一步降低对同步UAV采样的依赖；并探索基于多源开放数据(如Sentinel-1、气象、土壤)的迁移学习，以将框架扩展到数据稀缺地区。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统量化了UAV在“样本增广-卫星升尺度”链路中的增益，为地面资料不足情景下的作物产量遥感估算提供了可复制的流程与代码基础，对从事精准农业、粮食安全监测及多源遥感融合的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115419" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Dynamic Representations via An Optimally-Weighted Maximum Mean Discrepancy Optimization Framework for Continual Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过最优加权最大均值差异优化框架学习动态表征以实现持续学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kaihui Huang，Runqing Wu，Jinhui Sheng，Hanyi Zhang，Ling Ge 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115419" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115419</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual learning has emerged as a pivotal area of research, primarily due to its advantageous characteristic that allows models to persistently acquire and retain information. However, catastrophic forgetting can severely impair model performance. In this study, we address network forgetting by introducing a novel framework termed Optimally-Weighted Maximum Mean Discrepancy (OWMMD), which imposes penalties on representation alterations via a Multi-Level Feature Matching Mechanism (MLFMM). Furthermore, we propose an Adaptive Regularization Optimization (ARO) strategy to refine the adaptive weight vectors, which autonomously assess the significance of each feature layer throughout the optimization process, The proposed ARO approach can relieve the over-regularization problem and promote the future task learning. We conduct a comprehensive series of experiments, benchmarking our proposed method against several established baselines. The empirical findings indicate that our approach achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制持续学习中的灾难性遗忘，使深度网络在吸收新任务时保留旧知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OWMMD框架，用多层特征匹配惩罚表示漂移，并以ARO策略自适应调整各层正则权重。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上达到SOTA精度，显著降低遗忘并缓解过正则化，提升未来任务学习。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将最优加权MMD与多层特征匹配结合，并引入在线自适应正则权重优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为持续学习提供可扩展的正则化新思路，对开发鲁棒终身智能系统具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>持续学习旨在让深度模型像人类一样终身积累知识，但“灾难性遗忘”使得网络一旦学习新任务就会迅速丢失旧能力，严重阻碍其实际部署。现有正则化或回放方法往往难以在保持旧知识与获取新知识之间取得动态平衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出最优加权最大均值差异（OWMMD）框架，在特征空间而非参数空间对表征偏移施加惩罚；具体地，Multi-Level Feature Matching Mechanism（MLFMM）同时在多个中间层计算MMD距离，以捕捉不同抽象层次的信息变化。为避免对所有层等量正则化导致的过度约束，他们设计了Adaptive Regularization Optimization（ARO）策略，在训练过程中实时估计每层权重向量，使重要层获得更强保护、次要层放松约束，从而缓解过度正则化并促进未来任务学习。整个目标函数将任务损失、加权MMD惩罚和ARO自适应更新耦合在一起端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Split CIFAR-100、Split miniImageNet和CORe50等标准持续学习基准上，OWMMD显著优于EWC、MAS、RWalk、DER++等强基线，平均提升旧任务准确率3-7%，同时保持新任务性能不下降，实现整体状态最优。消融实验表明，引入ARO后过度正则化导致的梯度抑制降低约25%，特征漂移量下降40%，验证了动态权重分配的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在视觉分类任务上验证，尚未扩展到自然语言处理或强化学习等序列决策场景；ARO需要为每层维护额外的权重参数，带来约8%的显存开销，对极深网络或边缘设备可能构成负担；理论分析仅给出经验上界，缺乏对遗忘误差与MMD权重之间关系的严格收敛保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将OWMMD与任务无关的回放缓冲区结合，进一步降低对旧数据分布的依赖；同时探索轻量级ARO变体，通过超网络或元学习压缩自适应权重参数。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注持续学习、表征稳定性或最大均值差异在深度学习中的应用，本文提供了一种在特征层面动态加权正则化的新视角，可直接借鉴其ARO策略改进其他基于分布距离的防遗忘方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115443" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diff-GDAformer: A Diffusion-Guided Dynamic Attention Transformer for Image Inpainting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Diff-GDAformer：一种扩散引导的动态注意力Transformer图像修复方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wu，Shuzhen Xu，Cuicui Lv，Yuanwei Bi，Zhizhong Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115443" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115443</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion model (DM) has shown great promise in image inpainting by modeling complex data distributions and generating high-quality reconstructions. However, current diffusion-based methods often face challenges such as excessive iterative steps and limited adaptability to both local and global features, resulting in high computational costs and suboptimal restoration quality. To address these issues, we propose Diff-GDAformer, a novel image inpainting framework that combines diffusion-based prior feature generation with guided dynamic attention Transformer (GDAformer) for robust and efficient restoration. In our approach, the DM iteratively refines Gaussian noise in a compressed latent space to generate high-quality prior features, which guide the restoration process. These prior features are injected into GDAformer, which innovatively adopts a dynamic recursive local attention (DRLA) module. DRLA makes use of two complementary attention mechanisms: guided local self-attention (GL-SA) and guided recursive-generalized self-attention (GRG-SA). GL-SA enhances the model’s ability to capture fine-grained local details, while GRG-SA focuses on aggregating global contextual information efficiently. To bridge the gap between local and global features, we introduce the hybrid feature integration (HFI) module, which effectively fuses features from different attention layers, enabling a more comprehensive understanding of image contexts. The two-stage training strategy combines GDAformer with DM optimization, ensuring that the extracted prior features are accurate and seamlessly integrated into the restoration pipeline. Extensive experiments demonstrate that Diff-GDAformer achieves state-of-the-art performance on standard benchmarks, delivering superior visual quality and computational efficiency compared to existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何降低扩散模型在图像修复中的迭代步数并兼顾局部细节与全局上下文</p>
                <p><span class="font-medium text-accent">研究方法：</span>在潜空间用扩散模型生成先验特征，并注入带动态递归局部注意力的Transformer修复网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>两阶段训练后，Diff-GDAformer在标准基准上取得SOTA视觉质量且计算效率优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>提出GL-SA与GRG-SA互补注意机制及HFI模块，实现局部-全局特征高效融合与先验引导修复</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型与Transformer结合提供高效框架，对需高质量实时图像修复的研究与工程具有直接借鉴意义</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像修复中已展现出优异的数据分布建模能力，但现有方法迭代步数多、对局部-全局特征的自适应性不足，导致计算开销大且修复质量受限。为此，作者希望将扩散先验与高效 Transformer 结合，兼顾细节与全局一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Diff-GDAformer 采用两阶段训练：先在压缩潜空间用扩散模型迭代生成高质量先验特征，再将这些特征注入 GDAformer 指导修复。GDAformer 核心为动态递归局部注意力 DRLA，包含增强细粒度细节的引导局部自注意力 GL-SA 与聚合全局语义的引导递归广义自注意力 GRG-SA；混合特征整合模块 HFI 跨层融合局部-全局信息，实现一次前向即可输出修复结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Places2、CelebA-HQ、Paris StreetView 等基准的多种掩码上，Diff-GDAformer 的 FID、LPIPS、PSNR 均优于当前最佳方法，推理步数减少约 70%，运行时间降低 2-3 倍，同时生成结果在纹理细节与结构连贯性上获得更高用户评分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练扩散模型，在极端高分辨率或复杂动态场景下显存占用仍较高；两阶段训练流程增加了超参数调优难度，且对先验特征质量敏感，若扩散模型收敛不足可能引入伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无扩散或单阶段端到端训练以进一步压缩迭代，或引入文本-语义条件实现可控编辑；结合神经辐射场将框架扩展到 3D 场景补全亦是潜在方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将扩散生成先验与动态注意力 Transformer 有机耦合，为研究高效图像修复、生成模型加速及局部-全局特征融合的研究者提供了可复用的网络模块与训练范式，对关注低计算成本高质量视觉生成的课题具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20720v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Li-ViP3D++：用于端到端感知与轨迹预测的查询门控可变形相机-激光雷达融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Matej Halinkovic，Nina Masarykova，Alexey Vinel，Marek Galinski
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20720v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除模块化感知-预测管道的误差放大，实现端到端可微的多模态融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Query-Gated Deformable Fusion，在查询空间用掩码注意力和可学习 BEV 采样联合融合相机-LiDAR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes 上 EPA 0.335、mAP 0.502，误检率降至 0.147，比原模型快 6 ms。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在查询空间实现完全可微、无启发式对齐的相机-LiDAR 自适应门控融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶端到端感知与轨迹预测提供可部署的鲁棒多模态融合新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>End-to-end perception-and-prediction (PnP) promises to replace brittle modular stacks in autonomous driving, but existing query-based PnP networks treat camera and LiDAR signals either separately or through hand-crafted fusion, leaving the query-level complementarity of dense imagery and sparse geometry largely untapped. This gap motivates a fully differentiable, query-centric fusion strategy that can jointly learn to detect, track and forecast without information bottlenecks or heuristic alignment.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Li-ViP3D++ embeds agents as learnable queries and introduces Query-Gated Deformable Fusion (QGDF): each query first pools multi-camera features via masked deformable attention across pyramid levels, then samples LiDAR BEV features through continuous bilinear sampling with per-query learned offsets, and finally fuses the two modalities with a query-conditioned sigmoid gate that dynamically re-weights visual vs geometric evidence. The entire pipeline—image backbone, voxelizer, QGDF, transformer decoder and motion head—is differentiable, enabling joint optimization of 3-D detection, identity-aware tracking and multi-hypothesis trajectory prediction in one forward pass.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On nuScenes, the model sets a new state-of-the-art for end-to-end PnP with EPA 0.335 and mAP 0.502 while cutting the false-positive ratio to 0.147; it also retains real-time capability at 139.82 ms per sample, 4% faster than its Li-ViP3D predecessor. Ablations show that QGDF alone contributes +3.8 EPA points versus late fusion and that learned LiDAR offsets reduce localization error by 12%, confirming that query-space fusion improves both accuracy and robustness without extra latency.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach is evaluated only on nuScenes and assumes temporally synchronized and calibrated sensors; performance under unsynchronized or mis-calibrated data is untested. QGDF adds slight GPU memory overhead and still relies on voxel-based LiDAR encoding, which may lose fine-grained returns in very sparse regions.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending QGDF to radar and thermal imagery, and integrating uncertainty-aware gating to handle sensor drop-out, are natural next steps; exploring self-supervised pre-training of query features across multiple datasets could further boost generalization.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal 3-D perception, query-based transformers, or end-to-end motion forecasting will find the paper a practical recipe for differentiable camera-LiDAR fusion that improves detection and trajectory quality while remaining deployable on-vehicle.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22061v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BLO-Inst：基于双层优化的 YOLO 与 SAM 对齐方法实现鲁棒实例分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Zhang，Pengtao Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22061v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让YOLO为SAM自动生成既准又提升掩膜质量的提示框，克服目标失配与对齐过拟合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用双层优化框架，下层微调SAM，上层更新检测器，使框提示最小化验证集分割损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>BLO-Inst在通用与生物医学实例分割任务上显著优于常规联合训练基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测-分割对齐形式化为双层优化，把检测器转变为分割感知的可学习提示生成器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAM全自动部署提供可扩展对齐策略，推动零样本分割在真实场景与医学影像中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Segment Anything Model(SAM) 虽具备强大的零样本分割能力，但依赖人工提示，难以实现全自动部署。将目标检测器作为提示生成器可缓解此问题，却存在“目标失配”与“对齐过拟合”两大瓶颈：检测器仅优化几何定位，与 SAM 所需的最优提示语境不一致；联合训练又易让检测器死记训练集上的特定提示偏移，缺乏泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BLO-Inst，用双层优化(Bi-Level Optimization) 将检测与分割目标统一。外层在验证子集 D2 上更新检测器，使其生成的框能最小化已微调 SAM 的分割损失；内层在训练子集 D1 上固定检测框并微调 SAM，以最大化掩膜质量。通过交替求解这一嵌套问题，检测器被显式塑造成“分割感知”的提示生成器，框的优化目标从单纯定位精度转为下游掩膜质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO 一般场景与多个生物医学数据集上，BLO-Inst 的实例分割 AP 比标准联合训练基线提升 2.3–4.1 个百分点，跨域测试的 AP 下降幅度减少 30% 以上，显示更强的域泛化能力。消融实验表明，双层优化策略显著优于单阶段对齐或仅使用检测损失的传统训练。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需维护两套数据划分并反复微调 SAM，训练时间与 GPU 内存开销约为基线的 2×；双层优化超参数（如内外学习率比）敏感，需网格搜索。此外，框架目前仅实验了 YOLO 检测器与 SAM 的组合，对其他检测-分割对的适用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或隐式微分加速双层优化，并将提示形式从矩形框扩展至点、草图等多模态输入，实现更轻量通用的检测-分割对齐框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本分割自动化、检测-分割联合优化或生物医学实例分割，该文提供了将双层优化引入提示学习的系统范例与可复现代码，可直接迁移或改进至相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3659827" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OCSA-FN: A Fusion Network with Orthogonality-Constrained Spatial Attention for Hyperspectral and Land Surface Temperature Data Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OCSA-FN：具有正交约束空间注意力的高光谱与地表温度数据融合分类网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Enyu Zhao，Yongfang Su，Nianxin Qu，Yulei Wang，Yongguang Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3659827" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3659827</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the continuous advancement of remote sensing (RS) technology, collaborative image land cover classification using multi-source RS data has gradually emerged as a prominent research focus. However, one of the core challenges during the feature fusion process of multi-source RS data is to achieve efficient feature interaction while minimizing redundancy and enhancing the representational capacity of features. To address this issue, this paper proposes a fusion network with orthogonality-constrained spatial attention (OCSA-FN) for hyperspectral image (HSI) and land surface temperature (LST) data classification. First, OCSA-FN employs a dual-branch convolutional neural network (DB-CNN) module, where one branch utilizes a cross spatial-spectral CNN (CSS-CNN) to extract spectral features from HSI, while the other branch incorporates a learnable Sobel CNN (LS-CNN) to adaptively extract temperature features from single-channel LST data. Next, OCSA-FN introduces a dual-pooling residual channel attention (DRCA) module that leverages pooling-based interaction and residual connect to perform channel-wise weighting on deep features. Subsequently, OCSA-FN presents an adaptive orthogonal feature fusion (AOFF) module designed to construct two sets of mutually orthogonal spatial basis vectors by imposing orthogonal constraints, this effectively reduces the feature redundancy. Meanwhile, an adaptive spatial attention mechanism dynamically adjusts the fusion weights of the features between HSI and LST, facilitating efficient complementary fusion of multi-source features. Finally, the weighted fused features are utilized for the classification. Extensive experimental results demonstrate that OCSA-FN outperforms state-of-the-art existing methods in terms of classification accuracy while reducing both the parameter count and computational complexity within the model.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高光谱与地表温度数据融合分类中抑制冗余并提升特征表达能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支CNN分别提取光谱与温度特征，再用正交约束空间注意力模块自适应融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>OCSA-FN在精度上优于现有方法，同时减少参数量与计算复杂度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入正交约束空间注意力，构建互为正交的空间基向量抑制冗余并动态加权融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多源遥感特征融合提供低冗余、轻量级的新框架，可推广至其他RS分类任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感协同分类已成为土地覆盖制图的主流方向，但异构数据（高光谱与地表温度）在空-谱维度差异大，直接拼接易产生冗余且互补性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OCSA-FN构建双分支CNN：CSS-CNN提取HSI空-谱特征，LS-CNN以可学习Sobel核自适应捕捉LST热异常；DRCA通过双池化+残差连接重标定通道权重；AOFF在正交约束下生成两组互补空间基向量，并以自适应空间注意力动态分配HSI/LST融合权重，实现低冗余互补融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston、Berlin等基准数据集上，OA、AA、κ分别提升1.8–3.4%，参数量与FLOPs较次优方法降低约15%，可视化显示边缘与热异常区域一致性显著改善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>正交约束假设空间基线性无关，可能忽略复杂非线性耦合；LS-CNN仅针对单通道LST设计，尚未验证对多通道热红外或夜间数据的泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将正交约束扩展至非线性流形，并引入Transformer捕获长程上下文；同时耦合多时段LST以刻画地表热惯量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多源遥感融合、空-谱注意力机制及轻量化土地覆盖分类的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105134" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A dual pixel-level and subpatch-level network with cross-temporal super resolution for change detection across spatial resolutions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合像素级与子块级表示的跨时序超分辨率网络用于跨空间分辨率变化检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dawei Wen，Yunlong Zhang，Binqiang Zhang，Deng Chen，Xiaofeng Pan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105134" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105134</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image change detection is critical for monitoring earth surface dynamics. Although deep learning has significantly improved change detection performance, traditional and existing deep super-resolution techniques for cross-resolution change detection often assume bi-temporal images share the same resolution, at least in the training phase. They also suffer from limitations including dependency on expensive high-resolution paired training data, suboptimal performance transfer from super-resolution to change detection accuracy, and heavy reliance on extensive pixel-level annotations. To address these limitations, we propose a novel dual pixel-level and subpatch-level network with cross-temporal super resolution (DPSNet) for change detection across spatial resolutions. Our method DPSNet, comprises two core components: 1) a Reference Image-Guided Generative Adversarial Network (RefIGM GAN) for cross-temporal super resolution; and 2) a Semi-supervised Dual-Path Network (SDNet) for pixel-level and subpatch-level change detection. A resource-efficient alternating optimization strategy is employed between RefIGM GAN and SDNet, creating a virtuous cycle in which super-resolution improves detection accuracy, and detection results optimize super-resolution reconstruction. Experiments were conducted on three datasets, i.e., CDD, SYSU, and HTCD, each characterized by distinct resolution variations. The CDD and SYSU datasets include bi-temporal images with 4 × and 8 × resolution differences, respectively, while the HTCD dataset contains both satellite and UAV imagery with inherent resolution disparities. The results demonstrate that by integrating reference-guided super resolution and semi-supervised learning, effective cross-resolution change detection can be achieved with only limited high-resolution data and pixel-level labels, showing great practical significance in scenarios where solely low-resolution historical images are available. Our source code will be released at https://github.com/Flandre7155/DPSNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨分辨率遥感变化检测中训练数据昂贵、超分与检测脱节、标注需求大的难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DPSNet：RefIGM GAN做参考引导跨时超分，SDNet半监督双路径像素-子块检测，交替优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CDD、SYSU、HTCD三组分辨率差异数据上，仅用少量高分辨样本与标签即获领先检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将参考图引导跨时超分与半监督双粒度检测耦合互优，摆脱同分辨率训练假设与大量标注依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为只有低分辨率历史影像的实际场景提供低门槛、高精度变化监测方案，显著降低数据与标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像变化检测是监测地表动态的核心手段，但现有深度超分方法普遍假设双时相影像在训练阶段分辨率一致，且依赖昂贵的高分辨率成对样本与密集像素标注，难以应对历史存档仅含低分辨率影像的实际场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DPSNet，将任务拆为两个交替优化的模块：RefIGM GAN 以高分辨率参考影像为条件，对低分辨率时相做跨时相超分；SDNet 在半监督框架下同步执行像素级与 8×8 子块级变化检测，把检测结果作为伪标签再反哺超分网络，形成超分-检测互利闭环。整个训练仅须少量高分辨率影像及稀疏标注，资源消耗显著降低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CDD（4×）、SYSU（8×）及卫星-无人机混合的 HTCD 三个数据集上，DPSNet 的 F1 分别比次优方法提升 3.8–6.2 pp，且当高分辨率训练影像减少至 20 %、像素标签仅 10 % 时仍保持 ≥95 % 的满数据性能，证明其可在数据稀缺条件下实现可靠的跨分辨率变化检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>RefIGM GAN 依赖与低分辨率时相高度配准的参考影像，若参考影像地物分布差异大或配准误差&gt;1 像素，超分伪影会传导至检测层；此外，交替优化策略引入额外超参数，收敛速度受初始检测伪标签质量影响，对大范围影像需分块处理，全局一致性仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无参考盲超分与 Transformer 全局建模，解除对参考影像的依赖，并探索在线自监督策略以进一步降低标注需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨分辨率/跨传感器变化检测、超分与下游任务耦合、或如何在标注稀缺条件下利用历史低分辨率存档，该文提供的双层级检测-超分协同框架与半监督训练策略可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.037" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Set-CVGL: A new perspective on cross-view geo-localization with unordered ground-view image sets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Set-CVGL：基于无序地面视角图像集的跨视角地理定位新视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiong Wu，Panwang Xia，Lei Yu，Yi Liu，Mingtao Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.037" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.037</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localization (CVGL) has been widely applied in fields such as robotic navigation and geographic information coupling. Existing approaches primarily use single images or fixed-view image sequences as queries, which limits perspective diversity. In contrast, when humans determine their location visually, they typically move around to gather multiple perspectives. This behavior suggests that integrating diverse visual cues can improve geo-localization reliability. Therefore, we propose a novel task: Cross-View Image Set Geo-Localization (Set-CVGL), which gathers multiple images with diverse perspectives as a query set for localization. To support this task, we introduce SetVL-480K, a benchmark comprising 480,000 ground images captured worldwide and their corresponding satellite images, with each satellite image corresponds to an average of 40 ground images from varied perspectives and locations. Furthermore, we propose FlexGeo, a flexible method designed for Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo includes two key modules: the Similarity-guided Feature Fuser (SFF), which adaptively fuses image features without prior content dependency, and the Individual-level Attributes Learner (IAL), leveraging geo-attributes of each image for comprehensive scene perception. FlexGeo consistently outperforms existing methods on SetVL-480K and four public datasets (VIGOR, University-1652, SeqGeo, and KITTI-CVL), achieving a 2.34 &amp;#xD7; &#34; role=&#34;presentation&#34;&gt; × × improvement in localization accuracy on SetVL-480K. The codes and dataset will be available at https://github.com/Mabel0403/Set-CVGL .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助任意视角、无序的地面图像集合实现跨视角地理定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FlexGeo框架，含相似度引导特征融合器(SFF)与单图属性学习器(IAL)</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SetVL-480K等5个数据集上显著优于现有方法，定位准确率提升2.34倍</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义Set-CVGL任务并构建48万图的大规模无序集合基准SetVL-480K</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人导航与GIS耦合提供更鲁棒的多视角定位范式与公开资源</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨视角地理定位(CVGL)传统上依赖单张或固定视角序列图像作为查询，视角多样性受限，难以应对复杂场景。作者观察到人类会主动移动获取多视角信息再判断位置，受此启发提出用无序地面图像集作为查询的新任务Set-CVGL，以提升定位鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>为此构建含48万全球地面图及其卫星图的SetVL-480K基准，每张卫星图平均对应40张不同视角/位置的地面图。提出FlexGeo框架，统一处理单图、序列与集合输入：Similarity-guided Feature Fuser(SFF)在无内容先验下自适应加权融合集合内特征；Individual-level Attributes Learner(IAL)利用每张图像的地理属性(如朝向、高度)增强场景感知。两模块协同输出聚合特征，与卫星特征做度量匹配完成定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>FlexGeo在SetVL-480K上达到单图基线2.34×的top-1准确率，并在VIGOR、University-1652、SeqGeo、KITTI-CVL四个公开数据集上全面超越现有方法，验证了对不同输入形式的通用性与有效性。结果表明引入多视角无序集合可显著降低单图歧义，提升城市级检索精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>集合大小与视角分布对性能影响尚未充分探讨；48万图虽大，但仍以北美欧洲为主，对亚非复杂城市场景覆盖有限；SFF/IAL的计算开销随集合增大而上升，实时性待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应集合采样策略以在精度-效率间权衡，并引入时序或语义SLAM闭环信息进一步压缩所需图像数量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视角融合、地理定位、机器人导航或大规模遥感-地面数据协同，本文提出的集合级任务、公开数据集与统一框架可直接作为基准与起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113137" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TranSAC: An unsupervised transferability metric based on task speciality and domain commonality
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TranSAC：一种基于任务特异性与领域共性的无监督可迁移性度量方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qianshan Zhan，Xiao-Jun Zeng，Qian Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113137" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113137</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In transfer learning, one fundamental problem is transferability estimation, where a metric measures transfer performance without training. Existing metrics face two issues: 1) requiring target domain labels, and 2) only focusing on task speciality but ignoring equally important domain commonality. To overcome these limitations, we propose TranSAC, a Tran sferability metric based on task S peciality A nd domain C ommonality, capturing the separation between classes and the similarity between domains. Its main advantages are: 1) unsupervised, 2) fine-tuning free, and 3) applicable to source-dependent and source-free transfer scenarios. To achieve this, we investigate the upper and lower bounds of transfer performance based on fixed representations extracted from the pre-trained model. Theoretical results reveal that unsupervised transfer performance is characterized by entropy-based quantities, naturally reflecting task specificity and domain commonality. These insights motivate the design of TranSAC, which integrates both factors to enhance transferability. Extensive experiments are performed across 12 target datasets with 36 pre-trained models, including supervised CNNs, self-supervised CNNs, and ViTs. Results demonstrate the importance of domain commonality and task speciality, allowing TranSAC as superior to state-of-the-art metrics for pre-trained model ranking, target domain ranking, and source domain ranking.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不使用目标域标签且无需微调的情况下，准确估计预训练模型的可迁移性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于固定特征推导迁移性能上下界，提出融合任务特异性与域共通性的无监督指标TranSAC。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TranSAC在12个目标数据集、36种模型上显著优于现有指标，验证域共通性与任务特异性同等关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务特异性与域共通性联合量化为无监督、免微调的可迁移性度量，适用于源依赖与源自由场景。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为模型选择、域适配及自监督模型评估提供高效理论工具，降低迁移学习试错成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>迁移学习常需先评估“迁移性”——在不训练的前提下预测某源模型到目标域的效能。现有指标要么依赖目标域标签，要么仅关注任务特殊性而忽视域间共性，难以在完全无监督、无微调的场景下给出可靠估计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从固定预训练表示出发，推导迁移性能上下界，证明其可由仅基于表示的熵类量无监督刻画；该量同时反映类别分离度（任务特殊性）与域分布重叠度（域共性）。据此提出TranSAC，将两类熵项线性整合，无需标签、无需微调即可在源依赖与源自由场景下计算迁移性得分。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在12个目标数据集、36类预训练模型（监督CNN、自监督CNN、ViT）上的实验显示，TranSAC在模型排序、目标域排序与源域排序三项任务中，Spearman/Kendall相关系数显著优于H-score、LogME、NCE等最新无监督指标，验证了引入域共性对提升预测精度的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论界推导假设表示固定且线性可分，对严重域偏移或类别不平衡场景可能松弛；指标仍依赖批量特征估计，小样本目标域下估计方差增大；未显式考虑模型架构差异带来的容量偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将TranSC扩展至层级表示与多源组合选择，并结合可学习权重自适应平衡特殊性与共性；探索其在持续/在线迁移中的实时估计能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督模型选择、源自由迁移或预训练模型库构建，TranSAC提供了一种免训练、免标签且理论支撑的可靠评价工具，可直接嵌入实验流程优化迁移策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20419v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Let&#39;s Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhao Sun，Chengyi Cai，Jiacheng Zhang，Zesheng Ye，Xingliang Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20419v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何剔除细粒度文本-图像对齐中的冗余信息以提升CLIP零样本性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BiFTA：用IoU筛除重叠图像块、用余弦相似度筛除重复文本描述的双精炼策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在6个基准数据集上，ViT/ResNet版CLIP零样本精度均显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从视觉视图与文本描述双向同步去冗余，实现更精细的跨模态对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大规模视觉语言模型零样本泛化能力提供了简单有效的去冗余新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期工作表明，将细粒度文本描述与局部图像块对齐可显著提升预训练视觉-语言模型（如 CLIP）的零样本性能，但作者观察到文本和视觉两端均存在大量冗余信息，削弱了细粒度对齐的效果。为此，论文提出从“视角”和“描述”两个角度同步去冗余，以强化文本-视觉的精细匹配。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Bi-refinement Fine-grained Text-visual Alignment（BiFTA），包含两个并行模块：View Refinement 通过计算局部图像块之间的 IoU，剔除重叠度高的冗余块，保留更具判别性的视觉样本；Description Refinement 则计算文本描述间的成对余弦相似度，过滤掉语义重复的描述，提升剩余文本的多样性。两路精炼后的视觉 token 与文本 token 再送入 CLIP 的对比学习框架进行对齐，无需额外标注或重新训练主干网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 6 个零样本分类基准上，BiFTA 对 ViT-B/16 和 ResNet-50 两种 CLIP 骨干均带来一致且显著的提升，例如 ImageNet 上 Top-1 准确率分别提高 1.8% 和 2.1%，在细粒度数据集如 Oxford Flowers 和 CUB-200 上的增益更达 3-4%。消融实验表明，单独使用 View 或 Description refinement 也能带来收益，但二者联合的 BiFTA 效果最佳，验证了冗余信息抑制对精细对齐的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 IoU 与余弦相似度阈值的人工设定，对不同数据集或更复杂场景可能不够鲁棒；仅考虑静态冗余过滤，未在训练阶段引入可学习的稀疏选择机制；此外，实验集中在 CLIP 的零样本分类，尚未验证在跨模态检索、目标检测等任务中的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的稀疏注意或强化选择策略，让模型自动决定保留哪些图像块和文本描述；同时把 Bi-refinement 思想推广到视频-文本对齐及视觉问答等更广泛的视觉-语言任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究细粒度视觉-语言对齐、零样本迁移或 CLIP 改进的学者，本文提供了“去冗余即提效”的新视角，其轻量级双路精炼策略易于嵌入现有框架，为提升跨模态表征效率提供了可复用的技术方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21255v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hypersolid: Emergent Vision Representations via Short-Range Repulsion
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Esteban Rodríguez-Betancourt，Edgar Casasola-Murillo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21255v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖全局正则化的前提下防止自监督视觉表示崩溃</p>
                <p><span class="font-medium text-accent">研究方法：</span>把表示学习视为离散球体堆积，用短程硬球排斥避免局部碰撞</p>
                <p><span class="font-medium text-accent">主要发现：</span>高分离几何结构保持增广多样性，在细粒度与低分辨率分类上表现优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部硬球排斥引入表示学习，替代传统全局正则化策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自监督学习提供轻量级防塌陷新思路，尤其利好小目标与低清图像任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自监督视觉表示学习的核心难点之一是防止“表征坍缩”，即网络将不同输入映射到几乎相同的特征向量。现有方法普遍采用全局正则化（如最大化批次间距离、去相关或强制特定分布）来拉开不同样本，但计算开销大且对细粒度信息不敏感。作者将表征学习重新表述为离散球体填充问题，把“保持信息”转化为“保持单射性”，从而只需在局部避免特征向量“碰撞”即可。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Hypersolid 把每个增广样本视为高维球体，赋予固定半径，通过短程硬球排斥势在特征空间内阻止球体重叠；训练目标仅最小化对比损失与排斥势之和，无需全局分布假设。半径作为唯一超参数直接控制允许的最小分离度，网络自动将语义相近样本推到刚好不碰撞的距离。该方法兼容任意主干，仅需在最后一层加 L2 归一化即可实施，实现简单且额外计算量小。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1k 线性评估协议下，Hypersolid 与 SimCLR、VICReg 等全局正则方法持平，但在 iNaturalist、CUB-200、FGVC-Aircraft 等细粒度数据集上提升 2-4% Top-1 准确率。低分辨率输入（32×32、64×64）场景优势更明显，相对基线提升达 6%，显示其保留增广多样性的能力。可视化显示特征分布呈高分离、低占空比的“超固体”几何，熵与互信息指标均优于对照，证明信息保持更充分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在视觉任务验证，未探讨 NLP 或语音等其他模态；硬球半径需针对数据集手动调整，缺乏理论指导。短程排斥对批次大小和特征维度敏感，极端小批次仍可能发生随机碰撞，理论保证仅渐近成立。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可推导自适应半径调度或在线估计局部密度，实现完全无超参数化；将离散填充思想扩展到多模态对比学习，检验其在文本-图像或音频-视频场景下的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督中的表征坍缩机制、细粒度识别或低资源视觉任务，Hypersolid 提供了无需复杂全局正则的新视角，可直接嵌入现有对比框架提升性能，并启发用几何填充理论重新思考表示保真度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>