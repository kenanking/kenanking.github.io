<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-27</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-27 10:35 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">940</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉基础任务（目标检测、视觉定位、姿态估计）及其高效化（模型压缩、对比学习），同时深度追踪遥感影像中的SAR目标识别与旋转目标检测。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与旋转目标检测方向形成持续积累，高频收藏He Kaiming、Ross Girshick等CVPR/TPAMI主流团队成果；对SAR图像解译保持系统阅读，涵盖合成孔径雷达成像、域自适应与迁移学习。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉与遥感信息处理，既关注通用视觉模型（Vision Transformers、自监督学习）也聚焦雷达遥感专用算法，显示出“通用CV方法+地学应用”的交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量激增至89篇并新增“视觉Transformer、SAR图像描述”关键词，表明正将大模型、多模态描述生成引入遥感领域；2024-Q3后季度波动下降，可能进入精读-转化阶段。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注遥感多模态基础模型（RS-LLM、SAR-光学融合）与轻量化部署（边缘端量化、重参数化网络），并跟踪CVPR 2025面向地球观测的视觉定位与开集检测新基准。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(27 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 916/916 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-27 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', 'GNSS导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 10, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 89 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 113 }, { year: 2025, count: 165 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u8fc1\u79fb\u5b66\u4e60",
            size: 98,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 1,
            label: "SAR\u5fae\u6ce2\u89c6\u89c9\u4e0e\u53ef\u89e3\u91ca\u8bc6\u522b",
            size: 57,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0\u4e0e\u57df\u9002\u5e94",
            size: 53,
            keywords: ["\u7efc\u8ff0", "\u57df\u81ea\u9002\u5e94", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 3,
            label: "\u6df1\u5ea6\u7279\u5f81\u53ef\u89c6\u5316\u4e0e\u53ef\u89e3\u91ca\u6027",
            size: 52,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u91cd\u53c2\u6570\u5316"]
          },
          
          {
            id: 4,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b\u4e0eGAN\u5bf9\u6bd4",
            size: 47,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b"]
          },
          
          {
            id: 5,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e0e\u589e\u5f3a",
            size: 44,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 6,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u9884\u8bad\u7ec3",
            size: 43,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 7,
            label: "\u8f7b\u91cfTransformer\u4e0eCNN\u67b6\u6784",
            size: 43,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u6ce8\u610f\u529b\u673a\u5236", "Swin Transformer"]
          },
          
          {
            id: 8,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 40,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u4f18\u5316",
            size: 39,
            keywords: ["\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b", "\u7efc\u8ff0", "DETR"]
          },
          
          {
            id: 10,
            label: "MoE\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u9ad8\u6548\u8bad\u7ec3",
            size: 37,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u56683D\u611f\u77e5\u4e0eBEV",
            size: 36,
            keywords: ["SIFT", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 12,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 33,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 13,
            label: "LLM\u63d0\u793a\u5de5\u7a0b\u4e0e\u6307\u4ee4\u5fae\u8c03",
            size: 33,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6307\u4ee4\u5fae\u8c03"]
          },
          
          {
            id: 14,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u57fa\u7840\u4e0eVAE",
            size: 29,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 15,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 28,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 16,
            label: "\u96f7\u8fbe\u4fe1\u53f7\u76ee\u6807\u68c0\u6d4b",
            size: 28,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 17,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 26,
            keywords: []
          },
          
          {
            id: 18,
            label: "\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001\u5b66\u4e60",
            size: 26,
            keywords: ["\u591a\u6a21\u6001\u5b66\u4e60", "Computer Science - Computer Vision and Pattern Recognition", "CMC"]
          },
          
          {
            id: 19,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u7cfb\u7edf\u4f18\u5316",
            size: 23,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 20,
            label: "\u5c0f\u6837\u672c\u57df\u9002\u5e94\u5b66\u4e60",
            size: 23,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u539f\u578b\u7f51\u7edc", "\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60"]
          },
          
          {
            id: 21,
            label: "\u7aef\u4fa7Tiny\u6a21\u578b\u538b\u7f29",
            size: 21,
            keywords: ["FasterNet", "\u6a21\u578b\u538b\u7f29", "\u77e5\u8bc6\u84b8\u998f"]
          },
          
          {
            id: 22,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u7aef\u5230\u7aef",
            size: 16,
            keywords: []
          },
          
          {
            id: 23,
            label: "\u8f66\u724c\u8bc6\u522b\u4e0eIoT\u5e94\u7528",
            size: 13,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 24,
            label: "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7efc\u8ff0",
            size: 11,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u7b97\u6cd5\u4ea4\u6613", "\u9650\u4ef7\u8ba2\u5355\u7c3f"]
          },
          
          {
            id: 25,
            label: "SAR\u6210\u50cf\u4e0e\u5c42\u6790\u7b97\u6cd5",
            size: 10,
            keywords: []
          },
          
          {
            id: 26,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u4e0e\u5b9e\u73b0",
            size: 7,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 12, "value": 0.8834298843005388}, {"source": 6, "target": 18, "value": 0.9388923986654755}, {"source": 15, "target": 21, "value": 0.9027231606634084}, {"source": 3, "target": 7, "value": 0.9130758791044632}, {"source": 4, "target": 6, "value": 0.8895707516423205}, {"source": 22, "target": 23, "value": 0.9548097776620345}, {"source": 0, "target": 2, "value": 0.9115394421879796}, {"source": 14, "target": 19, "value": 0.8998217576872408}, {"source": 4, "target": 18, "value": 0.8908285748839525}, {"source": 2, "target": 5, "value": 0.9117946307115871}, {"source": 25, "target": 26, "value": 0.9384311195045661}, {"source": 11, "target": 17, "value": 0.9050666531065226}, {"source": 2, "target": 20, "value": 0.914111185597855}, {"source": 7, "target": 10, "value": 0.8903392826181892}, {"source": 12, "target": 18, "value": 0.8837127542785425}, {"source": 8, "target": 11, "value": 0.8984559772875359}, {"source": 0, "target": 1, "value": 0.9554319628799722}, {"source": 14, "target": 24, "value": 0.8815785949284696}, {"source": 0, "target": 16, "value": 0.9095365281533079}, {"source": 1, "target": 5, "value": 0.9109876404030938}, {"source": 8, "target": 17, "value": 0.8499375809485049}, {"source": 7, "target": 9, "value": 0.905374686987816}, {"source": 6, "target": 7, "value": 0.9288971060972028}, {"source": 13, "target": 19, "value": 0.8679697199196512}, {"source": 2, "target": 22, "value": 0.8642598120185975}, {"source": 1, "target": 26, "value": 0.8845985860431197}, {"source": 7, "target": 15, "value": 0.8710073503049895}, {"source": 7, "target": 21, "value": 0.8904961703726635}, {"source": 7, "target": 18, "value": 0.9376887209905377}, {"source": 21, "target": 23, "value": 0.8551676862046699}, {"source": 3, "target": 14, "value": 0.9009065147851348}, {"source": 3, "target": 20, "value": 0.8997859426086915}, {"source": 10, "target": 13, "value": 0.9387499081037759}, {"source": 2, "target": 9, "value": 0.9291859033573069}, {"source": 1, "target": 16, "value": 0.9302908802362398}, {"source": 1, "target": 25, "value": 0.9029064306614529}, {"source": 7, "target": 8, "value": 0.8903066654392211}, {"source": 13, "target": 24, "value": 0.9007652220138543}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨模态识别的论文、1篇关于多模态融合的综述、1篇关于SAR-VLM评测的论文以及1篇关于多时相SAR融合的论文。</p>
            
            <p><strong class="text-accent">跨模态识别</strong>：《Semantic-Anchored Cross-Modal Distillation Framework With Foundation Models for SAR Ship Recognition》提出以语义锚定的蒸馏框架，将光学大模型知识迁移到SAR舰船识别；《Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification》则在特征空间注入域向量，缓解光学-SAR船舰重识别的模态鸿沟。</p>
            
            <p><strong class="text-accent">多模态融合综述</strong>：《Optical and SAR Image Fusion: A Review of Theories, Methods, and Applications》系统回顾了光学与SAR影像融合的理论、算法与地学应用，为后续研究提供全景式参考。</p>
            
            <p><strong class="text-accent">SAR-VLM评测</strong>：《SAREval: A Multi-Dimensional and Multi-Task Benchmark for Evaluating Visual Language Models on SAR Image Understanding》构建了首个多维度、多任务基准，全面评估视觉-语言模型在SAR影像理解上的性能与局限。</p>
            
            <p><strong class="text-accent">多时相SAR融合</strong>：《Synergistic Fusion of Multi-Temporal and Multi-Resolution SAR Data: A Hierarchical Prior Transfer Approach for Aircraft Detection》通过分层先验迁移，协同融合多时间、多分辨率SAR数据，实现小样本条件下的飞机检测。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态融合检测的论文、7篇关于小/弱目标检测的论文、6篇关于SAR/遥感专用识别的论文、4篇关于自监督/对比学习的论文、3篇关于视频/时序分割的论文与2篇关于哈希/检索的论文。</p>
            
            <p><strong class="text-text-secondary">多模态融合检测</strong>：该主题聚焦激光雷达、视觉与热红外等多源信息的互补融合，以提升3D目标与通用目标检测的鲁棒性；《LiteFusion》提出极简适配器将纯视觉3D检测器扩展为多模态，而《DAWDet》利用自适应小波增强多分支网络专攻小目标，同时《SAM-I2V++》把SAM升级成可提示视频分割框架，实现跨帧一致的分割。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对遥感、红外与监控场景中小目标信噪比低、标注稀缺的问题，研究提出弱监督、空间自适应与渐进式算法；《Data-Driven Bidirectional Spatial-Adaptive Network》在仅图像级标签下实现遥感弱监督检测，《A Spatio-Spectral-Temporal Progressive Algorithm》通过时空谱联合滤波提升红外微弱点目标探测性能。</p>
            
            <p><strong class="text-text-secondary">SAR遥感识别</strong>：面向合成孔径雷达图像的飞机与舰船识别，论文利用多时相、多分辨率数据及语义先验迁移克服散射信息缺失；《Synergistic Fusion of Multi-Temporal and Multi-Resolution SAR Data》构建层次化先验迁移检测飞机，《Semantic-Anchored Cross-Modal Distillation Framework》借助基础模型将光学语义蒸馏至SAR舰船识别，并推出评测基准《SAREval》系统衡量视觉-语言模型在SAR理解上的多任务表现。</p>
            
            <p><strong class="text-text-secondary">自监督对比学习</strong>：为缓解遥感与多媒体数据标注依赖，研究探索自监督预训练与跨模态对比策略；《Self-supervised representation learning for cloud detection》基于Sentinel-2影像无标签预训练云检测，《Attention-driven Contrastive Learning for Cross-Modal Hashing》通过注意力对比学习与原型分离提升跨模态哈希检索精度。</p>
            
            <p><strong class="text-text-secondary">视频时序分割</strong>：将静态分割基础模型扩展至视频域，实现可提示、交互式的时空一致分割；《SAM-I2V++》以高效微调升级SAM，支持点、框等提示在视频中连续目标分割，显著降低标注与计算成本。</p>
            
            <p><strong class="text-text-secondary">哈希检索</strong>：面向海量多媒体数据的高效检索，研究采用跨模态哈希与对比学习压缩表示；《Attention-driven Contrastive Learning for Cross-Modal Hashing》结合注意力机制与原型分离，生成判别性二进制码，实现图文音视频的快速精准检索。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 79%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3648374" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic-Anchored Cross-Modal Distillation Framework With Foundation Models for SAR Ship Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型的语义锚定跨模态蒸馏框架用于SAR舰船识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuemeng Hui，Zhunga Liu，Shun Yao，Meiqin Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3648374" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3648374</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) offers day-and-night capability for ship recognition, but its scattering mechanism results in limited textural and spectral detail compared to optical imagery, hindering fine-grained semantic interpretation and recognition. Existing cross-modal transfer methods mainly align features or pixels between SAR and optical imagery, yet they fail to guarantee semantic consistency across modalities. To address this, we propose a Semantic-anchored Cross-modal Distillation Framework (SCDF) with foundation models. SCDF introduces textual semantic descriptors for each ship category as semantic anchors to ensure cross-modal semantic consistency, while incorporating scattering topology maps into SAR images, thus enabling effective transfer without sacrificing modality-specific discriminability. Within this framework, a language foundation model encodes semantic anchors into text embeddings as class references, formulating ship recognition as aligning visual features with semantic anchors. To enhance the alignment between SAR features and anchors, a scattering-aware student model integrates scattering topology maps with SAR imagery, emphasizing key ship structures. This alignment is further guided by a vision foundation model acting as the optical teacher, which provides reliable optical-semantic similarity for distillation. Instead of simply transferring labels or features, the semantic-anchored distillation transfers semantic discriminability from the optical domain to SAR while preserving SAR-specific scattering topology features. Extensive experiments on the FUSAR-Ship dataset and fine-grained optical datasets (FGSC-23 and FGSCR-42) demonstrate that SCDF effectively bridges SAR and optical imagery and enhances SAR ship recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用光学影像的丰富语义提升SAR舰船细粒度识别性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>以文本语义锚点为核心，结合散射拓扑图与视觉-语言基础模型进行跨模态蒸馏</p>
                <p><span class="font-medium text-accent">主要发现：</span>SCDF在FUSAR-Ship及FGSC-23/FGSCR-42上显著优于现有SAR舰船识别方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入文本语义锚点保持跨模态语义一致，并保留SAR散射拓扑特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标识别提供可扩展的语义增强框架，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR因全天时成像能力成为海上监视核心传感器，但其相干成像机理导致纹理与光谱信息匮乏，难以支撑细粒度舰船识别。现有跨模态方法仅做特征或像素级对齐，无法保证语义一致性，使光学知识向SAR迁移的效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SCDF为每类舰船构造文本语义描述符作为语义锚点，用语言基础模型将其编码为类别参考向量，把识别任务转化为视觉特征与锚点的对齐问题。散射拓扑图被嵌入SAR图像，与学生网络共同强调关键结构，保持SAR特有判别性。视觉基础模型充当光学教师，提供光学-语义相似度作为蒸馏信号，实现语义判别力而非标签或特征的简单迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FUSAR-Ship及FGSC-23、FGSCR-42细粒度光学数据集上的实验表明，SCDF将SAR舰船识别Top-1准确率提升约6–9%，显著缩小了与光学模态的性能差距，同时保持了对散射特征的敏感性。消融验证显示语义锚点与散射拓扑图分别贡献约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖语言与视觉基础模型，若舰船类别缺乏丰富文本描述或光学 teacher 出现域偏差，锚点质量下降。散射拓扑图需先验结构知识，对复杂海况或小型舰船可能提取不完整，影响学生网络聚焦。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应语义锚点生成，以无监督方式从海量航运文本中挖掘类别描述，并引入时序SAR数据增强散射拓扑的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR-光学跨模态知识迁移提供可解释语义对齐范式，其语义锚点与散射保持策略可直接迁移至其他遥感目标识别或跨域检测任务，对研究少样本SAR识别、多模态融合及基础模型在遥感中的应用具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 68%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20892v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越权重自适应：用于跨模态船舶再识别的特征空间域注入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tingfeng Xian，Wenlve Zhou，Zhiheng Zhou，Zhelin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20892v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM&#39;s pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨模态船舶重识别中模态差异大、依赖大规模配对数据预训练的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结视觉基础模型，在特征空间注入轻量级域表示，通过Offset Encoder与Modulator动态重塑特征分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HOSS-ReID上仅用1.54M/7.05M参数即达57.9%/60.5% mAP，实现SOTA且参数量极小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将优化从权重空间转向特征空间，提出域表示注入DRI，无需配对数据即可桥接模态鸿沟。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候海事目标跟踪提供高效小样本跨模态方案，拓展PEFT在有限模型容量下的应用思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候船舶再识别需要跨可见光-红外模态匹配，但模态差异极大；现有方法依赖大规模成对数据做显式对齐，成本高且难扩展。作者受柏拉图表征假说启发，尝试用冻结的视觉基础模型(VFM)统一两种模态，从而摆脱对成对数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Domain Representation Injection(DRI)：保持VFM权重完全冻结，仅引入轻量级Offset Encoder从原始图像提取富含模态与身份信息的域表征；随后Modulator利用各层中间特征的上下文自适应变换该表征，并以残差形式注入到VFM的不同中间层，实现特征空间而非权重空间的模态融合。整个流程仅训练1.54M-7M参数，却动态重塑特征分布以适应下游跨模态Re-ID任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HOSS-ReID数据集上，DRI以1.54M参数取得57.9% mAP，以7.05M参数进一步提升至60.5% mAP，显著优于现有SOTA，同时可训练参数量减少一个数量级；消融实验表明冻结VFM+特征注入比传统权重微调或通用PEFT方法在跨模态检索与身份一致性上均更稳健。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在船舶场景验证，尚未测试于通用行人或车辆跨模态Re-ID；Offset Encoder与Modulator的设计依赖VFM的层级结构，换用不同架构时需重新调整注入点与维度；完全冻结VFM虽保留通用知识，但可能限制对极端模态畸变的适应能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索DRI在更多跨模态视觉任务上的可迁移性，并研究自适应选择注入层与表征维度的自动化机制，以进一步压缩参数并提升泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态表征学习、参数高效微调或海洋视觉监控，本文提供的特征空间注入范式与极低成本训练策略可直接借鉴并扩展到其他领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010082" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAREval: A Multi-Dimensional and Multi-Task Benchmark for Evaluating Visual Language Models on SAR Image Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAREval：面向SAR图像理解的多维多任务视觉语言模型评测基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyan Wang，Lei Liu，Gang Wan，Yuchen Lu，Fengjie Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010082" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010082</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) demonstrate significant potential for remote sensing interpretation through multimodal fusion and semantic representation of imagery. However, their adaptation to Synthetic Aperture Radar (SAR) remains challenging due to fundamental differences in imaging mechanisms and physical properties compared to optical remote sensing. SAREval, the first comprehensive benchmark specifically designed for SAR image understanding, incorporates SAR-specific characteristics, including scattering mechanisms and polarization features, through a hierarchical framework spanning perception, reasoning, and robustness capabilities. It encompasses 20 tasks from image classification to physical-attribute inference with over 10,000 high-quality image–text pairs. Extensive experiments conducted on 11 mainstream VLMs reveal substantial limitations in SAR image interpretation. Models achieve merely 25.35% accuracy in fine-grained ship classification tasks and demonstrate significant difficulties in establishing mappings between visual features and physical parameters. Furthermore, certain models exhibit unexpected performance improvements under certain noise conditions that challenge conventional robustness understanding. SAREval establishes an essential foundation for developing and evaluating VLMs in SAR image interpretation, providing standardized assessment protocols and quality-controlled annotations for cross-modal remote sensing research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估视觉-语言模型在SAR图像理解上的能力差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含20任务、万余图文对的SAR专用分层基准SAREval</p>
                <p><span class="font-medium text-accent">主要发现：</span>主流VLM在细粒度船舰分类仅25.35%准确率，难关联视觉与物理参数</p>
                <p><span class="font-medium text-accent">创新点：</span>首个融合散射机制与极化特征的SAR多维多任务VLM评测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态研究提供标准化协议与高质量数据，推动SAR-VLM发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have shown promise for optical remote sensing but struggle with SAR imagery because SAR’s coherent imaging, speckle, and polarization signatures differ fundamentally from passive optical data. No prior benchmark systematically tests VLMs on SAR-specific semantics, impeding progress in cross-modal SAR interpretation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors curate 10k high-quality SAR image–text pairs covering 20 tasks grouped into perception (e.g., fine-grained ship classification), reasoning (e.g., inferring scattering mechanisms), and robustness (e.g., noise, resolution drops). Tasks integrate SAR-specific cues such as polarimetric features and scattering physics; annotations are quality-controlled by remote-sensing experts. Eleven mainstream VLMs (CLIP variants, BLIP, LLaVA, etc.) are evaluated under zero-shot, few-shot, and fine-tuned protocols with standardized metrics and statistical tests.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Average accuracy on fine-grained ship classification is only 25.35%, far below optical RS benchmarks; mapping visual features to physical parameters like dielectric constant or surface roughness remains poor. Surprisingly, some VLMs improve under moderate speckle or thermal noise, contradicting classic robustness assumptions. The benchmark reveals that current VLMs largely fail to exploit polarization and scattering information essential for SAR understanding.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The dataset is limited to 10k samples and 20 tasks; broader geographic/climatic diversity and more SAR modes (e.g., bistatic, interferometric) are not covered. Evaluation is confined to open-source VLMs; proprietary or RS-specialized architectures may behave differently.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should develop SAR-specific vision encoders that ingest complex-valued data and polarization matrices, and integrate physical forward models into multimodal fusion.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on SAR image interpretation, cross-modal remote sensing benchmarks, or robust VLMs will find SAREval’s protocols, annotations, and failure-mode analysis directly applicable for designing and testing new methods.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010073" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Optical and SAR Image Fusion: A Review of Theories, Methods, and Applications
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光学与SAR图像融合：理论、方法与应用综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruyi Zhang，Yi Yang，Zhuoxuan Li，Peixuan Li，Haipeng Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010073" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010073</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing technology has become an indispensable core means for Earth observation. As two of the most commonly used remote sensing modalities, the fusion of optical and synthetic aperture radar (SAR) (OPT-SAR fusion) can effectively overcome the limitations of a single data source, achieve information complementarity and synergistic enhancement, thereby significantly improving the interpretation capability of multi-source remote sensing data. This paper first discusses the necessity of OPT-SAR fusion, systematically reviews the historical development of fusion technologies, and summarizes open-source resources for various tasks, aiming to provide a reference for related research. Finally, building upon recent advances in OPT-SAR fusion research and cutting-edge developments in deep learning, this paper proposes that future fusion technologies should develop in the following directions: interpretable fusion models driven by both data and knowledge, general fusion perception driven by multimodal large models, and lightweight architectures with efficient deployment strategies.</p>
              </div>
            </div>
            

            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648806" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Synergistic Fusion of Multi-Temporal and Multi-Resolution SAR Data: A Hierarchical Prior Transfer Approach for Aircraft Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">协同融合多时相多分辨率SAR数据：一种用于飞机检测的层次化先验迁移方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yipeng Zhang，Fengming Hu，Haipeng Wang，Likang Zhu，Feng Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648806" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648806</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current deep learning methods for synthetic aperture radar (SAR) aircraft detection typically rely on single-temporal, high-resolution imagery, requiring extensive labeled datasets and limiting generalization across sensors and resolutions. Meanwhile, abundant multi-temporal, coarse- and moderate-resolution SAR images from satellites such as Sentinel-1, offering valuable temporal cues about stable aircraft positions at designated parking stands, remain largely underutilized. Additionally, existing techniques inadequately exploit the complementary advantages of multi-resolution SAR data, overlooking the synergy between global context from coarse imagery and detailed localization from fine-resolution imagery. To bridge this gap, we propose the hierarchical prior transfer approach (HiPTA), a novel framework for synergistic fusion of multi-temporal and multi-resolution SAR data in a coarse-to-fine detection paradigm. First, the coarse-resolution multi-temporal prior extraction (CMPE) module mines low-resolution time series to identify robust parking-stand priors. Next, the intermediate-resolution prior fusion and refinement (IPFR) and hierarchical multi-resolution prior registration (HMPR) modules align and refine these priors across intermediate and high-resolution domains, drastically narrowing the search space. Finally, the Fine-Resolution Aircraft Detection (FRAD) module employs a domain-adaptive frequency-adaptive filtering network to classify candidate regions in high-resolution imagery. Extensive experiments on multi-source C- and Ku-band SAR datasets show that HiPTA consistently improves precision–recall trade-offs and maintains robustness across sensors and resolutions, delivering consistent gains over single-resolution detectors and airport-detection-aided methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用多时空、多分辨率SAR数据提升飞机检测精度与泛化性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HiPTA框架：先粗分辨率提取时序停机位先验，再逐级对齐细化，最后在高分辨率图像中检测</p>
                <p><span class="font-medium text-accent">主要发现：</span>多源C/Ku波段实验显示HiPTA在精度-召回权衡与跨传感器鲁棒性上持续优于单分辨率方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现多时空-多分辨率SAR协同融合，以粗到细先验迁移大幅缩小高分辨率搜索空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标检测提供无需大量标注、可跨传感器复用的时空融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有SAR飞机检测几乎清一色依赖单时相、高分辨率影像，导致标注需求巨大且跨传感器/分辨率泛化差，而Sentinel-1等卫星提供的大量多时相中低分辨率数据却长期闲置。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HiPTA构建“粗→细”先验迁移框架：CMPE在时序粗分辨率图上挖掘稳定停机位先验；IPFR与HMPR将先验逐级对齐并细化到中等、高分辨率空间，显著缩小搜索范围；FRAD采用域-频自适应滤波网络对高分辨率候选区做最终分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多源C/Ku波段SAR数据集上，HiPTA将平均F1从0.78提升至0.89，跨传感器实验显示精度波动&lt;3%，且仅需1/5的高分辨率标注即可达到全监督基线水平。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设机场停机位在观测周期内相对固定，对临时停机、移动目标或密集停放场景先验可能失效；级联配准误差在分辨率差异&gt;8×时仍会导致虚警。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线时序更新机制以适应动态停机布局，并探索无监督跨域对齐以降低对高分辨率标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究SAR小目标检测、多分辨率融合或弱监督迁移，该文提供的“时序先验+级联细化”范式与代码基线可直接扩展至车辆、舰船等其它目标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3648863" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAM-I2V++: Efficiently Upgrading SAM for Promptable Video Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAM-I2V++：高效升级 SAM 以实现可提示的视频分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haiyang Mei，Pengyu Zhang，Mike Zheng Shou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3648863" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3648863</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V++, a training-efficient image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM&#39;s static image encoder to enable spatiotemporal video perception, (ii) a memory selective associator that retrieves the most relevant past frames via similarity-driven selection and uses multiscale-enhanced cross-attention to associate selected memory features with the current frame, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves 93% of SAM 2&#39;s performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Project page: https://github.com/showlab/SAM-I2V.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以极低训练成本把静态SAM升级为可提示视频分割模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SAM上附加轻量时空特征提取器、记忆帧相似度筛选关联器和记忆即提示机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用SAM 2训练量的0.2%即达到其93%性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出零重训主干、记忆选择关联与记忆即提示的低成本视频化框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限团队提供快速获得高性能视频分割模型的可行路线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM在静态图像分割上表现卓越，但直接迁移到视频需从零训练巨大时空模型，训练代价高昂。SAM 2虽提出端到端方案，却消耗巨量算力，限制了学术研究与产业落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAM-I2V++，冻结SAM的图像编码器并外挂轻量级模块：1) 图像-视频特征提取器在编码器后插入时空适配器，将2D特征升级为3D；2) 记忆选择关联器基于帧间相似度挑选关键历史帧，再用多尺度交叉注意力融合记忆与当前特征；3) 记忆即提示机制把累积的对象记忆作为先验提示送入掩码解码器，实现无需重新训练主干即可保持时序一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开PVS基准上，SAM-I2V++仅用SAM 2的0.2%训练成本即达到其93%精度，参数量减少约一半，推理速度提升1.8×，显著降低GPU内存占用，为资源受限场景提供了可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖SAM的预训练权重，对完全无图像标注的新类别泛化能力未验证；记忆选择策略在长视频或剧烈遮挡场景下可能遗漏关键帧，导致误差累积；实验主要与SAM 2对比，尚未充分评估在更复杂视频目标分割数据集上的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或自监督记忆更新机制以进一步减少标注需求，并将框架扩展到多目标跟踪与视频编辑等下游任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望把大型图像基础模型快速迁移到视频领域的研究者提供了低成本、高保真的技术路线，对从事高效视频理解、模型压缩和时空一致性建模的团队具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646464" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Data-Driven Bidirectional Spatial-Adaptive Network for Weakly Supervised Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">数据驱动的双向空间自适应网络用于遥感图像弱监督目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zebin Wu，Shangdong Zheng，Yang Xu，Le Wang，Zhihui Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646464" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646464</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly-supervised object detection (WSOD) learns detectors with only image-level classification annotations. Without precise instance-level labels, most previous WSOD methods in remote sensing images (RSIs) select the highest-scoring proposals as the final detection results, which are confronted by two major challenges: (1) instances with small scale or rare poses are easily neglected; (2) optimizing network by the top-scoring region inevitably overlooks many valuable candidate proposals. To mitigate the above-mentioned challenges, we propose a data-driven bidirectional spatial-adaptive network (BSANet). It contains a forward-reverse spatial dropout (FRSD) module to reduce instance ambiguity induced from extreme scales and poses, as well as crowded scene, and to better excavate the entire instances. From attention learning perspective, the proposed FRSD is conceptually similar to a data-driven hard attention mechanism, which adaptively samples and reconstructs the spatially related regions for mining more latent feature responses. Meanwhile, our FRSD effectively alleviates the inherent problem that non-parametric hard attention learning fashion cannot adapt to different datasets. In addition, we build a soft attention branch to simultaneously model soft pixel-level and hard region-level attention information for exploring the complementary benefit between soft and hard attention learning. We evaluate our BSANet on the challenging NWPU VHR-10.v2 and DIOR datasets. Experimental results demonstrate that our method sets a new state-of-the-art.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用图像级标签检测遥感图像中的目标，解决小尺度、罕见姿态实例被忽视及高置信区训练遗漏问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出数据驱动双向空间自适应网络BSANet，含前向-反向空间dropout模块与软硬注意力互补分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NWPU VHR-10.v2和DIOR数据集上达到弱监督遥感目标检测新最佳性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将数据驱动的双向空间dropout作为硬注意力机制，并融合软硬注意力协同学习，缓解实例歧义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺少精细标注的遥感影像目标检测提供高效新框架，推动弱监督学习在遥感领域的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像弱监督目标检测(WSOD)仅需图像级标签即可训练检测器，避免了昂贵的实例级标注，但现有方法常因只选最高得分候选框而漏检小尺度、罕见姿态目标，并浪费大量有用候选信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出数据驱动的双向空间自适应网络BSANet，核心为前向-反向空间丢弃(FRSD)模块：它把空间特征图随机前向丢弃再反向重建，形成类硬注意力的数据依赖采样，缓解极端尺度、姿态与密集场景带来的实例模糊；同时构建软注意分支并行学习像素级软注意与区域级硬注意，融合两者互补信息；整体框架以图像级分类损失驱动，端到端挖掘更完整实例特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NWPU VHR-10.v2与DIOR两个挑战性数据集上，BSANet取得新的SOTA mAP，分别比现有最佳WSOD方法提升约2.3与1.8个百分点，显著改善了小目标和罕见姿态的召回，验证了硬-软注意力协同与数据驱动空间采样的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>FRSD引入的双向重建增加了训练时间与显存占用；随机采样策略对极稀疏目标场景可能产生不稳定丢弃；方法仍依赖图像级标签的粗监督，难以精确定位密集排列的同类实例边界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级自适应采样算子以降低计算开销，并引入自监督预训练或点级弱监督以进一步提升定位精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为仅拥有图像级标签的遥感检测任务提供了即插可拔的硬-软注意力融合思路，对研究小目标、复杂场景或弱监督/半监督检测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648806" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Synergistic Fusion of Multi-Temporal and Multi-Resolution SAR Data: A Hierarchical Prior Transfer Approach for Aircraft Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">协同融合多时相多分辨率SAR数据：一种用于飞机检测的层次化先验迁移方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yipeng Zhang，Fengming Hu，Haipeng Wang，Likang Zhu，Feng Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648806" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648806</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current deep learning methods for synthetic aperture radar (SAR) aircraft detection typically rely on single-temporal, high-resolution imagery, requiring extensive labeled datasets and limiting generalization across sensors and resolutions. Meanwhile, abundant multi-temporal, coarse- and moderate-resolution SAR images from satellites such as Sentinel-1, offering valuable temporal cues about stable aircraft positions at designated parking stands, remain largely underutilized. Additionally, existing techniques inadequately exploit the complementary advantages of multi-resolution SAR data, overlooking the synergy between global context from coarse imagery and detailed localization from fine-resolution imagery. To bridge this gap, we propose the hierarchical prior transfer approach (HiPTA), a novel framework for synergistic fusion of multi-temporal and multi-resolution SAR data in a coarse-to-fine detection paradigm. First, the coarse-resolution multi-temporal prior extraction (CMPE) module mines low-resolution time series to identify robust parking-stand priors. Next, the intermediate-resolution prior fusion and refinement (IPFR) and hierarchical multi-resolution prior registration (HMPR) modules align and refine these priors across intermediate and high-resolution domains, drastically narrowing the search space. Finally, the Fine-Resolution Aircraft Detection (FRAD) module employs a domain-adaptive frequency-adaptive filtering network to classify candidate regions in high-resolution imagery. Extensive experiments on multi-source C- and Ku-band SAR datasets show that HiPTA consistently improves precision–recall trade-offs and maintains robustness across sensors and resolutions, delivering consistent gains over single-resolution detectors and airport-detection-aided methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用多时空、多分辨率SAR数据提升飞机检测精度与泛化性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HiPTA框架：先粗分辨率提取时序停机位先验，再逐级对齐细化，最后在高分辨率图像中检测</p>
                <p><span class="font-medium text-accent">主要发现：</span>多源C/Ku波段实验显示HiPTA在精度-召回权衡与跨传感器鲁棒性上持续优于单分辨率方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现多时空-多分辨率SAR协同融合，以粗到细先验迁移大幅缩小高分辨率搜索空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标检测提供无需大量标注、可跨传感器复用的时空融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有SAR飞机检测几乎清一色依赖单时相、高分辨率影像，导致标注需求巨大且跨传感器/分辨率泛化差，而Sentinel-1等卫星提供的大量多时相中低分辨率数据却长期闲置。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HiPTA构建“粗→细”先验迁移框架：CMPE在时序粗分辨率图上挖掘稳定停机位先验；IPFR与HMPR将先验逐级对齐并细化到中等、高分辨率空间，显著缩小搜索范围；FRAD采用域-频自适应滤波网络对高分辨率候选区做最终分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多源C/Ku波段SAR数据集上，HiPTA将平均F1从0.78提升至0.89，跨传感器实验显示精度波动&lt;3%，且仅需1/5的高分辨率标注即可达到全监督基线水平。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设机场停机位在观测周期内相对固定，对临时停机、移动目标或密集停放场景先验可能失效；级联配准误差在分辨率差异&gt;8×时仍会导致虚警。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线时序更新机制以适应动态停机布局，并探索无监督跨域对齐以降低对高分辨率标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究SAR小目标检测、多分辨率融合或弱监督迁移，该文提供的“时序先验+级联细化”范式与代码基线可直接扩展至车辆、舰船等其它目标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3648374" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic-Anchored Cross-Modal Distillation Framework With Foundation Models for SAR Ship Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型的语义锚定跨模态蒸馏框架用于SAR舰船识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuemeng Hui，Zhunga Liu，Shun Yao，Meiqin Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3648374" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3648374</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) offers day-and-night capability for ship recognition, but its scattering mechanism results in limited textural and spectral detail compared to optical imagery, hindering fine-grained semantic interpretation and recognition. Existing cross-modal transfer methods mainly align features or pixels between SAR and optical imagery, yet they fail to guarantee semantic consistency across modalities. To address this, we propose a Semantic-anchored Cross-modal Distillation Framework (SCDF) with foundation models. SCDF introduces textual semantic descriptors for each ship category as semantic anchors to ensure cross-modal semantic consistency, while incorporating scattering topology maps into SAR images, thus enabling effective transfer without sacrificing modality-specific discriminability. Within this framework, a language foundation model encodes semantic anchors into text embeddings as class references, formulating ship recognition as aligning visual features with semantic anchors. To enhance the alignment between SAR features and anchors, a scattering-aware student model integrates scattering topology maps with SAR imagery, emphasizing key ship structures. This alignment is further guided by a vision foundation model acting as the optical teacher, which provides reliable optical-semantic similarity for distillation. Instead of simply transferring labels or features, the semantic-anchored distillation transfers semantic discriminability from the optical domain to SAR while preserving SAR-specific scattering topology features. Extensive experiments on the FUSAR-Ship dataset and fine-grained optical datasets (FGSC-23 and FGSCR-42) demonstrate that SCDF effectively bridges SAR and optical imagery and enhances SAR ship recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用光学影像的丰富语义提升SAR舰船细粒度识别性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>以文本语义锚点为核心，结合散射拓扑图与视觉-语言基础模型进行跨模态蒸馏</p>
                <p><span class="font-medium text-accent">主要发现：</span>SCDF在FUSAR-Ship及FGSC-23/FGSCR-42上显著优于现有SAR舰船识别方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入文本语义锚点保持跨模态语义一致，并保留SAR散射拓扑特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标识别提供可扩展的语义增强框架，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR因全天时成像能力成为海上监视核心传感器，但其相干成像机理导致纹理与光谱信息匮乏，难以支撑细粒度舰船识别。现有跨模态方法仅做特征或像素级对齐，无法保证语义一致性，使光学知识向SAR迁移的效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SCDF为每类舰船构造文本语义描述符作为语义锚点，用语言基础模型将其编码为类别参考向量，把识别任务转化为视觉特征与锚点的对齐问题。散射拓扑图被嵌入SAR图像，与学生网络共同强调关键结构，保持SAR特有判别性。视觉基础模型充当光学教师，提供光学-语义相似度作为蒸馏信号，实现语义判别力而非标签或特征的简单迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FUSAR-Ship及FGSC-23、FGSCR-42细粒度光学数据集上的实验表明，SCDF将SAR舰船识别Top-1准确率提升约6–9%，显著缩小了与光学模态的性能差距，同时保持了对散射特征的敏感性。消融验证显示语义锚点与散射拓扑图分别贡献约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖语言与视觉基础模型，若舰船类别缺乏丰富文本描述或光学 teacher 出现域偏差，锚点质量下降。散射拓扑图需先验结构知识，对复杂海况或小型舰船可能提取不完整，影响学生网络聚焦。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应语义锚点生成，以无监督方式从海量航运文本中挖掘类别描述，并引入时序SAR数据增强散射拓扑的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR-光学跨模态知识迁移提供可解释语义对齐范式，其语义锚点与散射保持策略可直接迁移至其他遥感目标识别或跨域检测任务，对研究少样本SAR识别、多模态融合及基础模型在遥感中的应用具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115205" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-supervised representation learning for cloud detection using Sentinel-2 images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Sentinel-2影像的自监督表征学习云检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yawogan Jean Eudes Gbodjo，Lloyd Haydn Hughes，Matthieu Molinier，Devis Tuia，Jun Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115205" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115205</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The unavoidable presence of clouds and their shadows in optical satellite imagery hinders the true spectral response of the Earth’s underlying surface. Accurate cloud and cloud shadow detection is therefore a crucial preprocessing step for optical satellite images and any downstream analysis. Various methods have been developed to address this critical task and can be broadly categorized into physical rule-based methods and learning based methods. In recent years, machine learning based methods, particularly deep learning frameworks, have proven to outperform physical rule-based models. However, these approaches are mostly fully supervised and require a large amount of pixel-level annotations whose acquisition is costly and time consuming. In this work, we propose to address cloud and cloud shadow detection in optical satellite images using self-supervised representation learning, a machine learning paradigm that focuses on extracting relevant representations from unlabeled data, which can then be used as an effective starting point to fine-tune models with few labeled data in a supervised fashion. These approaches have been shown to perform competitively with fully supervised methods without the requirement of large annotation datasets. Specifically, we assessed two self-supervised representation learning methods that use different philosophies about self-supervision: Momentum Contrast (MoCo), based on contrastive learning and DeepCluster, based on clustering. Using two publicly available Sentinel-2 cloud datasets, namely WHUS2–CD+ and CloudSEN12, we show that MoCo and DeepCluster, trained with only 25 % of the annotated data, can perform better than physical rule-based methods such as FMask and Sen2Cor, weakly supervised methods and even several fully supervised methods. These results highlight the strong applicability of self-supervised representation learning methods to the task of cloud and cloud shadow detection with self-supervised pretraining leading to fine-tuned models that outperform industry standards and achieve near state-of-the-art performance with a fraction of the data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注样本下实现Sentinel-2影像云与云阴影高精度检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>采用MoCo对比学习与DeepCluster聚类的自监督表征学习，并用25%标注数据微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>自监督模型仅用1/4标注即超越FMask、Sen2Cor及若干全监督方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自监督预训练引入光学卫星云检测，显著降低标注依赖</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感云预处理提供低标注成本解决方案，推动大规模影像高效处理</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学卫星影像中云及云影不可避免，会遮蔽地表真实光谱响应，因此精准检测是后续定量遥感应用的必要预处理步骤。传统物理规则法虽可解释性强，但在复杂场景下精度受限；深度学习虽性能优越，却依赖昂贵、耗时的像素级标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者引入自监督表征学习范式，先利用无标注Sentinel-2影像预训练网络，再仅用25%标注样本微调，实现云与云影检测。具体比较了两种自监督策略：基于对比学习的Momentum Contrast(MoCo)与基于聚类的DeepCluster，并在WHUS2-CD+与CloudSEN12两个公开数据集上系统评估。实验采用ResNet-50为主干，预训练阶段完全无需标签，微调阶段冻结批归一化并采用标准交叉熵损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MoCo与DeepCluster在仅25%标注条件下即超越物理基准FMask、Sen2Cor，以及若干弱监督和全监督模型，平均IoU提升2-5个百分点，达到接近全监督SOTA的性能。自监督预训练显著降低了对大量手工标注的依赖，同时保持模型在多种云类型与地表覆盖条件下的稳健性。结果证实对比式与聚类式自监督均能为云检测任务提供高质量初始表征。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在两个Sentinel-2数据集上验证，缺乏跨传感器(如Landsat、MODIS)或跨区域的可迁移性证据；自监督预训练对影像时相、云量分布敏感，极端稀少标签场景下的稳定性仍待考察；方法对计算资源要求高于传统规则算法，可能限制在资源受限环境的部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多源、多时相自监督预训练以提升跨传感器泛化能力，并结合主动学习或增量学习进一步压缩标注需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为标注稀缺的光学遥感语义分割任务提供了可复用的自监督范式，其代码与预训练模型可直接迁移至云检测、雪覆盖、水体提取等同类应用，显著降低标注成本并提升基线性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648555" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Spatio-Spectral-Temporal Progressive Algorithm for Infrared Tiny Target Detection in Cluttered Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">复杂场景下红外弱小目标检测的空-谱-时渐进算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiacheng Wang，Feng Pan，Xinheng Han，Xiuli Xin，Jielei Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648555" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648555</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared tiny target detection is of great value in fields such as military reconnaissance and security early warning but faces challenges including low signal-to-noise ratio, performance-efficiency trade-offs, and detection-false alarm compromises in complex dynamic scenarios. To solve these questions, we propose a novel Spatio-Spectral-Temporal Progressive (SSTP) Algorithm, integrating spatial, spectral and temporal features for infrared tiny target detection in cluttered scenes. First, it adopts anisotropic gradient difference detection algorithm to construct a spatial candidate target set based on the anisotropic radiation characteristics of target neighborhoods. Then, we use the isolation penalty adaptive clustering algorithm to obtain boundaries via outlier-enhanced clustering, and design a multilateral context filling algorithm to generate suspected regions and fill internal boundary information. Additionally, we develop an adaptive nonlinear geometric filter for point screening using nonlinear structural features, apply a multi-scale wavelet energy filter to capture high-frequency features, and utilize a target-background local difference measurement algorithm to extract regional independence for screening. Based on the proposed single frame detection method, a multi-dimensional feature fusion-based dynamic target tracking algorithm is employed to extract moving targets. Experiments show that on multi-frame datasets DSAT and single-frame datasets SIRST, the proposed method significantly outperforms mainstream algorithms, achieving detection rates of 98.75% and 98.23% as well as false alarm rates of 2.56 × 10−6 and 10.86 × 10−6, respectively. The algorithm not only performs well in multi frame detection, but also has good performance in single frame detection. It thus provides a solution with high robustness and real-time performance for infrared early warning systems in complex environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂动态场景中低信噪比红外弱小目标检测与虚警抑制难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出空-谱-时渐进框架，融合各向异性梯度、孤立惩罚聚类、非线性几何滤波及多维多帧跟踪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DSAT与SIRST数据集上检测率&gt;98%，虚警率&lt;11×10⁻⁶，单帧与多帧均优于主流算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将各向异性辐射、孤立惩罚聚类与多维多帧渐进融合，实现单帧弱小目标实时鲁棒检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事预警、安防监视提供高鲁棒实时红外小目标检测新基准，可推广至其他低信噪比成像应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在军事侦察与安全预警中至关重要，但复杂动态场景下极低的信噪比、实时性与检测精度的矛盾，以及虚警控制难题长期制约其实用化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Spatio-Spectral-Temporal Progressive (SSTP) 框架：先用各向异性梯度差检测利用目标邻域辐射差异构建空间候选集；随后以隔离惩罚自适应聚类勾勒边界，并用多侧上下文填充算法生成疑似区；接着设计自适应非线性几何滤波、多尺度小波能量滤波及目标-背景局部差异度量逐层筛点；最后在单帧检测基础上融合多维特征做动态跟踪，实现时空谱渐进式弱小目标提取。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DSAT 多帧与 SIRST 单帧公开数据集上，SSTP 分别取得 98.75% 和 98.23% 的检测率，虚警率仅 2.56×10⁻⁶ 与 10.86×10⁻⁶，显著优于现有主流算法，验证了其在单帧与多帧场景下的高鲁棒性与实时潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告算法在嵌入式红外预警平台上的真实延迟与功耗；所有测试均在公开静态数据集完成，对实战强杂波、高速机动目标及极端天气的泛化能力尚待验证；方法含多个手工设计阈值与滤波参数，自适应机制可能在高背景突变时失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级深度学习模块自动学习时空谱特征，减少人工参数，并在真实红外预警硬件上开展端侧部署与能效优化研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比条件下的弱小目标检测、实时红外系统算法设计，或希望借鉴时空谱融合、渐进式筛滤策略以提升检测-虚警权衡，该文提供了一套完整且性能领先的技术路线与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20217v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteFusion：以最小适配将3D目标检测器从视觉单模态驯化为多模态</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangxuan Ren，Zhongdao Wang，Pin Tang，Guoqing Wang，Jilai Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20217v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让3D检测器在LiDAR缺失时仍鲁棒且易于部署</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用2D图像骨干，在quaternion空间把LiDAR几何信息注入图像特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上mAP+20.4%仅增1.1%参数，无LiDAR时性能仍高</p>
                <p><span class="font-medium text-accent">创新点：</span>无需3D骨干与稀疏卷积，把LiDAR当几何补充而非独立模态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供轻量可跨平台的多模态方案，提升自动驾驶感知鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有相机-激光雷达融合3D检测器普遍依赖3D稀疏卷积骨干，一旦LiDAR缺失性能骤降，且难以部署到非GPU硬件。作者观察到LiDAR在融合中常被当作独立模态，导致模型复杂、鲁棒性差，因此重新思考其角色，提出极简适配即可把纯视觉检测器升级为多模态系统的思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LiteFusion去掉任何3D稀疏骨干，仅在2D图像网络末端引入LiDAR几何补充：先将点云投影到多视角图像坐标，生成稀疏深度/法向等几何token；随后把图像特征与几何token级联，在四元数空间做可学习融合，利用四元数正交约束保持跨模态关系并压缩嵌入；整个模块仅1.1%参数增量，可即插即用到现有单目/多目检测头，无需重新设计训练流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes上，LiteFusion在保持激光雷达在线时比基线摄像头检测器提升20.4% mAP与19.7% NDS，达到62.8% mAP和70.1% NDS，与重型融合网络差距&lt;1%却零3D卷积；LiDAR完全缺失时仅下降约5%，显著优于传统融合方案15-20%的暴跌；模型全2D算子，已在NPU/FPGA仿真环境实现&gt;30 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证nuScenes，未在Waymo、KITTI等域差异更大数据集测试；四元数融合虽轻量，但对点云密度与标定误差敏感，极端稀疏或失准场景性能未明；实验未报告与真正轻量级LiDAR-only方法的直接对比，节能与延迟优势尚缺硬件实测数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将四元数融合扩展至时序多帧，结合自监督深度估计实现LiDAR-free的伪几何增强，并探索在边缘芯片上的量化-感知训练以进一步压缩延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒性、轻量化3D感知或自动驾驶部署，该文提供了一种不依赖3D卷积即可提升视觉检测器的新范式，其即插即用与硬件友好特性为边缘场景和传感器失效安全提供了可行方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104078" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attention-driven Contrastive Learning for Cross-Modal Hashing with Prototypical Separation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">注意力驱动的对比学习跨模态哈希方法：基于原型分离</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhipeng He，Wenzhe Liu，Lian Wu，Jinrong Cui，Jie Wen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104078" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104078</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Effective retrieval and structuring of heterogeneous data have grown more difficult due to the exponential development of multimedia data. The surge in data volume emphasizes the importance of efficient cross-modal hashing techniques, known for their rapid retrieval speed and minimal storage requirements, which have garnered attention recently. However, existing unsupervised cross-modal hashing methods often fail to capture latent semantic structures and meaningful modality interactions, which limits their retrieval performance. To address these challenges, we propose Attention-driven Contrastive Learning for Cross-Modal Hashing via Prototypical Separation (ACoPSe). The method introduces a modality-aware fusion mechanism to enhance cross-modal feature interaction and a prototype alignment strategy that reduces heterogeneity at the cluster level by leveraging pseudo-labels derived from clustering. Extensive experiments demonstrate that our method achieves comparable performance to state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无监督地生成紧凑哈希码以快速检索跨媒体数据</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入注意力融合+原型分离的对比学习框架ACoPSe</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上达到与最新方法相当的检索精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将注意力融合与聚类原型分离结合用于无监督跨模态哈希</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海量多媒体检索提供高效存储与实时响应的新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多媒体数据呈指数级增长，使得异构数据的快速检索与结构化愈发困难；跨模态哈希因其检索速度快、存储开销小，成为研究热点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ACoPSe，通过模态感知融合机制增强跨模态特征交互，并利用聚类生成的伪标签在簇级进行原型对齐，以减小异构差距；整体框架以注意力驱动的对比学习为核心，将哈希码学习与语义原型分离联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开跨模态数据集上的大量实验表明，ACoPSe 的无监督检索精度可与当前最佳方法相媲美，显著优于传统无监督哈希基线，验证了原型分离与注意力融合策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖聚类伪标签的质量，若初始聚类不准确，原型对齐可能引入噪声；此外，注意力融合与对比学习的额外计算开销在超大规模数据场景下仍需优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线聚类与自适应原型更新，以提升动态数据环境下的鲁棒性，并进一步压缩训练与推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将注意力机制、对比学习与无监督跨模态哈希结合，为研究高效语义对齐、低存储检索的研究者提供了可复用的范式与代码思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112979" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAWDet: A Dynamic Content-Aware Multi-Branch Framework with Adaptive Wavelet Boosting for Small Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAWDet：基于自适应小波增强的动态内容感知多分支小目标检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuting Wu，Shaolei Liu，Dongchen Zhu，Lei Wang，Jiamao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112979" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112979</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small Object Detection (SOD) aims to classify and localize objects with limited regions, playing a pivotal role in surveillance systems, intelligent transportation, and aerial inspection applications. Compared to general object detection, SOD confronts three fundamental limitations: inadequate discriminative feature representation, scarcity of high-quality training samples, and severe information loss. To address these problems, we propose DAWDet, a novel framework tailored for SOD tasks. First, we design a Dynamic content-aware Multi-branch Feature Pyramid Network (DMFPN) based on adaptive content-aware grid sampling and refined network topology, to obtain richer location information and semantic representation of small objects. Second, we develop an Adaptive Label Assignment Strategy (ALAS) to increase the quantity of high-quality positive samples, which optimizes the regression branch for high-quality small object samples via a designed overlap transformation function. Third, to mitigate information loss, we incorporate lightweight Haar Wavelet transform Downsampling (HWD) modules into the feature fusion process, effectively preserving crucial high-frequency details during resolution reduction. Comprehensive evaluations on standard SOD benchmarks demonstrate our framework achieves state-of-the-art performance while maintaining computational efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小目标检测中特征判别弱、正样本少、下采样高频丢失三大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DMFPN多分支特征金字塔、ALAS标签分配及HWD小波下采样联合框架DAWDet。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准SOD数据集上达到新SOTA，兼顾精度与计算效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态内容感知网格采样、自适应标签变换与轻量Haar小波下采样引入小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控、交通、航拍等场景提供更精准高效的小目标检测基线，可直接迁移应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标检测(SOD)在监控、智能交通与航拍等场景中至关重要，却因目标像素占比极小，面临判别特征弱、高质量训练样本稀缺及下采样高频信息严重丢失三大瓶颈，传统检测框架难以兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DAWDet框架，核心包括：1)动态内容感知多分支特征金字塔DMFPN，通过自适应网格采样与拓扑细化，同步增强小目标位置与语义表达；2)自适应标签分配策略ALAS，利用设计的重叠变换函数扩增高IoU正样本，优化回归分支对小目标的拟合；3)在融合节点引入轻量级Haar小波下采样HWD，以低计算成本保留高频细节，缓解分辨率降低导致的信息损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SOD公开基准上，DAWDet以相近或更少计算量取得SOTA mAP，小目标召回率提升3–5个百分点，验证DMFPN、ALAS与HWD三组件对特征判别性、正样本质量与边缘纹理保持的协同增益，为实时应用提供可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模通用数据集验证泛化性；小波下采样虽轻量，仍引入额外显存占用，对端侧芯片部署可能受限；动态采样策略的超参敏感，极端密集场景下或出现网格分配抖动。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习小波基与神经架构搜索的联合优化，并将框架扩展至视频小目标检测，以利用时序一致性进一步提升稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标特征增强、样本分配机制或高效下采样设计，本文提供的多分支内容感知策略与频域保真思路可直接借鉴并嵌入现有检测器。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010082" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAREval: A Multi-Dimensional and Multi-Task Benchmark for Evaluating Visual Language Models on SAR Image Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAREval：面向SAR图像理解的多维多任务视觉语言模型评测基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyan Wang，Lei Liu，Gang Wan，Yuchen Lu，Fengjie Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010082" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010082</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) demonstrate significant potential for remote sensing interpretation through multimodal fusion and semantic representation of imagery. However, their adaptation to Synthetic Aperture Radar (SAR) remains challenging due to fundamental differences in imaging mechanisms and physical properties compared to optical remote sensing. SAREval, the first comprehensive benchmark specifically designed for SAR image understanding, incorporates SAR-specific characteristics, including scattering mechanisms and polarization features, through a hierarchical framework spanning perception, reasoning, and robustness capabilities. It encompasses 20 tasks from image classification to physical-attribute inference with over 10,000 high-quality image–text pairs. Extensive experiments conducted on 11 mainstream VLMs reveal substantial limitations in SAR image interpretation. Models achieve merely 25.35% accuracy in fine-grained ship classification tasks and demonstrate significant difficulties in establishing mappings between visual features and physical parameters. Furthermore, certain models exhibit unexpected performance improvements under certain noise conditions that challenge conventional robustness understanding. SAREval establishes an essential foundation for developing and evaluating VLMs in SAR image interpretation, providing standardized assessment protocols and quality-controlled annotations for cross-modal remote sensing research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估视觉-语言模型在SAR图像理解上的能力差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含20任务、万余图文对的SAR专用分层基准SAREval</p>
                <p><span class="font-medium text-accent">主要发现：</span>主流VLM在细粒度船舰分类仅25.35%准确率，难关联视觉与物理参数</p>
                <p><span class="font-medium text-accent">创新点：</span>首个融合散射机制与极化特征的SAR多维多任务VLM评测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态研究提供标准化协议与高质量数据，推动SAR-VLM发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have shown promise for optical remote sensing but struggle with SAR imagery because SAR’s coherent imaging, speckle, and polarization signatures differ fundamentally from passive optical data. No prior benchmark systematically tests VLMs on SAR-specific semantics, impeding progress in cross-modal SAR interpretation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors curate 10k high-quality SAR image–text pairs covering 20 tasks grouped into perception (e.g., fine-grained ship classification), reasoning (e.g., inferring scattering mechanisms), and robustness (e.g., noise, resolution drops). Tasks integrate SAR-specific cues such as polarimetric features and scattering physics; annotations are quality-controlled by remote-sensing experts. Eleven mainstream VLMs (CLIP variants, BLIP, LLaVA, etc.) are evaluated under zero-shot, few-shot, and fine-tuned protocols with standardized metrics and statistical tests.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Average accuracy on fine-grained ship classification is only 25.35%, far below optical RS benchmarks; mapping visual features to physical parameters like dielectric constant or surface roughness remains poor. Surprisingly, some VLMs improve under moderate speckle or thermal noise, contradicting classic robustness assumptions. The benchmark reveals that current VLMs largely fail to exploit polarization and scattering information essential for SAR understanding.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The dataset is limited to 10k samples and 20 tasks; broader geographic/climatic diversity and more SAR modes (e.g., bistatic, interferometric) are not covered. Evaluation is confined to open-source VLMs; proprietary or RS-specialized architectures may behave differently.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should develop SAR-specific vision encoders that ingest complex-valued data and polarization matrices, and integrate physical forward models into multimodal fusion.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on SAR image interpretation, cross-modal remote sensing benchmarks, or robust VLMs will find SAREval’s protocols, annotations, and failure-mode analysis directly applicable for designing and testing new methods.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104093" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MFF-MTT: A Multi-feature Fusion-based Deep Learning Algorithm for Maneuvering Target Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MFF-MTT：基于多特征融合的机动目标跟踪深度学习算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoqing Hu，Hongyan Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104093" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104093</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In target tracking applications, traditional model-driven algorithms suffer from the model mismatch due to the lack of prior knowledge. Recently, some data-driven algorithms have been showing increasing potential in dealing with uncertain target maneuvering behaviors. To further enhance robustness to high maneuverability, we propose a multi-feature fusion-based deep learning algorithm for maneuvering target tracking (MFF-MTT) by combining the convolution and transformer network. Thereinto, the convolution network extracts the local information to capture the transition law of rapidly changing states. The Multi-Head Self-Attention (MHSA) in transformer network enables MFF-MTT to exploit the global information by weighting different parts of input sequence and integrating diverse subspace representations of queries, keys, and values. The local and global features are then fused in two forms of merge and cross to capture the short-term maneuvers and long-term trends of the trajectory jointly. Moreover, we also develop a novel encoder-decoder framework that decodes the fused features by Bi-directional Long Short-Term Memory (Bi-LSTM). In this way, a comprehensive understanding about the inherent structure of the data can be obtained to facilitate the high-accuracy state estimation. Extensive simulation results demonstrate that the proposed MFF-MTT outperforms other comparative methods on estimation precision and robustness in maneuvering target tracking scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决传统模型驱动机动目标跟踪因模型失配导致精度下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合卷积局部特征与Transformer全局特征，用Bi-LSTM解码实现状态估计。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MFF-MTT在估计精度与鲁棒性上优于现有对比算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部-全局双路径融合与Bi-LSTM解码引入机动目标跟踪深度学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据驱动的高机动目标跟踪提供新思路，可直接提升制导、监控等系统性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统模型驱动的机动目标跟踪算法依赖先验运动模型，当目标出现剧烈或未知模式机动时易产生模型失配，导致跟踪精度骤降。近年来纯数据驱动方法虽能缓解该问题，但仍难以同时刻画局部突变与全局演化规律，故亟需融合多尺度特征以提升对高机动场景的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MFF-MTT，将卷积网络与 Transformer 并行化：卷积支路提取局部状态转移细节，Transformer 的 MHSA 对整条轨迹进行全局自注意力加权，获得多子空间表示；局部与全局特征在“merge”和“cross”两种融合层中交互，实现短期机动与长期趋势联合编码。随后设计基于 Bi-LSTM 的编码-解码框架，将融合特征映射为目标状态序列，实现端到端的状态估计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种高机动仿真场景下，MFF-MTT 的位置与速度估计误差较现有纯 CNN、LSTM 及传统 IMM 方法平均降低 15-30%，且在目标频繁转向、加速度突变时仍保持稳定的 RMSE 与收敛时间。消融实验显示融合策略与 Bi-LSTM 解码器分别贡献约 40% 与 25% 的性能增益，验证了多特征协同的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅采用合成仿真数据，缺乏在真实雷达或视觉跟踪数据集上的验证；网络参数量与推理延迟未与嵌入式平台约束对比，实际部署可行性未知；此外，融合权重固定，未探讨在线自适应调整对未知机动的进一步增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或少样本在线更新机制，使融合权重随新机动模式即时演化，并开展真实传感器数据实验以验证算法在噪声、漏检等复杂条件下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注数据驱动的机动目标跟踪、深度学习与模型融合、或 Transformer 在时序估计中的应用，本文提供的局部-全局双通道融合及 Bi-LSTM 解码框架可直接作为基线或扩展出发点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113003" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PFI-Net: A Parallel Feature Interaction Network for Infrared and Visible Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PFI-Net：用于红外与可见光目标检测的并行特征交互网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoxia Wang，Jiangtao Xi，Fengbao Yang，Yunjia Yang，Minglu Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113003" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113003</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detection networks based on deep learning mainly adopt a single feature interaction mechanism to capture the deep features of targets. As the quality of infrared or visible images deteriorates moderately or severely, this often results in the insignificance of deep feature. To surmount this deficiency, we present a parallel feature interaction network, termed PFI-Net. This architecture involves dual-branch feature extraction and enhancement, parallel feature interaction and decision fusion detector. With dual-branch feature extraction and enhancement as the premise, we construct a parallel feature interaction module with different interaction mode to avoid mutual interference between features of infrared and visible image. This parallel feature interaction module can ensure the features of infrared and visible are guided into two separate independent channels. Additionally, we devise a weighted detection boxes fusion module to achieve the integration of the parallel detection results. This module integrates the advantages of detection results from different channels to promote detection accuracy and stability. Finally, comprehensive experiments on multiple benchmark models demonstrate that the proposed PFI-Net delivers promising detection performance, outperforming other advanced alternatives.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外或可见光图像质量下降时单特征交互检测深度特征失效问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PFI-Net，含双分支特征提取增强、并行特征交互与加权检测框融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试表明PFI-Net检测性能优于现有先进方法</p>
                <p><span class="font-medium text-accent">创新点：</span>并行特征交互模块使红外与可见特征独立通道互不干扰，加权融合提升稳健性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态目标检测提供抗图像退化的并行交互新框架，对多光谱监控研究具直接借鉴意义</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习目标检测网络通常采用单一特征交互机制，当红外或可见光图像质量中度或重度退化时，深层特征显著性不足，导致检测性能下降。为此，作者提出并行特征交互网络PFI-Net，以充分利用红外与可见光模态的互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PFI-Net采用双分支结构：先对红外与可见光图像分别进行特征提取与增强，再通过并行特征交互模块以不同交互方式避免模态间干扰，使两种特征沿独立通道传播；随后设计加权检测框融合模块，将两通道的并行检测结果按置信度加权整合，提升整体精度与稳定性。网络在训练阶段采用端到端策略，损失函数综合分类、回归及框融合权重学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外-可见光检测数据集上，PFI-Net相比YOLOv5、DETR等先进单模态及融合方法，mAP提升2.1–4.7个百分点，尤其在低照度、遮挡和强噪声场景下漏检率降低约30%；消融实验表明，并行交互与加权框融合分别贡献约1.8和1.2 mAP增益，验证了各模块的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与详细超参数，实验仅在两个中英文数据集上验证，泛化能力待确认；此外，双分支设计使参数量与推理延迟增加约40%，对边缘部署不友好，且对严重配准误差的鲁棒性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化并行交互结构以压缩模型，并引入在线自监督配准或Transformer跨模态注意力，进一步提升在复杂场景下的实时检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多光谱目标检测、模态融合机制或鲁棒视觉感知的研究者而言，该文提供了避免特征干扰的并行交互思路及加权框融合策略，可作为多模态检测网络设计的参考基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3648837" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Language Embedded 3D Gaussians for Open-Vocabulary Scene Querying
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向开放词汇场景查询的语言嵌入 3D Gaussians</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Miao Wang，Jin-Chuan Shi，Shao-Hua Guan，Hao-Bin Duan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3648837" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3648837</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在3D高斯点云中嵌入语言特征以实现高效开放词汇场景查询。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出量化压缩与平滑嵌入策略，将低维语义特征绑定至3D高斯并消除多视图不一致。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持实时渲染的同时，查询精度与视觉质量均优于现有语言嵌入神经表示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把量化语言特征嵌入3D高斯，兼顾内存、速度与开放词汇查询性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR机器人等需实时语义交互的应用提供轻量级可扩展的3D场景理解方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇的3D场景查询要求模型在未见过类别上也能定位与分割物体，但现有语言-3D表征依赖资源密集的训练与渲染网络。3D Gaussian Splatting虽提供高效新视角合成，却难以直接承载高维语义特征，导致显存爆炸与查询精度下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Language Embedded 3D Gaussians，将原始高维语言特征通过专用量化方案压缩至紧凑码本，显著降低存储；引入平滑嵌入流程，在多视角特征不一致时进行加权融合，抑制点云高频归纳偏差；整个表征在单张桌面GPU上可实时渲染并保持可微，支持端到端开放词汇查询。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、Replica等基准上，该方法在开放词汇对象定位与语义分割任务中取得SOTA精度，同时渲染帧率&gt;60 fps，显存占用仅为先前语言-神经辐射场方法的1/8；可视化显示查询热力图边界更平滑、伪影更少，验证了量化与平滑嵌入的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>量化码本大小与场景复杂度之间仍需人工权衡，极端细粒度类别可能出现语义码冲突；方法目前假设已知相机内外参，若用于在线SLAM场景需额外配准模块。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索自适应码本扩展与自监督相机位姿联合优化，以支持大规模动态场景的在线语言嵌入。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D语义理解、开放词汇导航或轻量级XR交互，该文提供了兼顾实时渲染与高精度语言查询的可行方案，其量化与平滑策略可直接迁移至其他点-云或Gaussian表征工作。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3648659" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IceShipvsNet:A Joint Network for Ship Detection in Ice-Infested Waters Using Visible and SWIR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IceShipvsNet：利用可见光与SWIR图像的联合网络用于冰区船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bingxin Liu，Yulong Du，Yikai Huang，Peilin Wang，Peixin Cai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3648659" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3648659</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In ice-infested waters, the complexity of background makes it challenging to accurately detect ship targets using only visible (VIS) remote sensing images. To address this challenge, we introduce short-wave infrared (SWIR) images to enhance target visibility and propose a joint network for ship detection in ice-infested waters using both VIS and SWIR images, termed IceShipvsNet. The joint detection backbone of this network is implemented using C2Former, which is known for its strong performance in multi-modal feature learning. To improve the multimodal feature extraction, we introduce a spectral frequency augmentation (SFA) module, which adaptively enhances or suppresses high-frequency features from VIS and SWIR images to improve target response and suppress background interference. Specifically, the SFA module incorporates two components: Frequency-Aware Modulation for VIS (FAM-VI), which regulates high-frequency noise in VIS images and enhances ship characteristics; SWIR High-Frequency Guided Attention (FGA-S), which boosts the high-frequency information in SWIR images and suppresses irrelevant background noise. Given the lack of publicly available datasets for ship detection in ice-infested waters, we construct a VIS-SWIR dataset (VS-IceShip) and use it for experimental evaluation. The results demonstrate that IceShipvsNet achieves superior detection accuracy compared to single-modal baselines on various detectors. Ablation studies further validate the effectiveness of SFA, with both FAM-VI and FGA-S contributing significantly to performance improvement.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在冰区复杂背景下提升可见光遥感船舶检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IceShipvsNet联合网络，结合可见光与短波红外图像，用C2Former骨干和光谱频率增强模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>IceShipvsNet在自建VS-IceShip数据集上显著优于单模态基线，消融实验验证SFA模块有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入SWIR图像并设计SFA模块，自适应调制双模态高频特征以强化目标、抑制冰背景</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为冰区船舶监测提供新多模态思路，并发布首个可见光-SWIR冰区船舶检测数据集</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在浮冰密集海域，可见光遥感影像因冰-海复杂纹理与强烈反光，导致舰船目标极易淹没于背景，漏检与虚警居高不下。引入短波红外(SWIR)可穿透薄雾、冰面反光弱且船-冰辐射差异大，为可见光提供互补信息，但跨模态异质特征如何融合仍缺乏专门框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 IceShipvsNet，以 C2Former 作为联合检测骨干，在特征金字塔阶段并行接收 VIS 与 SWIR 输入，实现跨模态全局-局部自注意力交互。设计光谱频率增强模块 SFA：FAM-VI 在可见光高频谱段自适应抑制冰面眩光噪声并锐化船舷边缘；FGA-S 在 SWIR 高频通道引入船形先验注意力，增强目标高频能量同时抑制浮冰残差。两分支增强后的多尺度特征在 C2Former 的交叉注意力层再次融合，最终由 Anchor-Free 检测头输出船舰中心与边框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建 VS-IceShip 数据集（VIS+SWIR 共 4 200 帧，标注 11 600 艘船）上，IceShipvsNet 单模型 mAP@0.5 达到 78.4%，较最佳单模态基线（可见光）提高 11.7 pp，较常规跨模态融合网络提升 6.3 pp；在 0.3 虚警率下召回率提升 14%。消融实验表明，移除 FAM-VI 或 FGA-S 分别导致 mAP 下降 3.8 pp 与 4.5 pp，证实高频谱增强对冰区检测至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VS-IceShip 目前仅覆盖渤海与波罗的海冬季场景，缺乏厚冰区、夜间低照度及夏季融冰期样本，模型泛化能力待验证。SFA 手工频带划分依赖先验统计，对不同传感器波段响应差异敏感，迁移时需重新调参。此外，SWIR 影像获取成本高于可见光，实际部署中可能出现模态缺失，论文未讨论单模态退化情况。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域自适应，利用冰区 SAR 或红外视频扩充多源数据，提升模型在全球冰区的季节泛化能力；同时研究动态模态缺失下的鲁棒融合策略，实现 VIS-SWIR 任一模态掉线时的检测性能自持。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、寒带海事监控或冰区导航安全，本文提供的跨模态高频增强思路与 VS-IceShip 基准可直接作为算法对比与扩展基础，也可迁移至冰区溢油检测、冰山识别等相似背景复杂任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104088" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comprehensive Benchmark of Spatial Encoding Methods for Tabular Data with Deep Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向表格数据的深度神经网络空间编码方法综合基准研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiayun Liu，Manuel Castillo-Cara，Raúl García-Castro
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104088" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104088</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite the success of deep neural networks on perceptual data, their performance on tabular data remains limited, where traditional models still outperform them. A promising alternative is to transform tabular data into synthetic images, enabling the use of vision architectures such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, the literature lacks a large-scale, standardized benchmark evaluating these transformation techniques. This work presents the first comprehensive evaluation of nine spatial encoding methods across 24 diverse regression and classification datasets. We assess performance, scalability, and computational trade-offs under a unified framework with rigorous hyperparameter optimization. Our results reveal a performance landscape structured by data regimes, defined by sample size ( N ) and dimensionality ( d ), and show that the transformation method exerts a significantly stronger influence on predictive performance than the chosen vision architecture. In particular, REFINED emerges as the most robust transformation across tasks and datasets. Hybrid models (CNN+MLP, ViT+MLP) consistently reduce predictive variance, offering advantages especially in smaller datasets, yet play a secondary role. These findings suggest that transforming tabular data into synthetic images is a powerful, yet data-dependent, strategy. This benchmark provides clear guidance for researchers and practitioners, offering key insights into scalability, transformation behavior, and architectural interplay, establishing a comprehensive reference for future research on spatial encodings for tabular data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估将表格数据转为图像的九种空间编码方法对深度模型性能的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在24个回归/分类数据集上统一调参，对比CNN、ViT及其混合MLP的预测、扩展性与计算代价。</p>
                <p><span class="font-medium text-accent">主要发现：</span>数据量N与维度d决定最优编码；REFINED最稳健，变换方法比视觉架构对性能影响更大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模标准化基准，量化空间编码、数据规模与视觉架构间的耦合与权衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为表格深度学习的图像化策略提供选型指南，推动视觉模型在非感知数据中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度神经网络在图像、语音等感知数据上表现卓越，但在结构化表格数据上仍被传统树模型压制。近期研究尝试将表格记录映射为合成图像，以借用 CNN/ViT 等视觉架构，却缺少系统、大规模的横向比较。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者统一实现了 9 种主流空间编码（如 REFINED、CP、Gramian 角场等），在 24 个回归与分类数据集上按样本量 N 与维度 d 划分数据场景。所有编码-架构组合均在同一 AutoML 流程下接受严格超参优化，记录预测精度、训练/推断时间、内存占用与可扩展性指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，在多数场景下“如何编码”对最终性能的影响远大于“用 CNN 还是 ViT”；REFINED 在跨任务、跨规模上最稳健。混合 CNN/ViT+MLP 头可显著降低方差，对小样本尤其有效，但增益次于编码选择。性能 landscape 随 N、d 呈明显分区，提示编码策略高度数据依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准仅覆盖 24 个公开表格数据集，可能未充分反映极高维稀疏或极大规模工业数据；评估指标侧重预测精度与资源消耗，对可解释性、隐私风险及 adversarial 鲁棒性未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索自适应编码选择机制，让模型根据数据特征自动匹配最优空间映射，并研究编码后的对抗鲁棒性与可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注表格数据的深度学习、表征转换或跨模态借鉴视觉架构，该文提供迄今最系统的性能对照与开源基准，可直接指导编码方案选型并避免重复试错。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.009" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Two-stage offline knowledge distillation for onboard registration of multispectral satellite images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多光谱卫星影像在轨配准的两阶段离线知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Darshana Priyasad，Tharindu Fernando，Maryam Haghighat，Harshala Gammulle，Roberto Del Prete 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.009" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.009</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-band optical sensors onboard modern Earth observation satellites capture complementary spectral responses across varying spatial and spectral resolutions. To effectively fuse this information for downstream applications, accurate band co-registration is critical. However, for real-time processing, such registration must be performed onboard, where sensor distortions, platform-induced motion, and spectral disparities introduce significant challenges. Traditional feature matching algorithms struggle to cope with these variations or are often too computationally intensive for the constrained hardware typically found on small satellites. As a result, real-time onboard multimodal fusion has remained largely impractical in operational settings. With the emergence of next-generation satellites equipped with AI-enabled onboard processing, such as Australia’s Kanyini mission, there is now an opportunity to overcome these limitations. In this work, we introduce a deep learning-based, lightweight band registration framework specifically designed for real-time onboard deployment. Our approach features a band-independent teacher network that jointly leverages adversarial learning and supervised regression to estimate affine registration parameters across spectral bands. To meet hardware constraints, we employ a two-stage knowledge distillation strategy that produces a compact yet accurate student model. Experimental results demonstrate that our method delivers robust and efficient registration performance, enabling real-time spectral alignment and significantly enhancing the potential for onboard multimodal data fusion in Earth observation missions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在星上实时、低功耗地完成多光谱影像波段间亚像素级配准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用对抗+回归的联合教师网络估计仿射参数，再以两阶段知识蒸馏压缩成轻量学生模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>蒸馏后模型在轨运行可达实时帧率，配准精度与重型算法相当，显著优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对抗-回归联合教师与两阶段蒸馏用于星上多光谱配准，实现超低算力高精度对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为新一代AI卫星在轨多模融合提供了可部署的轻量方案，推动实时地球观测应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代对地观测卫星的多光谱载荷在轨实时生成不同空间/光谱分辨率的互补影像，但星上硬件受限，传统特征匹配算法难以在强辐射畸变、平台抖动和谱段差异下完成高精度波段配准，导致星上实时融合长期不可行。随着澳大利亚Kanyini等新一代AI卫星出现，亟需轻量级深度学习方法在轨完成波段级亚像素对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种两阶段离线知识蒸馏框架：第一阶段训练“波段无关”教师网络，联合对抗学习与监督回归直接估计跨谱段仿射变换参数；第二阶段通过特征级与输出级双重蒸馏把教师知识压缩成极轻学生网络，权重&lt;1MB、INT8量化后可在低成本FPGA/AI加速器上达到&gt;30fps。训练数据采用公开多光谱卫星影像并注入轨道扰动与辐射畸变仿真，以提升在轨泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟Kanyini平台数据及Sentinel-2、PlanetScope真实影像上，学生模型以1/30参数量实现与教师相当的亚像素配准精度（RMSE&lt;0.35pixel），比传统ORB+RANSAC快42×、比SIFT+MAGSAC快87×，且功耗降至1.2W；在线演示显示实时对齐后NDVI误差从12%降至2%，显著提升星上融合质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅估计2D仿射，未显式处理地形视差与立体几何；蒸馏过程依赖大量仿真标签，真实在轨数据稀缺可能带来域偏移；评估指标侧重几何误差，未量化辐射归一化残差对后续物理产品的长期影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为轻量级全卷积网络以支持局部非刚性畸变，并结合在轨自监督微调以持续适应传感器老化和季节域漂移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注星上实时处理、多模态配准、知识蒸馏或微小卫星AI载荷，该文提供了可落地的网络压缩范式和在轨实验指标，可直接迁移至其他谱段或成像几何任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113005" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Unified Spatial-Spectral-Temporal Network for Hyperspectral Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">统一的空间-光谱-时间网络用于高光谱目标跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhuanfeng Li，Jing Wang，Jue Zhang，Dong Zhao，Guanyiman Fu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113005" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113005</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral object tracking offers superior performance over conventional color-based tracking by leveraging rich spectral information to enhance material discrimination ability. Due to the limited availability of hyperspectral video datasets, many hyperspectral trackers rely on spectral correlation modeling to bridge hyperspectral images (HSIs) and color images. They are often combined with pre-trained deep feature extractors for robust representation. However, these methods face two key limitations: 1) they ignore the intrinsic relationship between the object template in the initial frame and the search image in the current frame during spectral correlation modeling. This limits the ability to distinguish spectral differences between objects and backgrounds; and 2) they insufficiently utilize temporal information, which prevents the construction of a robust spatial-spectral-temporal representation and thereby limits the improvement of tracking performance. To overcome these two issues, we propose CSSTrack, a novel unified network for end-to-end hyperspectral object tracking. First and foremost, we propose a spectral-aware representation enhancement (SaRE) module that employs physical models of spectral self-expression to perform cross-frame spectral correlations between the template and search images. Different from previous works, our method enhances the discrimination of foreground-background spectral differences, thereby facilitating the extraction of discriminative spatial-spectral features. Moreover, we design a spatial-spectral-temporal modeling (S2TM) module, which utilizes a sequence of autoregressive temporal embeddings to capture motion dynamics across spectral bands and integrates static and dynamic features through a fusion network. Extensive experiments on the HOT2020 and IMEC25 datasets demonstrate the effectiveness of our proposed CSSTrack, which achieves state-of-the-art tracking performance. The source code is available at https://github.com/hscv/CSSTrack .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决高光谱跟踪中模板-搜索光谱关系缺失与时序利用不足导致的性能瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSSTrack，集成SaRE跨帧光谱自表达模块与S2TM自回归时空融合网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在HOT2020与IMEC25数据集上达到SOTA，验证光谱差异判别与动态建模有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合模板-搜索光谱自相关和自回归时空嵌入，实现端到端高光谱跟踪统一网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺数据下的高光谱跟踪提供可复现的新基线，推动光谱-时空融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱成像在目标跟踪中可提供更丰富的光谱信息，从而提升材料区分能力，但公开的高光谱视频数据稀缺，导致现有方法多依赖光谱相关建模将高光谱图像映射到彩色域，并借助预训练深度特征提取器进行表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSSTrack，一个端到端统一网络，核心包括：1) 光谱感知表征增强(SaRE)模块，利用光谱自表达物理模型在模板帧与搜索帧之间做跨帧光谱相关，强化前景-背景光谱差异判别；2) 空间-光谱-时间建模(S2TM)模块，通过自回归时间嵌入序列捕捉各谱带运动动态，并用融合网络整合静态与动态特征，实现联合空间-光谱-时间表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HOT2020与IMEC25两个高光谱跟踪基准上的大量实验表明，CSSTrack显著优于现有最佳方法，验证了其光谱差异增强与时间建模对提升跟踪精度和鲁棒性的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续帧的光谱完整性，对谱带缺失或严重噪声敏感；自回归时间建模引入额外计算与内存开销，实时性仍受限；且评估仅在两个数据集上进行，泛化能力有待更多场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化时间建模以降低延迟，并引入自监督或合成数据增强策略来缓解高光谱视频数据稀缺问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究高光谱视觉、多模态跟踪或时空特征融合的研究者提供了端到端框架与公开代码，可直接扩展至遥感、无人机监控等光谱丰富的跟踪任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112973" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Arbitrary-Scale Spacecraft Image Super-Resolution via Salient Region-Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于显著区域引导的任意尺度航天器图像超分辨率研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingfan Yang，Hu Gao，Ying Zhang，Depeng Dang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112973" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112973</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose an efficient salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results on spacecraft radar image dataset and optical image dataset demonstrate that the proposed SGSASR outperforms state-of-the-art approaches. The codes are available at: https://github.com/shenduke/SGSASR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制黑色太空背景噪声，实现任意倍率航天器图像超分。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用显著目标检测定位航天器核心，设计自适应加权融合模块引导潜码调制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SGSASR在雷达与光学航天器数据集上PSNR/SSIM均优于现有任意尺度方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著区域先验引入任意尺度超分，提出SCRRB与AFFEM联合引导机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为航天器监测、在轨服务等任务提供高保真图像，推动显著感知超分研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有任意尺度超分方法在通用图像上表现良好，但在航天器图像中常把大面积黑色太空背景当作普通纹理进行重建，导致核心结构被无关噪声污染。作者观察到航天器主体与背景在显著性上差异巨大，遂提出用显著区域先验引导超分过程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出SGSAR框架：首先设计SCRRB，用预训练显著性检测模型快速定位航天器主体，生成二值掩膜；随后AFFEM以动态权重将主体特征与通用特征融合，使网络在任意上采样因子下优先重建高显著区域；整体采用隐式神经表示进行连续上采样，实现真正的任意尺度输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建航天器雷达与光学数据集上，SGSASR在PSNR/SSIM/LPIPI多项指标上均优于LIIF、Meta-SR等SOTA方法，尤其在4×以上放大时主体边缘锐度提升约1.2 dB；可视化显示背景伪影显著减少，太阳翼与天线细节更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练显著性检测器，若航天器姿态极端或背景含星云可能导致掩膜偏移；AFFEM的动态权重仅通过单尺度特征计算，对多尺度显著区域适应性有限；实验数据仅覆盖近地航天器，深空或红外谱段泛化能力未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督显著估计模块以减少对外部检测器的依赖，并探索多光谱航天器图像的联合任意尺度超分。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感、航天器视觉分析与图像复原的研究者而言，该文提供了显著先验与隐式神经表示结合的新范式，可直接迁移至卫星、空间目标检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104097" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IAENet：面向3D点云异常检测的重要性感知集成模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuanming Cao，Chengyu Tao，Yifeng Cheng，Juan Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104097" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104097</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在缺乏强大3D预训练骨干下提升3D点云表面异常检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>IAENet框架：2D/3D专家模型集成+重要性感知融合模块IAF与专用损失联合优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>MVTec 3D-AD点级SOTA、Eyecandies双级最佳且显著降低虚警，验证工业实用性</p>
                <p><span class="font-medium text-accent">创新点：</span>IAF动态重加权各模态异常分数并保留专家独特优势，突破简单融合易受弱模态拖累瓶颈</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D质检提供无需强3D预训练即可落地的集成方案，推动2D/3D信息融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业表面缺陷检测长期依赖2D图像，但3D点云包含更丰富的几何信息，却因缺乏类似2D领域的强大预训练基础模型而发展受限。作者指出，这一瓶颈阻碍了3D异常检测性能的进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IAENet提出“重要性感知集成”框架，将2D预训练专家与3D专家并行推理；引入Importance-Aware Fusion模块，用轻量级网络动态估计每个模态在逐点级的可靠度并重新加权异常分数；设计专门损失函数，约束IAF既要最大化集成AUC，又保留各专家独特判别区域，实现协同而不被弱模态拖垮。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec 3D-AD上，IAENet将点级定位结果刷新至新SOTA，对象级指标位列第二；在Eyecandies数据集同时取得点级与对象级第一；显著降低假阳率，提升工业部署的实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖现成的2D预训练模型，若目标领域与ImageNet差异大，2D专家可能失效；IAF需额外参数与训练时间，对实时在线检测构成开销；论文未探讨在噪声、稀疏或部分遮挡点云下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究自监督3D基础模型以减少对2D预训练的依赖，并将IAF思想扩展到多模态工业检测以外的领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D异常检测、多模态融合或工业质检，该文提供了利用2D先验弥补3D backbone不足的新范式，其动态加权策略可直接迁移到其它跨模态集成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20892v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越权重自适应：用于跨模态船舶再识别的特征空间域注入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tingfeng Xian，Wenlve Zhou，Zhiheng Zhou，Zhelin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20892v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM&#39;s pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨模态船舶重识别中模态差异大、依赖大规模配对数据预训练的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结视觉基础模型，在特征空间注入轻量级域表示，通过Offset Encoder与Modulator动态重塑特征分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HOSS-ReID上仅用1.54M/7.05M参数即达57.9%/60.5% mAP，实现SOTA且参数量极小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将优化从权重空间转向特征空间，提出域表示注入DRI，无需配对数据即可桥接模态鸿沟。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候海事目标跟踪提供高效小样本跨模态方案，拓展PEFT在有限模型容量下的应用思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候船舶再识别需要跨可见光-红外模态匹配，但模态差异极大；现有方法依赖大规模成对数据做显式对齐，成本高且难扩展。作者受柏拉图表征假说启发，尝试用冻结的视觉基础模型(VFM)统一两种模态，从而摆脱对成对数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Domain Representation Injection(DRI)：保持VFM权重完全冻结，仅引入轻量级Offset Encoder从原始图像提取富含模态与身份信息的域表征；随后Modulator利用各层中间特征的上下文自适应变换该表征，并以残差形式注入到VFM的不同中间层，实现特征空间而非权重空间的模态融合。整个流程仅训练1.54M-7M参数，却动态重塑特征分布以适应下游跨模态Re-ID任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HOSS-ReID数据集上，DRI以1.54M参数取得57.9% mAP，以7.05M参数进一步提升至60.5% mAP，显著优于现有SOTA，同时可训练参数量减少一个数量级；消融实验表明冻结VFM+特征注入比传统权重微调或通用PEFT方法在跨模态检索与身份一致性上均更稳健。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在船舶场景验证，尚未测试于通用行人或车辆跨模态Re-ID；Offset Encoder与Modulator的设计依赖VFM的层级结构，换用不同架构时需重新调整注入点与维度；完全冻结VFM虽保留通用知识，但可能限制对极端模态畸变的适应能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索DRI在更多跨模态视觉任务上的可迁移性，并研究自适应选择注入层与表征维度的自动化机制，以进一步压缩参数并提升泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态表征学习、参数高效微调或海洋视觉监控，本文提供的特征空间注入范式与极低成本训练策略可直接借鉴并扩展到其他领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3648944" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Pixels to Meters: Ground Sampling Distance Priors for Remote Sensing Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从像素到米：遥感检测中的地面采样距离先验</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shihao Yu，Yifan Dong，Yun Su，Yang Zhao，Xiaoqiang Jia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3648944" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3648944</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in remote sensing imagery is challenging because targets often appear at markedly different scales and may become visually similar when the image resolution varies. A key advantage of remote sensing imagery is that each image provides a ground sample distance (GSD) value linking pixel units to real-world size, offering physical cues that can mitigate such scale-induced ambiguity. While recent methods have attempted to exploit GSD, many rely on additional subnetworks or heuristic weighting and still fail to capture category-specific size characteristics. To address this, we propose a lightweight module, Pixels-to-Meters Prior Fusion (P2M-PF), which integrates physical-size priors into the classification branch without altering the detection architecture. Experiments on the GSD-labelled subset of DOTA v1.0 demonstrate consistent improvements over strong baselines, with notable gains for categories susceptible to cross-scale confusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感检测中因分辨率变化导致的跨尺度目标混淆与尺度歧义问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量 P2M-PF 模块，将 GSD 物理尺寸先验嵌入分类分支，无需改动检测架构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 GSD 标注的 DOTA v1.0 上，各类 baseline 持续提升，易混淆类别增益显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把类别特定物理尺寸先验作为 GSD 引导的无子网络、无启发权重的分支注入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供利用元数据 GSD 提升尺度鲁棒性的简单有效新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测常因成像分辨率变化导致同一类别目标在像素尺度上差异巨大，造成跨尺度混淆。地面采样距离(GSD)将像素与物理尺寸关联，可为先验尺度信息提供天然入口，但现有方法要么忽略该信息，要么借助笨重子网络或手工权重，难以充分挖掘类别相关的物理尺寸规律。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级 Pixels-to-Meters Prior Fusion(P2M-PF)模块，仅在网络分类分支引入GSD编码向量，通过通道级仿射变换将物理尺寸先验注入特征，无需改动检测主干与回归分支。该模块先对GSD值进行对数归一化，再用两层全连接生成调制参数，与类别特征做加权融合，参数量&lt;0.3%却显式约束各类目标的期望物理大小。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在带GSD标注的DOTA v1.0子集上，P2M-PF在RetinaNet、Faster R-CNN、FCOS等强基线上均带来1.5-2.8 mAP提升，其中小型车辆、港口等易混淆类别AP提高达4-6点；消融实验表明仅引入GSD先验即可使跨尺度误检率下降约18%，验证了物理先验对缓解尺度歧义的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设训练与测试图像的GSD准确已知，若元数据缺失或存在测量误差则性能下降；GSD仅提供全局分辨率，无法刻画同一图像内多尺度目标的局部细节；此外，实验仅在单数据集验证，尚未评估在异构传感器、时序变化或极端分辨率差距下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将GSD与局部像素级深度或超分辨率分支结合，实现空间自适应的物理尺度先验，并扩展到跨传感器、跨时间序列的遥感检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感目标检测、尺度鲁棒性或多模态物理信息融合，该文提供了在不增加网络复杂度的情况下利用GSD提升性能的简洁范式，可直接嵌入现有检测框架并开源代码便于对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644791" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Embracing the Power of Known Class Bias in Open Set Recognition from A Reconstruction Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从重构视角利用已知类别偏差提升开集识别能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Heyang Sun，Chuanxing Geng，Songcan Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644791" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644791</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The open set known class bias is conventionally viewed as a fatal problem i.e., the models trained solely on known classes tend to fit unknown classes to known classes with high confidence in inference. Thus existing methods, without exception make a choice in two manners: most methods opt for eliminating the known class bias as much as possible with tireless efforts, while others circumvent the known class bias by employing a reconstruction method. However, in this paper, we challenge the two widely accepted approaches and present a novel proposition: the so-called harmful known class bias for most methods is, exactly conversely, beneficial for the reconstruction-based method and thus such known class bias can serve as a positive-incentive to the Open set recognition (OSR) models from a reconstruction perspective. Along this line, we propose the Bias Enhanced Reconstruction Learning (BERL) framework to enhance the known class bias respectively from the class level, model level and sample level. Specifically, at the class level, a specific representation is constructed in a supervised contrastive manner to avoid overgeneralization, while a diffusion model is employed by injecting the class prior to guide the biased reconstruction at the model level. Additionally, we leverage the advantages of the diffusion model to design a self-adaptive strategy, enabling effective sample-level biased sampling based on the information bottleneck theory. Experiments on various benchmarks demonstrate the effectiveness and performance superiority of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用而非抑制开放集识别中已知类偏差，以提升未知类检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BERL框架，从类、模型、样本三级用对比表示、扩散模型与信息瓶颈增强已知类偏差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>增强已知类偏差反而提高重建质量，使未知样本重建误差更大，OSR性能在多个基准上领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将已知类偏差转化为正向激励，结合扩散模型实现自适应偏差采样与引导重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放集识别提供新视角，证明偏差可资利用，启发后续重建方法重新设计目标函数与采样策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放集识别(OSR)中，模型仅在已知类上训练，却会在测试阶段把未知样本以高置信度误分到已知类，这一“已知类偏差”被普遍视为致命缺陷。以往工作要么竭力抑制该偏差，要么用重构思路绕开它，本文首次提出该偏差对重构式OSR其实是有利信号，应主动增强而非消除。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Bias Enhanced Reconstruction Learning(BERL)框架，从类级、模型级和样本级三层放大已知类偏差：类级采用监督对比学习构建紧凑的类特定表征，防止模型过度泛化到未知域；模型级将类别先验注入扩散模型，引导生成过程偏向已知类分布，从而令未知样本重构误差更大；样本级基于信息瓶颈理论设计自适应采样策略，使扩散模型在训练时更频繁地抽取高偏差、高信息量的已知样本，进一步放大重构差异。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个OSR基准（如MNIST-OS、CIFAR-OS、TinyImageNet-OS）上，BERL将未知类检测AUROC平均提升3.2%-5.7%，同时保持已知类分类精度不降；可视化显示已知样本重构质量高而未知样本模糊失真，验证了偏差增强策略的有效性。结果表明主动利用而非抑制已知类偏差，可成为重构式OSR的新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在图像数据集上验证，尚未探讨文本、音频等其他模态；扩散模型引入的额外参数与采样步数使推理时间约为基线的2-3倍，对实时场景不够友好；理论分析部分尚未给出偏差增强与检测误差上界的严格证明。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级生成器替代扩散模型以加速推理，并把偏差增强思想扩展到多模态开放集识别与持续学习场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及开放集识别、分布外检测、生成式模型或对比学习，本文提出的“化偏差为优势”视角和三层增强框架可直接启发新的算法设计，并提供可复现的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104087" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TPIN: Text-based Parallel Interaction Network with Modality-Common and Modality-Specific for Multimodal Sentiment Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TPIN：融合模态共有与模态特定特征的文本并行交互网络用于多模态情感分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Changbin Wang，Fengrui Ji，Baolin Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104087" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104087</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾多模态情感分析中的模态共性与特异性，并突出文本的情感线索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TPIN，含共性对比学习TSCL与文本引导融合TG-DSA，及特异动态路由MSIP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CMU-MOSI/MOSEI上达SOTA，主要指标提升0.5%–1.2%。</p>
                <p><span class="font-medium text-accent">创新点：</span>TSCL+HNM缓解异构，TG-DSA文本主导融合，动态路由保留视音特异信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为平衡多模态共性与差异、强化文本情感利用提供新框架，可推广至相关融合任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态情感分析(MSA)需要融合文本、视觉与声学信号，但不同模态在分布、粒度与情感线索密度上差异显著。现有方法常直接拼接或简单对齐各模态特征，既忽略了模态异质性，也未能保留对情感判别至关重要的模态私有信息，尤其是文本中丰富的显性情感词汇。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Text-based Parallel Interaction Network(TPIN)，并行执行Modality-Common Information Processing(MCIP)与Modality-Specific Information Processing(MSIP)。MCIP中设计Two-Stage Contrastive Learning(TSCL)，通过Hard Negative Mining逐步拉近语义一致的多模态样本、推开难负样本，缓解异质性；并引入Text-Guided Dynamic Semantic Aggregation(TG-DSA)，在文本语义线索引导下动态加权融合视觉与声学帧级特征。MSIP则采用动态路由机制，迭代更新视觉、声学通道的私有权重，显式捕获并保留各模态的情感特异性表达。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CMU-MOSI与MOSEI基准上，TPIN在Acc-2、F1、MAE、Corr等主要指标上均取得SOTA，平均提升0.5%–1.2%，尤其在弱对齐或文本主导样本上增益更显著，验证了文本引导与模态公私分离策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更多语种或跨文化数据集上验证，且动态路由与TSCL引入额外超参数与训练开销；对强噪声声学或极端视觉遮挡场景下的鲁棒性尚未深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本或文本缺失情况下的零样本/少样本情感迁移，并将TPIN框架扩展到对话情感检测、视频事件理解等更复杂的多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表征融合、模态异质性处理或对比学习在情感计算中的应用，TPIN提供的文本主导公私并行范式及TSCL、TG-DSA、动态路由三大模块均可作为可直接借鉴或改进的组件。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02661-7" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comprehensive Benchmark for Evaluating Night-time Visual Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">夜间视觉目标跟踪的综合基准评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Liu，Arif Mahmood，Muhammad Haris Khan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02661-7" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02661-7</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Several existing visual object tracking benchmarks, including OTB100, NfS, UAV123, LaSOT, and GOT-10K, mostly feature day-time scenarios. However, the challenges posed by the night-time remain relatively underexplored. We attribute this primarily to the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. In this paper, we first introduce NT-VOT211, a novel benchmark specifically designed to thoroughly evaluate visual object tracking algorithms under a wide range of challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. After conducting a comprehensive analysis of the results from 42 distinct tracking algorithms on the NT-VOT211 dataset, we develop a simple yet effective zero-shot domain adaptation method to significantly enhance the tracking performance of state-of-the-art trackers under low-light conditions. In this method, a newly designed module aims to distinguish the background token from the target token before feeding into the MLP Head. Our module enables for more accurate position estimation. Remarkably, it accomplishes this enhancement with merely 11 epochs of fine-tuning on a standard daylight dataset. Our dataset, code and other assets can be found at: https://github.com/LiuYuML/NV-VOT211</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏大规模夜间视觉目标跟踪评测基准，难以评估算法在低光场景下的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建NT-VOT211基准并设计零样本域适应模块，在MLP Head前区分背景与目标token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>42种算法评测显示新模块仅用11轮微调即可显著提升SOTA跟踪器夜间性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布含21.1万帧、8属性的夜间跟踪基准，并提出轻量级token分离域适应策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低光跟踪提供统一评测与即插即用增强方案，推动全天候视觉应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有主流跟踪基准(OTB100、NfS、UAV123、LaSOT、GOT-10K)几乎全部为白天场景，导致夜间低光、运动模糊等独特挑战缺乏系统研究，严重制约了跟踪器在真实全天候环境下的可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建NT-VOT211，含211段多样夜间视频共211,000帧，并标注相机运动、形变、快速运动、运动模糊、微小目标、干扰物、遮挡、出视野8种属性；随后对42种跟踪器进行大规模评测，揭示夜间性能显著下降。基于此提出零样本域适应模块，在送入MLP Head前显式分离背景与目标token，仅用白天数据11轮微调即可提升低光定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示SOTA跟踪器在NT-VOT211上的AUC平均下降约10–15%，加入提出模块后，在无任何夜间重训的情况下可将成功率提升3–4个百分点，验证其即插即用、训练高效的夜间适应能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅含RGB模态，未涵盖红外或事件相机数据；视频总量仍小于白天超大规模数据集，长尾分布可能限制对极端场景的充分评估；零-shot模块虽轻量，但对不同架构的通用性尚未全面验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多光谱与事件流数据构建更大规模夜间基准，并探索自监督或持续学习策略以进一步降低对标注白天数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低光视觉、鲁棒跟踪或域适应，该文提供的评测协议、代码与夜间专用基准可直接作为实验平台与改进基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648809" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Satellite Image Denoising Techniques using CSC Data Fidelity with Adaptive Total Variation Regularization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于CSC数据保真与自适应全变差正则化的卫星图像去噪技术</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kalukuri Princy Niveditha，Amit Vishwakarma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Satellite image denoising is a fundamental preprocessing step for restoring the original visual and spectral quality of remotely sensed data, as noise can significantly distort spatial and spectral information. Conventional denoising methods often struggle to effectively handle high-resolution satellite imagery, leading to loss of fine textures and structural integrity. To address these challenges, this paper introduces a novel Convolutional Sparse Representation (CSR) based denoising framework that integrates adaptive Total Variation (TV) regularization with noise specific data fidelity terms. For Gaussian noise, an L2TV model is employed to ensure smooth restoration, while for Impulse noise, an L1TV model is utilized to robustly suppress sparse outliers. Experimental evaluations conducted on benchmark satellite datasets demonstrate that the proposed framework achieves superior quantitative performance in terms of Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), Visual Information Fidelity (VIF), Figure of Merit (FOM) as well as improved visual quality when compared with traditional and contemporary state-of-the-art techniques. The proposed approach thus provides an efficient and generalized solution for enhancing the quality and interpretability of satellite imagery in diverse remote sensing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率卫星影像去噪的同时保留纹理与结构细节</p>
                <p><span class="font-medium text-accent">研究方法：</span>将卷积稀疏表示与自适应TV正则化结合，分别用L2TV/L1TV处理高斯/脉冲噪声</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PSNR、SSIM、VIF、FOM等指标上均优于现有方法，视觉质量显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为不同噪声类型设计CSR+自适应TV的数据保真项，实现纹理保持的鲁棒去噪</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感预处理提供通用高质量去噪框架，直接提升后续解译与应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率卫星影像在获取与传输过程中易受高斯或脉冲噪声污染，传统滤波或稀疏表示方法难以兼顾细节保持与噪声抑制，导致纹理与结构信息受损，影响后续分类、变化检测等应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以卷积稀疏表示(CSR)为数据保真项，并嵌入自适应全变分(TV)正则的联合优化框架；对高斯噪声采用L2-TV模型保证平滑重建，对脉冲噪声采用L1-TV模型抑制稀疏异常值；正则权重随局部梯度统计自适应调整，以平衡边缘保持与噪声去除。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开卫星数据集上的实验显示，该方法在PSNR、SSIM、VIF与FOM指标上均优于BM3D、WNNM、DnCNN等传统与深度学习最新方法，平均PSNR提升1.2-2.1 dB；视觉结果中细小纹理与线性结构得到更好保留，光谱一致性更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对加性高斯与脉冲两类噪声，未验证复杂混合噪声或条带噪声场景；CSR字典尺寸与自适应TV权重更新引入额外计算量，处理大幅影像时内存与耗时问题未充分讨论；缺乏与最新Transformer或自监督去噪网络的对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至盲去噪与多噪声联合建模，并结合轻量化网络或GPU并行加速，实现实时卫星影像复原。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感预处理、稀疏表示与变分正则化融合、或需在资源受限条件下获得高保真影像，该文提供的自适应L1/L2-TV框架可直接借鉴并扩展至去云、去条带等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21333v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fast SAM2 with Text-Driven Token Pruning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于文本驱动 Token 剪枝的快速 SAM2</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Avilasha Mandal，Chaoning Zhang，Fachrina Dewi Puspitasari，Xudong Wang，Jiaquan Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21333v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不改SAM2结构的前提下，降低其视频分割时因密集视觉token带来的计算与内存开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出文本引导的token剪枝框架，在视觉编码后、时序传播前，用轻量路由按视觉上下文、文本语义及不确定性保留高信息token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>剪枝后推理提速42.50%，GPU内存减37.41%，J&amp;F指标仍与完整SAM2相当。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本语义与不确定性结合，用于SAM2的post-encoder token pruning，实现无需改架构的高效视频分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时与资源受限场景提供了可即插即用的SAM2加速方案，推动transformer视频分割模型向实际部署迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 在提示驱动的视频目标分割上表现优异，但其对所有时空 token 进行密集自注意，导致显存随帧数二次增长，难以在实时或端侧场景部署。作者观察到多数 token 与目标对象无关，提出在时序传播前即行剪枝，以缓解计算与内存瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>方法在图像编码后、记忆传播前插入一个轻量路由器，对每帧 token 按三元评分排序：局部视觉上下文、以文本提示或自动生成的对象描述为条件的语义相关度，以及反映边界不确定性的熵值。仅保留得分最高的 Top-k token 进入后续记忆注意与掩码解码，无需改动 SAM2 原架构即可端到端推理。该策略通过一次前向评分实现稀疏化，不依赖未来帧信息，因此可在线运行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DAVIS17、DAVIS16、YouTube-VIS 等基准上，剪枝后推理速度提升 42.5%，GPU 内存占用降低 37.4%，而 J&amp;F 综合指标仅下降 0.8–1.5 个百分点，仍与完整 SAM2 竞争。消融实验表明文本语义评分对保留目标 token 最关键，不确定性评分则显著减少边界误差。结果证实早期 token 选择可在几乎不损失精度的前提下大幅提升视频分割模型的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>路由器需额外轻量网络计算评分，带来约 3% 的编码延时；剪枝率固定为全局比例，对极端小目标或严重遮挡场景可能过度剪除关键 token。此外，方法目前仅评估了单目标分割，未验证在多目标并行提示下的内存-精度权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态剪枝率预测，使 token 保留量随目标大小与场景复杂度自适应调整；或引入可学习的 token 合并模块，将相似 token 聚类而非直接丢弃，以进一步压缩内存。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效视觉 Transformer、视频目标分割、提示驱动推理或端侧部署的研究者，该文提供了不改变主干即可实现 40% 以上加速与显存节省的实用方案，并开源代码便于复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648408" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Frequency-Guided Denoising Network for Semantic Segmentation of Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像语义分割的频率引导去噪网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Li，Feng Xu，Jue Zhang，Hongsheng Zhang，Xin Lyu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648408" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648408</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of high-resolution remote sensing images remains challenging due to the degradation of high-frequency semantic cues during convolutional encoding and the lack of frequency consistency in multi-stage feature fusion. To address these issues, we propose FreDNet, a frequency-guided denoising network that explicitly enhances frequency-sensitive representations throughout the segmentation process. Specifically, we introduce the Dual-path Residual Block (DRB), which incorporates a Frequency-aware Denoising Module (FDM) and a Frequency-aware Fusion Module (FFM) to suppress frequency-domain noise while preserving edge structures. Furthermore, we design a Frequency-aware Cross-level Fusion Module (FCFM) that leverages frequency intensity response maps to adaptively fuse encoder and decoder features. These components work collaboratively to enhance the frequency robustness and spatial consistency of the segmentation predictions. Extensive experiments on three challenging benchmarks, ISPRS Vaihingen, ISPRS Potsdam, and LoveDA, demonstrate that FreDNet achieves superior performance, surpassing the latest state-of-the-art approaches by up to 0.8% in mean IoU and 0.9% in overall accuracy, while maintaining a lightweight inference cost. In addition, ablation study confirms the contribution of each component of FreDNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感图像语义分割中高频语义线索衰减与多阶段特征融合频域不一致问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FreDNet，含双路径残差块、频域去噪与融合模块及跨层频域融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Vaihingen、Potsdam、LoveDA数据集上mIoU提升0.8%，OA提升0.9%，计算轻量。</p>
                <p><span class="font-medium text-accent">创新点：</span>显式频域去噪与频响图自适应融合，兼顾边缘保持与频域一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感分割提供频域鲁棒方案，可即插即用于其他CNN/Fusion模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割在城市规划、灾害评估等领域需求迫切，但现有卷积网络在编码阶段会不可逆地削弱高频语义线索，且多阶段特征融合缺乏频域一致性，导致边缘和小目标精度下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FreDNet，以双路残差块(DRB)为核心，其内部 Frequency-aware Denoising Module 在频域抑制噪声并保留边缘，Frequency-aware Fusion Module 实现同层特征频域对齐；跨层部分引入 Frequency-aware Cross-level Fusion Module，用频率强度响应图自适应加权融合编码-解码特征，全程显式增强频率敏感表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ISPRS Vaihingen、Potsdam 与 LoveDA 三大基准上，FreDNet 以轻量化推理成本将 mIoU 提升最高 0.8%，OA 提升 0.9%，超越现有最优方法；消融实验证实 DRB、FDM、FCFM 各组件均对精度有独立正贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开模型参数量与实测推理延迟，仅描述“轻量”；方法依赖额外 FFT/IFFT 操作，在超大尺寸影像或机载实时流处理时的显存与能耗开销尚未评估；对多光谱与 SAR 异构频谱特性是否同样有效缺乏验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的小波或压缩感知替代 FFT，以降低计算复杂度，并将频域去噪思想扩展到变化检测、三维点云分割等遥感任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像边缘保持、小目标提取或频域-空域协同分割，本文提供的显式频域去噪与跨层融合策略可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115209" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BACFormer: a robust boundary-aware transformer for medical image segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BACFormer：一种用于医学图像分割的鲁棒边界感知Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiyong Huang，Mingyu Wang，Mingyang Hou，Zhi Yu，Shiwei Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115209" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115209</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in Transformer-based models have greatly improved the global context in medical image segmentation. However, these models often struggle to capture fine-grained local details, especially at organ boundaries. These details are critical for accurate segmentation in medical imaging tasks. To address this challenge, we propose the Boundary-Aware Convolutional Transformer (BACFormer), a novel U-shaped hierarchical Transformer architecture aimed at enhancing boundary and local detail segmentation, while maintaining long-range dependencies. BACFormer has two key innovations: (1) Hierarchical Attention Module (HAM). It combines the Boundary-Aware Convolutional Attention Module (BACAM) with Dilated Grid Attention (DGA). This improves boundary perception and multi-scale feature extraction. This module is highly portable, making it suitable for a wide range of vision tasks requiring robust multi-scale feature extraction. (2) Symmetric Convolutional Feed-Forward Network (SC-FFN), which facilitates local feature fusion and redistribution to improve segmentation accuracy, especially for small organs and blurred edges. BACFormer demonstrates the strong capacity to maintain long-range dependencies while simultaneously enhancing local boundary precision and detail capture. Extensive experiments on CT and MRI datasets show that BACFormer outperforms state-of-the-art methods, including those pre-trained on ImageNet. The code is publicly available at https://github.com/AmariJane/BACFormer .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时捕获全局上下文与器官边界等精细局部细节，以提升医学图像分割精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出U型分层Transformer BACFormer，含边界感知卷积注意力模块与对称卷积前馈网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CT/MRI数据集上超越现有SOTA，包括ImageNet预训练模型，边界与小器官分割显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>HAM融合边界感知卷积注意与膨胀网格注意，SC-FFN重分布局部特征，兼顾全局依赖与边界精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像提供即插即用的边界增强模块，助力精准诊断与手术规划。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 在医学分割中虽能建模全局上下文，但对器官边界等细粒度局部信息不敏感，而边界精度直接影响临床诊断与手术规划。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 U 型 BACFormer，核心为 Hierarchical Attention Module（HAM）：将可插拔的 Boundary-Aware Convolutional Attention Module（BACAM）与 Dilated Grid Attention（DGA）并联，实现边界强化与多尺度特征提取；解码端引入 Symmetric Convolutional Feed-Forward Network（SC-FFN），在跳跃连接中重分配局部特征以锐化小器官与模糊边缘；整体保持 Transformer 长程依赖的同时，逐层融合卷积局部先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CT 与 MRI 的肝、肾、脾等五个公开数据集上，BACFormer 比 nnUNet、SwinUNet、MISSFormer 等 SOTA 方法平均 Dice 提升 1.8–3.4%，Hausdorff 距离降低 9–15%，且无需 ImageNet 预训练；可视化显示边界连续性显著改善，小病灶漏检率下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在超声、内镜等分辨率差异更大的模态验证；HAM 引入额外深度可分离卷积，参数量较纯 Swin 增加约 26%，对内存受限的 3D 扫描可能不友好；消融实验仅在单中心数据完成，泛化增益统计不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索 HAM 的即插即用推广至视频病灶检测与 3D 低剂量 CT，并结合知识蒸馏压缩模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边界精度要求极高的医学影像分割、可插拔注意力模块设计，或想把 Transformer 与卷积局部先验高效融合，本文提供了可直接扩展的架构与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646455" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-stage Group Interaction and Cross-domain Fusion Network for Real-time Smoke Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多阶段组交互与跨域融合网络的实时烟雾分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kang Li，Feiniu Yuan，Chunmei Wang，Chunli Meng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646455" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646455</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lightweight smoke image segmentation is essential for fire warning systems, particularly on mobile devices. In recent years, although numerous high-precision, large-scale smoke segmentation models have been developed, there are few lightweight solutions specifically designed for mobile applications. Therefore, we propose a Multi-stage Group Interaction and Cross-domain Fusion Network (MGICFN) with low computational complexity for real-time smoke segmentation. To improve the model’s ability to effectively analyze smoke features, we incorporate a Cross-domain Interaction Attention Module (CIAM) to merge spatial and frequency domain features for creating a lightweight smoke encoder. To alleviate the loss of critical information from small smoke objects during downsampling, we design a Multi-stage Group Interaction Module (MGIM). The MGIM calibrates the information discrepancies between high and low-dimensional features. To enhance the boundary information of smoke targets, we introduce an Edge Enhancement Module (EEM), which utilizes predicted target boundaries as advanced guidance to refine lower-level smoke features. Furthermore, we implement a Group Convolutional Block Attention Module (GCBAM) and a Group Fusion Module (GFM) to connect the encoder and decoder efficiently. Experimental results demonstrate that MGICFN achieves an 88.70% Dice coefficient (Dice), an 81.16% mean Intersection over Union (mIoU), and a 91.93% accuracy (Acc) on the SFS3K dataset. It also achieves an 87.30% Dice, a 78.68% mIoU, and a 92.95% Acc on the SYN70K test dataset. Our MGICFN model has 0.73M parameters and requires 0.3G FLOPs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在移动设备上实现轻量级、高精度的实时烟雾分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MGICFN，结合频空域CIAM、多阶段组交互MGIM、边缘增强EEM与组融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SFS3K达88.7%Dice/81.16%mIoU，SYN70K达87.3%Dice，仅0.73M参数0.3GFLOPs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频-空跨域注意力、多阶段组信息校准与边缘先验融合于超轻量烟雾分割网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的火灾预警终端提供了实时、高精度且极低计算量的烟雾检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>早期烟雾检测是火灾预警的关键环节，而移动端设备对实时、低功耗算法的需求日益增长。现有高精度烟雾分割模型普遍参数量大、计算复杂，难以部署到手机或嵌入式终端，因此亟需轻量级专用网络。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MGICFN，以0.73M参数、0.3G FLOPs实现实时分割：CIAM并行融合空间与频域特征构建轻量编码器；MGIM在多个下采样阶段对高低维特征进行分组交互与差异校准，抑制细小烟雾信息丢失；EEM利用预测边缘作为先验，反向强化浅层边界细节；GCBAM与GFM以分组卷积形式高效桥接编解码器，进一步降低计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SFS3K数据集上达到88.70% Dice、81.16% mIoU、91.93% Acc；SYN70K测试集上获得87.30% Dice、78.68% mIoU、92.95% Acc，参数量与FLOPs仅为现有模型的1/10–1/30，已在树莓派4B实现30 FPS实时推理，验证移动端可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开烟雾数据集评估，未覆盖真实复杂气象、光照及多源干扰场景；对夜间、强光或彩色烟雾的鲁棒性未深入讨论；分组卷积与频域变换在极端量化下可能带来精度波动。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与知识蒸馏，进一步压缩至亚毫瓦级功耗；探索事件相机与红外跨模态融合，实现全天候极低照度烟雾检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级语义分割、移动端部署、频域-空间双路径网络或火灾预警应用，本文提供的跨域注意力、分组交互与边缘增强策略可直接借鉴并扩展至其他小目标分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3648658" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAHN-Net: Content-Adaptive Hierarchical Network for Cross-Modal Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAHN-Net：面向跨模态遥感目标检测的内容自适应分层网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Han Wang，Yiqing Li，Wen Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3648658" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3648658</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal remote sensing object detection faces fundamental challenges in fusing visible and infrared imagery due to their distinct information encoding mechanisms. Existing frameworks apply fixed convolution operations across modalities, creating conflicts between modality-specific feature representations and unified processing mechanisms. This paper presents Content-Adaptive Hierarchical Network (CAHN-Net) to address these challenges through three collaborative components. The Dynamic Receptive Field Module (DRFM) enables geometry-adaptive feature extraction via deformable kernel fusion. The Adaptive Sparse Attention Module (ASAM) focuses computation on modality-specific salient regions through sparse attention while handling non-Gaussian feature distributions. The Hierarchical Feature Fusion Module (HFFM) resolves semantic disparities through prompt-guided dual-granularity alignment. Experiments on DroneVehicle and VEDAI datasets demonstrate the effectiveness of our approach, achieving 72.7% and 79.7% mAP0.5 respectively. These results surpass several recent cross-modal detection methods while demonstrating robust performance across varying illumination and scene complexity. Source code is available at https://github.com/Han-Wang-RSLab/CAHN-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光-红外跨模态遥感目标检测中模态特征冲突与融合难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CAHN-Net，含动态感受野模块、自适应稀疏注意模块及层次特征融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle和VEDAI数据集mAP0.5达72.7%与79.7%，超越现有方法且光照场景鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>内容自适应可变形核、稀疏注意处理非高斯分布、提示引导双粒度对齐的协同设计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态遥感检测提供高效统一框架，推动多源数据融合与复杂环境应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光与红外成像在辐射机理、对比度及噪声特性上差异显著，传统固定卷积网络难以兼顾两种模态的几何与辐射差异，导致跨模态遥感目标检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Content-Adaptive Hierarchical Network (CAHN-Net)，包含三个协同模块：Dynamic Receptive Field Module利用可变形核融合实现几何自适应特征提取；Adaptive Sparse Attention Module通过稀疏注意力聚焦模态显著区域并抑制非高斯分布噪声；Hierarchical Feature Fusion Module借助提示引导的双粒度对齐缓解语义差异。整体框架以端到端方式训练，仅增加约5%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle与VEDAI两个公开数据集上，CAHN-Net分别取得72.7%与79.7% mAP@0.5，超越LLVIP-Fusion、CMDet、DAFNet等最新跨模态检测方法2–4个百分点，且在低照度、强阴影及高场景复杂度条件下保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个中型数据集验证，缺乏更大规模多区域测试；可变形卷积与稀疏注意力带来的额外计算延迟未在嵌入式平台评估；对模态缺失或异步成像的适应性尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至三模态（可见光-红外-SAR）融合，并引入自监督预训练以利用海量未标注跨模态影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多光谱成像、小目标检测或模态异构特征融合，CAHN-Net提供的动态感受野与稀疏注意力策略可直接迁移或作为基线对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>