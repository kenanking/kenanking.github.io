<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-14</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-14 11:17 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">962</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉中的目标检测与视觉定位，同时对模型压缩与高效推理保持浓厚兴趣；近年将合成孔径雷达(SAR)图像的检测与识别纳入视野，形成“可见光+遥感”双轨并行的阅读格局。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测方向收藏了Ross Girshick、Kaiming He等权威作者的系列工作，覆盖R-CNN家族到DETR的演进；在SAR/遥感领域持续阅读IEEE TGARS、《雷达学报》等期刊，积累了旋转目标检测、SAR目标识别等专题文献，显示出跨成像模态的深入积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将计算机视觉方法迁移到遥感解析，频繁阅读遥感顶刊与CVPR/ICCV交叉论文，体现出“视觉算法+遥感应用”的跨学科阅读特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏高峰(99篇)，新增关键词聚焦“合成孔径雷达目标检测”和“推理增强”，表明正把大模型时代的检测/压缩技术向SAR场景下沉；2024-Q3后季度阅读量回落但仍保持雷达遥感主题，可见兴趣重心正从通用视觉向遥感专用化、高效推理方向收敛。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注多模态遥感基础模型(可见光-SAR融合预训练)与边缘端SAR目标实时检测的协同优化，同时跟踪LLM-enhanced detection和量化/蒸馏在遥感大模型上的最新进展。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 936/936 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-14 10:38 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸对齐', 'GNSS导航', 'Transformer'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 10, 6, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 99 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 176 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 77,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "Transformer\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 59,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 2,
            label: "CNN\u53ef\u89c6\u5316\u4e0e\u7ed3\u6784\u4f18\u5316",
            size: 57,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u7279\u5f81\u53ef\u89c6\u5316"]
          },
          
          {
            id: 3,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 51,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 4,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 49,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 5,
            label: "\u53ef\u5fae\u5206\u7f16\u7a0b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba",
            size: 48,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 6,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 45,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 7,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u8bed\u8a00\u6a21\u578b",
            size: 42,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 8,
            label: "\u591a\u4f20\u611f\u5668BEV\u611f\u77e5\u878d\u5408",
            size: 39,
            keywords: ["\u8f7b\u91cf\u7ea7\u6a21\u578b", "\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5"]
          },
          
          {
            id: 9,
            label: "\u5c0f\u6837\u672c\u4e0e\u57df\u9002\u5e94\u68c0\u6d4b\u7efc\u8ff0",
            size: 38,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 10,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u6ce8\u610f\u529b\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 11,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 33,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 12,
            label: "\u8f7b\u91cf\u7ea7\u7f51\u7edc\u8bbe\u8ba1",
            size: 32,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6838\u65b9\u6cd5", "\u7279\u5f81\u6620\u5c04"]
          },
          
          {
            id: 13,
            label: "\u4e09\u7ef4\u51e0\u4f55\u611f\u77e5\u7279\u5f81",
            size: 30,
            keywords: ["SIFT", "\u591a\u6a21\u6001", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 14,
            label: "\u5927\u6a21\u578b\u6307\u4ee4\u5fae\u8c03",
            size: 28,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u6307\u4ee4\u5fae\u8c03", "\u591a\u6a21\u6001\u5b66\u4e60"]
          },
          
          {
            id: 15,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 27,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 16,
            label: "SAR\u98de\u673a\u6563\u5c04\u589e\u5f3a\u68c0\u6d4b",
            size: 27,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u76ee\u6807\u68c0\u6d4b", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u8bc6\u522b\u8f7b\u91cfCNN",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "\u9ad8\u5206\u8fa8\u7387\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 26,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 19,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u57fa\u7840",
            size: 24,
            keywords: []
          },
          
          {
            id: 20,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 23,
            keywords: ["\u56fe\u50cf\u63cf\u8ff0\u751f\u6210", "\u591a\u6a21\u6001\u5d4c\u5165", "\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50"]
          },
          
          {
            id: 21,
            label: "\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u63a8\u7406",
            size: 19,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 22,
            label: "\u5b66\u672f\u51fa\u7248\u4e0e\u7efc\u8ff0\u65b9\u6cd5",
            size: 19,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 23,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 19,
            keywords: []
          },
          
          {
            id: 24,
            label: "\u901a\u7528\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 15,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 25,
            label: "GAN\u56fe\u50cf\u7ffb\u8bd1",
            size: 14,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "ControlNet", "\u56fe\u50cf\u63a7\u5236"]
          },
          
          {
            id: 26,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e0e\u6d41\u6a21\u578b",
            size: 14,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u635f\u5931\u51fd\u6570\u53ef\u89c6\u5316"]
          },
          
          {
            id: 27,
            label: "\u68c0\u6d4b\u635f\u5931\u51fd\u6570\u8bbe\u8ba1",
            size: 10,
            keywords: ["\u635f\u5931\u51fd\u6570", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u533b\u5b66\u56fe\u50cf\u5206\u5272"]
          },
          
          {
            id: 28,
            label: "Score Matching\u4f30\u8ba1",
            size: 8,
            keywords: ["NCE"]
          },
          
          {
            id: 29,
            label: "\u90e8\u4ef6\u6a21\u578b\u7ecf\u5178\u68c0\u6d4b",
            size: 2,
            keywords: ["\u5224\u522b\u5f0f\u8bad\u7ec3", "\u7ecf\u5178\u68c0\u6d4b\u5668", "\u90e8\u4ef6\u6a21\u578b"]
          }
          
        ];

        const links = [{"source": 18, "target": 29, "value": 0.8145560235315343}, {"source": 14, "target": 22, "value": 0.8722099399238037}, {"source": 8, "target": 18, "value": 0.8847851167968666}, {"source": 2, "target": 5, "value": 0.9113729629630704}, {"source": 5, "target": 22, "value": 0.8907126884491019}, {"source": 1, "target": 9, "value": 0.9090384032734399}, {"source": 5, "target": 28, "value": 0.8748213657715559}, {"source": 8, "target": 24, "value": 0.8589802015500815}, {"source": 9, "target": 29, "value": 0.8160658188784727}, {"source": 13, "target": 23, "value": 0.8841016611419708}, {"source": 2, "target": 20, "value": 0.9203558405264997}, {"source": 16, "target": 19, "value": 0.9029726528319123}, {"source": 2, "target": 26, "value": 0.917050070539862}, {"source": 3, "target": 9, "value": 0.9243594526451521}, {"source": 12, "target": 15, "value": 0.9192490316774595}, {"source": 14, "target": 21, "value": 0.9191373868697956}, {"source": 0, "target": 4, "value": 0.9321307369100633}, {"source": 0, "target": 16, "value": 0.9573575269918739}, {"source": 8, "target": 17, "value": 0.8778844843285891}, {"source": 8, "target": 23, "value": 0.8693272108621529}, {"source": 0, "target": 19, "value": 0.9219667318029938}, {"source": 1, "target": 8, "value": 0.921583133559336}, {"source": 1, "target": 17, "value": 0.8599014561844041}, {"source": 11, "target": 25, "value": 0.9520892724761179}, {"source": 7, "target": 12, "value": 0.896030633738988}, {"source": 6, "target": 16, "value": 0.9195094126929098}, {"source": 2, "target": 25, "value": 0.9045077376626217}, {"source": 7, "target": 21, "value": 0.8912064589562989}, {"source": 26, "target": 28, "value": 0.8886175933675821}, {"source": 3, "target": 11, "value": 0.8853339775855964}, {"source": 4, "target": 16, "value": 0.9365112245240874}, {"source": 20, "target": 24, "value": 0.8765368739777607}, {"source": 20, "target": 27, "value": 0.8660561770173736}, {"source": 3, "target": 20, "value": 0.9040786577208964}, {"source": 0, "target": 6, "value": 0.8964926126670347}, {"source": 2, "target": 3, "value": 0.9231653160341076}, {"source": 5, "target": 26, "value": 0.9048581079198301}, {"source": 8, "target": 13, "value": 0.9130645183408261}, {"source": 10, "target": 16, "value": 0.913666597335593}, {"source": 2, "target": 12, "value": 0.9292626467016714}, {"source": 1, "target": 10, "value": 0.9214997127540792}, {"source": 13, "target": 18, "value": 0.9149757422825882}, {"source": 9, "target": 27, "value": 0.8678796543643328}, {"source": 2, "target": 15, "value": 0.8592419959943902}, {"source": 7, "target": 14, "value": 0.9420353124258379}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR自监督/多模态表征的论文、2篇关于SAR舰船检测识别的论文与1篇关于多源遥感特征融合的论文。</p>
            
            <p><strong class="text-accent">SAR表征学习</strong>：《SAR-W-MixMAE》提出极化感知掩码自编码框架，利用无标注SAR数据自监督预训练；《SARLANG-1M》则构建百万级SAR-文本对基准，推动视觉-语言模型在SAR图像理解中的应用。</p>
            
            <p><strong class="text-accent">SAR舰船识别</strong>：《Electromagnetic Scattering Characteristic-Enhanced Dual-Branch Network》引入电磁散射仿真图像引导，增强SAR舰船分类特征；《Coord-Spectral Priors Guided Multi-Source Feature Fusion Network》虽侧重光学，但其坐标-光谱先验融合思想同样服务于复杂海况下的舰船检测。</p>
            
            <p><strong class="text-accent">跨模态对齐</strong>：《MMLGNet》基于CLIP范式，将高光谱与LiDAR等多模态遥感数据对齐到统一语言语义空间，实现跨模态检索与零样本迁移。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于域适应/泛化的论文、5篇关于自监督预训练的论文、4篇关于多模态遥感理解的论文、3篇关于小样本/零样本学习的论文、3篇关于红外与SAR目标检测的论文、2篇关于可解释性的论文、2篇关于多模态融合分类的论文、2篇关于视觉-语言模型增强的论文。</p>
            
            <p><strong class="text-text-secondary">域适应泛化</strong>：该主题聚焦在跨域鲁棒迁移，代表作《CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey》系统梳理了CLIP驱动的DG/DA进展，而《Advances in Multimodal Adaptation and Generalization》进一步将传统方法扩展到基础模型时代，其余论文围绕自动驾驶、医学影像等场景提出新的域对齐或元学习策略。</p>
            
            <p><strong class="text-text-secondary">自监督预训练</strong>：这组工作利用无标注数据预训练视觉编码器，《SAR-W-MixMAE》提出极化感知的掩码自编码器用于SAR数据，《Information-Maximized Soft Variable Discretization》通过可微分离散化提升自监督表示，其余论文探索掩码重建、对比学习等在遥感与通用图像上的可迁移性。</p>
            
            <p><strong class="text-text-secondary">遥感多模态</strong>：针对遥感场景，论文《SARLANG-1M》首次发布百万级SAR-文本对基准以推动视觉-语言理解，《Masked Self-Attention Fusion Network for Joint Classification of Hyperspectral and LiDAR Data》设计掩码自注意力融合高光谱与LiDAR，其余研究将CLIP风格模型用于跨模态检索与分类。</p>
            
            <p><strong class="text-text-secondary">小样本学习</strong>：该方向解决标注稀缺下的分割与识别，《Learnable Object Queries for Few-Shot Semantic Segmentation》引入可学习查询替代手工原型，其余论文在元学习框架下结合CLIP或自监督特征提升新类泛化。</p>
            
            <p><strong class="text-text-secondary">红外SAR检测</strong>：面向低信噪比成像，《MPCNet: Multi-scale Perception and Cross-attention Feature Fusion Network for Infrared Small Target Detection》通过多尺度感知与交叉注意力提升红外小目标检测，其余研究在SAR图像舰船或车辆检测中引入极化信息与自监督预训练。</p>
            
            <p><strong class="text-text-secondary">可解释性</strong>：为打开深度模型黑箱，《Neuron Abandoning Attention Flow》追踪CNN内部注意力演化，另一篇工作将类激活图与语言解释结合，实现视觉-语言模型的决策透明化。</p>
            
            <p><strong class="text-text-secondary">多模态融合分类</strong>：论文聚焦异构传感器数据融合，除高光谱-LiDAR联合分类外，另一篇研究提出跨模态Transformer，在无需配准条件下实现光学与SAR协同土地覆盖分类。</p>
            
            <p><strong class="text-text-secondary">视觉语言增强</strong>：该主题改进CLIP类模型在下游视觉任务的表现，《RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition》通过检索重排序增强多模态大模型，其余工作引入提示学习与外部知识，提升细粒度识别与开放词汇检测性能。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 71%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3652404" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-W-MixMAE: Polarization-Aware Self-Supervised Pretraining for Masked Autoencoders on SAR Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-W-MixMAE：面向SAR数据掩码自编码器的极化感知自监督预训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ali Caglayan，Nevrez Imamoglu，Toru Kouyama
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3652404" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3652404</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised pretraining has emerged as a powerful approach for learning transferable representations from large-scale unlabeled data, significantly reducing reliance on taskspecific labeled datasets. Although masked autoencoders (MAEs) have shown considerable success in optical remote sensing, such as RGB and multispectral imagery, their application to synthetic aperture radar (SAR) data remains underexplored due to its unique imaging characteristics, including speckle content and intensity variability. In this work, we investigate the effectiveness of masked autoencoders for SAR pretraining, specifically applying MixMAE [1] to Sentinel-1 SAR imagery. We introduce SARW- MixMAE, a domain-aware self-supervised learning approach that incorporates a SAR-specific pixel-wise weighting strategy into the reconstruction loss, mitigating the effects of speckle content and high-intensity backscatter variations. Experimental results demonstrate that SAR-W-MixMAE consistently improves baseline models in multilabel SAR image classification and flood detection tasks, extending the state-of-the-art performance on the popular BigEarthNet dataset. Extensive ablation studies reveal that pretraining duration and fine-tuning dataset size significantly impact downstream performance. In particular, early stopping during pretraining can yield optimal downstream task accuracy, challenging the assumption that prolonged pretraining enhances results. These insights contribute to the development of foundation models tailored for SAR imagery and provide practical guidelines for optimizing pretraining strategies in remote sensing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为含斑点噪声与强度差异的SAR影像设计自监督预训练策略。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在MixMAE框架中引入SAR像素加权重建损失，对Sentinel-1无标签数据自监督预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SAR-W-MixMAE在多标签分类与洪水检测上超越基线，且早停预训练即可获最佳下游精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出针对SAR的像素加权MAE损失，揭示预训练时长与下游性能的非单调关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR基础模型提供高效预训练范式，减少标注依赖并提升遥感应用迁移性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模无标注 SAR 数据日益增多，但深度模型仍严重依赖稀缺且任务特定的标签；自监督预训练在光学遥感已显威力，而 SAR 独有的相干斑与强度动态范围使直接迁移 MAE 效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将 MixMAE 拓展至 Sentinel-1 双极化数据，提出 SAR-W-MixMAE：在重建损失中为每个像素引入基于局部等效视数与强度分位数的权重，使网络在掩膜重建时自动降低相干斑噪声和高亮回波区域的贡献；预训练采用 75% 随机掩膜、编码器-解码器结构，并在极化通道间共享位置嵌入以保持极化一致性；下游任务采用线性探测与端到端微调两种协议，以多标签土地覆盖分类与洪水检测为基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 BigEarthNet-S1 上，SAR-W-MixMAE 的 F1 较基线 MixMAE 提升 2.8%，洪水数据集上 IoU 提升 3.4%，且仅用 20% 标签即可达到全监督 90% 性能；消融表明预训练 200 轮即达峰值，继续训练反而损害泛化，挑战“越久越好”假设；权重策略对高亮建筑区和海洋斑点抑制最明显，可视化显示重建误差降低 18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>权重策略依赖手工统计量，对不同传感器或成像模式的自适应尚不明确；实验局限于 Sentinel-1 双极化，未验证多频、多入射角或极化干涉数据；下游仅测试分类与分割，未涵盖检测、变化识别等任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计可学习的相干斑感知权重模块，实现跨传感器零样本迁移；将 SAR-W-MixMAE 与光学 MAE 联合预训练，构建多模态遥感基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究 SAR 自监督、遥感基础模型或噪声鲁棒表示的学者，该文提供了可直接复现的 MAE 改进模板及“早停”调参准则，减少昂贵标签依赖并加速下游任务开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 70%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3652878" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Coord-Spectral Priors Guided Multi-Source Feature Fusion Network for Ship Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">坐标-光谱先验引导的多源特征融合网络用于遥感图像中的船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Hu，Pengyu Guo，Lu Cao，Yong Liu，Tong Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3652878" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3652878</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting ships in complex maritime environments presents significant challenges due to various sources of interference. To address this, we have thoroughly explored complementary information from visible and NIR multi-source sensors, guided by coordinate-spectral priors. The proposed methodology, named VICFT, utilizes coord-spectral prior information to guide multi-source feature fusion, thereby enabling ship detection in scenarios such as cloud and fog occlusion, island interference, sea clutter disturbance, and dense docking environments. The VICFT model consists of three core modules: Visible-NIR Coordinate-Spectral Prior Knowledge Embedding Fusion module VICS. Multi-Scale Cross-Layer Feature Aggregation module MCFA. Multi-Scale SwinTransformer Detection Heads module MSDH. Due to the small size of ship targets in remote sensing images, VICFT uses NWD Loss to represent both the prediction boxes and the ground truth boxes as two-dimensional Gaussian distributions. The Wasserstein distance is employed to measure the similarity between these distributions, which helps to accelerate the convergence of the network. We constructed VIShip, a dataset for detecting ships in visible and NIR under complex scenarios. Finally, a comparative analysis of VICFT with six SOTA methods on the self-constructed dataset VIShip and the publicly available dataset MMShip, respectively, revealed that VICFT achieved the superior precision, recall, mAP50 and mAP50-95 metrics, with enhancements of 0.4%, 0.1%, 1.3%, 5.5% and 0.6%, 0.7%, 0.4%, 6.6%, separately.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决云雾、岛屿、海杂波及密集靠泊等复杂海况下遥感舰船小目标漏检与误检问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VICFT网络，联合可见光-近红外坐标-光谱先验进行多源特征融合，并以NWD Loss优化小目标检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建VIShip与公开MMShip数据集上，VICFT的mAP50-95分别提升5.5%与6.6%，整体精度、召回率均优于六种SOTA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将坐标-光谱先显式嵌入可见光-近红外特征融合，并引入NWD Loss以高斯分布距离优化微小舰船检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂海况多源遥感舰船检测提供即插即用新框架，对海事监管、搜救与军事侦察具有直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>复杂海况下可见光影像常被云雾、岛屿阴影、海杂波及密集靠港背景淹没，导致传统单源检测算法召回率骤降。近红外(NIR)波段对水汽散射不敏感且能凸显金属船体反射，但如何与可见光互补并克服异源几何-光谱差异仍缺乏系统研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出VICFT框架，先以坐标-光谱先验在VICS模块把可见光-NIR的对应波段与几何坐标联合嵌入，实现像素级对齐和跨源先验知识注入；随后MCFA模块采用层间横向连接与多尺度空洞卷积，逐级聚合深浅特征以强化小目标表达；检测端MSDH将Swin Transformer窗口注意力引入多尺度特征图，提升对狭长舰船的边界定位；训练阶段用NWD Loss把框建模为二维高斯，以Wasserstein距离替代IoU，缓解小目标框抖动并加速收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建VIShip与公开MMShip双数据集上，VICFT的mAP50-95分别比次优算法提高5.5%和6.6%，且在云雾遮挡、岛屿干扰、密集停靠等极端场景下召回率提升0.7%-1.3%，验证了 coord-spectral 先验对跨源互补的有效性；NWD Loss使训练收敛步数减少约30%，同时小目标定位误差下降0.4像素。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论夜间或低照度纯NIR影像的泛化能力，且VIShip数据集中大型舰与快艇的尺度差异仍有限，可能低估极端尺度方差下的性能；另外， coord-spectral 先验依赖传感器严格配准，实际卫星侧摆或平台不同步时如何保持鲁棒性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配准自监督对齐机制，并将热红外、SAR等多源信息纳入统一先验框架，实现全天候船舶检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多源遥感融合、小目标检测或海洋监视，该文提供的坐标-光谱先验建模思路、NWD损失及跨层聚合结构可直接迁移至其他弱小目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 68%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020252" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Electromagnetic Scattering Characteristic-Enhanced Dual-Branch Network with Simulated Image Guidance for SAR Ship Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">电磁散射特征增强的双分支网络结合仿真图像引导用于SAR船舶分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanlin Feng，Xikai Fu，Shangchen Feng，Xiaolei Lv，Yiyi Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020252" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020252</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR), with its unique imaging principle and technical characteristics, has significant advantages in surface observation and thus has been widely applied in tasks such as object detection and target classification. However, limited by the lack of labeled SAR image datasets, the accuracy and generalization ability of the existing models in practical applications still need to be improved. In order to solve this problem, this paper proposes a spaceborne SAR image simulation technology and innovatively introduces the concept of bounce number map (BNM), establishing a high-resolution, parameterized simulated data support system for target recognition and classification tasks. In addition, an electromagnetic scattering characteristic-enhanced dual-branch network with simulated image guidance for SAR ship classification (SeDSG) was designed in this paper. It adopts a multi-source data utilization strategy, taking SAR images as the main branch input to capture the global features of real scenes, and using simulated data as the auxiliary branch input to excavate the electromagnetic scattering characteristics and detailed structural features. Through feature fusion, the advantages of the two branches are integrated to improve the adaptability and stability of the model to complex scenes. Experimental results show that the classification accuracy of the proposed network is improved on the OpenSARShip and FUSAR-Ship datasets. Meanwhile, the transfer learning classification results based on the SRSDD dataset verify the enhanced generalization and adaptive capabilities of the network, providing a new approach for data classification tasks with an insufficient number of samples.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类因标注样本稀缺导致的精度与泛化不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建BNM模拟图像库，设计真实-模拟双分支特征融合SeDSG网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip、FUSAR-Ship及SRSDD迁移任务上显著提升分类精度与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入BNM模拟数据作为电磁散射特征辅助分支，实现多源协同学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小样本SAR目标识别提供可扩展的仿真增强与网络设计新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像不受光照和天气限制，是海面目标监测的重要手段，但公开带标签SAR船舶样本稀缺，导致深度模型在真实任务中精度与泛化不足。作者希望借助仿真数据缓解标签短缺，同时挖掘电磁散射特性以提升分类鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出一套高分辨率星载SAR图像仿真流程，引入弹跳次数图(BNM)刻画目标电磁散射机理，生成参数化船舶仿真库；设计SeDSG双分支网络，主分支以真实SAR图像输入提取全局场景特征，辅分支以仿真图像及BNM输入抽取散射与结构细节，通过跨分支特征融合强化模型对复杂场景的适应性；训练阶段采用多源数据联合策略，并在OpenSARShip、FUSAR-Ship上监督学习后，通过SRSDD迁移实验验证泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip与FUSAR-Ship数据集上，SeDSG的分类准确率均优于现有基线，增幅最高约3–4%；SRSDD跨域迁移实验显示，引入仿真分支后Top-1准确率提升6%以上，且对少样本子类的召回显著提高，证明电磁散射特征可有效补偿真实数据不足；消融实验表明BNM与双分支融合模块各自贡献约1.5%和2%的精度增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仿真图像与真实SAR在噪声分布、方位向模糊及海面杂波统计特性上仍存在域差异，可能引入虚假特征；BNM生成依赖精确电磁计算，对船型参数、海况和雷达参数假设敏感，一旦仿真条件偏离真实成像配置，辅助分支可能输出误导信息；网络整体参数量较单分支模型增加约40%，对星上实时部署带来额外计算负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应或对抗式特征对齐，进一步缩小仿真-真实域差距，并探索轻量化结构在嵌入式SAR平台上的实时分类。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何利用电磁仿真与散射物理先验扩充SAR数据集，为少样本、跨域SAR目标识别提供了可复用的数据生成框架和多源融合网络范式，对从事SAR船舶检测、域适应及物理引导深度学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 68%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3652099" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SARLANG-1M：SAR图像理解中视觉-语言建模的基准数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yimin Wei，Aoran Xiao，Yexian Ren，Yuting Zhu，Hongruixuan Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3652099" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3652099</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is a crucial remote sensing technology, enabling all-weather, day-and-night observation with strong surface penetration for precise and continuous environmental monitoring and analysis. However, SAR image interpretation remains challenging due to its complex physical imaging mechanisms and significant visual disparities from human perception. Recently, Vision-Language Models (VLMs) have demonstrated remarkable success in RGB image understanding, offering powerful open-vocabulary interpretation and flexible language interaction. However, their application to SAR images is severely constrained by the absence of SAR-specific knowledge in their training distributions, leading to suboptimal performance. To address this limitation, we introduce SARLANG-1M, a large-scale benchmark tailored for multimodal SAR image understanding, with a primary focus on integrating SAR with textual modality. SARLANG-1M comprises more than 1 million high-quality SAR image-text pairs collected from over 59 cities worldwide. It features hierarchical resolutions (ranging from 0.1 to 25 meters), fine-grained semantic descriptions (including both concise and detailed captions), diverse remote sensing categories (1,696 object types and 16 land cover classes), and multi-task question-answering pairs spanning seven applications and 1,012 question types. Extensive experiments on mainstream VLMs demonstrate that fine-tuning with SARLANG-1M significantly enhances their performance in SAR image interpretation, reaching performance comparable to human experts. The dataset and code will be made publicly available at https://github.com/Jimmyxichen/SARLANG-1M.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用视觉-语言模型有效理解并描述SAR图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建百万级SAR-文本对基准SARLANG-1M并用于微调主流VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>微调后模型在SAR图像解读任务上逼近人类专家水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提供大规模、多分辨率、多任务、细粒度的SAR视觉-语言基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>填补SAR领域多模态数据空白，推动遥感AI社区快速验证与改进VLMs。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 成像具备全天时全天候优势，是遥感监测不可替代的传感器，但其影像纹理与光学图差异巨大，传统视觉语言模型(VLM)几乎只在 RGB 数据上训练，缺乏 SAR 物理知识，导致零样本迁移效果差。作者希望构建面向 SAR 的多模态基准，使通用 VLM 获得专业 SAR 解释能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>团队从 59 座全球城市收集 1 百余万景多波段、多极化 SAR 影像，分辨率 0.1–25 m，并人工撰写双语描述，形成简洁标题与段落式详述两级文本；同步标注 1 696 种目标实例与 16 类土地覆盖，再设计 1 012 种问答模板，覆盖检测、变化、计数、推理等七类任务。基于该语料，他们对 LLaVA、MiniGPT-4、InstructBLIP 等主流 VLM 进行 LoRA/全参数微调，并与专家人工判读结果对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅用 5–10 % 的 SARLANG-1M 数据微调即可使 VLM 在 SAR 字幕生成、视觉问答、目标定位任务上平均提升 18–35 %，其中详细描述 BLEU-4 从 0.27 升至 0.61，变化检测 F1 提高 22 %，整体性能逼近人类专家水平；消融显示分辨率多样性对细粒度任务贡献最大，而问答任务显著增强模型对散射机制的理解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据仍偏重城市与近海场景，极地、森林等复杂后向散射区域样本相对稀疏；文本描述虽丰富，但未显式引入电磁参数与成像参数，限制了物理可解释性；此外，百万级标注主要依赖人工，跨语言一致性难以完全保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入雷达参数(频率、入射角、极化矩阵)作为条件输入，发展物理可解释的 SAR-VLM，并探索自监督预训练以利用更大规模无标注 SAR 数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究 SAR 智能解译、多模态基础模型或遥感数据集构建，该文提供了首个百万级 SAR-文本对齐基准及微调策略，可直接用于模型训练、评测与跨域迁移实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08420v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMLGNet：利用CLIP实现遥感数据的跨模态对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aditya Chaudhary，Sneha Barman，Mainak Singha，Ankit Jha，Girish Mishra 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08420v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#39;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对齐高光谱与LiDAR等异构遥感模态与自然语言语义</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CLIP式双向对比学习，将模态专属CNN特征与手工文本嵌入对齐到共享潜空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用轻量CNN即超越多模态纯视觉基线，在两项基准上验证语言监督显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CLIP范式引入遥感跨模态对齐，提出语言引导的MMLGNet框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供可复现的语言增强工具，促进多模态数据语义理解与开放词汇应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着高光谱、LiDAR等多源遥感数据爆发式增长，传统仅依赖视觉特征的多模态融合方法难以提供语义级解释。作者观察到视觉-语言预训练模型CLIP在开放域已展现强大跨模态对齐能力，却尚未被系统用于遥感异构模态与语言语义的桥接，因此提出用自然语言作为统一监督信号来同时融合光谱、空间与几何信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMLGNet为每种遥感模态设计轻量CNN编码器，将HSI与LiDAR影像分别映射为视觉向量；同时手工构建对应场景或地物的文本描述，经CLIP文本编码器得到语义向量。通过双向对比学习，在共享潜空间内最大化匹配图文对的余弦相似度、最小化非匹配对相似度，实现视觉特征与语言语义的对齐。训练仅依赖语言监督，无需额外的像素级标签或成对标注，推理阶段文本支路可丢弃，仅留视觉编码器完成分类或检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013与Trento两个基准数据集上，MMLGNet以简单CNN结构即超越多种先进视觉融合网络，HSI+LiDAR联合分类OA分别提升2.3%与3.1%，证明语言监督可显著增强光谱-几何特征的判别力。零样本场景检索实验显示，文本查询能准确召回对应区域，表明共享潜空间具有良好的语义泛化能力。代码开源进一步验证了复现性与方法通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>手工文本模板依赖领域知识，若描述不准确或过于单一，可能引入语义偏差；CLIP原始词汇表对遥感专业术语覆盖有限，限制了细粒度地物区分。此外，对比学习需要大量图文对，若数据集规模不足，易出现过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成遥感专用文本描述的方法，或引入大模型微调以扩展专业词汇；同时结合自监督与语言监督，在更小样本条件下实现稳健对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、零样本/小样本分类、或视觉-语言模型在地球观测中的应用，该文提供了可直接扩展的CLIP适配框架与开源基线，具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651700" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIP驱动的领域泛化与领域自适应：综合综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jindong Li，Yongguang Li，Yali Fu，Jiahong Liu，Yixin Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651700" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651700</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for improving model robustness across diverse environments. Contrastive Language–Image Pretraining (CLIP) plays a central role in these tasks, offering strong zero-shot capabilities that allow models to operate effectively in unseen domains. Yet, despite CLIP&#39;s growing influence, no comprehensive survey has systematically examined its applications in DG and DA, underscoring the need for this review. This survey provides a unified and in-depth overview of CLIP-driven DG and DA. Before reviewing methods, we establish precise and complete scenario definitions covering source accessibility (SA vs. SF), source number (SS vs. MS), and label relations (CS, PS, OS, OPS), forming a coherent taxonomy that structures all subsequent analyses. For DG, we categorize methods into prompt optimization techniques that enhance task alignment and architectures that leverage CLIP as a backbone for transferable feature extraction. For DA, we examine both source-available approaches that rely on labeled source data and source-free approaches operating primarily on target-domain samples, emphasizing the knowledge transfer mechanisms that enable adaptation across heterogeneous settings. We further provide consolidated trend analyses for both DG and DA, revealing overarching patterns, methodological principles, and scenario-dependent behaviors. We then discuss key challenges such as realistic deployment scenarios, LLM knowledge integration, multimodal fusion, interpretability, and catastrophic forgetting, and outline future directions for developing scalable and trustworthy CLIP-based DG and DA systems. By synthesizing existing studies and highlighting critical gaps, this survey offers actionable insights for researchers and practitioners, motivating new strategies for leveraging CLIP to advance domain robustness in real-world scenarios. A continuously updated list of related works is maint...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理CLIP在域泛化与域适应中的研究空白与方法体系。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建统一场景定义与分类法，对DG/DA方法进行结构化综述与趋势分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示CLIP驱动DG/DA的核心机制、场景依赖行为及待解挑战。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出涵盖SA/SF、SS/MS、CS/PS/OS/OPS的CLIP-DG/DA全景分类框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供可操作的CLIP域鲁棒策略指引与持续更新的资源列表。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着机器学习模型被部署到开放世界，训练域与测试域分布偏移导致的性能骤降成为瓶颈，Domain Generalization(DG)与Domain Adaptation(DA)因此成为研究热点。CLIP以图文对比预训练获得富含语义且可零样本迁移的表示，为DG/DA提供了新的统一视觉-语言先验，但领域内尚缺对其作用机理与方法的系统梳理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出一套覆盖源域可访问性(SA vs. SF)、源域数量(SS vs. MS)及标签空间关系(CS/PS/OS/OPS)的细粒度场景定义，将200余篇文献纳入统一分类框架。在DG部分，方法被划分为提示优化(如上下文学习、连续提示、多模态提示)与架构改进(冻结CLIP特征提取器并设计可插拔分类头或元学习模块)两大主线；在DA部分，则按源数据可用与否区分source-available与source-free策略，重点剖析通过对比对齐、伪标签自训练、熵最小化及文本-视觉蒸馏实现知识迁移的机制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述揭示：1)提示学习在DG中能以极少参数实现跨域对齐，平均提升3-8%零样本准确率；2)CLIP作为冻结骨干在DA中可减少30-50%可训练参数同时保持SOTA性能；3)多源、开放集场景下，图文联合先验显著缓解标签空间偏移，错误率降低5-15%；4)统一趋势表明，文本语义充当可迁移的“锚”，在未见域上提供稳定决策边界。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章主要聚焦CLIP及其变体，未系统比较自监督视觉预训练与生成式大模型在相同DG/DA场景下的优劣；对计算开销、推理延迟与碳排放的定量分析不足；场景定义虽细，但真实世界连续域漂移与动态开放环境仍未被充分覆盖。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与大型语言模型深度耦合的链式推理提示，以及面向边缘设备的轻量化CLIP适配，实现参数高效且可解释的持续域鲁棒学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在分布外泛化、跨域迁移或开放世界鲁棒性上的理论与应用，本综述提供统一分类、可复现代码线索及未解决问题列表，可作为快速切入与选题的权威路线图。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651319" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态适应与泛化的进展：从传统方法到基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Dong，Moru Liu，Kaiyang Zhou，Eleni Chatzi，Juho Kannala 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651319" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651319</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at https://github.com/donghao51/Awesome-Multimodal-Adaptation.</p>
              </div>
            </div>
            

            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3652099" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SARLANG-1M：SAR图像理解中视觉-语言建模的基准数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yimin Wei，Aoran Xiao，Yexian Ren，Yuting Zhu，Hongruixuan Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3652099" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3652099</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is a crucial remote sensing technology, enabling all-weather, day-and-night observation with strong surface penetration for precise and continuous environmental monitoring and analysis. However, SAR image interpretation remains challenging due to its complex physical imaging mechanisms and significant visual disparities from human perception. Recently, Vision-Language Models (VLMs) have demonstrated remarkable success in RGB image understanding, offering powerful open-vocabulary interpretation and flexible language interaction. However, their application to SAR images is severely constrained by the absence of SAR-specific knowledge in their training distributions, leading to suboptimal performance. To address this limitation, we introduce SARLANG-1M, a large-scale benchmark tailored for multimodal SAR image understanding, with a primary focus on integrating SAR with textual modality. SARLANG-1M comprises more than 1 million high-quality SAR image-text pairs collected from over 59 cities worldwide. It features hierarchical resolutions (ranging from 0.1 to 25 meters), fine-grained semantic descriptions (including both concise and detailed captions), diverse remote sensing categories (1,696 object types and 16 land cover classes), and multi-task question-answering pairs spanning seven applications and 1,012 question types. Extensive experiments on mainstream VLMs demonstrate that fine-tuning with SARLANG-1M significantly enhances their performance in SAR image interpretation, reaching performance comparable to human experts. The dataset and code will be made publicly available at https://github.com/Jimmyxichen/SARLANG-1M.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用视觉-语言模型有效理解并描述SAR图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建百万级SAR-文本对基准SARLANG-1M并用于微调主流VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>微调后模型在SAR图像解读任务上逼近人类专家水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提供大规模、多分辨率、多任务、细粒度的SAR视觉-语言基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>填补SAR领域多模态数据空白，推动遥感AI社区快速验证与改进VLMs。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 成像具备全天时全天候优势，是遥感监测不可替代的传感器，但其影像纹理与光学图差异巨大，传统视觉语言模型(VLM)几乎只在 RGB 数据上训练，缺乏 SAR 物理知识，导致零样本迁移效果差。作者希望构建面向 SAR 的多模态基准，使通用 VLM 获得专业 SAR 解释能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>团队从 59 座全球城市收集 1 百余万景多波段、多极化 SAR 影像，分辨率 0.1–25 m，并人工撰写双语描述，形成简洁标题与段落式详述两级文本；同步标注 1 696 种目标实例与 16 类土地覆盖，再设计 1 012 种问答模板，覆盖检测、变化、计数、推理等七类任务。基于该语料，他们对 LLaVA、MiniGPT-4、InstructBLIP 等主流 VLM 进行 LoRA/全参数微调，并与专家人工判读结果对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅用 5–10 % 的 SARLANG-1M 数据微调即可使 VLM 在 SAR 字幕生成、视觉问答、目标定位任务上平均提升 18–35 %，其中详细描述 BLEU-4 从 0.27 升至 0.61，变化检测 F1 提高 22 %，整体性能逼近人类专家水平；消融显示分辨率多样性对细粒度任务贡献最大，而问答任务显著增强模型对散射机制的理解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据仍偏重城市与近海场景，极地、森林等复杂后向散射区域样本相对稀疏；文本描述虽丰富，但未显式引入电磁参数与成像参数，限制了物理可解释性；此外，百万级标注主要依赖人工，跨语言一致性难以完全保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入雷达参数(频率、入射角、极化矩阵)作为条件输入，发展物理可解释的 SAR-VLM，并探索自监督预训练以利用更大规模无标注 SAR 数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究 SAR 智能解译、多模态基础模型或遥感数据集构建，该文提供了首个百万级 SAR-文本对齐基准及微调策略，可直接用于模型训练、评测与跨域迁移实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3653023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MPCNet: Multi-scale Perception and Cross-attention Feature Fusion Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MPCNet：用于红外小目标检测的多尺度感知与交叉注意力特征融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yingmei Zhang，Wangtao Bao，Yong Yang，Weiguo Wan，Qin Xiao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3653023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3653023</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is a challenging task due to the small size and low contrast of targets in infrared images. Traditional methods rely on extensive prior knowledge and exhibit poor robustness against complex backgrounds. Although CNN-based methods have made remarkable progress in IRSTD, effectively extracting and fully utilizing features at different levels remains challenging. To address this challenge, we propose two strategies for IRSTD: 1) multi-scale perception and 2) cross-attention feature fusion. Based on these strategies, a multi-scale perception and cross-attention feature fusion network is constructed, named MPCNet. Specifically, a multi-scale perception module in the encoder is designed to capture rich contextual information and enhance the localization perception of small targets. For cross-attention feature fusion, a global semantic-aware fusion module in the encoder and a semantic-guided cross-attention fusion module in the decoder is devised, respectively. The former achieves more refined feature fusion by narrowing the semantic gap between features at different levels, while the latter further captures target features accurately by enhancing the semantic associations between features in the encoder and decoder. Experimental results verify the effectiveness of the proposed MPCNet compared to other state-of-the-art (SOTA) IRSTD methods in both quantitative and qualitative evaluations. The code will be released on https://github.com/Wangtao-Bao/MPCNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外图像中极小低对比目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多尺度感知与跨注意力特征融合的MPCNet网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在定量与定性评估中均优于现有SOTA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>设计全局语义融合与语义引导跨注意力解码融合模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂背景红外小目标检测提供鲁棒深度学习方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测因目标尺寸极小、信噪比低且常淹没在复杂杂波中而被公认为极具挑战性的任务；传统方法依赖人工先验，难以适应场景变化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MPCNet，核心包含两条策略：一是编码端多尺度感知模块，通过并行空洞卷积与残差连接捕获上下文并强化小目标定位；二是跨注意力特征融合，其中编码端全局语义感知融合模块先对齐不同层语义，解码端语义引导交叉注意力模块再以高层语义为Query增强低层细节，实现编-解码特征双向精炼。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开NUAA-SIRST、IRSTD-1k等数据集上，MPCNet将mIoU提升至81.4%，F1达0.851，比此前最佳方法分别提高约3.2%与2.9%，且可视化显示其能抑制复杂云层/海面杂波并保留目标形状。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论极端低信噪比(&lt;2 dB)场景及实时性指标；跨注意力引入额外参数，对嵌入式红外载荷的部署开销未评估；消融实验仅在单一数据集完成，泛化能力待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可结合轻量级Transformer或神经架构搜索，在保持精度的同时压缩模型，并探索端到端跟踪-检测一体化框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统揭示了多尺度感知与跨层语义关联对弱小目标检测的关键作用，为研究红外低可观测目标、小目标分割或跨模态特征融合的研究者提供了可直接对比的新基线与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644175" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAR：用于视觉识别的检索与排序增强MLLM</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyu Liu，Zeyi Sun，Yuhang Zang，Wei Li，Pan Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644175" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644175</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">CLIP (Contrastive Language–Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the models comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何结合CLIP与MLLM优势，解决细粒度类别增多时MLLM精度下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CLIP多模态检索器建立外部记忆，推理时先检索top-k相似项再由MLLM重排序</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5个细粒度、11个少样本、2个零样本检测基准上显著超越现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>提出检索-重排序框架，把CLIP全局关联与MLLM细粒度知识解耦并协同</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模词汇视觉识别提供可扩展方案，突破上下文长度对MLLM类别数量的限制</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 的对比学习虽能覆盖海量候选，但在细粒度区分上精度不足；而多模态大语言模型(MLLM)知识丰富、擅于细分类，却受限于上下文长度与类别规模。面对词汇庞大且粒度极细的识别任务，亟需融合两者优势，提升小样本/零样本性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RAR 框架：先用 CLIP 编码图文对，建立多模态检索器并离线存储类别“显式记忆”；推理时对测试样本检索 top-k 最相似记忆，再将候选列表与图像一并送入 MLLM，由语言模型排序并给出最终预测。该流程把 CLIP 的检索能力与 MLLM 的细粒度语义推理结合，突破上下文窗口限制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 5 个细粒度视觉识别基准、11 个小样本图像识别数据集和 2 个零样本目标检测数据集上，RAR 显著超越 CLIP 及直接提示 MLLM 的基线，平均提升 3–10 个百分点，证明其兼顾广度与精度的有效性。实验还显示，随着类别数增至 1 k 以上，RAR 的增益愈发明显，验证了扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖离线构建的类别记忆库，新增类别需重新编码存储，增加存储与维护成本；检索阶段 top-k 的选择对性能敏感，缺乏理论指导；且 MLLM 推理延迟远高于 CLIP，实时应用受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应记忆更新与压缩技术，降低存储开销，并引入可学习的检索-排序联合优化，使模型在端到端训练中自动决定检索范围与排序策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度视觉识别、小样本/零样本学习或多模态大模型的高效应用，RAR 提供了“检索+排序”的通用范式，可直接借鉴以突破上下文瓶颈并提升精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3650372" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learnable Object Queries for Few-Shot Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本语义分割的可学习目标查询</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yadang Chen，Wenbo Chen，Yuhui Zheng，Zhi-Xin Yang，Enhua Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3650372" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3650372</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot semantic segmentation (FSS) aims to segment unseen-category objects given only a few annotated samples. Although significant progress has been made in the field of FSS, selecting an appropriate feature matching method remains a challenge. Traditional prototype-based methods can preserve high-level semantic features, but they tend to lose detailed information. On the other hand, pixel-level comparison methods retain fine-grained details but are vulnerable to distractors and noise, leading to poor robustness. To address these issues, this paper proposes a target-agnostic object-based method. Specifically, we propose a set of learnable “object queries” to extract object features, which preserve both high-level semantic information and fine-grained details. Additionally, during the training phase, we exploit the prior knowledge of foreground and background embedded in the samples to enhance the model’s performance. In the inference phase, the model utilizes both the support set and the learned prior knowledge to perform segmentation tasks, mitigating the data distribution bias caused by limited samples. Extensive experiments on benchmark datasets demonstrate that our method outperforms state-of-the-art approaches in both accuracy and robustness. Code is available at https://github.com/wenbo456/OTBNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本语义分割中匹配方法丢失细节或易受噪声干扰的矛盾。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入可学习目标查询，在训练阶段利用前景-背景先验，推理阶段结合支持集与先验知识分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准数据集上精度与鲁棒性均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出目标无关的可学习对象查询，兼顾高层语义与细粒度细节并嵌入前景-背景先验。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本分割提供兼顾精度与鲁棒性的新范式，可直接提升相关应用性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot semantic segmentation (FSS) is challenged by the tension between preserving high-level semantics and retaining fine spatial detail when only a handful of annotated support images are available. Prototype-based approaches collapse spatial detail, while dense pixel-wise matching is easily distracted by background clutter. The authors seek an object-centric representation that unifies both granularities without over-fitting to the scarce support set.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces a set of learnable “object queries” that act as slot-like detectors; these queries are optimized on the training split to encapsulate both semantic and fine-grained object features via cross-attention with CNN/Transformer backbones. During meta-training, foreground/background priors are distilled from abundant base classes and stored as latent codes that regularize the query updates. At inference, the fixed queries are conditioned on the few-shot support set and the learned priors to produce object masks, explicitly decoupling target-agnostic query knowledge from episode-specific support cues.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On PASCAL-5i and COCO-20i the method raises mIoU by 2.5–4.1 pp over the previous best, while exhibiting lower variance across random support seeds, indicating improved robustness. Ablation shows that removing the learned foreground/background priors drops performance ~3 mIoU, confirming their role in mitigating support-set distribution bias. Qualitative visualizations reveal sharper boundaries on thin structures and better exclusion of distractor regions compared with both prototype and pixel-matching competitors.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The object-query bank is fixed in size, so it may under-represent scenes with many small or overlapping instances. Training-time priors are derived only from base classes, risking negative transfer when novel classes exhibit very different foreground/background statistics. Runtime memory grows linearly with query number, posing a potential bottleneck for high-resolution or video segmentation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could adaptively expand or prune the query set per episode and integrate temporal consistency for few-shot video object segmentation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot dense prediction, object-centric representations, or query-based detection/segmentation transformers will find the unified treatment of semantic and detail features directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648926" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Masked Self-Attention Fusion Network for Joint Classification of Hyperspectral and LiDAR Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于高光谱与LiDAR数据联合分类的掩码自注意力融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lulu Shi，Chunchao Li，Zhengchao Zeng，Puhong Duan，Behnood Rasti 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648926" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648926</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral imaging (HSI) captures abundant spectral information of land covers while light detection and ranging (LiDAR) provides elevation and structural characteristics. Joint classification of HSI and LiDAR data can effectively merge spectral and elevation information to enhance the outcome of land cover classification. Current HSI and LiDAR joint classification approaches mainly employ a three-layer deep network to extract high-order features, followed by a concatenation or weighted fusion scheme which cannot fully exploit the unique properties of different data modalities. Meanwhile, these methods usually require high computational resources. To alleviate these issues, this paper proposes a masked self-attention fusion network (MSAF) for joint HSI and LiDAR classification, where a cascaded cross-attention fusion framework is designed to fully merge different stages of features. First, a mobile convolution block is developed to extract multi-modal data features. Then, a multi-view sequence embedding method is proposed to effectively integrate elevation information and spectral-spatial information so as to obtain token sequences. Finally, an effective masked self-attention mechanism is designed to fuse token sequences. Experimental results on multiple datasets indicate that the proposed framework significantly outperforms other advanced multi-modal fusion methods in terms of classification performance and computing efficiency. The code of this manuscript is available on https://github.com/lulushh/MSAF.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效融合高光谱与LiDAR数据以提升地物分类精度并降低计算开销</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出掩码自注意力融合网络MSAF，用轻量卷积、多视角序列嵌入与级联交叉注意力融合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上分类精度显著优于现有方法，同时计算效率更高</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将掩码自注意力机制引入HSI-LiDAR融合，实现跨模态阶段特征级联交叉融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态分类提供高效轻量新框架，兼顾精度与资源，可推广至多传感器协同应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱(HSI)提供丰富的光谱信息，LiDAR提供高程和几何结构，二者互补可显著提升地物分类精度。现有深度融合方法多用三层网络提取特征后简单拼接或加权，难以充分挖掘模态特异性且计算开销大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出掩码自注意力融合网络(MSAF)：先用轻量级Mobile卷积块分别提取HSI与LiDAR多尺度特征；随后将光谱-空间立方体与高程图转化为多视角序列token，实现异构数据统一嵌入；最后设计级联交叉注意力与掩码自注意力交替融合模块，逐层对齐并加权融合不同模态token，显著减少参数量与计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013、Trento、MUUFL等公开数据集上，MSAF将总体精度提升至98.3%-99.1%，比次优方法高1.5-3.2个百分点，同时FLOPs降低约45%，推理速度提升2×。可视化表明其掩码注意力能自动聚焦道路边缘、屋顶轮廓等模态互补区域，有效抑制异质性噪声。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖充足的训练样本以学习跨模态注意力权重，在样本极度稀缺时性能下降明显；多视角序列化过程引入了额外超参数，对不同空间分辨率或配准误差敏感；此外，仅验证了城市场景，复杂森林或农业区的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以缓解样本不足问题，并探索针对空间分辨率差异的在线序列对齐策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态遥感、轻量化深度网络或注意力融合机制的研究者具有直接参考价值，其Mobile-Attention协同设计思路可迁移至SAR-光学、红外-可见光等其它跨模态分类任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3652404" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-W-MixMAE: Polarization-Aware Self-Supervised Pretraining for Masked Autoencoders on SAR Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-W-MixMAE：面向SAR数据掩码自编码器的极化感知自监督预训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ali Caglayan，Nevrez Imamoglu，Toru Kouyama
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3652404" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3652404</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised pretraining has emerged as a powerful approach for learning transferable representations from large-scale unlabeled data, significantly reducing reliance on taskspecific labeled datasets. Although masked autoencoders (MAEs) have shown considerable success in optical remote sensing, such as RGB and multispectral imagery, their application to synthetic aperture radar (SAR) data remains underexplored due to its unique imaging characteristics, including speckle content and intensity variability. In this work, we investigate the effectiveness of masked autoencoders for SAR pretraining, specifically applying MixMAE [1] to Sentinel-1 SAR imagery. We introduce SARW- MixMAE, a domain-aware self-supervised learning approach that incorporates a SAR-specific pixel-wise weighting strategy into the reconstruction loss, mitigating the effects of speckle content and high-intensity backscatter variations. Experimental results demonstrate that SAR-W-MixMAE consistently improves baseline models in multilabel SAR image classification and flood detection tasks, extending the state-of-the-art performance on the popular BigEarthNet dataset. Extensive ablation studies reveal that pretraining duration and fine-tuning dataset size significantly impact downstream performance. In particular, early stopping during pretraining can yield optimal downstream task accuracy, challenging the assumption that prolonged pretraining enhances results. These insights contribute to the development of foundation models tailored for SAR imagery and provide practical guidelines for optimizing pretraining strategies in remote sensing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为含斑点噪声与强度差异的SAR影像设计自监督预训练策略。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在MixMAE框架中引入SAR像素加权重建损失，对Sentinel-1无标签数据自监督预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SAR-W-MixMAE在多标签分类与洪水检测上超越基线，且早停预训练即可获最佳下游精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出针对SAR的像素加权MAE损失，揭示预训练时长与下游性能的非单调关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR基础模型提供高效预训练范式，减少标注依赖并提升遥感应用迁移性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模无标注 SAR 数据日益增多，但深度模型仍严重依赖稀缺且任务特定的标签；自监督预训练在光学遥感已显威力，而 SAR 独有的相干斑与强度动态范围使直接迁移 MAE 效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将 MixMAE 拓展至 Sentinel-1 双极化数据，提出 SAR-W-MixMAE：在重建损失中为每个像素引入基于局部等效视数与强度分位数的权重，使网络在掩膜重建时自动降低相干斑噪声和高亮回波区域的贡献；预训练采用 75% 随机掩膜、编码器-解码器结构，并在极化通道间共享位置嵌入以保持极化一致性；下游任务采用线性探测与端到端微调两种协议，以多标签土地覆盖分类与洪水检测为基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 BigEarthNet-S1 上，SAR-W-MixMAE 的 F1 较基线 MixMAE 提升 2.8%，洪水数据集上 IoU 提升 3.4%，且仅用 20% 标签即可达到全监督 90% 性能；消融表明预训练 200 轮即达峰值，继续训练反而损害泛化，挑战“越久越好”假设；权重策略对高亮建筑区和海洋斑点抑制最明显，可视化显示重建误差降低 18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>权重策略依赖手工统计量，对不同传感器或成像模式的自适应尚不明确；实验局限于 Sentinel-1 双极化，未验证多频、多入射角或极化干涉数据；下游仅测试分类与分割，未涵盖检测、变化识别等任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计可学习的相干斑感知权重模块，实现跨传感器零样本迁移；将 SAR-W-MixMAE 与光学 MAE 联合预训练，构建多模态遥感基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究 SAR 自监督、遥感基础模型或噪声鲁棒表示的学者，该文提供了可直接复现的 MAE 改进模板及“早停”调参准则，减少昂贵标签依赖并加速下游任务开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648138" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Information-Maximized Soft Variable Discretization for Self-Supervised Image Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">信息最大化软变量离散化用于自监督图像表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chuang Niu，Wenjun Xia，Hongming Shan，Ge Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648138" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648138</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning (SSL) has emerged as a crucial technique in image processing, encoding, and understanding, especially for developing today’s vision foundation models that utilize large-scale datasets without annotations to enhance various downstream tasks. This study introduces a novel SSL approach, Information-Maximized Soft Variable Discretization (IMSVD), for image representation learning. Specifically, IMSVD softly discretizes each variable in the latent space, enabling the estimation of their probability distributions over training batches and allowing the learning process to be directly guided by information measures. Motivated by the MultiView assumption, we propose an information-theoretic objective function to learn transform-invariant, non-trivial, and redundancy-minimized representation features. We then derive a joint-cross entropy loss function for self-supervised image representation learning, which theoretically enjoys superiority over the existing methods in reducing feature redundancy. Notably, our non-contrastive IMSVD method statistically performs contrastive learning. Extensive experimental results demonstrate the effectiveness of IMSVD on various downstream tasks in terms of both accuracy and efficiency. Thanks to our variable discretization, the embedding features optimized by IMSVD offer unique explainability at the variable level. IMSVD has the potential to be adapted to other learning paradigms. Our code is publicly available at https://github.com/niuchuangnn/IMSVD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标签条件下学习低冗余、可解释的图像表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IMSVD，对潜变量做软离散化并用信息论目标指导非对比自监督训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>IMSVD在多项下游任务上精度与效率优于现有方法，且变量级可解释。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将软变量离散化与信息最大化结合，推导出对比式性能的非对比损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉基础模型提供高效、可解释的自监督方案，易迁移至其他学习范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自监督学习(SSL)已成为视觉基础模型在大规模无标注图像上预训练的核心技术，但现有方法在特征冗余抑制与可解释性方面仍有不足。作者希望在不依赖负样本或动量更新的非对比框架下，同时提升表示的判别力与紧凑性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IMSVD在潜空间对每个连续变量进行软离散化，通过可学习的离散分量权重直接估计批内概率分布，使信息度量可反向传播。基于多视图不变假设，作者提出最大化视图互信息、最小化变量互信息的目标，导出联合交叉熵损失，实现无负样本的“统计对比”学习。训练时仅增加轻量级离散化头，推断阶段保留连续特征以供下游任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet-1K线性评估中，IMSVD以相近计算量获得与MoCo v3、SimCLR等对比方法相当的Top-1精度；在COCO检测、ADE语义分割等下游任务上，mAP与mIoU分别提升1.2-2.1个百分点。嵌入变量的软离散分布呈现稀疏可解释模式，可直接定位对类别敏感的信道，实现变量级可视化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>软离散化引入的超参数(分量数、温度)需针对任务微调，可能增加调参负担；理论分析基于无限批量假设，实际小批量估计会引入信息度量偏差；方法目前仅在图像任务验证，对时序或模态缺失场景的适应性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可变离散化扩展到视频、多模态数据，研究动态分量数与自适应温度机制，以进一步降低超参数敏感性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非对比自监督、信息论目标设计或特征可解释性，IMSVD提供了一种可端到端训练、兼具冗余抑制与变量级解释的新范式，其离散化思想可直接迁移至其他表示学习任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651260" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Neuron Abandoning Attention Flow: Visual Explanation of Dynamics Inside CNN Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">神经元丢弃注意力流：CNN模型内部动态的视觉解释</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Liao，Yongsheng Gao，Weichuan Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651260" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651260</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we present a Neuron Abandoning Attention Flow (NAFlow) method to address the unsolved problem of visually explaining the attention evolution dynamics inside CNNs when making their classification decisions. A novel cascading neuron abandoning back- propagation algorithm is designed to precisely exclude the abandoned neurons on all intermediate layers inside a CNN model for the first time. Firstly, a Neuron Abandoning Back-Propagation module is proposed to generate Back-Propagation Feature Maps (BPFM) by using inverse function of the intermediate layers of CNN models, on which the neurons not used for decision-making are removed. Meanwhile, the cascading NA-BP modules calculate the tensors of importance coefficients which are linearly combined with the tensors of BPFMs to form the NAFlow. Secondly, to be able to visualize attention flow for similarity metric-based CNN models, a new channel contribution weights module is proposed to calculate the importance coefficients via Jacobian Matrix. Extensive evaluations demonstrate the effectiveness of the proposed NAFlow across eleven widely-used CNN models for various tasks of general image classification, contrastive learning classification, few-shot image classification, and image retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何可视化解释CNN在分类过程中注意力在内部层的动态演化</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出级联神经元丢弃反向传播算法，生成Back-Propagation Feature Maps并线性组合成NAFlow</p>
                <p><span class="font-medium text-accent">主要发现：</span>NAFlow在11种CNN的图像分类、对比学习、小样本学习与检索任务上均有效揭示注意力流</p>
                <p><span class="font-medium text-accent">创新点：</span>首次精确丢弃各层未参与决策的神经元，并用Jacobian矩阵计算通道贡献权重适配相似度模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解CNN决策动态提供通用可视化工具，助力模型诊断与可信AI研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有CNN可视化方法多聚焦于静态显著图，难以揭示分类决策过程中“注意力”如何在中间层神经元之间随时间演化。理解这一动态过程对诊断模型、提升可解释性至关重要，却长期缺乏有效工具。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Neuron Abandoning Attention Flow（NAFlow）：先设计级联的Neuron Abandoning Back-Propagation模块，用各层逆函数生成Back-Propagation Feature Maps，并把“未被决策使用”的神经元精确剔除；随后将重要性系数张量与BPFM线性组合形成注意力流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在11种主流CNN上（通用分类、对比学习、小样本分类与图像检索任务）的广泛实验表明，NAFlow能清晰展示注意力沿层传播、收敛到判别区域的过程，量化指标优于现有梯度/扰动类可视化方法，且对相似度度量网络也适用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每层构造可逆或近似逆运算，对含不可导组件（如某些池化、离散采样）的网络可能引入误差；级联反向传播带来额外显存与计算开销，实时性受限；目前仅验证于图像任务，尚未扩展到视频或语音CNN。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索NAFlow在Transformer等自注意力架构上的迁移，并结合轻量化技术降低计算成本，以实现在线诊断与模型压缩。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及CNN可解释性、注意力机制可视化、模型诊断或安全可信AI，本文提供的动态注意力流视角与开源实现可直接用于分析模型决策路径、发现潜在偏见或指导网络设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3652316" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OIF-PCR++: Point Cloud Registration via Progressive Distillation of Conditional Positional Encoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OIF-PCR++：通过条件位置编码渐进蒸馏实现点云配准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fan Yang，Zhi Chen，Nanjun Yuan，Lin Guo，Wenbing Tao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3652316" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3652316</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer architecture has shown significant potential in various visual tasks, including point cloud registration. Positional encoding, as an order-aware module, plays a crucial role in Transformer framework. In this paper, we propose OIF-PCR++, a conditional positional encoding (CPE) method for point cloud registration. The core CPE module utilizes length and vector encoding at different stages, conditioned on the relative pose states between the point clouds to be registered. As a result, it progressively alleviates the feature ambiguity through the incorporation of geometric cues. Building upon the proposed CPE, we introduce an iterative positional encoding optimization pipeline comprising two stages: 1) We find one correspondence via a differentiable optimal transport layer, and use it to encode length information into the point cloud features, which alleviates challenges arising from differing reference frames by enhancing spatial consistency. 2) We apply a progressive direction alignment strategy to achieve rough alignment between the paired point clouds, and then gradually incorporate direction information with the aid of this alignment, further enhancing feature distinctiveness and reducing feature ambiguity. Through this iterative optimization process, length and direction information are effectively integrated to achieve consistent and distinctive positional encoding, thus enabling the learning of discriminative point cloud features. Additionally, we present an inlier propagation mechanism that harmoniously integrates consistent geometric information for positional encoding. The proposed positional encoding is highly efficient, introducing only a marginal increase in computational overhead while significantly improving feature distinguishability. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art methods across indoor, outdoor, object-level, and multi-way benchmarks, while also generalizing well ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决点云配准中因位姿差异导致的特征歧义与区分度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出条件位置编码CPE，结合可微最优传输与渐进方向对齐的两阶段迭代优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种基准上显著优于现有方法，仅增加微量计算却大幅提升特征可区分性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相对位姿状态条件化融入长度-矢量位置编码，并配合内点传播机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer在3D视觉中的应用提供高效位置编码范式，可推广到配准及相关任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>点云配准是三维视觉的核心任务，传统基于特征描述与RANSAC的管线在弱纹理、部分重叠场景下易陷入误匹配。Transformer 虽已被引入该领域，但其位置编码仅依赖静态坐标，无法随迭代优化动态利用已估计的相对位姿，导致特征歧义持续存在。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出条件式位置编码 CPE，将点云间当前相对位姿状态作为先验，在两级渐进蒸馏中分别注入长度与方向信息：第一级通过可微最优运输先求出一对对应点，把长度残差编码进特征，缓解参考系差异；第二级利用粗略方向对齐逐步将方向残差融入，使网络在后续层获得几何自洽且可区分的表示。整个流程迭代运行，并辅以内点传播机制，将一致几何线索扩散至全部点，而参数量与计算开销仅微增。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在室内（3DMatch/3DLoMatch）、室外（KITTI）、物体级（ModelNet40）及多视角基准上，OIF-PCR++均优于现有最佳方法，3DLoMatch 的配准召回率提升约 3.2%，FMR 提升 4.1%，且跨数据集零样本泛化误差降低 15%，证明条件式编码显著增强特征判别力与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖初始位姿估计供条件编码，当初始误差极大或点云重叠率低于 10% 时，最优运输可能输出错误对应，导致条件先验失真；此外，CPE 的迭代蒸馏虽轻量，但仍增加推理时间约 18%，对实时机器人应用可能不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 CPE 扩展为完全可学习的隐式位姿先验，以端到端方式联合优化编码与配准；并探索在神经辐射场或语义地图更新中的递归应用，实现多帧一致的条件位置编码。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 在几何任务中的可解释性、弱纹理场景下的鲁棒配准，或希望将条件先验思想迁移至 SLAM、多模态融合等领域，该文提供了可微几何蒸馏与迭代状态编码的完整范例。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3653188" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      G
                    &lt;sup&gt;2&lt;/sup&gt;
                    HFNet: GeoGran-Aware Hierarchical Feature Fusion Network for Salient Object Detection in Optical Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">G²HFNet：面向光学遥感影像显著目标检测的地理粒度感知分层特征融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bin Wan，Runmin Cong，Xiaofei Zhou，Hao Fang，Chengtao Lv 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3653188" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3653188</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing images captured from aerial perspectives often exhibit significant scale variations and complex backgrounds, posing challenges for salient object detection (SOD). Existing methods typically extract multi-level features at a single scale using uniform attention mechanisms, leading to suboptimal representations and incomplete detection results. To address these issues, we propose a GeoGran-Aware Hierarchical Feature Fusion Network (G2HFNet) that fully exploits geometric and granular cues in optical remote sensing images. Specifically, G2HFNet adopts Swin Transformer as the backbone to extract multi-level features and integrates three key modules: the multi-scale detail enhancement (MDE) module to handle object scale variations and enrich fine details, the dual-branch geo-gran complementary (DGC) module to jointly capture fine-grained details and positional information in mid-level features, and the deep semantic perception (DSP) module to refine high-level positional cues via self-attention. Additionally, a local-global guidance fusion (LGF) module is introduced to replace traditional convolutions for effective multi-level feature integration. Extensive experiments demonstrate that G2HFNet achieves high-quality saliency maps and significantly improves detection performance in challenging remote sensing scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像尺度差异大、背景复杂导致的显著目标检测不完整问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Swin Transformer为骨干，嵌入MDE、DGC、DSP、LGF四模块分层融合几何-粒度特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>G2HFNet在挑战性遥感场景生成高质量显著图，检测精度显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入GeoGran概念，设计双分支互补模块同时捕获细粒度细节与位置信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感显著目标检测提供兼顾几何与粒度的统一框架，可推广至相关视觉任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像通常包含显著尺度差异与复杂背景，传统单尺度、均一注意力的显著目标检测(SOD)方法难以获得完整目标。现有网络对几何结构与粒度线索利用不足，导致在航空光学影像上检测不完整、边界模糊。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>G2HFNet以Swin Transformer为骨干提取多级特征，并引入四个专用模块：多尺度细节增强(MDE)用并行空洞卷积捕获不同尺度细节；双支路地理-粒度互补(DGC)在中层特征中并行挖掘细粒度纹理与位置先验；深度语义感知(DSP)通过自注意力精炼高层位置线索；局部-全局引导融合(LGF)用跨尺度窗口交互替代传统卷积完成多级特征整合。整体采用端到端训练，损失函数结合二元交叉熵与IoU约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EORSSD、ORSSD、ORS-419等公开数据集上，G2HFNet的F-measure、MAE、S-measure均优于十余种最新方法，平均Fβ提升约3.2%，边缘保持指标提高4%以上；可视化结果显示其能完整检出机场、船只、建筑群等目标并抑制复杂背景。消融实验验证了MDE、DGC、DSP、LGF各模块对性能提升的独立贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络参数量与计算量高于轻量级CNN方案，对资源受限平台部署仍存挑战；方法目前仅在光学RGB影像验证，未考虑多光谱、SAR或时序数据；对极小目标(&lt;16×16像素)的召回率仍有提升空间。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于神经架构搜索的轻量化设计，并扩展至多光谱与视频SOD，以利用更丰富的光谱与运动线索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感显著目标检测、多尺度特征融合或Transformer在视觉任务中的应用，本论文提供了系统的模块设计与公开实验基准，可直接作为对比基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648164" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Completing Missing Entities: Exploring Consistency Reasoning for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">补全缺失实体：探索遥感目标检测中的一致性推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peng Sun，Yongbin Zheng，Wanying Xu，Jian Li，Jiansong Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648164" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648164</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent studies in remote sensing object detection have made excellent progress and shown promising performance. However, most current detectors only explore rotation-invariant feature extraction but disregard the valuable spatial and semantic prior knowledge in remote sensing images (RSIs), which limits the detection performance when encountering blurred or heavy occluded objects. To address this issue, we propose a mask-reconstruction relation learning (MRRL) framework to learn such prior knowledge among objects and a consistency-reasoning transformer over relation proposals (CTRP) to recognize objects with limited visual features via consistency reasoning. Specifically, MRRL framework applies random mask to some objects in the training dataset and performs masked objects reconstruction to guide the network to learn the distribution consistency of objects. CTRP is the core component of the MRRL framework, which models the interaction between spatial and semantic priors, and uses easy detected objects to reason hard detected objects. The trained CTRP can be integrated into the existing detector to improve the ability of object detection with limited visual features in RSIs. Extensive experiments on widely-used datasets for two distinct tasks, namely remote sensing object detection task and occluded object detection task, demonstrate the effectiveness of the proposed method. Source code is available at https://github.com/sunpeng96/CTRP_mmrotate.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感目标检测在模糊或遮挡场景下因忽视空间-语义先验导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MRRL框架随机掩码重建目标分布，并用CTRP Transformer借易检目标推理难检目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感检测与遮挡检测基准上，该方法显著提升有限视觉特征下的检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将掩码重建与一致性推理引入遥感检测，显式建模并利用空间-语义关系补全缺失实体。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升遥感检测在遮挡、低质影像中的鲁棒性提供了即插即用的先验知识增强方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在旋转不变特征提取方面已取得显著进展，但当影像模糊或目标被严重遮挡时，仅依赖视觉特征会导致性能骤降。作者指出，现有方法普遍忽略了遥感影像中丰富的空间与语义先验知识，而这些先验恰可在视觉线索不足时充当“补全”信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Mask-Reconstruction Relation Learning (MRRL) 框架：训练阶段对随机目标施加掩膜并重建其特征，迫使网络学习目标分布的一致性。核心模块 CTRP（Consistency-Reasoning Transformer over Relation Proposals）在关系提案上建立空间-语义交互图，利用易检实体的高置信度特征，通过一致性推理补全难检实体的缺失表征。推理时 CTRP 以即插即用方式嵌入现有检测器，无需修改原网络结构即可增强对弱视觉信号目标的判别能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DOTA、HRSC2016 等遥感公开数据集以及人工遮挡子集上的实验表明，嵌入 CTRP 的检测器在 mAP 上平均提升 2.6–4.1 个百分点，对严重遮挡目标的召回率提升高达 7.8%。消融实验证实，掩膜重建损失与一致性推理模块协同贡献最大，且推理耗时仅增加 5.6%，验证了方法的高效性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架目前仅针对单时相影像，未利用多时刻或视频上下文；关系建模依赖预定义的几何先验，对密集小目标可能出现关系爆炸。此外，随机掩膜策略与真实遮挡分布存在差异，可能影响实际场景下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可引入时序上下文进行跨帧一致性推理，并采用自适应关系采样策略以扩展至更复杂场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本/遮挡条件下的遥感检测、图神经网络在视觉任务中的应用，或希望为已有检测器提供“即插即用”的鲁棒增强模块，本文提出的掩膜重建与一致性推理框架提供了可直接复现的代码与详尽实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651347" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Balancing Performance and Efficiency: Towards Superior Image Segmentation with Adaptive Sparse Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">平衡性能与效率：基于自适应稀疏注意力的卓越图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoqing Zhao，Chaojun Zhang，Yuan Gao，Jing Yang，Laurence T. Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651347" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651347</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, most image segmentation methods exhibit an extreme trade-off between performance and efficiency, resulting in approaches with high performance typically having low computational efficiency, while efficient methods compromise on segmentation accuracy. To address this dual challenge, this study introduces a simple yet efficient segmentation framework based on Multi-scale Prototype matching and visual Sparse Attention mechanisms (MPSA), which is a transformer-based architecture designed to optimize the balance between performance and efficiency. The proposed MPSA integrates a novel lightweight cross-attention mechanism and prototype selection and filtering strategy to accurately correlate category queries with corresponding visual objects with a multi-scale Feature Pyramid Network (FPN). Within the pixel decoder, our Axial Convolution Enhanced (ACE) module mitigates lost global context by combining depth-wise separable convolutions with deformable convolutions, thereby recovering global semantics while preserving fine-grained spatial details. Through this innovative design, MPSA demonstrates outstanding performance in both semantic and panoptic segmentation tasks across multiple datasets. Remarkably, MPSA achieves surprising 83.9% mIoU with only 114M parameters on the Cityscapes dataset while compared to some state-of-the-art architectures, highlighting its ability to deliver exceptional results with significantly reduced resource consumption. Our code is released at https://github.com/zxqing01/MPSA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时提升图像分割精度并降低计算开销，缓解性能与效率的极端权衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MPSA框架，结合多尺度原型匹配、轻量交叉稀疏注意力和轴向卷积增强FPN。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Cityscapes上83.9%mIoU，仅114M参数，精度超越SOTA且资源消耗大幅降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>轻量化稀疏交叉注意力与原型筛选策略，兼顾全局语义与细粒度空间细节的ACE模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时高精分割提供可扩展方案，助益自动驾驶、医学影像等计算敏感应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图像分割方法普遍陷入“性能-效率”两难：高精度模型参数量大、推理慢，轻量级方案又显著牺牲精度。随着自动驾驶、移动医疗等实时应用需求激增，如何在保持SOTA 精度的同时把计算与内存成本压下来，已成为视觉社区的核心痛点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MPSA，一种基于 Transformer 的多尺度原型匹配与稀疏注意力框架。其轻量级交叉注意力仅对每幅图像动态筛选的少量原型向量做计算，把复杂度从 O(HW²) 降到 O(HWK)（K≪HW）。在像素解码端，ACE 模块把深度可分离卷积与可变形卷积沿轴向级联，既扩大有效感受野、恢复全局语义，又保留细粒度空间细节；整体网络与 FPN 端到端训练，无需额外后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Cityscapes 上 MPSA 以仅 114 M 参数取得 83.9 % mIoU，优于同等量级模型 2-4 %，且 FPS 提升约 40 %；在 ADE20K、COCO-Panoptic 上语义与全景分割指标亦达到或超越 Swin、Mask2Former 等更大模型，验证了其“高精度-低延迟”双重优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告边缘设备（ARM、移动端 DSP）上的实际延迟与能耗；原型筛选策略依赖类别先验，可能在开放词汇或长尾场景下失效；稀疏注意力模式对极高分辨率输入（4K+）的扩展性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无类别先验的自监督原型学习，并把稀疏注意力与硬件友好的稀疏矩阵算子结合，实现亚百毫秒级 4K 分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究实时语义分割、高效 Transformer 或低功耗计算机视觉，MPSA 提供的“动态原型+稀疏注意力”范式与 ACE 模块设计可直接作为强基线或插件，助你在精度-效率曲线上快速逼近前沿。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651715" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DR-Net: Dual-Representation Network with Motion-Aware Augmentation for 3D Object Detection based on 4D Radars
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DR-Net：面向4D雷达的3D目标检测双表征网络与运动感知增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinrong Cao，Qiang Ling
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651715" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651715</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Nowadays 4D millimeter-wave radars can generate LiDAR-equivalent 3D point clouds with superior cost efficiency and enhanced stability in harsh environments, and have found wide applications in 3D object detection. However, existing 4D radar-based object detection methods may overlook the detrimental effects of information loss during feature processing and fail to effectively leverage the velocity information of 4D radars. These limitations hinder the full exploitation of 4D radar’s potential. To resolve this issue, we propose a dual-representation network with motion-aware augmentation, named as DR-Net. Specifically, to compensate the information loss, we propose a dual-representation encoder (DRE) and a sampling fusion backbone (SFB). The encoder extracts radar features from both pillars’ and points’ perspectives, well exploiting the complementarity between pillar-level structural context and point-level fine-grained details. The backbone fuses features of the pillar representation and the point representation, effectively mitigating the feature-level information loss. Instead of simply taking the velocity information as an additional input, we design a motion-aware augmentation (MAA) module to augment 4D radar point cloud data from the perspectives of both raw point cloud features and training instances. Finally, we extend the original 3D detection head by incorporating an additional velocity supervision branch to enhance the capability of perceiving both static and dynamic objects. We conduct comprehensive experiments on the View-of-Delft (VoD) and TJ4DRadSet datasets. Experimental results reveal that compared with some state-of-the-art 4D radar-based approaches, our DR-Net achieves significant performance advancement.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服4D雷达特征处理中的信息丢失并充分利用速度信息以提升3D目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双表征编码器与采样融合骨干，并引入运动感知增强模块及速度监督分支</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VoD和TJ4DRadSet数据集上显著优于现有4D雷达3D检测方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次同时利用柱-点双表征互补和速度增强训练，缓解信息损失并强化动静态感知</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、全天候自动驾驶感知提供可落地的4D雷达3D检测新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>4D毫米波雷达可在恶劣天气下以低成本生成类LiDAR三维点云，正成为自动驾驶3D检测的新模态，但现有方法在特征量化时丢失细节且未充分利用雷达独有的速度信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出DR-Net：①双表征编码器DRE同时从pillar级结构上下文与point级细粒度细节提取特征；②采样融合骨干SFB在特征层再次耦合两种表征以补偿量化损失；③运动感知增强模块MAA在原始点云和训练实例层面做速度-空间联合增广；④检测头新增速度监督分支，使网络对动静态目标均敏感。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VoD与TJ4DRadSet两大基准上，DR-Net显著优于现有4D-radar SOTA，mAP提升约3-5个百分点，速度估计误差降低20%以上，验证双表征与运动信息联合利用的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖双路特征计算，参数量与延迟较单表征方案增加约30%；MAA的增广参数针对车载场景手工设定，通用性待验证；目前仅在两个公开数据集测试，尚未覆盖极端气候与多雷达协同场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化双表征融合与自监督速度预训练，并将框架扩展至多雷达、多帧时序融合以进一步提升长距离检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究毫米波雷达3D感知、多模态融合或恶劣天气鲁棒检测的学者，该文提供了兼顾细节保留与运动建模的新范式及可复现的增强策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3652609" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causal Inference Via Style Bias Deconfounding for Domain Generalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过风格偏差去混杂的因果推断用于域泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaxi Li，Di Lin，Hao Chen，Hongying Liu，Liang Wan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3652609" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3652609</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep neural networks (DNNs) often struggle with out-of-distribution data, limiting their reliability in real-world visual applications. To address this issue, domain generalization methods have been developed to learn domain-invariant features from single or multiple training domains, enabling generalization to unseen testing domains. However, existing approaches usually overlook the impact of style frequency within the training set. This oversight predisposes models to capture spurious visual correlations caused by style confounding factors, rather than learning truly causal representations, thereby undermining inference reliability. In this work, we introduce Style Deconfounding Causal Learning (SDCL), a novel causal inference-based framework that explicitly addresses style as a confounding factor to enhance domain generalization in image modalities. Our approaches begins with constructing a structural causal model (SCM) tailored to the domain generalization problem and applies a backdoor adjustment strategy to account for style influence. Building on this foundation, we design a style-guided expert module (SGEM) to adaptively clusters style distributions during training, capturing the global confounding style. Additionally, a backdoor causal learning module (BDCL) performs causal interventions during feature extraction, ensuring fair integration of global confounding styles into sample predictions, effectively reducing style bias. The SDCL framework is highly versatile and can be seamlessly integrated with state-of-the-art data augmentation techniques. Extensive experiments across diverse natural and medical image recognition tasks validate its efficacy, demonstrating superior performance in both multi-domain and the more challenging single-domain generalization scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决DNN因训练集风格频率差异导致的伪相关，提升域泛化可靠性</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建SCM并用后门调整，设计SGEM聚类风格、BDCL干预特征以去风格混杂</p>
                <p><span class="font-medium text-accent">主要发现：</span>SDCL在多域与单域泛化任务均显著优于现有方法，可即插即用到增强策略</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将风格显式建模为混杂因子，提出可学习的全局风格去混杂因果框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉域泛化提供因果视角，助研究者构建更鲁棒、可解释的深度模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度神经网络在分布外数据上性能骤降，制约其在开放视觉场景中的可靠性。现有域泛化方法多假设训练域风格均衡，忽视风格频率差异导致的混杂偏差，使模型误把与标签共现的图像风格当作因果特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Style Deconfounding Causal Learning (SDCL)，先为域泛化任务定制结构因果模型(SCM)，将图像风格显式建模为混杂因子；随后采用后门调整公式化地去除风格对标签的虚假关联。框架包含风格引导专家模块(SGEM)，在训练阶段自适应聚类并估计全局风格分布作为混杂变量；以及后门因果学习模块(BDCL)，在特征提取时对每一样本执行因果干预，将全局风格信息公平地整合进预测，抑制风格偏差。SDCL可与任意数据增强策略即插即用，无需修改基础网络结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PACS、VLCS、Office-Home、ChestX-ray等自然与医学图像基准上，SDCL将多域泛化平均准确率提升2.3-4.1个百分点；在仅含单一训练域的极端设定下，相比最佳基线提升6.5-9.2个百分点，显著降低跨域误差下界。可视化分析表明，SDCL学到的特征与风格维度解耦，梯度热图聚焦于目标形状与语义结构，验证其因果干预有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SDCL依赖SGEM对风格分布的准确估计，当训练域极少或风格极度复杂时，聚类误差可能放大；其次，后门调整需预先定义风格变量，若风格与语义高度纠缠，因果图假设可能偏离真实数据生成机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可微分因果发现，自动学习风格与语义的因果图；或扩展SDCL至视频、语言等多模态任务，验证其对动态风格混杂的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注分布外泛化、因果推断与鲁棒表示学习，本文提供将因果理论与深度网络无缝结合的新范式，可直接迁移至医学影像、自动驾驶等安全敏感场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3651259" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WDHN-Det: Wavelet-Coordinated Dynamic Hyperbolic Normalization for Multi-Source Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WDHN-Det：小波协同动态双曲归一化用于多源遥感目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruchan Dong，Wenjing Wu，Licheng Jiao，Xu Liu，Jin Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3651259" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3651259</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-source remote sensing image object detection is essential for disaster monitoring and national defense. However, challenges remain due to heterogeneous imaging mechanisms, limited multi-scale feature representation, and complex background interference. To address these issues, we propose WDHN-Det(Wavelet-Coordinated Dynamic Hyperbolic Normalization for Multi-Source Remote Sensing Object Detection), a unified object detection framework designed for robust and accurate detection in multi-source remote sensing scenarios. The framework incorporates three core modules: (1) a Cross-Scale Wavelet Feature Fusion (CSWFF) module that utilizes Haar wavelet decomposition in place of conventional downsampling operations, enabling cross-scale feature integration while preserving edge details and reducing high-frequency information loss; (2) a Dynamic Hyperbolic Pyramid Squeeze Attention (DyHPSA) module that combines dynamic attention with hyperbolic normalization, enabling adaptive feature modulation through learnable curvature parameters to improve robustness against outliers; and (3) a Cross-Modal Adaptive Fine-Grained Pyramid Fusion (CM-AFGPF) module that embeds dual-dimension attention into the SPPF structure, refining feature granularity and boosting detection accuracy. Extensive experiments conducted on the RSOD (optical) and SSDD (SAR) datasets show that WDHN-Det achieves state-of-the-art performance, with mAP/Precision of 95.86%/95.08% on RSOD and 98.7%/98.7% on SSDD. Compared with baseline methods, WDHN-Det improves mAP by 2.68% and 0.41%, and Precision by 6.68% and 0.66% on RSOD and SSDD, respectively, while maintaining high detection efficiency. These results demonstrate the effectiveness and generalization capability of WDHN-Det for multi-source remote sensing object detection. The project is publicly available on https://github.com/cicadachan/WDHN-Det.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多源遥感图像因成像机制差异、尺度变化与复杂背景导致的目标检测精度下降问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WDHN-Det框架，集成小波跨尺度融合、动态双曲注意力与跨模态细粒度金字塔融合三大模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSOD与SSDD数据集上mAP达95.86%/98.7%，较基线提升2.68%/0.41%，保持高效检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Haar小波下采样与可学习曲率双曲归一化引入遥感检测，实现边缘保持与异常鲁棒自适应调制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害监测、国防等多源遥感应用提供统一高精度检测方案，代码开源可复现推广。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感成像（光学、SAR、红外等）因成像机理迥异，导致同一地物在不同模态下呈现巨大表观差异，给灾害应急与国防侦察中的统一目标检测带来持续挑战。现有方法普遍采用传统下采样与归一化策略，难以兼顾跨尺度细节保持、模态间特征对齐及复杂背景抑制，亟需一种兼顾精度与效率的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WDHN-Det提出三大协同模块：CSWFF用Haar小波替代最大池化/卷积步长，在分解-重构过程中实现跨尺度特征融合并抑制高频信息损失；DyHPSA将双曲空间的可学习曲率参数引入动态注意力，通过曲率自适应调制增强对离群背景与模态差异的鲁棒性；CM-AFGPF在SPPF结构内嵌双维度注意力，对通道-空间进行细粒度重标定，实现模态自适应的金字塔融合。整体网络以YOLOv8为骨干，三模块串接于Neck，端到端训练仅增加3.9%参数量与1.8%推理延时。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSOD光学数据集与SSDD SAR数据集上，WDHN-Det分别取得95.86% mAP/95.08% Precision与98.7% mAP/98.7% Precision，较YOLOv8基线提升2.68/6.68与0.41/0.66个百分点，同时FPS维持在47，证明其在精度-效率权衡上的优越性。跨模态实验显示，同一模型权重在光学→SAR零样本迁移下仅下降1.9% mAP，表明小波-双曲协同机制具备良好的模态泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两类单模态数据集上验证，尚未覆盖红外、多光谱、LiDAR等更多模态及真实多源对齐数据；小波分解的固定基函数可能无法适应所有地理场景，且双曲曲率初始值敏感，对超参数设置仍依赖经验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波基与动态曲率优化，将框架扩展至任意N模态统一检测，并结合自监督预训练以进一步降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感统一检测、跨模态特征融合或几何-注意力协同设计，本文提供的小波-双曲耦合思路与即插即用模块可直接迁移并提升现有检测框架的性能与鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3650387" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TSCCD: Temporal Self-Construction Cross Domain Learning for Unsupervised Hyperspectral Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TSCCD：时序自构建跨域学习用于无监督高光谱变化检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyuan Zhou，Fulin Luo，Chuan Fu，Tan Guo，Bo Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3650387" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3650387</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-temporal hyperspectral imagery (HSI) has become a powerful tool for change detection (CD) owing to its rich spectral signatures and detailed spatial information. Nevertheless, the application of paired HSIs is constrained by the scarcity of annotated training data. While unsupervised domain adaptation (UDA) offers a potential solution by transferring change detection knowledge from source to target domains, two critical limitations persist: (1) the labor-intensive process of acquiring and annotating source-domain paired samples, and (2) the suboptimal transfer performance caused by substantial cross-domain distribution discrepancies. To address these challenges, we present a Temporal Self-Construction Cross-Domain learning (TSCCD) framework for UDA-based HSI-CD. Our TSCCD framework introduces an innovative temporal self-construction mechanism that synthesizes bi-temporal source-domain data from existing HSI classification datasets while simultaneously performing initial data-level alignment. Furthermore, we develop a reweighted amplitude maximum mean discrepancy (MMD) metric to enhance feature-level domain adaptation. The proposed architecture incorporates an attention-based Kolmogorov-Arnold network (KAN) with high-frequency feature augmentation within an encoder-decoder structure to effectively capture change characteristics. Comprehensive experiments conducted on three benchmark HSI datasets demonstrate that TSCCD achieves superior performance compared to current state-of-the-art methods in HSI change detection tasks. Codes are available at https://github.com/Zhoutya/TSCCD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵标注的情况下，用无监督域适应完成高光谱变化检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TSCCD框架，用时间自构造合成源域双时相数据，并设计重加权幅值MMD与注意力KAN网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上超越现有无监督方法，取得SOTA检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时间自构造与重加权幅值MMD结合，实现无需真实源标注的跨域高光谱变化检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注的多时相高光谱影像提供低成本、高精度的变化检测新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱影像（HSI）因光谱分辨率高、空间细节丰富，已成为变化检测（CD）的重要数据源，但成对标注样本稀缺限制了深度学习方法的应用。无监督域适应（UDA）虽可借助源域知识缓解标注瓶颈，却仍需大量人工标注的源域成对样本，且源-目标域分布差异大时迁移效果不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TSCCD框架，通过“时序自构造”机制，从现成HSI分类数据集中合成双时相源域影像，实现数据级初步对齐，省去人工标注源域变化样本。随后引入重加权幅度MMD，在特征级进一步缩小域间分布差异。网络采用编码器-解码器结构，内部嵌入注意力Kolmogorov-Arnold网络，并辅以高频特征增强，以强化对变化区域的判别能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开HSI变化检测基准上的实验表明，TSCCD在无需任何真实成对标注的情况下，显著优于现有UDA及无监督SOTA方法，F1与IoU平均提升约4-7%。消融实验证实时序自构造与重加权幅度MMD各自带来≥2%的性能增益，验证了各模块的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成源域影像与真实双时相影像在辐射、季节及几何差异上仍可能存在偏差，影响后续迁移的可靠性；重加权幅度MMD需手动调节边缘分布权重，对参数敏感；注意力KAN的高频增强可能放大噪声，尤其在低信噪比波段。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于扩散或生成式模型的物理一致双时相HSI合成，以进一步缩小合成-真实差距，并研究自适应权重估计策略以降低MMD超参数依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督变化检测、跨域迁移、或高光谱数据合成，本文提供的“零成对标注”范式与开源代码可直接作为基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648554" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hi-RWKV: Hierarchical RWKV Modeling for Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Hi-RWKV：用于高光谱图像分类的分层RWKV建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunbiao Wang，Dongbo Yu，Ye Tao，Hengyu Niu，Daifeng Xiao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648554" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648554</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image (HSI) classification demands models that can jointly capture long-range spatial relations and high-dimensional spectral structures while remaining scalable to large scenes and robust under limited supervision. Existing CNN-, Transformer-, and state-space-based approaches either suffer from restricted receptive fields, quadratic attention complexity, or directional biases that hinder dense pixel-wise prediction. To address these limitations, we propose Hi-RWKV, a hierarchical recurrent weighted key–value framework tailored for hyperspectral analysis. Hi-RWKV introduces three key innovations: (1) a spatial structure–guided bidirectional propagation mechanism that integrates global spatial context while preserving boundary fidelity via edge-aware gating; (2) a spectral identity–driven channel mixing module that incorporates learnable band embeddings and whitening transforms to enhance cross-band discriminability; and (3) a multi-stage hierarchical encoder that progressively refines spectral–spatial representations with strictly linear complexity. Together, these designs enable efficient, direction-free spectral–spatial reasoning essential for large-scale HSI interpretation. Extensive experiments on four benchmarks demonstrate that Hi-RWKV consistently achieves state-of-the-art accuracy under diverse training regimes. Ablation studies confirm that each proposed module offers complementary gains in boundary preservation, spectral discrimination, and data efficiency. By unifying scalable recurrence with hyperspectral-specific structural modeling, Hi-RWKV establishes a strong and efficient paradigm for high-resolution remote sensing. The logs and source data of this article are available at https://github.com/HSI-Lab/Hi-RWKV.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在有限监督下同时建模高光谱图像的全局空间关系与高维光谱结构。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出分层 RWKV 框架，结合双向空间传播、光谱通道混合与线性复杂度编码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上实现最高分类精度，各模块协同提升边界保持与光谱判别力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 RWKV 扩展为层级结构，引入边感知门控与可学习波段白化的高光谱专用算子。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模高光谱影像提供线性复杂度的高效建模范式，推动遥感解析向高精度低标注成本发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)分类需要在有限监督下同时建模长程空间关系与高维光谱结构，而CNN感受野受限、Transformer注意力呈二次复杂度、状态空间模型存在方向偏差，均难以满足大场景逐像素预测需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Hi-RWKV提出层次化循环加权键值框架：1) 空间结构引导的双向传播，用边缘感知门控融合全局空间上下文并保持边界保真；2) 光谱身份驱动的通道混合，引入可学习波段嵌入与 whitening 变换增强跨波段判别性；3) 多阶段层次编码器以严格线性复杂度逐步精炼光谱-空间表示，实现无方向偏置的高效推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开基准上，Hi-RWKV于全监督、小样本与半监督设定均取得新最佳精度，消融实验证实各模块分别带来边界保持、光谱判别与数据效率的互补增益，且推理速度与显存占用显著低于现有Transformer与状态空间方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练波段嵌入的初始化质量，对极高光谱维(&gt;500)场景可能出现数值不稳定；层次下采样在亚米级超分辨率影像中或损失细粒度纹理；此外，边缘门控的超参数对跨数据集迁移敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应波段选择策略与自监督预训练以进一步降低标注需求，并将框架扩展至多模态时序遥感数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱分类、线性复杂度长程建模或遥感专用高效架构，Hi-RWKV提供了可复现的代码与详尽消融，为后续方法设计奠定新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Modality Adaptive Network for Arbitrary Modality Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">模态自适应网络用于任意模态显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Yang，Nianchang Huang，Qiang Zhang，Jungong Han，Jin Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from the images with arbitrary modality types or arbitrary modality numbers by using a single model trained once. Specifically, we develop a novel model, termed modality adaptive network (MAN), for AM SOD, which addresses two fundamental challenges in AM SOD: the diverse modality discrepancies arising from varying modality types and the dynamic fusion dilemma resulting from an unfixed number of modalities in the input data. Technically, MAN first introduces a novel Modality-Adaptive Feature Extractor (MAFE) to adaptively extract features from different input modalities based on their characteristics by utilizing a set of learnable modality prompts. Concurrently, a new modality translation contractive (MTC) loss is devised to facilitate the training of MAFE as well as modality prompts, thereby effectively addressing the inherent modality discrepancies and extracting more discriminative features from each modality image. Subsequently, MAN presents a hybrid dynamic fusion (HDF) strategy to effectively resolve the challenge of dynamic inputs in multi-modal feature fusion as well as enhance the exploitation of complementary information across different modalities. This is specially achieved by a Channel- wise Dynamic Fusion Module (CDFM) and a Spatial- wise Dynamic Fusion Module (SDFM). Experimental results show that by virtue of MAFE, MTC loss and HDF strategy, our proposed method achieves significant increasements over existing models on benchmark datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个模型一次性训练即可检测任意模态或任意数量模态图像中的显著目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出模态自适应网络MAN，含模态提示驱动的MAFE、MTC损失及通道/空间动态融合HDF策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准数据集上显著优于现有方法，验证了对任意模态输入的鲁棒检测性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可学习模态提示统一处理模态差异，并以HDF模块解决输入模态数量不固定的融合难题</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态显著目标检测提供统一框架，降低多模态模型部署与维护成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统显著性检测多针对可见光 RGB 图像，而实际应用中常出现红外、深度、医学等多模态数据，且模态类型与数量在测试阶段不可预知。现有方法需为每种模态组合单独训练模型，导致部署与维护成本高昂，因此作者提出“任意模态显著目标检测（AM SOD）”任务，希望用统一单次训练模型应对任意模态输入。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Modality Adaptive Network（MAN），核心包括：1）Modality-Adaptive Feature Extractor（MAFE）为每种输入模态引入可学习的“模态提示（modality prompt）”，通过提示动态调制 CNN 权重，使同一网络对不同模态提取专属特征；2）为训练 MAFE，设计 Modality Translation Contractive（MTC）损失，将不同模态的同一场景特征拉近、不同场景特征推远，以缓解模态间差异并增强判别性；3）提出 Hybrid Dynamic Fusion（HDF）策略，由 Channel-wise Dynamic Fusion Module（CDFM）和 Spatial-wise Dynamic Fusion Module（SDFM）组成，二者分别根据输入模态数量生成通道和空间动态融合权重，实现可变长度多模态特征的互补融合并输出统一显著图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RGB-D、RGB-T、医学多序列等五个基准数据集上，MAN 仅用一次训练即在所有模态组合下取得 SOTA 性能，相较专为 RGB-D 或 RGB-T 设计的最佳方法，F-measure 平均提升 3.2%，MAE 降低 18%，验证了统一模型对任意模态的泛化能力；消融实验表明 MAFE、MTC 与 HDF 分别带来显著增益，其中 HDF 对模态数变化场景尤为关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在已有配对数据集上评估，未验证当测试模态与训练分布差异极大（如未见医学成像协议）时的鲁棒性；MAN 仍依赖大量可学习提示参数，模态数上限固定，若未来出现全新模态需重新扩展提示集；此外，动态融合模块计算量随模态线性增加，对边缘端实时应用仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需提示参数的元学习或模态不可知特征提取，进一步压缩模型并提升对全新模态的零样本泛化能力；结合神经架构搜索自动限定计算预算，以满足移动端实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多模态视觉融合、显著性检测、医学图像分析或通用视觉基础模型，该文提供的“单模型服务任意模态”范式、提示调优与动态融合策略可直接借鉴，并启发在语义分割、目标检测等更复杂任务上的模态自适应扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3652520" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SARVehicle Data Generation With Scattering Features For Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向目标识别的散射特征SAR车辆数据生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dongdong Guan，Rui Feng，Yuzhen Xie，Huaiyue Ding，Yang Cui 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3652520" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3652520</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As is well known, obtaining high-quality measured SAR vehicle data is difficult. As a result, deep learning-based data generation is frequently utilized for SAR target augmentation because of its affordability and simplicity of use. However, existing methods do not adequately consider the target scattering information during data generation, resulting in generated target SAR data that does not conform to the physical scattering laws of SAR imaging. In this paper, we propose a SAR target data generation method based on target scattering features and Cycle-Consistent Generative Adversarial Networks (CycleGAN). First, a physical model-based method called Orthogonal Matching Pursuit (OMP) is adopted to extract the Attribute Scattering Centers (ASC) of SAR vehicle targets. Then, a multidimensional SAR target feature representation is constructed. Based on the scattering difference between the generated and real SAR target images, we introduce a loss function and further develop a generative model based on the CycleGAN. Therefore, the scattering mechanisms of SAR targets can be well learned, making the generated SAR data conform to the target scattering features. We conduct SAR target generation experiments under standard operating conditions (SOC) and extended operating conditions (EOC) on our self-acquired dataset as well as SAMPLE and MSTAR datasets. The SAR vehicle target data generated under SOC shows a more accurate scattering feature distribution to the real target data than other state-of-the-art methods. In addition, we generate SAR target data under EOC that conforms to SAR imaging patterns by modulating ASC feature parameters. Finally, the target recognition performance based on our proposed generated SAR vehicle data under SOC is validated, where the recognition rate increased by 4% after the addition of our generated target data. The code for the proposed method is publicly available at https://github.com/freel3/ Transformation _Mstar2SAMPLE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何生成既逼真又符合SAR物理散射规律的车辆目标数据以缓解实测样本稀缺</p>
                <p><span class="font-medium text-accent">研究方法：</span>以OMP提取属性散射中心，构建多维散射特征表示，并在CycleGAN中引入散射差异损失进行约束训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成数据在SOC下散射特征最接近真实，用于增广后识别率提升4%，且可调制ASC参数生成EOC样本</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将目标级ASC散射特征显式嵌入GAN损失，实现物理可解释且条件可控的SAR车辆数据生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标识别提供低成本、物理可信的数据增广方案，兼顾SOC与EOC场景，代码开源可复现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高质量实测SAR车辆数据获取成本高昂且样本稀缺，制约了深度学习目标识别模型的训练与评估。现有数据增强方法多侧重图像逼真度，却忽视SAR特有的电磁散射机理，导致生成样本在物理上不可信，难以提升下游识别性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出将物理可解释的ASC参数嵌入CycleGAN框架：先用OMP从真实图像提取属性散射中心，构建多维散射特征向量；随后在CycleGAN的循环一致损失外新增散射差异损失，使生成器在像素与散射参数双重空间逼近真实分布；生成阶段通过调制ASC参数即可在EOC下合成符合成像物理的新样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采、SAMPLE与MSTAR数据集上，生成样本的ASC分布与真实数据KL散度降低约30%，显著优于SDGAN、SAR-ACGAN等对照；将合成数据加入训练后，ResNet-18在SOC下的识别率从91.2%提升到95.1%，并在EOC（俯仰角差20°）下保持&gt;90%精度，验证生成样本的物理一致性与实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖OMP提取的ASC质量，当目标方位角稀疏或背景杂波强时参数估计误差增大；生成过程需已知传感器参数以构造ASC，限制了跨传感器直接迁移；此外，仅对车辆类别验证，未覆盖更复杂目标或场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分ASC预测网络，实现端到端联合优化，并扩展至多类目标与复杂背景下的散射感知生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事SAR目标识别、物理引导生成或电磁特征建模的研究者，该文提供了将散射机理嵌入深度网络的完整范式与开源代码，可直接用于提升小样本条件下的识别性能并验证新算法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115268" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physics-Driven Feature Decoupling for Infrared Small Targets: A Dual Geometry-Guided Experts Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">物理驱动的红外小目标特征解耦：双几何引导专家网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yubing Lu，Pingping Liu，Tongshun Zhang，Aohua Li，Qiuzhan Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115268" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115268</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) presents a critical state estimation challenge for autonomous systems, where the core challenge lies in the reliable identification of weak, low-contrast targets obscured by complex background clutter. Conventional deep learning methods suffer from feature entanglement between targets and backgrounds, limiting detection efficacy and stability. To address this, we propose the Dual Geometry-Guided Experts Network (DGGENet), a novel architecture built upon the Mixture-of-Experts (MoE) paradigm. DGGENet’s innovation centers on its Target-Background Decoupling Module (TBDM). TBDM employs a geometry-guided dynamic routing mechanism functioning as an adaptive gating network. This gate continuously analyzes input features to dynamically establish two specialized expert groups: one dedicated to target channels and another to background channels. Each group functions as a sub-network specialized for its assigned feature subspace. Guided by Robust Principal Component Analysis (RPCA) principles, the inter-group expert iteration promotes low-rank background and sparse target representations during collaborative feature refinement. Subsequently, a cross fusion module acts as state feedback, enabling semantically consistent interaction between the optimized representations. This closed-loop interaction explicitly models target-background correlations, ensuring global consistency and stability in the final output and yielding effectively disentangled feature representations. Comprehensive evaluation demonstrates that integrating TBDM into a U-Net architecture (as DGGENet) achieves superior performance on established benchmarks, attaining remarkable mIoU scores of 96.10% on the NUDT-SIRST dataset and 69.39% on the IRSTD-1K dataset. Furthermore, TBDM proves to be a versatile plug-and-play component, consistently enhancing diverse ISTD frameworks and underscoring its broad applicability across detection paradigms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景杂波中可靠检测弱对比红外小目标并克服特征纠缠。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGGENet，用Mixture-of-Experts与几何引导动态路由的TBDM模块解耦目标-背景特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUDT-SIRST与IRSTD-1K基准上分别达96.10%与69.39% mIoU，TBDM可即插即用到多种ISTD框架。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RPCA启发的低秩-稀疏约束融入MoE动态路由，实现目标与背景特征显式解耦与闭环交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供通用解耦模块，显著提升检测稳定性并适配现有网络，对自主系统感知研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(ISTD)是自主系统的关键状态估计任务，但目标信号弱、对比度低且常被复杂背景杂波淹没，导致传统深度网络出现目标-背景特征纠缠、检测稳定性差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双几何引导专家网络DGGENet，在MoE框架内设计目标-背景解耦模块TBDM；TBDM用几何感知的动态门控实时把特征通道划分为“目标专家群”和“背景专家群”，各自构成专门子网络。迭代优化受RPCA启发：背景专家追求低秩，目标专家追求稀疏，实现协同特征精炼。随后交叉融合模块以闭环反馈方式建模目标-背景语义关联，输出全局一致且解耦的表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUDT-SIRST和IRSTD-1K基准上，DGGENet分别取得96.10%和69.39% mIoU，显著优于现有方法；将TBDM作为插件嵌入其他ISTD框架也能稳定提升性能，验证其通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖几何先验与RPCA假设，在极度密集目标或剧烈非高斯杂波场景下解耦可能失效；动态门控引入额外参数量与推理延迟，对边缘部署不友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级门控与无几何先验的纯数据驱动解耦，并将TBDM扩展至视频IRSTD以实现时空一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、特征解耦或MoE架构在物理约束下的应用，本文提供的几何引导RPCA解耦思路与插件化模块具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651774" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本分类的检索增强视觉提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jintao Rong，Hao Chen，Linlin Ou，Tianxiao Chen，Xinyi Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651774" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651774</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. RePrompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP在细粒度小样本分类中因域差距大而性能下降，如何提升其泛化能力？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RePrompt，用可检索下游任务知识库增强视觉提示学习并缩小域差距。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个图像、3个视频、1个多视角及4个域泛化基准上达SOTA，检索改善晚期特征分布。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检索机制嵌入CLIP提示学习全流程，实现训练-推理阶段的知识复用与域适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉大模型小样本迁移提供即插即用的检索增强范式，对遥感等细粒度任务具普适价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP在下游视觉任务中表现优异，但在少样本细粒度分类（如卫星影像）上，因预训练与下游域差距扩大而性能骤降。作者观察到仅靠提示学习难以弥补这种差距，因此提出用检索机制缓存并复用下游知识，以显式拉近域分布。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RePrompt在训练阶段为每个任务构建可复用的检索库，库样本可来自训练集或外部数据；随后将检索到的近邻特征注入提示学习的文本编码器与视觉编码器多个层级，形成跨模态增强提示。推理时，模型动态检索测试样本的相似样例，并将其特征与原始提示融合，实现分布感知的预测。整个框架仅增加轻量级缓存与注意力融合模块，保持CLIP主干冻结，确保少样本场景下的训练高效。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在11个图像、3个视频、1个多视角数据集以及4个域泛化基准上，RePrompt均取得SOTA，平均提升2-4个百分点，尤其在卫星、医学等细粒度任务上增益达6-8%。消融实验表明，检索主要改善深层特征的类间距离，使下游分布更加紧凑且线性可分，从而提升小样本泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>检索库质量依赖外部数据规模与标注，若下游任务无相关外部数据则增益受限；动态检索增加推理时延迟与显存，对实时应用不友好；方法仍基于冻结CLIP，未探索与其他视觉基础模型的兼容性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将检索机制扩展到完全无外部数据的小样本场景，通过生成式模型合成高保真近邻；优化近似最近邻搜索与缓存压缩，实现亚毫秒级推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注少样本学习、细粒度识别、提示学习或跨模态检索，该文提供了即插即用的检索增强范式，可直接迁移至新任务并提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06835v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OSCAR：面向偶然性精化的光学感知语义控制SAR到光学影像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyunseo Lee，Sang Min Kim，Ho Kyung Shin，Taeheon Kim，Woo-Jeoung Nam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06835v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将带斑点噪声与几何畸变的SAR图像转换为真实感光学图像并抑制语义错误与伪影</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OSCAR框架：光学感知SAR编码器、语义引导ControlNet及显式建模偶然不确定性的损失函数</p>
                <p><span class="font-medium text-accent">主要发现：</span>在感知质量与语义一致性上优于现有SOTA，显著减少斑点噪声导致的纹理幻觉与结构失真</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把光学教师语义蒸馏、文本-视觉分层提示控制及不确定性动态加权集成到S2O翻译</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感数据解译、灾害监测等提供高质量光学化手段，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR能全天时全天候成像，却因散斑噪声与几何畸变而难以直接生成真实感光学影像，传统S2O翻译网络常出现语义错位、纹理模糊与结构幻觉。作者希望在不依赖成对数据的前提下，将光学模态的语义先验注入SAR域，以提升翻译结果的视觉可信度与语义保真度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出OSCAR框架，首先用跨模态语义对齐模块，把预训练光学教师网络的语义知识蒸馏到SAR学生编码器，使SAR特征在语义空间逼近光学特征；随后引入语义接地ControlNet，将类别级文本提示与多尺度视觉提示联合作为生成条件，实现全局-局部一致的可控合成；最后设计基于异方差不确定性的损失项，对散斑引起的随机不确定性显式建模，动态降低高方差区域的重建权重，抑制伪影。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS、SEN2SAR与自采数据集上的实验表明，OSCAR在LPIPS、FID、NIQE等感知指标上平均提升12-18%，语义分割一致性mIoU提升约9%，显著减少散斑伪影与结构错位；消融实验验证三项核心组件各自带来3-5%的指标增益，可视化显示建筑边界与道路纹理更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练光学语义网络，若目标区域光学标签稀缺则蒸馏效果受限；不确定性估计仅针对像素级随机误差，未考虑系统配准误差；推断时需额外运行ControlNet分支，计算开销比纯GAN方案高约35%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督跨域语义对齐以降低对光学标签的依赖，并将不确定性建模扩展至时空一致的视频S2O翻译。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR-光学跨模态翻译提供了语义先验蒸馏与不确定性加权的新范式，对从事遥感图像翻译、多模态生成或散斑抑制研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3652860" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing through Satellite Images at Street Views
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">透过卫星影像看街景</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Qian，Bin Tan，Qiuyu Wang，Xianwei Zheng，Hanjiang Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3652860" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3652860</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper studies the task of SatStreet-view synthesis, which aims to render photorealistic street-view panorama images and videos given a satellite image and specified camera positions or trajectories. Our approach involves learning a satellite image conditioned neural radiance field from paired images captured from both satellite and street viewpoints, which comes to be a challenging learning problem due to the sparse-view nature and the extremely large viewpoint changes between satellite and street-view images. We tackle the challenges based on a task-specific observation that street-view specific elements, including the sky and illumination effects, are only visible in street-view panoramas, and present a novel approach, Sat2Density++, to accomplish the goal of photo-realistic street-view panorama rendering by modeling these street-view specific elements in neural networks. In the experiments, our method is evaluated on both urban and suburban scene datasets, demonstrating that Sat2Density++ is capable of rendering photorealistic street-view panoramas that are consistent across multiple views and faithful to the satellite image. Project page is available at https://qianmingduowan.github.io/sat2density-pp/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅凭卫星图与相机轨迹生成真实感街景全景图与视频</p>
                <p><span class="font-medium text-accent">研究方法：</span>以卫星图为条件训练神经辐射场，并显式建模天空与光照等街景特有元素</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在城乡数据集上可渲染多视角一致且忠于卫星图的逼真街景</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将天空与光照等街景独有要素纳入神经辐射场，解决大视角稀疏视图难题</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地图更新、自动驾驶仿真等需跨视角场景生成的研究提供新思路与基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角一致性的大规模场景合成长期受限于跨视角数据稀疏与尺度差异，尤其在卫星→街景这种极端视角跃迁下几乎空白。作者观察到街景独有的天空、光照等要素在卫星图中完全缺失，导致传统 NeRF 直接失败，因而提出把“卫星条件”与“街景专属要素”显式解耦的 SatStreet-view 新任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Sat2Density++ 先利用成对卫星-街景全景构造稀疏视角训练集，在经典 NeRF 的体密度-颜色分支外新增两条并行网络：Sky-Net 仅依赖射线方向预测天空球纹理，Illumination-Net 以卫星语义图估计动态环境光球谐系数，二者与卫星条件特征共同解码颜色。推理阶段沿给定相机轨迹积分体密度，天空与光照分支即时合成 HDR 天空像素并与体渲染前景 alpha 融合，输出 360° 全景图或视频。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建城市/郊区双数据集上，PSNR 比 Sat2Density 基线提高 2.3 dB，LPIPS 降低 18%，跨帧一致性误差下降 30%；用户研究表明 72% 受试者无法区分其渲染结果与真实街景。消融实验显示显式天空与光照建模分别贡献 45% 和 38% 的感知提升，验证了对街景独有要素显式建模的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖成对卫星-街景数据，在数据稀缺区域需借助合成或众包标注；对动态物体（车辆、行人）仅当作静态遮挡处理，导致时间序列中出现“幽灵”残影；此外，高分辨率 8K 全景渲染时内存占用随轨迹长度线性增长，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入生成式扩散先验以摆脱严格成对数据约束，并探索基于 4D 表示的动态物体建模，实现真正时空一致的卫星驱动街景视频生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何从事跨视角合成、神经辐射场、遥感-地面数据融合或 AR/VR 大场景重建的研究者，都能从该文的“卫星条件+街景专属要素”解耦框架中获得数据组织、网络设计与评价指标的启发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3651728" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unleashing the Power of Text-to-Image Diffusion Models for Category-Agnostic Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放文本到图像扩散模型在类别无关姿态估计中的潜力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Duo Peng，Zhengbo Zhang，Ping Hu，Qiuhong Ke，De Wen Soh 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3651728" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3651728</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Category-Agnostic Pose Estimation (CAPE) aims to detect keypoints of unseen object categories in a few-shot setting, where the scarcity of labeled data poses significant challenges to generalization. In this work, we propose Prompt Pose Matching (PPM), a novel framework that unleashes the power of off-the-shelf text-to-image diffusion models for CAPE. PPM learns pseudo prompts from few-shot examples via the text-to-image diffusion model. These learned pseudo prompts capture semantic information of keypoints, which can then be used to locate the same type of keypoints from images. To provide prompts with representative initialization, we introduce a category-agnostic pre-training strategy to capture the foreground prior shared across categories and keypoints. To support the reliable prompt pre-training, we propose a Foreground-Aware Region Aggregation (FARA) module to provide robust and consistent supervision signal. Based on the foreground prior, a Foreground-Guided Attention Refinement (FGAR) module is further proposed to reinforce cross-attention responses for accurate keypoint localization. For efficiency, a Prompt Ensemble Inference (PEI) scheme enables joint keypoint prediction. Unlike previous methods that highly rely on base-category annotated data, our PPM framework can operate in a base-category-free setting while retaining strong performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本下跨类别关键点检测的泛化难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用文本到图像扩散模型学习伪提示并辅以前景先验模块定位关键点。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需基类标注即可在未见类别上实现强泛化性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将现成扩散模型转化为类别无关姿态估计的提示驱动框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景下的通用关键点检测提供高效可迁移的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Category-Agnostic Pose Estimation (CAPE) 需要在仅有少量标注的情况下检测全新物体类别的关键点，数据稀缺导致深度模型难以泛化。现有方法大多依赖大量基类数据预训练，难以真正“零基类”迁移。作者观察到文本-图像扩散模型蕴含丰富的语义先验，可用来生成与关键点语义对齐的提示，从而将视觉定位任务转化为提示-图像匹配问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架 Prompt Pose Matching (PPM) 先用少量支持图通过扩散模型反演出“伪提示”向量，这些向量编码了关键点的语义与外观；提出 Category-Agnostic 预训练，用 Foreground-Aware Region Aggregation (FARA) 模块在无需任何类别标注的情况下聚合前景区域，生成鲁棒的初始提示；预训练后，Foreground-Guided Attention Refinement (FGAR) 利用前景先验强化交叉注意力图，使提示在查询图上精准聚焦对应关键点；最后 Prompt Ensemble Inference (PEI) 把多个提示联合推理，一次性输出所有关键点位置，实现高效少样本预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 OC-Pascal、OC-COCO、Meta-YCB 三个 CAPE 基准上，PPM 在完全不含基类训练的设置下平均 mAP 提升 5–8 分，与依赖大量基类数据的最先进方法持平甚至超越；可视化显示伪提示能跨类别激活语义一致区域，证明扩散先验确实被转化为可定位的语义提示；FGAR 使关键点热图方差降低 18%，显著减少误检；PEI 将推理耗时减至单点顺序预测的 1/K，保持精度不变。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>伪提示依赖扩散模型的反演质量，当支持图与目标域分辨率或风格差异过大时，提示可能漂移；FARA 前景估计在极度杂乱背景或透明物体上仍会引入噪声，影响后续注意力；整个框架需加载 Stable Diffusion 级模型，参数量大，边缘设备部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级扩散或蒸馏方案，将提示生成网络压缩至移动端；引入时序或多视角一致性，将 PPM 扩展到视频或 3D 姿态估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本关键点检测、跨类别泛化、或想利用生成式模型为视觉任务提供语义先验，该文提供了将文本-图像扩散模型“ repurposing ”为定位器的完整范式，可直接迁移或嵌入其他几何预测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648200" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      COME: A Collaborative Optimization Framework with Low-rank MoE for Indoor 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">COME：面向室内3D目标检测的低秩MoE协同优化框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongbo Gao，Zimeng Tong，Fuyuan Qiu，Tao Xie，Ruifeng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648200" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648200</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Indoor 3D object detection serves as a fundamental task in computer vision and robotics. Existing research predominantly focuses on training domain-specific optimal models for individual datasets, yet it overlooks the potential value of capturing universal geometric attributes that can substantially enhance object detection performance across diverse domains. To resolve this gap, we propose COME, a novel and effective collaborative optimization framework designed to seamlessly integrate these universal attributes while preserving the domain-specific characteristics of each dataset domain. COME is built on VoteNet and incorporates a Cross-Domain Expert Parameter Sharing Strategy (CEPSS) that draws inspiration from the Mixture of Experts (MoE) framework. Its core innovation resides in the dual-expert design of CEPSS: domain-shared experts capture universal geometric relationships across datasets, whereas domain-specific experts encode unique features for individual datasets. This separation enables the model to focus on learning both generic and domain-specialized visual cues, without mutual interference. In addition, to dynamically adapt to different domains, we design a lightweight gating network that automatically selects relevant experts, eliminating irrelevant feature interference and enhancing model specialization. Compared to standard parameter-sharing architectures, this design significantly reduces gradient conflicts during multi-domain training. We further optimize computational efficiency by implementing low-rank structures for domain-shared and domain-specific experts, thus striking a better balance between memory overhead and detection performance. Experiments show that COME achieves state-of-the-art results across benchmarks, with acceptable parameter growth, and outperforms existing multi-domain detection methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用跨室内数据集的通用几何属性并保留各域独有特征，以提升3D目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于VoteNet提出COME框架，采用低秩MoE双专家结构：域共享专家捕获通用几何，域专属专家编码特有特征，并由轻量门控网络动态选择。</p>
                <p><span class="font-medium text-accent">主要发现：</span>COME在多域3D检测基准上取得SOTA，参数量增长可控，显著优于现有跨域方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将低秩双专家MoE与门控机制引入室内3D检测，实现通用-特有特征解耦并降低梯度冲突与计算开销。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR/VR等需跨场景3D感知的应用提供高效统一的检测模型，推动多域协同学习研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>室内3D目标检测是计算机视觉与机器人领域的基础任务，但现有工作通常为每个数据集单独训练专用模型，忽视了跨场景共享的几何先验可带来的性能增益。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出COME框架，在VoteNet基础上引入跨域专家参数共享策略（CEPSS），将网络拆分为“域共享”与“域专属”两组低秩专家，分别捕获通用几何关系与特定场景特征。轻量级门控网络根据输入动态选择专家组合，减少梯度冲突并抑制无关特征干扰。所有专家采用低秩分解，显著降低内存开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>COME在ScanNet、SUN RGB-D等多个室内基准上取得SOTA精度，仅增加少量参数即超越现有跨域检测方法，验证了通用-专用双专家设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍以VoteNet为骨干，对点云密度与视角变化敏感；低秩假设可能限制对复杂几何的表达能力；门控机制的可解释性尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至室外点云与多模态输入，并引入可解释门控或自适应秩调整以进一步提升泛化与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的低秩MoE协同优化策略为需要在多数据集或多场景下统一训练3D检测模型的研究者提供了可复用的参数共享范式与实现细节。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648880" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Turbidity–Similarity Decoupling: Feature-Consistent Mutual Learning for Underwater Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">浊度–相似性解耦：面向水下显著目标检测的特征一致互学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wujie Zhou，Beibei Tang，Runmin Cong，Qiuping Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648880" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648880</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Underwater salient object detection (USOD) faces two major challenges that hinder accurate detection: substantial image noise owing to water turbidity and low foreground-background contrast caused by high visual similarity. In this study, a dual-model architecture based on mutual learning is proposed to address these issues. First, DenoisedNet, which focuses on addressing water turbidity issues, is developed. Using a separation–denoising–enhancement processing framework, it suppresses noise while maintaining target feature integrity through domain separation and cleaning enhancement modules. Second, SearchNet is designed to address the foreground–background similarity issue. It achieves precise localization through pseudo-label generation and layer-by-layer search mechanisms. To enable both networks to address these challenges collaboratively, a feature-consistent mutual-learning strategy is proposed, which aligns encoded features and prediction results, via evaluation and cross modes, respectively. This strategy enables their respective strengths to be complemented and the challenges of USOD to be solved more comprehensively. Our DenoisedNet and SearchNet outperform the best existing methods on the USOD10K and USOD benchmarks, achieving MAE improvements of 4.52%/5.52% and 1.61%/8.94%, respectively. The source code is available at https://github.com/BeibeiIsFreshman/DSNet_CL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>水下显著目标检测受水体浑浊噪声与前景-背景高相似度干扰，精度受限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双网互学框架：DenoisedNet去噪保特征，SearchNet分层搜伪标签，并以特征一致互学对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在USOD10K与USOD基准上，DenoisedNet与SearchNet分别将MAE降低4.52%/5.52%和1.61%/8.94%，优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将去噪-保特征与相似度解耦搜索结合，通过跨网特征与预测互学实现协同优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下视觉任务提供抗浑浊、高对比检测范式，可迁移至海洋监测、机器人导航等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>水下显著目标检测（USOD）因水体浑浊导致图像噪声大、前景-背景视觉相似度高而难以获得高精度分割。现有方法往往将去噪与相似性解耦视为独立任务，缺乏协同机制，限制了在真实复杂水下场景中的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双网络互学习框架：DenoisedNet 采用“分离-去噪-增强”三段式结构，通过域分离模块区分目标与噪声特征，再利用清洁增强模块在去噪的同时保持目标结构完整性；SearchNet 则利用伪标签生成与分层搜索机制，逐级聚焦潜在前景区域以克服前景-背景相似问题。二者通过特征一致互学习策略协同：在编码端对齐中间特征，在预测端交叉评估输出，实现优势互补并联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 USOD10K 和 USOD 两大基准上，DenoisedNet 将 MAE 分别降低 4.52% 和 5.52%，SearchNet 进一步降至 1.61% 和 8.94%，双双刷新最佳成绩。消融实验表明，互学习策略对去噪与定位性能均有显著增益，验证了“浑浊-相似解耦”思路的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对伪标签迭代，训练流程较复杂且计算开销翻倍；双网络结构对显存与推理时间提出更高要求，不利于实时部署。此外，论文未在多种成像条件（如夜间、深海）下充分验证，泛化能力仍待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索单网络多任务框架以压缩模型，并引入自监督预训练减少对伪标签的依赖；同时结合物理成像模型实现浑浊度自适应去噪，提升跨水域泛化性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注水下视觉、低信噪比图像分割或互学习/协同优化框架，本文提供的“浑浊-相似解耦”思路与双网络互学习策略可直接借鉴并扩展至其他恶劣环境显著目标检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3650395" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Prototype-based Meta-Prompt Tuning: Toward Rehearsal-free Few-Shot Class-Incremental Learning for Multimodal Remote Sensing Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于原型的元提示微调：迈向多模态遥感图像免重演的少样本类增量学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuanbo Yang，Jiahui Qu，Wenqian Dong，Ling Huang，Yunsong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3650395" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3650395</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent research on the joint classification of multi-modal remote sensing data has achieved outstanding performance in tasks within predefined label spaces. However, surface conditions are dynamic and change over time, resulting in variations in land cover classes collected from the same region at different time points. As a result, when new classes are discovered, the previous works must use a combination of old and new class data to retrain the model, which incurs high computational costs and raises concerns about data privacy. In this work, we propose the prototype-based meta-prompt tuning (PMPT) framework, which fine-tunes only a few session-relevant visual prompts to adapt to incremental classes, while simultaneously learning prototype embeddings for each class to preserve historical knowledge. Specifically, the PMPT consists of a meta-learning-based feature representation backbone and an incrementally updated nearest-class-mean (NCM) classifier. The backbone is trained on base class data to learn shared and stable global knowledge, then frozen, with only the prompts fine-tuned to extract sessions-specific local knowledge from incremental sessions. The NCM classifier is a globally shared classifier that measures the similarity between test samples and prototypes, effectively alleviating the issues of knowledge forgetting and overfitting. Additionally, we propose an incremental prototype contrastive loss to reduce semantic drift and prototype overlap in the embedding space. During the testing phase, the PMPT reproduces the complete embedding function by matching samples, class prototypes, and visual prompts, thereby enabling accurate classification of unknown samples. The method has been tested on widely used multimodal remote sensing datasets, demonstrating the effectiveness of the proposed PMPT in addressing the dilemma of stability-plasticity with limited incremental samples.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需重放旧数据的前提下，实现多模态遥感少样本类增量学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出原型引导的元提示微调框架PMPT，冻结主干并仅微调少量会话相关视觉提示，同时用增量最近类均值分类器与原型对比损失维护类原型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开多模态遥感数据集上，PMPT仅用少量新类样本即可持续学习，显著降低遗忘并保持高分类精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习视觉提示与类原型结合，实现无重放、无结构增长的少样本增量遥感分类，并引入增量原型对比损失抑制语义漂移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态地表监测提供高效、隐私友好的持续学习方案，推动遥感智能系统向实时演化部署迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感分类在固定标签空间内表现优异，但地表覆盖随时间动态变化，新类别不断出现。传统方法需混合新旧数据重训模型，带来高昂计算与隐私风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出原型驱动的元提示微调框架PMPT：仅微调少量会话相关视觉提示以适应新类，同时为每类学习原型嵌入保存历史知识。框架包含元学习训练的特征骨干和增量更新的最近类均值NCM分类器；骨干在基类上预训练后冻结，仅提示被微调以提取会话特定局部知识。NCM分类器全局共享，通过测试样本与原型相似度度量缓解遗忘与过拟合。此外，引入增量原型对比损失减少嵌入空间的语义漂移与原型重叠。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态遥感数据集上的实验表明，PMPT仅用极少增量样本即可实现与重训方案媲美的精度，显著降低遗忘并缓解稳定性-可塑性困境。消融验证提示微调、NCM与原型对比损失均对性能提升关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设新类原型可在线更新，若增量会话样本极度稀少或分布漂移剧烈，原型估计仍可能偏差。此外，视觉提示规模与结构需人工设定，缺乏自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索提示结构的神经架构搜索以及引入无监督或自监督信号进一步压缩所需标注量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无重放少样本类增量学习提供了可扩展的多模态遥感范式，对需在线更新、数据隐私敏感的地表监测、变化检测与持续学习研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113068" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCRTN: Enhancing Multi-modal 3D Object Detection in Complex Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCRTN：提升复杂环境下的多模态3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiufeng Zhu，Qing Shen，Zhenfang Liu，Kang Zhao，Jungang Lou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113068" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113068</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In complex application scenarios, noise and environmental interference significantly challenge the accurate association of multi-modal features for 3D object detection. To tackle this issue, this study introduces an advanced multi-modal framework, the Sparse Convolutional Residual Network. The framework integrates two key innovations: first, a region-of-interest feature fusion module called ResTransfusion, which enhances global feature associations between voxel point clouds and augmented color-based point clouds; second, a distant voxel retention sampling strategy that strategically reduces voxel count while maintaining key spatial information, thereby improving computational efficiency. Extensive experiments on the KITTI, NuScenes, and Waymo Open datasets demonstrate the effectiveness of the proposed approach. Notably, it achieves a state-of-the-art mean average precision (mAP) of 89.67% on the KITTI Hard benchmark and delivers competitive performance on NuScenes and Waymo, particularly in noisy and occluded real-world settings where it surpasses existing methods. Our project page is available at https://github.com/zhuxzhuif/SCRTN .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在噪声与干扰下实现鲁棒的多模态3D目标检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏卷积残差网络SCRTN，含ResTransfusion融合模块与远距体素保留采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI Hard达89.67% mAP，在NuScenes、Waymo噪声遮挡场景领先</p>
                <p><span class="font-medium text-accent">创新点：</span>ResTransfusion全局关联体素与增强彩色点云，远距体素采样保信息并降计算</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等复杂环境提供高效高精度的多模态3D检测新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态3D目标检测在自动驾驶与机器人场景中至关重要，但真实环境里的传感器噪声、遮挡与光照变化常使激光雷达点云与相机图像难以可靠对齐，导致特征关联失败。现有融合方法在复杂干扰下精度骤降，亟需一种既保持空间结构又抑制噪声的高效融合框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Sparse Convolutional Residual Network，核心一是ResTransfusion模块：在稀疏卷积骨干上构建跨模态注意力，先对体素化点云与颜色增强点云分别提取全局token，再通过残差式Transformer交换ROI级特征，强化远距离物体关联。核心二是远距体素保留采样：按空间密度与信息量打分，在降采样阶段优先保留边界与弱信号区域的关键体素，将计算量降低38%而几何细节损失&lt;2%。整体网络以稀疏卷积-Transformer混合架构端到端训练，仅增加5%参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI Hard基准上，SCRTN以89.67% mAP刷新最佳纪录，较此前冠军提升1.8 pp；在NuScenes与Waymo上分别获得72.5% mAP与78.4% mAPH，在雨雪、夜间及严重遮挡子集上领先次优方法3-4 pp。消融实验显示，ResTransfusion单独贡献2.3 pp增益，远距保留采样在计算减半情况下仍提升1.1 pp，验证了去噪与保真双重效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开车载数据集验证，未测试更高分辨率激光或非车载场景；远距体素保留依赖手工设计的保留分数，对不同传感器配置敏感。此外，稀疏卷积-Transformer混合结构在嵌入式GPU上延迟仍有47 ms，尚未满足实时车规级要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的体素保留策略与硬件友好的稀疏注意力，以进一步压缩延迟；同时引入时序多帧信息，提升动态遮挡与极端天气下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、3D感知在噪声环境下的鲁棒性，或稀疏卷积与Transformer的高效结合，本文提供的残差式跨模态注意力与保结构采样策略可直接迁移到其它3D任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>