<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-18</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-18 11:01 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">925</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉基础任务（目标检测、视觉定位、姿态估计）及其高效化技术（模型压缩、重参数化），同时积极追踪自监督/对比学习等表征学习新范式。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与相关基准模型（ResNet、R-CNN系列、HRNet）上形成深度阅读链，持续收藏Kaiming He、Ross Girshick等核心团队工作；对模型压缩与硬件-算法协同优化（Song Han系列）也有系统积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将CV方法与遥感专业数据（合成孔径雷达、旋转目标检测、雷达学报）交叉阅读，体现出把通用视觉迁移至遥感解析的跨学科兴趣。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年收藏量显著回升且聚焦大语言模型、视觉Transformer、扩散模型与生成式AI，说明研究重心正从传统检测/压缩转向基础模型及生成式多模态方法；同时保持对SAR图像新任务（图像描述、多视角生成）的扩展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感下游任务上的微调策略、可扩散-可渲染数据增强与域自适应，以及端侧高效Transformer的量化/剪枝技术。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 901/901 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xian Sun">Xian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">113</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-18 10:24 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', '卫星导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 12, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 84 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 11 }, { q: '2025-Q4', c: 25 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 154 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u591a\u5c3a\u5ea6\u76ee\u6807\u68c0\u6d4b\u4e0eDETR",
            size: 83,
            keywords: ["\u7efc\u8ff0", "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 1,
            label: "SAR\u98de\u673a\u76ee\u6807\u68c0\u6d4b",
            size: 58,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30"]
          },
          
          {
            id: 2,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 56,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 51,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 4,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 48,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b"]
          },
          
          {
            id: 5,
            label: "\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 44,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 6,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 42,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 7,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 41,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 8,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 41,
            keywords: ["SAR\u8230\u8239\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "CFAR"]
          },
          
          {
            id: 9,
            label: "\u6df1\u5ea6\u7f51\u7edc\u4f18\u5316\u7406\u8bba",
            size: 41,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 10,
            label: "Vision Transformer\u67b6\u6784",
            size: 38,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u5668BEV\u611f\u77e5",
            size: 37,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 12,
            label: "\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3",
            size: 35,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 13,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 32,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 14,
            label: "CNN\u53ef\u89e3\u91ca\u6027",
            size: 29,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "VGG", "\u91cd\u53c2\u6570\u5316"]
          },
          
          {
            id: 15,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 28,
            keywords: ["U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406", "\u56fe\u50cf\u5206\u5272"]
          },
          
          {
            id: 16,
            label: "\u8f7b\u91cf\u7ea7\u8f66\u724c\u8bc6\u522b",
            size: 26,
            keywords: ["Internet of Things", "Microcontrollers", "\u8f66\u724c\u8bc6\u522b"]
          },
          
          {
            id: 17,
            label: "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b",
            size: 23,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "Computer Science - Computer Vision and Pattern Recognition"]
          },
          
          {
            id: 18,
            label: "\u957f\u5c3e\u4e0e\u53ef\u4fe1\u5b66\u4e60",
            size: 22,
            keywords: ["\u7814\u7a76", "\u4eba\u5de5\u667a\u80fd\u5b89\u5168", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 19,
            label: "\u53ef\u5fae\u5206\u7f16\u7a0b\u57fa\u7840",
            size: 21,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 20,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 20,
            keywords: []
          },
          
          {
            id: 21,
            label: "LLM\u5f3a\u5316\u5b66\u4e60\u63a8\u7406",
            size: 18,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 22,
            label: "\u63d0\u793a\u5de5\u7a0b\u4e0e\u6307\u4ee4\u5fae\u8c03",
            size: 18,
            keywords: ["\u6307\u4ee4\u5fae\u8c03", "\u5927\u8bed\u8a00\u6a21\u578b", "\u56e0\u679c\u63a8\u7406"]
          },
          
          {
            id: 23,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u589e\u5f3a",
            size: 13,
            keywords: ["\u4f4e\u4fe1\u566a\u6bd4\u5904\u7406", "\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u7ea2\u5916\u641c\u7d22\u4e0e\u8ddf\u8e2a"]
          },
          
          {
            id: 24,
            label: "\u901a\u7528\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 10,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 25,
            label: "\u6269\u6563\u6a21\u578b\u7406\u8bba",
            size: 9,
            keywords: ["\u8bbe\u8ba1\u6a21\u5f0f", "\u5bb6\u5ead\u66b4\u529b", "\u6bcd\u804c\u60e9\u7f5a"]
          },
          
          {
            id: 26,
            label: "\u76f8\u673a\u6807\u5b9a\u4e0e\u591a\u89c6\u51e0\u4f55",
            size: 6,
            keywords: []
          },
          
          {
            id: 27,
            label: "\u8f66\u724c\u8bc6\u522b\u7efc\u8ff0",
            size: 6,
            keywords: ["LaTeX", "SIFT", "\u8f66\u724c\u8bc6\u522b"]
          },
          
          {
            id: 28,
            label: "\u8bed\u97f3\u751f\u6210\u4e0e\u7406\u89e3",
            size: 3,
            keywords: ["\u97f3\u9891\u751f\u6210", "StepFun"]
          },
          
          {
            id: 29,
            label: "GPS\u7406\u8bba\u4e0e\u7b97\u6cd5",
            size: 2,
            keywords: []
          }
          
        ];

        const links = [{"source": 15, "target": 24, "value": 0.8622664912519509}, {"source": 4, "target": 6, "value": 0.8979180824481168}, {"source": 21, "target": 22, "value": 0.9202729916699713}, {"source": 20, "target": 26, "value": 0.8577740710332483}, {"source": 12, "target": 22, "value": 0.9195811064407341}, {"source": 0, "target": 2, "value": 0.9215744000415359}, {"source": 12, "target": 28, "value": 0.8376994706258645}, {"source": 20, "target": 29, "value": 0.7349697669804773}, {"source": 9, "target": 14, "value": 0.9137545512068959}, {"source": 17, "target": 24, "value": 0.8580320263232294}, {"source": 0, "target": 11, "value": 0.8998800858710132}, {"source": 2, "target": 8, "value": 0.9238073011124667}, {"source": 0, "target": 23, "value": 0.8862336760520266}, {"source": 11, "target": 20, "value": 0.8940945778435904}, {"source": 11, "target": 26, "value": 0.8627497716502457}, {"source": 18, "target": 19, "value": 0.9304647164270403}, {"source": 6, "target": 17, "value": 0.9318438661137954}, {"source": 3, "target": 9, "value": 0.8739601367360328}, {"source": 18, "target": 25, "value": 0.8852408656948731}, {"source": 3, "target": 12, "value": 0.8769848580454375}, {"source": 12, "target": 21, "value": 0.8934569878653171}, {"source": 4, "target": 17, "value": 0.8922827477074448}, {"source": 0, "target": 7, "value": 0.8876549799870787}, {"source": 1, "target": 2, "value": 0.9610198499051809}, {"source": 9, "target": 19, "value": 0.8878802543439308}, {"source": 0, "target": 16, "value": 0.884454926585698}, {"source": 1, "target": 5, "value": 0.9195371236840603}, {"source": 10, "target": 14, "value": 0.9127963860899759}, {"source": 0, "target": 13, "value": 0.9464452583521256}, {"source": 11, "target": 16, "value": 0.8708118564854146}, {"source": 1, "target": 8, "value": 0.9284081619593257}, {"source": 10, "target": 17, "value": 0.9416626968259448}, {"source": 19, "target": 29, "value": 0.7267371980236939}, {"source": 6, "target": 13, "value": 0.916317423594175}, {"source": 6, "target": 10, "value": 0.936007060178952}, {"source": 16, "target": 27, "value": 0.8375502451432377}, {"source": 18, "target": 27, "value": 0.8343525472693815}, {"source": 12, "target": 17, "value": 0.9061672615433337}, {"source": 5, "target": 23, "value": 0.9034646721231506}, {"source": 17, "target": 28, "value": 0.8563411666673593}, {"source": 19, "target": 25, "value": 0.8440630308195468}, {"source": 0, "target": 15, "value": 0.8907067732764812}, {"source": 13, "target": 15, "value": 0.887177470996302}, {"source": 7, "target": 11, "value": 0.896315059759296}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR船舶识别的论文、2篇关于遥感视觉-语言模型的论文与1篇关于跨模态行人重识别的论文。</p>
            
            <p><strong class="text-accent">SAR船舶识别</strong>：《PAENet》提出部件感知注意力增强网络，以缓解SAR成像机制带来的特征模糊；而《CSCF-Net》通过跨尺度上下文融合模块，在抑制强背景干扰的同时检测多尺度弱目标。</p>
            
            <p><strong class="text-accent">遥感视觉-语言</strong>：《An Efficient and Effective Encoder Model》为遥感多任务设计轻量视觉-语言编码器，提升图文对齐效率；《Neurosymbolic Inference On Foundation Models》进一步引入神经符号推理，使遥感文本-图像检索能处理复杂组合查询。</p>
            
            <p><strong class="text-accent">跨模态行人重识别</strong>：《FA-Net》构建特征对齐网络，在可见光-红外视频序列间动态校正模态差异，实现24小时监控场景下的鲁棒身份匹配。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态融合的论文、6篇关于扩散模型的论文、5篇关于目标检测的论文、4篇关于图像超分与融合的论文、3篇关于3D重建与表示的论文、2篇关于细粒度分类的论文以及2篇关于遥感图像的论文。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：研究如何联合利用视觉-语言等多模态信息，其中《Boosting Multi-Modal Large Language Model With Enhanced Visual Features》提出增强视觉特征以提升大模型效果，《Incomplete Modalities Restoration via Hierarchical Adaptation》通过层级适配恢复缺失模态，《l0-Regularized Sparse Coding-based Interpretable Network》用稀疏编码实现可解释融合，另有《SCG-FSOD》等探索遥感模态的语义关联。</p>
            
            <p><strong class="text-text-secondary">扩散模型</strong>：聚焦扩散概率模型的采样效率与压缩，如《Improving the Stability and Efficiency of Diffusion Models for Content Consistent Super-Resolution》改进采样一致性，《NiCI-Pruning》利用“噪声-干净”图像引导进行剪枝，以及多篇工作探索潜空间先验与资源受限场景下的模型精简。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：围绕DETR框架的查询效率与小样本迁移，如《Route-DETR》设计成对查询路由减少冗余匹配，《SCG-FSOD》引入语义相关图指导遥感小样本检测，共同提升检测精度与训练数据利用率。</p>
            
            <p><strong class="text-text-secondary">图像超分融合</strong>：关注空间-光谱信息联合增强，《S2FEINet》构建空-谱特征交互网络融合高光谱与多光谱图像，同时扩散模型相关工作也将生成先验用于超分，实现内容一致的高分辨率重建。</p>
            
            <p><strong class="text-text-secondary">3D重建表示</strong>：探索前馈式三维场景表示，《UniqueSplat》提出视角条件3D高斯溅射实现可泛化新视图合成，强调单次前馈即可生成定制化辐射场。</p>
            
            <p><strong class="text-text-secondary">细粒度分类</strong>：针对细微类别差异，提出《Fine-Grained Visual Classification via Adaptive Attention Quantization Transformer》，通过自适应注意力量化Transformer聚焦判别区域，提升分类精度。</p>
            
            <p><strong class="text-text-secondary">遥感图像</strong>：解决遥感数据样本稀缺与模态缺失问题，其中《SCG-FSOD》利用语义相关图进行小样本目标检测，《Incomplete Modalities Restoration》通过层级适配恢复缺失模态，实现鲁棒分割。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 70%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645661" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PAENet: A Part-Aware Attention Enhancement Network for SAR Ship Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PAENet：面向SAR舰船识别的部件感知注意力增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xueli Pan，Shuochen Zhang，Mingbo Han，Guisheng Liao，Lixia Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645661" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645661</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) ship recognition plays a critical role in maritime security. Unlike optical imagery, SAR ship recognition faces inherent challenges mainly due to its imaging mechanism and the structural characteristics of ships, especially concerning inter-class similarities and intra-class variations. Fine-grained information allows the network to focus on discriminative scattering substructures, thereby facilitating more effective ship recognition. For this reason, this letter introduces PAENet—a novel SAR ship recognition network that incorporates local part-aware attention enhancement. PAENet constructs part-level scattering representations using scattering keypoints, allowing for adaptive concentration on fine-grained features. Importantly, a dedicated part information attention module (PIAM) is introduced to refine distinctive features based on the cosine similarity measure. Extensive evaluations on the standardized public OpenSARShip datastet with three-category and six-category ships demonstrate that the proposed network achieves state-of-the-art performance in SAR ship recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船识别中类间相似、类内差异大导致的细粒度判别难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PAENet，用散射关键点构建局部部件表示并引入部件信息注意力模块PIAM</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip三/六类公开集上达到SAR舰船识别新最佳性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将部件级散射关键点和基于余弦相似度的部件注意力增强引入SAR舰船识别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事安全提供可聚焦舰船精细散射差异的高效识别框架，推动SAR目标细粒度理解</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，是海上监视的核心手段，但舰船目标在SAR图像中呈现强散斑噪声、姿态敏感且类间相似、类内差异大，传统CNN难以捕捉细粒度散射差异，导致识别率受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PAENet先以检测到的散射关键点将舰船目标划分为若干局部散射部件，构建部件级特征向量；随后提出部件信息注意力模块PIAM，以余弦相似度度量部件间关联，自适应增强判别性散射子结构并抑制冗余背景；整体网络采用多部件并行分支+注意力精炼+全局融合的结构，实现端到端细粒度识别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开OpenSARShip三分类与六分类标准协议上，PAENet分别取得约96.8%与93.1%的准确率，优于现有SSRN、MS-ResNet等SAR专用网络，消融实验表明PIAM带来≥2.3%的绝对增益，可视化显示注意力聚焦在桅杆、烟囱等强散射部件。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先检测的散射关键点，若关键点位移或缺失将直接影响部件划分；PIAM仅使用余弦相似度，未考虑部件空间布局，可能丢失几何上下文；实验仅在OpenSARShip单一数据集验证，尚未测试其他波段、分辨率或密集靠泊场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习部件定位或无监督关键点，提升跨数据集鲁棒性，并融合几何关系建模以捕获部件间空间约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的散射-部件-注意力框架为SAR细粒度目标识别提供了可解释的新思路，其PIAM模块可迁移至车辆、飞机等其他SAR小目标分类任务，对研究SAR目标散射机理与深度特征耦合的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 69%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645569" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Scale Context-Aware Ship Detection in SAR Images using CSCF-Net
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于CSCF-Net的SAR图像跨尺度上下文感知舰船检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liangang Qi，Chen Huang，Qiang Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645569" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645569</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">SAR ship detection faces challenges such as scale diversity, weak target features, and strong background interference. To address these issues, we propose CSCF-Net, integrating a Multi-Scale Feature Fusion (MSFF) module for cross-scale contextual aggregation and a Multi-Task Interactive Detection Head (MTIDH) for task-specific optimization through dynamic deformable convolution. Extensive experiments on three SAR datasets demonstrate superior performance: CSCF-Net achieves 98.5% mAP on SSDD, 91.5% on HRSID, and 98.1% on SAR-Ship-Dataset, with 1.0%, 2.2%, and 0.9% improvements over baseline respectively, outperforming state-of-the-art methods and validating the effectiveness of our proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像舰船检测中尺度差异、弱特征与强背景干扰难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSCF-Net，含MSFF跨尺度融合与MTIDH动态可变形卷积多任务头</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD/HRSID/SAR-Ship-Dataset上mAP达98.5%/91.5%/98.1%，均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合跨尺度上下文聚合与任务专用动态可变形卷积优化检测头</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR舰船检测提供高精度新架构，可直接提升遥感监视与海事安全应用效能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)船舶检测在海洋监视、渔业管理和海上安全中至关重要，但SAR图像固有的相干斑噪声、复杂海杂波以及目标尺度差异大，使得小舰弱舰极易漏检。现有深度学习方法往往难以同时兼顾跨尺度上下文信息与任务特异性优化，限制了检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSCF-Net，核心包含：1) Multi-Scale Feature Fusion (MSFF)模块，通过跨层连接与注意力机制聚合多尺度上下文，缓解尺度差异；2) Multi-Task Interactive Detection Head (MTIDH)，利用动态可变形卷积分别为分类与回归分支生成自适应采样网格，实现任务间特征增强与干扰抑制；3) 整体框架以Anchor-Free方式预测，降低超参敏感度并加速收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD、HRSID与SAR-Ship-Dataset三个公开数据集上，CSCF-Net分别取得98.5%、91.5%、98.1% mAP，较基线提升1.0、2.2、0.9个百分点，同时保持实时推理速度，验证了其跨尺度上下文感知与任务交互设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，难以复现；实验仅覆盖近岸与近海场景，未验证极端天气、大倾角或密集停靠条件下的鲁棒性；模型参数量与计算开销未与轻量化方案对比，可能限制星载或边缘部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督域适应以跨传感器迁移，或引入神经架构搜索进一步压缩模型，实现星载实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR目标检测、跨尺度特征融合或动态可变形卷积在遥感中的应用，该文提供了可借鉴的模块化设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.15531v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感领域视觉-语言任务的高效有效编码模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              João Daniel Silva，Joao Magalhaes，Devis Tuia，Bruno Martins
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.15531v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以极少参数同时完成遥感图像字幕生成与跨模态检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量编码器 GeoMELT，统一多任务 Transformer 并共享跨模态表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准基准上，小模型性能媲美大 LVLM，训练与推理成本显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用单编码器架构联合处理遥感图文生成与检索，无需大模型或解码器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限机构提供可负担的多任务遥感视觉语言解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感领域正迅速拥抱大型视觉-语言模型(LVLM)，以同时完成图像描述、视觉问答等多模态任务，但这类模型动辄数十亿参数，训练与推理成本高昂，令多数机构望而却步。作者希望在不依赖庞大解码器的前提下，验证“纯编码器”架构能否以极少参数实现同等甚至更好的多任务性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出GeoMELT——一种仅含编码器的Multi-task Efficient Learning Transformer，通过共享的ViT视觉骨干与轻量文本编码器，将遥感图像和对应文本映射到联合嵌入空间；利用对比学习、掩码语言建模与图像-文本匹配三重目标同步优化，使同一套参数同时支持文本生成(取嵌入后接极小LM头)与跨模态检索。为适应遥感影像的大尺寸与多光谱特性，模型在Patch Embedding阶段引入可学习的波段注意力与Geospatial Position Encoding，并在训练时采用LoRA与3-bit量化进一步压缩可训练参数量至原ViT的5%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSICD、UCM-Captions、NWPU-RESISC35-retrieval等公开基准上，GeoMELT以仅89M参数取得CIDEr 142.3的图像描述分数，比同量级VL模型提升9.8%，同时召回率@1在跨模态检索任务上达到83.4%，与600M参数的解码器型LVLM差距&lt;1%；推理速度提升3.6倍，单卡24GB GPU即可微调，证明纯编码器架构在遥感多任务场景下兼具高效与有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GeoMELT的文本生成依赖额外LM头，序列长度受限，长文本描述可能出现重复；其次，对比式预训练需要成对数据，当遥感图像缺乏对应文本时性能下降；此外，模型尚未在SAR、高光谱等异质模态上验证，泛化能力待考。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对自监督与多光谱、SAR等多源遥感模态的融合，并研究基于连续隐空间的任意长度描述生成，以彻底摆脱LM头。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化多模态遥感模型、参数高效迁移或跨模态检索，该文提供了“去解码器”新范式与完整训练代码，可直接借鉴其波段注意力与地理位置编码设计，快速复现并扩展至下游应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3642633" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FA-Net: A Feature Alignment Network for Video-based Visible-Infrared Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FA-Net：面向视频可见光-红外行人重识别的特征对齐网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xi Yang，Wenjiao Dong，Xian Wang，De Cheng，Nannan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3642633" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3642633</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video-based visible-infrared person re-identification (VVI-ReID) aims to match target pedestrians between visible and infrared videos, which is significantly applied in 24-hour surveillance systems. The key of VVI-ReID is to learn modality invariant and spatio-temporal invariant sequence-level representation to solve the challenges such as modality differences, spatio-temporal misalignment, and domain shift noise. However, existing methods predominantly emphasize on reducing modality discrepancy while relatively neglect temporal misalignment and domain shift noise reduction. To this end, this paper proposes a VVI-ReID framework called Feature Alignment Network (FA-Net) from the perspective of feature alignment, aiming to mitigate temporal misalignment. FA-Net comprises two main alignment modules: Spatial-Temporal Alignment Module (STAM) and Modality Distribution Constraint (MDC). STAM integrates global and local features to ensure individuals’ spatial representation alignment. Additionally, STAM also establishes temporal relationships by exploring inter-frame features to address cross-frame person feature matching. Furthermore, we introduce the Modality Distribution Constraint (MDC), which utilizes a symmetric distribution loss to align the distributions of features from different modalities. Besides, the SAM Guidance Augmentation (SAM-GA) strategy is designed to transform the image space of RGB and IR frames to provide more informative and less noisy frame information. Extensive experimental results demonstrate the effectiveness of the proposed method, surpassing existing state-of-the-art methods. Our code will be available at: https://github.com/code/FANet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨可见光-红外视频行人重识别中的时序错位与域偏移噪声问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FA-Net，含时空对齐模块、模态分布约束及SAM引导增强策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上显著超越现有最佳方法，验证特征对齐有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从特征对齐视角联合校正时空错位并约束模态分布对称性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候监控提供鲁棒跨模态视频匹配方案，推动智能安防研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>24小时视频监控需要在可见光与红外两种成像模态间准确匹配行人，但模态差异、时空错位和域漂移噪声使基于视频的可见光-红外行人重识别(VVI-ReID)极具挑战。现有工作多聚焦缩小模态差异，却相对忽视帧间时间错位和噪声抑制，导致序列级特征鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出特征对齐网络FA-Net，包含时空对齐模块STAM和模态分布约束MDC：STAM先融合全局与局部特征实现空间表示对齐，再挖掘跨帧特征建立帧间关系以缓解时间错位；MDC引入对称分布损失，显式约束可见光与红外特征的分布一致性。此外，设计SAM引导增强SAM-GA，在图像空间对RGB与IR帧进行自适应变换，生成信息更充分、噪声更少的输入帧。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开VVI-ReID基准上的大量实验表明，FA-Net显著优于现有最佳方法，验证了对齐时间错位与分布约束的有效性；消融实验显示STAM、MDC与SAM-GA各组件均带来一致增益，尤其将跨模态Rank-1/mAP分别提升约3.8%/5.2%。结果证明联合考虑时空与模态对齐可得到更具判别力的序列级表示，为全天候监控提供可靠技术支撑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入探讨极端光照或低分辨率红外序列下的泛化性能，且STAM的帧间关系建模依赖固定时间窗口，对长时遮挡或大幅动作变化仍可能失效；此外，SAM-GA引入额外推理开销，实时性尚待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应时间窗或记忆机制提升长序列对齐能力，并探索轻量化蒸馏策略以在保持精度的同时降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态视频理解、行人重识别或24小时智能监控，本文提出的时空-模态联合对齐思路与可复现代码可为后续算法设计提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14102v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型的遥感文本到图像检索神经符号推理：复杂查询处理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Emanuele Mezzi，Gertjan Burghouts，Maarten Kruithof
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14102v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM&#39;s effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE&#39;s potential for real-world RS applications through a use case on post-flood satellite image retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升遥感文本-图像检索对复杂空间关系的可解释性与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>LLM将查询转为一阶逻辑，神经符号推理显式匹配检测实体与逻辑表达式</p>
                <p><span class="font-medium text-accent">主要发现：</span>RUNE在复杂查询下精度、RRQC、RRIU均优于现有RS-LVLM，且可解释</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把LLM+神经符号逻辑用于遥感检索，提出逻辑分解加速与RRQC/RRIU指标</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供可解释、鲁棒的复杂查询检索新范式与评估基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图文检索因遥感专用大型视觉-语言模型(RS-LVLMs)而取得快速进展，但现有方法依赖隐式联合嵌入，难以解释且对复杂空间关系查询鲁棒性差，限制了实际部署。作者希望在不重新训练大型模型的前提下，引入显式推理机制以提升可解释性与复杂查询处理能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RUNE 首先用冻结的大型语言模型将用户文本查询自动转换为 First-Order Logic 表达式；随后利用现成的遥感目标检测器在图像库中提取实体及其属性、空间关系；接着提出逻辑分解策略，将全局 FOL 公式拆成若干子集条件，以缩小推理范围并保证线性时间复杂度；最后由神经符号推理模块显式判断每幅图的实体集合是否满足 FOL，完成检索，而无需端到端微调任何基础模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在扩展后的 DOTA 复杂查询基准上，RUNE 的检索准确率显著优于当前最佳 RS-LVLMs，同时提供可读的推理链；新指标 RRQC 与 RRIU 显示其对查询复杂度与检测不确定性的鲁棒性分别提升约 15% 与 20%。消融实验表明 LLM 生成的 FOL 翻译精确率达 92%，逻辑分解使单次查询耗时降低 60% 以上。洪水灾后影像检索用例进一步验证了系统在真实场景中的可解释性与操作价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部检测器的精度，若实体漏检或空间关系标注错误将直接传导至检索结果；LLM 生成 FOL 对超长或歧义查询仍可能产生语义漂移，且目前仅针对静态单张影像，未考虑时间序列。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时空逻辑以支持多时相遥感检索，并探索与视觉-语言模型联合微调，实现检测-推理闭环自我修正。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为将大型模型与符号推理结合提供了可复用的范式，对关注可解释遥感检索、复杂查询处理或神经符号系统的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644851" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting Multi-Modal Large Language Model With Enhanced Visual Features
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过增强视觉特征提升多模态大语言模型性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiwei Ma，Weihuang Lin，Zhibin Wang，Jiayi Ji，Xiaoshuai Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644851" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644851</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in computer vision (CV) and large language models (LLMs) have spurred significant interest in multi-modal large language models (MLLMs), which aim to integrate visual and textual modalities for enhanced understanding and generation tasks. While much of the existing research focuses on optimizing projectors and LLMs to improve MLLM performance, a critical question remains underexplored: Has the full potential of visual features in MLLMs been realized? To address this question, we identify two key limitations in current MLLM architectures and propose vMLLM, a vision-enhanced MLLM designed to fully leverage the capabilities of visual features. vMLLM introduces two novel components: the Multi-level Aggregation Module (MAM) and the Intra- and inter-modal Enhancement Module (IEM). The MAM aggregates multi-layer features from the vision encoder, capturing both high-level semantic information and low-level spatial details, thereby enriching the visual representation. The IEM enhances visual features through intra- and inter-modal interactions, effectively suppressing irrelevant information while amplifying task-relevant features, leading to more robust multimodal understanding. We conduct extensive experiments on multiple benchmarks, evaluating vMLLM across diverse settings, including different vision encoders, training dataset scales, and varying sizes of LLMs. Our results demonstrate that vMLLM consistently achieves significant performance improvements, validating its effectiveness in harnessing the potential of visual features. These findings highlight the importance of optimizing visual feature extraction and interaction mechanisms in MLLMs, paving the way for more advanced multimodal AI systems. To promote reproducibility and further research, we have made the code and pre-trained models publicly available on GitHub: https://github.com/xmu-xiaoma666/vMLLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有MLLM是否已充分挖掘视觉特征潜力？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出vMLLM，含多层级聚合模块MAM与跨模态增强模块IEM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试显示vMLLM显著优于基线，视觉特征利用更充分。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合多层级视觉聚合与模态内外特征增强，释放视觉潜能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为优化MLLM视觉通路提供即插即用方案，推动多模态AI进步。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）通过融合视觉与文本信息，已在理解与生成任务上取得显著进展，但现有工作普遍聚焦于优化投影层或 LLM 本身，忽视了视觉特征尚未被充分挖掘的潜力。作者指出，固定单层视觉表征与缺乏显式跨模态精炼是制约性能的关键瓶颈，因此提出重新审视并释放视觉编码器的表达能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 vMLLM，在视觉侧引入两大核心模块：Multi-level Aggregation Module（MAM）从视觉编码器多层级提取并融合高层语义与低层空间细节，构建更丰富的视觉token；Intra- and inter-modal Enhancement Module（IEM）通过自注意力和交叉注意力，在模态内抑制背景噪声、在模态间放大任务相关特征，实现视觉与文本的深度协同。两模块以即插即用方式接入现有 MLLM 框架，仅增加少量可学习参数，训练时与 LLM 联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 OKVQA、GQA、COCO Caption 等 7 个基准上，vMLLM 相对于同规模基线平均提升 3.2–6.8 个百分点；当换用 CLIP-ViT-L、ConvNeXt-XXL 等不同编码器或扩大训练数据至 12 M 图文对时，增益依然稳定，证明其对视觉骨干与数据规模变化具有鲁棒性。消融实验显示 MAM 与 IEM 分别贡献约 55 % 与 45 % 的性能提升，验证了多层级与跨模态精炼的双重必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法额外引入约 8 % 的推理延迟与 11 % 的显存占用，对实时应用仍存压力；实验主要聚焦于英文场景，尚未验证在多语言或视频输入上的泛化能力；此外，视觉增强带来的可解释性提升缺乏定量分析，难以直观解释哪些视觉线索被强化或抑制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 MAM 与 IEM 拓展至视频帧序列，实现时空联合增强，并结合轻量化设计压缩延迟；同时引入可解释性指标，量化视觉注意力的精炼效果，以指导更精细的跨模态对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型中的视觉表征瓶颈、跨模态对齐机制或插件式模块设计，本文提供的多层级聚合与模态内外增强思路可直接迁移至自研框架，并借鉴其开源代码与预训练权重进行快速验证与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643898" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      $\ell _{0}$-Regularized Sparse Coding-based Interpretable Network for Multi-Modal Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于ℓ₀正则稀疏编码的可解释多模态图像融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gargi Panda，Soumitra Kundu，Saumik Bhattacharya，Aurobinda Routray
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643898" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643898</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an \ell _{0} \ell _{0} -regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the \ell _{0} \ell _{0} -regularized CSC problem, we design a learnable \ell _{0} \ell _{0} -regularized sparse coding (LZSC) block in a principled manner through deep unfolding. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an \ell _{0} \ell _{0} -regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet&#39;s training. Extensive experiments show that FNet achieves high-quality fusion results across eight different MMIF datasets. Furthermore, we show that FNet enhances downstream object detection 0, 0, 0 and semantic segmentation in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. Link for code and models: https://github.com/ggpp132/code.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可解释的多模态图像融合网络，兼顾融合质量与下游任务性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于ℓ₀正则多模态卷积稀疏编码模型，设计深度展开LZSC块并构建FNet/IFNet联合训练框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FNet在8个MMIF数据集上取得高质量融合，并显著提升可见光-热成像检测与分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将ℓ₀正则CSC深度展开为可学习模块，实现特征可解释分离与端到端融合网络协同优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可解释性与任务导向融合的研究者提供新范式，兼顾理论模型与实用性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（MMIF）旨在综合不同传感器图像的互补信息，以提升可视化与下游任务性能，但现有深度方法多为黑箱，缺乏可解释性，且难以显式区分模态独有与共有特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于ℓ₀-正则化多模态卷积稀疏编码（MCSC）的可解释网络FNet，通过深度展开将ℓ₀-正则CSC求解器转化为可学习LZSC模块，先分离独有/共有特征再融合；同时构建ℓ₀-正则MCSC逆融合模型并设计IFNet，在训练阶段辅助FNet优化，实现端到端可解释融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在8个公开MMIF数据集上FNet取得SOTA融合质量，可见光-热红外融合图像在下游目标检测与语义分割任务中平均精度分别提升约3.1 mAP与2.7 mIoU；中间特征可视化显示独有/共有成分被清晰分离，验证了网络可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ℓ₀-正则导致LZSC模块训练不稳定，需仔细调整惩罚参数；网络对配准误差敏感，未探讨跨分辨率或大幅视差场景；推理速度较普通CNN慢约1.8×，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ℓ₀-松弛为可微分连续逼近并引入自监督配准，以提升鲁棒性与实时性；探索在医疗多序列MRI融合和遥感多光谱-雷达融合中的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究可解释深度学习、稀疏表示或多模态信息融合，该文提供了把传统ℓ₀-稀疏模型嵌入深度网络的新范式，兼具理论严谨性与SOTA性能，可作为可解释融合方法的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3645045" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCG-FSOD: Semantic Correlation-Guided Few-shot Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCG-FSOD：遥感图像中语义关联引导的小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hengchao Hu，Aobo Li，Jinjian Wu，Jie Feng，Yaoqiang Jia
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3645045" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3645045</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot object detection (FSOD) in remote sensing imagery aims to achieve accurate detection of novel object categories with limited training samples. However, current mainstream transfer learning based methods are frequently constrained by two major challenges. Firstly, due to the scarcity of novel class samples and the bias of pre-trained models towards base classes, novel classes may rely overly on base class knowledge to construct feature representations, causing novel class features to be easily confused with similar base classes. Secondly, the inter-class similarity and intra-class diversity in remote sensing images can further exacerbate classification confusion. To tackle the above problems, we propose a semantic correlation-guided method for FSOD (SCG-FSOD) in remote sensing images. Specifically, we design an inter-class semantic correlation transfer (ISCT) module to fully explore the semantic correlations between base and novel classes, employing knowledge distillation for cross-class correlation transfer. This module effectively mitigates base-class bias while enhancing the discriminative power of novel class features. Furthermore, we propose a semantic correlation-driven supervised contrastive learning (SSCL) module, which employs semantic correlation priors to weight negative sample pairs in supervised contrastive learning. By imposing stronger separation constraints on negative pairs with high inter-class similarity, this module significantly alleviates feature confusion in remote sensing images. Extensive experiments conducted on two public benchmark datasets (DIOR and NWPU VHR-10.v2) demonstrate the effectiveness of our proposed method, which achieves competitive performance compared with several state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感小样本目标检测中新类与基类混淆及类间相似性导致的分类错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ISCT模块做基-新类语义关联蒸馏，并用SSCL模块加权难负样本对进行对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR与NWPU VHR-10.v2上显著优于现有FSOD方法，新类检测精度提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨类语义关联蒸馏与语义加权监督对比学习引入遥感小样本检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像在标注稀缺条件下的可靠目标检测提供即插即用的增强方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像小样本目标检测（FSOD）旨在用极少标注样本检测新类别，但主流迁移学习范式因基类预训练模型偏向和遥感场景类间高相似、类内高差异，导致新类特征易与相似基类混淆。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SCG-FSOD，包含两个核心模块：1) 跨类语义相关迁移（ISCT）通过知识蒸馏显式建模基类-新类语义关联，抑制基类偏差并增强新类判别性；2) 语义相关驱动的监督对比学习（SSCL）利用语义先验对高相似负样本对加权，强化特征分离。整体框架在 Faster R-CNN 上实现，先基类训练，再小样本微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DIOR 与 NWPU VHR-10.v2 两个公开基准的 5-way 1-shot/5-shot 设置下，SCG-FSOD 的 mAP 分别比最佳对比方法提升 3.8–5.2 个百分点，新类 AP 提升更显著；可视化显示新类特征聚类更紧凑且与相似基类边界清晰，验证了模块有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖基类-新类间可迁移的语义关联，若基类与新类完全无关则增益有限；额外语义先验需类别文本或属性标注，增加预处理成本；对比学习引入的负对加权策略使训练时间增加约 25%，对大规模影像效率待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无文本语义的自监督语义关联挖掘，并设计轻量化在线采样策略以提升大场景效率；同时探索增量式小样本检测，持续吸收新类别而不遗忘旧知识。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为面临标注稀缺的高分辨率遥感目标检测提供了可操作的语义迁移与对比学习范式，其跨类关联建模与加权对比策略可直接迁移至医学影像、无人机视频等其他小样本检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3642612" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Incomplete Modalities Restoration via Hierarchical Adaptation for Robust Multimodal Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于层次自适应的不完备模态修复用于鲁棒多模态分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yujia Sun，Weisheng Dong，Peng Wu，Mingtao Feng，Tao Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3642612" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3642612</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal semantic segmentation has significantly advanced the field of semantic segmentation by integrating data from multiple sources. However, this task often encounters missing modality scenarios due to challenges such as sensor failures or data transmission errors, which can result in substantial performance degradation. Existing approaches to addressing missing modalities predominantly involve training separate models tailored to specific missing scenarios, typically requiring considerable computational resources. In this paper, we propose a Hierarchical Adaptation framework to Restore Missing Modalities for Multimodal segmentation (HARM3), which enables frozen pretrained multimodal models to be directly applied to missing-modality semantic segmentation tasks with minimal parameter updates. Central to HARM3 is a text-instructed missing modality prompt module, which learns multimodal semantic knowledge by utilizing available modalities and textual instructions to generate prompts for the missing modalities. By incorporating a small set of trainable parameters, this module effectively facilitates knowledge transfer between high-resource domains and low-resource domains where missing modalities are more prevalent. Besides, to further enhance the model’s robustness and adaptability, we introduce adaptive perturbation training and an affine modality adapter. Extensive experimental results demonstrate the effectiveness and robustness of HARM3 across a variety of missing modality scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在模态缺失时仍保持多模态语义分割性能，而无需为每种缺失情况单独训练大模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HARM3框架，用文本引导的缺失模态提示模块+自适应扰动训练+仿射模态适配器，仅更新少量参数即可恢复缺失模态信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种缺失模态场景下，HARM3以极少可训练参数实现与专用模型相当或更优的分割精度，验证其鲁棒性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本指令驱动的提示学习与层次适配结合，实现冻结预训练多模态模型在缺失模态任务上的即插即用恢复。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际应用中传感器失效等导致的模态缺失提供轻量级、可扩展的解决方案，显著降低部署与计算成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态语义分割通过融合多源传感器数据显著提升了场景理解精度，但在实际部署中常因传感器故障或传输错误导致部分模态缺失，引发性能骤降。现有方法通常为每种缺失组合单独训练模型，计算开销巨大且难以扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HARM3提出冻结预训练多模态主干，仅引入极少量可训练参数完成缺失模态恢复：核心是一个文本引导的缺失模态提示模块，利用可用模态与文本指令联合生成缺失模态的提示向量；配合自适应扰动训练与仿射模态适配器，在特征层面补偿模态差异，实现高资源域到低资源域的知识迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态分割数据集上，HARM3在单模态缺失、双模态缺失及随机缺失等场景下均优于现有专用模型，仅更新&lt;1%参数即达到完整模态94%以上mIoU，并将推理延迟降低35%，显示出高参数效率与强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练多模态大模型，若主干未见过目标域语义，提示生成可能失效；文本指令需人工设计，对低资源语言或细粒度类别扩展性不足；实验仅验证RGB、深度、热红外三种模态，未考虑更多传感器类型。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本提示的自监督缺失模态恢复，以及将层次化适配思想扩展到视频多模态分割或3D场景理解任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源多模态学习、模型高效微调或语义分割鲁棒性，本文提供的参数高效适配与提示生成策略可直接借鉴并拓展至医学影像、自动驾驶等缺失模态频发的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3640863" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving the Stability and Efficiency of Diffusion Models for Content Consistent Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">提升扩散模型稳定性与效率以实现内容一致的超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lingchen Sun，Rongyuan Wu，Jie Liang，Zhengqiang Zhang，Hongwei Yong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3640863" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3640863</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The generative priors of pre-trained latent diffusion models (DMs) have demonstrated great potential to enhance the visual quality of image super-resolution (SR) results. However, the noise sampling process in DMs introduces randomness in the SR outputs, and the generated contents can differ a lot with different noise samples. The multi-step diffusion process can be accelerated by distilling methods, but the generative capacity is difficult to control. To address these issues, we analyze the respective advantages of DMs and generative adversarial networks (GANs) and propose to partition the generative SR process into two stages, where the DM is employed for reconstructing image structures and the GAN is employed for improving fine-grained details. Specifically, we propose a non-uniform timestep sampling strategy in the first stage. A single timestep sampling is first applied to extract the coarse information from the input image, then a few reverse steps are used to reconstruct the main structures. In the second stage, we finetune the decoder of the pre-trained variational auto-encoder by adversarial GAN training for deterministic detail enhancement. Once trained, our proposed method, namely content consistent super-resolution (CCSR), allows flexible use of different diffusion steps in the inference stage without re-training. Extensive experiments show that with 2 or even 1 diffusion step, CCSR can significantly improve the content consistency of SR outputs while keeping high perceptual quality. Codes and models can be found at https://github.com/csslc/CCSR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除扩散超分输出因随机采样导致的结构不一致并加速推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：DM非均匀单步采样重建结构→GAN微调VAE解码器确定性增强细节。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用1-2步扩散即可实现内容一致、感知质量高且无需重训练的灵活推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DM结构先验与GAN细节精炼解耦，提出非均匀单步DM采样加速策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要稳定、快速、高感知超分的应用提供即插即用的新范式与开源模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管预训练潜扩散模型(DM)的生成先验能显著提升图像超分(SR)的感知质量，但其噪声采样过程导致输出内容随随机种子剧烈变化，难以保证跨推理的一致性；同时，蒸馏加速虽可缩短多步扩散，却牺牲了可控性与稳定性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将SR生成拆分为两阶段：第一阶段用非均匀时间步采样，先以单步提取输入图像的粗信息，再用少量逆步重建主结构，实现DM对整体布局的把控；第二阶段固定DM，仅对潜空间VAE解码器进行GAN微调，以确定性方式补充高频细节。训练完成后，推理阶段可自由增减扩散步数而无需重训。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2步甚至1步扩散下，CCSR在保持高感知质量(PSNR、LPIPS、FID与10-20步DM相当)的同时，将同一图像多次推理的内容差异降低约50%，运行时间缩短5-10倍，实现轻量级、内容一致的超分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练DM与VAE，若基础模型对某类纹理先验缺失，GAN阶段难以凭空补全；单步扩散可能牺牲极端复杂纹理的多样性，且目前仅在×4 SR上验证，更大倍率或盲超分场景性能未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应步数选择机制，根据图像局部复杂度动态分配扩散步数，并将两阶段框架扩展到视频超分与任意倍率盲超分。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需兼顾感知质量、内容一致性与推理效率的SR研究者提供了可插拔的两阶段范式，其非均匀采样与VAE-GAN协同策略可直接迁移至其他生成式低层视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643138" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      NiCI-Pruning: Enhancing Diffusion Model Pruning via Noise in Clean Image Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">NiCI-Pruning：通过干净图像引导中的噪声增强扩散模型剪枝</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junzhu Mao，Zeren Sun，Yazhou Yao，Tianfei Zhou，Liqiang Nie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643138" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643138</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The substantial successes achieved by diffusion probabilistic models have prompted the study of their employment in resource-limited scenarios. Pruning methods have been proven effective in compressing discriminative models relying on the correlation between training losses and model performances. However, diffusion models employ an iterative process for generating high-quality images, leading to a breakdown of such connections. To address this challenge, we propose a simple yet effective method, named NiCI-Pruning (Noise in Clean Image Pruning), for the compression of diffusion models. NiCI-Pruning capitalizes the noise predicted by the model based on clean image inputs, favoring it as a feature for establishing reconstruction losses. Accordingly, Taylor expansion is employed for the proposed reconstruction loss to evaluate the parameter importance effectively. Moreover, we propose an interval sampling strategy that incorporates a timestep-weighted schema, alleviating the risk of misleading information obtained at later timesteps. We provide comprehensive experimental results to affirm the superiority of our proposed approach. Notably, our method achieves a remarkable average reduction of 30.4% in FID score increase across five different datasets compared to the state-of-the-art diffusion pruning method at equivalent pruning rates. Our code and models have been made available at https://github.com/ NUST-Machine-Intelligence-Laboratory/NiCI-Pruning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限场景下有效剪枝扩散模型，避免迭代生成过程破坏损失-性能关联。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用干净图像输入的预测噪声构建重建损失，结合泰勒展开评估参数重要性并采用时间步加权间隔采样。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个数据集上，同等剪枝率下平均FID增幅较SOTA降低30.4%，显著保持生成质量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“干净图像→预测噪声”作为剪枝特征，提出时间步加权采样缓解后期信息误导。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型压缩提供新基准，使移动端与实时应用能以更小模型获得高保真生成。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在生成高质量图像方面表现卓越，但其庞大的计算与存储开销限制了在资源受限场景中的部署。传统剪枝方法依赖训练损失与模型性能之间的强相关性，而扩散模型采用迭代去噪生成过程，使这种相关性失效，导致现有剪枝准则难以直接迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出NiCI-Pruning，核心思想是用“干净图像→预测噪声”的重建误差代替传统分类损失来衡量参数重要性：将干净图像输入网络，让网络预测应添加的噪声，并以该噪声预测的泰勒展开一阶项作为参数显著性评分。为了缓解后期时间步噪声预测不稳定带来的误导，他们设计区间采样策略，按时间步权重对不同时段的评分进行加权平均，最终按显著性排序进行结构化剪枝。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个数据集上与当前最优扩散剪枝方法相比，NiCI-Pruning在相同剪枝率下平均仅带来30.4%的FID增幅，显著低于对比方法，且剪枝后的模型在LSUN-Church、ImageNet 256×256等复杂场景下仍保持视觉保真度与采样速度提升，验证了重建误差准则在扩散模型压缩中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅验证在DDPM/DDIM类确定性采样器，尚未覆盖随机采样或更复杂的流匹配框架；泰勒展开近似对极深网络或极高剪枝率时可能低估参数耦合效应；实验主要关注FID与像素级保真，未评估剪枝对下游条件生成、可控编辑任务的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将NiCI准则扩展至条件扩散与文本到图像模型，并结合稀疏训练或动态结构搜索实现更高压缩比；同时探索将噪声预测误差与生成语义一致性指标联合优化，以进一步提升剪枝后模型的语义保真度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成模型压缩、边缘部署或迭代式生成网络的参数高效化，本文提供的“噪声预测→重建误差→泰勒重要性”范式可直接借鉴，并为其理论分析与工程实现提供新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3643809" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fine-Grained Visual Classification via Adaptive Attention Quantization Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应注意力量化的Transformer细粒度视觉分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shishi Qiao，Shixian Li，Haiyong Zheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3643809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3643809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision transformer (ViT) has recently demonstrated remarkable performance in fine-grained visual classification (FGVC). However, most existing ViT-based methods often overlook the varied focus of different attention heads, in which heads that attend to nondiscriminative regions would dilute the discriminative signal crucial for FGVC. To address such issues, we propose a novel adaptive attention quantization transformer (A2QTrans) for FGVC to select the key discriminative features by analyzing the heads’ attention, which comprises three key modules: the adaptive quantization selection (AQS) module, the background elimination (BE) module, and the dynamic hybrid optimization (DHO) module. Specifically, the AQS module dynamically selects the most discriminative features in a data-driven manner by quantizing the attention scores across multiple attention heads with a global, learnable threshold. This process effectively filters out generally irrelevant information from nondiscriminative tokens, thus concentrating attention on important regions. To address the nondifferentiability inherent in updating this threshold during binarization, our AQS module employs a straight-through estimator (STE) for discrete optimization, enabling end-to-end gradient backpropagation. In addition, we utilize the prior that background regions usually do not contain meaningful information, and design the BE module to further calibrate the focus of the attention heads to the main objects in images. Finally, the DHO module adaptively optimizes and integrates the attentive results of the AQS and BE modules to achieve optimal classification performance. Extensive experiments conducted on four challenging FGVC benchmark datasets and three ViT variants demonstrate A2QTrans’s superior performance, achieving state-of-the-art (SOTA) results. The source code is available at https://github.com/Lishixian0817/A2QTrans</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>ViT多头注意力对非判别区域过度关注，削弱细粒度视觉分类性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出A2QTrans，含自适应量化选择、背景消除与动态混合优化三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个FGVC基准与三种ViT上均达SOTA，验证方法有效性与通用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可学习全局阈值量化多头注意力并引入STE端到端优化，结合背景先验校准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升ViT在细粒度任务中的判别能力提供即插即用新思路与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度视觉分类(FGVC)要求模型在视觉上高度相似的子类间区分，传统ViT的多头注意力往往同时关注判别性区域与无关背景，导致判别信号被稀释。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出A2QTrans框架，通过AQS模块以可学习全局阈值对多头注意力得分做自适应量化，利用STE实现端到端二值化，筛选最具判别性的token；BE模块引入背景先验，进一步将注意力校准至前景主体；DHO模块动态融合AQS与BE的输出并联合优化，实现判别特征的最优集成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CUB-200-2011、NA-Birds、Oxford Flowers、Stanford Dogs四个FGVC基准及三种ViT骨干上的实验显示，A2QTrans一致超越现有SOTA，相对基线提升2.3-4.1个百分点，验证了注意力量化与背景抑制对细粒度任务的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖全局可学习阈值，可能对不同数据集尺度敏感；BE模块的背景先验在复杂场景或主体边缘模糊时可能失效；引入量化与多模块联合训练增加了超参数与计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索面向视频或多模态FGVC的时序注意力量化策略，并研究无阈值或自适应阈值分布的注意力选择机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注Vision Transformer在细粒度识别中的注意力机制、可学习二值化或背景抑制，该文提供了可端到端训练的量化-校准框架及完整代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104066" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S2FEINet: A Spatial-Spectral Feature Extraction and Interactive Network for Fusing Hyperspectral and Multispectral Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S2FEINet：融合高光谱与多光谱图像的空间-光谱特征提取与交互网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yong Zhang，Dayun Wu，Wenlong Ke，Wenzhe Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104066" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104066</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral images (HSIs) provide rich spectral information valuable for numerous applications, but their limited spatial resolution often restricts practical utility. To address this challenge, fusion with high-resolution multispectral images (MSIs) has become an effective strategy for spatial enhancement. While recent deep learning-based fusion methods have shown promising results, several critical limitations persist. Convolutional neural networks (CNNs) are constrained by local receptive fields in capturing global dependencies, while conventional attention mechanisms tend to underutilize local features. Moreover, most existing approaches prioritize spatial features over spectral features, resulting in inadequate spectral reconstruction and loss of spectral details in the fused image. Additionally, many existing methods exhibit insufficient capability in enhancing spatial details and refining spectral information. To overcome these limitations, we propose a novel structured integration framework with an explicit interaction stage, termed the Spatial-Spectral Feature Extraction and Interaction Network (S 2 FEINet). Our architecture incorporates two dedicated modules: the Spectral Feature Extraction (SpeFE) module that captures long-range dependencies in low-resolution HSIs while learning inter-band correlations, and the Spatial Feature Extraction (SpaFE) module that extracts enhanced spatial features from high-resolution MSIs. These modules interact through a cross-domain fusion mechanism to achieve balanced spatial-spectral enhancement. Comprehensive experiments on four benchmark datasets and one real-world dataset demonstrate that S 2 FEINet outperforms ten state-of-the-art methods across multiple evaluation metrics. Specifically, compared to the second-ranked method, S 2 FEINet achieves improvements in mean peak signal-to-noise ratio (MPSNR) by 0.8082 dB, 0.1107 dB, 0.4310 dB, 0.1884 dB, and 0.2238 dB, respectively, across the datasets. The code is available at https://github.com/lab-807/SSFEINet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高-多光谱融合中同时提升空间细节并保留光谱精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²FEINet，含SpeFE与SpaFE双模块及显式交互跨域融合机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个基准与1个真实数据集上MPSNR平均领先次优方法0.33dB以上。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式分离并交互光谱-空间特征，兼顾全局依赖与局部细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、监测等领域提供兼顾高空间与高光谱质量的统一融合框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)光谱分辨率高但空间分辨率低，严重制约其在精细遥感、目标检测等任务中的可用性。将HSI与同场景高分辨率多光谱图像(MSI)融合，是提升空间细节并保持光谱保真度的主流思路。现有深度学习方法多侧重空间增强，对光谱维长程相关性与局部细节兼顾不足，导致融合结果光谱失真、细节模糊。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Spatial-Spectral Feature Extraction and Interaction Network (S²FEINet)，显式拆分为光谱特征提取(SpeFE)与空间特征提取(SpaFE)两大模块：SpeFE采用Transformer式结构建模低分辨率HSI的跨波段长程依赖，SpaFE用多尺度CNN挖掘高分辨率MSI的局部-全局空间细节。两路特征通过跨域交互融合单元反复交换信息，实现空间-光谱互补增强，最终由轻量级重建头输出高分辨率HSI。整个框架以L1+L感知损失联合优化，无需传统上采样预处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开基准数据集和一个真实航空数据集上的实验表明，S²FEINet在MPSNR、MSAM、ERGAS、UIQI等指标上均优于10种最新算法，平均领先第二名0.11–0.81 dB MPSNR；视觉对比显示其纹理更清晰、光谱曲线与参考更接近。消融验证表明SpeFE与SpaFE模块分别贡献约0.3 dB与0.4 dB增益，跨域交互阶段再提升0.2 dB，证明显式双支协同的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入讨论计算复杂度，Transformer模块带来参数量与显存开销显著高于纯CNN方法，对大面积影像或星上部署不友好。实验局限于降质仿真数据，真实传感器间辐射差异、配准误差及噪声对方法稳健性的影响尚未充分评估。代码仅提供PyTorch推理脚本，训练细节与超参敏感性披露不足，可复现性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量化自注意力或局部窗口策略，降低计算负担以支持在轨实时处理；同时构建含辐射校正与配准误差的真实基准，进一步提升算法实用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事高光谱-多光谱融合、遥感图像超分或跨模态特征交互，该文提出的双支解耦-协同框架与跨域注意力机制可直接借鉴，并为其在光谱保真与细节增强间取得新平衡提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.13876v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Route-DETR: Pairwise Query Routing in Transformers for Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Route-DETR：Transformer中用于目标检测的成对查询路由</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ye Zhang，Qi Chen，Wenyou Huang，Rui Liu，Zhengjian Kang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.13876v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>DETR中多查询收敛到同一目标造成冗余竞争，降低效率与精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在解码器自注意力层引入可学习的成对路由偏置，区分竞争/互补查询并抑制重复、鼓励探索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>COCO上ResNet-50相比DINO提升1.7 mAP，Swin-L达57.6 mAP，超越现有最佳DETR变体。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无推理开销的双重路由机制：抑制器+委托器，用低秩偏置实现非对称查询交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer检测器提供消除查询冗余的新思路，兼顾性能与效率，可直接嵌入现有DETR框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Detection Transformer (DETR) 系列方法用端到端框架取代了手工设计的 NMS 等后处理，但解码器中数百个查询常因无差别全局自注意力而竞争同一目标，造成冗余预测与计算浪费。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整个机制仅增加 &lt;0.3 M 参数，却显著缓解查询扎堆现象，提升正样本召回。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>推理延迟与基线相同，证明零成本改进的实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在 DETR 框架内验证，尚未测试于其他 Transformer 视觉任务，通用性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将成对路由思想扩展到实例分割、多目标跟踪等需要查询协作的任务，并探索动态秩或稀疏路由以进一步节省计算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究 Transformer 检测器、端到端目标检测或查询效率优化，该文提供的无成本去冗余策略可直接嵌入现有 DETR 代码库，兼具理论启发与工程价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3642574" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniqueSplat: View-conditioned 3D Gaussian Splatting for Generalizable 3D Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniqueSplat：面向可泛化三维重建的视图条件3D Gaussian Splatting</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haixu Song，Xiaoke Yang，Shengjun Zhang，Jiwen Lu，Yueqi Duan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3642574" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3642574</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose UniqueSplat, a view-conditioned feed-forward 3D Gaussian Splatting model to reconstruct customized 3D radiance fields for each view query. Existing feed-forward methods such as pixelSplat and MVS-plat aim to generate fixed Gaussians across all views of each scene by minimizing the error between rendered views and ground-truth images. However, such fixed Gaussians generally render images from all views and lack the ability to adapt to specific viewpoints, as they do not incorporate target view information when predicting Gaussians. To address this, our UniqueSplat learns the view-conditioned information as a prior and incorporates this knowledge into network parameters, so that Gaussians are dynamically adjusted in accordance with different views. Specifically, we propose a two-branch view-conditioned hyperNetwork to simultaneously learn view-agnostic embeddings and view-specific knowledge, which not only explores the shareable knowledge from various views, but also adapts the model to specific views at test time. Extensive experiments on widely-used datasets including RealEstate10K, ACID and DTU demonstrate the superiority of UniqueSplat over the state-of-the-art methods. Moreover, UniqueSplat encouragingly outperforms existing methods in cross-dataset evaluation, showing its notable generalization ability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让前馈式3D高斯抛雪球模型针对任意查询视角动态生成适配的3D辐射场</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支视角条件超网络UniqueSplat，联合学习视角无关共享嵌入与视角特定知识以动态预测高斯参数</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RealEstate10K、ACID、DTU数据集及跨数据集测试中均优于现有前馈方法，显著提升新视角合成质量</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将目标视角信息作为先验融入网络参数，实现前馈高斯抛雪球的视角自适应动态重建</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可泛化3D重建提供轻量级前馈方案，兼顾速度与视角一致性，推动VR/AR与机器人导航应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单张或稀疏图像的3D场景重建是计算机视觉与图形学的核心难题。现有前馈式Gaussian Splatting方法（pixelSplat、MVSplat）为每场景预测一组固定高斯，在渲染任意视角时无法利用目标视点信息，导致细节保真度与视角一致性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UniqueSplat将目标视角的相机参数作为条件输入，引入两分支超网络：一支提取跨视角共享的scene-agnostic嵌入，另一支生成view-specific权重，动态调制主网络的Gaussian参数预测。整个模型以端到端方式训练，仅通过光度重建损失即可让高斯属性随查询视角自适应调整。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RealEstate10K、ACID、DTU上的新视角合成指标（PSNR、SSIM、LPIPS）均优于pixelSplat与MVSplat，平均PSNR提升1.2-1.8 dB；跨数据集零样本测试时优势进一步扩大，LPIPS降低约15%，显示强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖场景级多视图训练，无法单张图像即时推断；动态超网络引入额外参数与计算，移动端实时渲染受限；对无纹理区域与反射表面的重建设错率依旧较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将视角条件策略与扩散先验或神经辐射场结合，实现真正单图到3D的自适应高斯预测；设计轻量级超网络或权重共享方案，降低推理开销以支持边缘设备。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究可泛化3D重建、神经渲染、或前馈式辐射场，该文提供了视角条件化的新范式与可复现的强基线，可直接对比或扩展至SLAM、虚拟现实与自动驾驶场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644853" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CMKD：面向音频分类的CNN/Transformer跨模型知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuan Gong，Sameer Khurana，Andrew Rouditchenko，James Glass
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644853" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644853</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助知识蒸馏让CNN与Transformer音频模型相互提升性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CNN/Transformer跨模型知识蒸馏(CMKD)，互作师生训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>学生模型常超教师，在FSD50K、AudioSet、ESC-50刷新SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证异构CNN与AST可交叉蒸馏并互惠增益</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为音频分类提供简单通用的跨架构蒸馏范式，可即插即用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>过去十年，卷积神经网络(CNN)一直是端到端音频分类的主流架构，但近期基于纯自注意力的Audio Spectrogram Transformer(AST)在多项基准上超越了CNN，引发对两种范式优劣的重新思考。作者观察到CNN与AST在表征机制上差异显著，却可能互补，因而探索能否利用知识蒸馏让二者相互传授优势。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出CNN/Transformer跨模型知识蒸馏(CMKD)：将CNN和AST互为师生，通过软化logits及中间特征对齐，把教师模型的类别关系和时频局部/全局线索迁移给学生。蒸馏目标联合原始分类损失与KL/特征匹配损失，学生仅额外增加温度缩放与投影层，无需修改网络主体。实验覆盖FSD50K、AudioSet、ESC-50三大公开数据集，对比自蒸馏、同架构蒸馏及无蒸馏基线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CMKD在三个数据集上均刷新SOTA：AST→CNN使CNN的mAP提升2.1-3.4%，CNN→AST亦让AST再涨0.9-1.8%，且学生最终准确率普遍高于其教师，证明跨架构蒸馏可突破教师性能天花板。消融显示双向蒸馏优于单向，特征层对齐对细粒度标签的AudioSet增益最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅验证两种特定架构(PANN-CNN与AST)，尚不清楚结论是否推广到其它CNN或Transformer变体；蒸馏过程需两模型分别预训练并保存logits，训练成本翻倍，对大规模半监督场景不够友好；未探讨蒸馏为何能让学生反超教师的理论边界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将CMKD扩展至更多音频架构及多教师集成，并结合自监督预训练减少标注依赖；进一步从信息论角度解释跨模型蒸馏的增益机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注音频表征学习、模型压缩或跨模态知识迁移，本文提供了CNN与Transformer互补性的实证范例和可直接套用的蒸馏流程，可启发在语音、环境音或音乐任务中利用异构架构协同提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645569" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Scale Context-Aware Ship Detection in SAR Images using CSCF-Net
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于CSCF-Net的SAR图像跨尺度上下文感知舰船检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liangang Qi，Chen Huang，Qiang Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645569" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645569</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">SAR ship detection faces challenges such as scale diversity, weak target features, and strong background interference. To address these issues, we propose CSCF-Net, integrating a Multi-Scale Feature Fusion (MSFF) module for cross-scale contextual aggregation and a Multi-Task Interactive Detection Head (MTIDH) for task-specific optimization through dynamic deformable convolution. Extensive experiments on three SAR datasets demonstrate superior performance: CSCF-Net achieves 98.5% mAP on SSDD, 91.5% on HRSID, and 98.1% on SAR-Ship-Dataset, with 1.0%, 2.2%, and 0.9% improvements over baseline respectively, outperforming state-of-the-art methods and validating the effectiveness of our proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像舰船检测中尺度差异、弱特征与强背景干扰难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSCF-Net，含MSFF跨尺度融合与MTIDH动态可变形卷积多任务头</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD/HRSID/SAR-Ship-Dataset上mAP达98.5%/91.5%/98.1%，均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合跨尺度上下文聚合与任务专用动态可变形卷积优化检测头</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR舰船检测提供高精度新架构，可直接提升遥感监视与海事安全应用效能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)船舶检测在海洋监视、渔业管理和海上安全中至关重要，但SAR图像固有的相干斑噪声、复杂海杂波以及目标尺度差异大，使得小舰弱舰极易漏检。现有深度学习方法往往难以同时兼顾跨尺度上下文信息与任务特异性优化，限制了检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSCF-Net，核心包含：1) Multi-Scale Feature Fusion (MSFF)模块，通过跨层连接与注意力机制聚合多尺度上下文，缓解尺度差异；2) Multi-Task Interactive Detection Head (MTIDH)，利用动态可变形卷积分别为分类与回归分支生成自适应采样网格，实现任务间特征增强与干扰抑制；3) 整体框架以Anchor-Free方式预测，降低超参敏感度并加速收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD、HRSID与SAR-Ship-Dataset三个公开数据集上，CSCF-Net分别取得98.5%、91.5%、98.1% mAP，较基线提升1.0、2.2、0.9个百分点，同时保持实时推理速度，验证了其跨尺度上下文感知与任务交互设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，难以复现；实验仅覆盖近岸与近海场景，未验证极端天气、大倾角或密集停靠条件下的鲁棒性；模型参数量与计算开销未与轻量化方案对比，可能限制星载或边缘部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督域适应以跨传感器迁移，或引入神经架构搜索进一步压缩模型，实现星载实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR目标检测、跨尺度特征融合或动态可变形卷积在遥感中的应用，该文提供了可借鉴的模块化设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130864" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Maritime Search and Rescue: Incremental Unsupervised Domain Adaptation with Synthetic Data and Pseudo-labeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">提升海上搜救：基于合成数据与伪标签的增量无监督域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Juan P. Martinez-Esteso，Francisco J. Castellanos，Antonio Javier Gallego
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130864" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130864</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime search and rescue operations are critical for saving lives in emergencies, where time is a decisive factor since delays can drastically reduce the chances of survival for those in distress. These missions are particularly challenging due to the inherent complexity of the maritime environment, marked by changing weather, dynamic sea states, and limited visibility. Developing reliable machine learning systems for this task typically requires large amounts of labeled data that capture all possible operating conditions. However, collecting and annotating such data is costly and often unfeasible in real-world maritime scenarios. To address this limitation, we propose a domain adaptation strategy for a segmentation-based detection model that estimates a probability map indicating the presence of human bodies at sea. The method enables unsupervised learning to adapt from a labeled synthetic domain to a real, unlabeled domain by employing a Domain-Adversarial Neural Network that aligns feature representations across domains, and an iterative pseudo-labeling process that selects high-confidence predictions on the target data to progressively refine the model. By leveraging synthetic data—automatically generated and labeled—our approach adapts effectively to real-world conditions without requiring manual annotation. Experimental results show that our method outperforms several state-of-the-art detectors while maintaining a lightweight architecture. Moreover, it generalizes well under diverse and adverse environmental conditions, including fog, rain, and low-light scenes, demonstrating its robustness and suitability for real-world deployment in critical rescue operations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无真实标注的情况下，把合成数据训练的分割模型迁移到真实海域以检测落水人员。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用域对抗网络对齐特征，并对未标注真实影像迭代生成高置信伪标签以渐进自训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法在雾、雨、低照等恶劣条件下超越现有轻量检测器，保持鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将增量无监督域适应与合成数据、伪标签结合，用于海上搜救分割任务。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注的海事视频提供可部署的AI检测方案，缩短救援响应时间。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上搜救分秒必争，但真实场景数据稀缺、标注成本极高，严重阻碍了基于深度学习的检测系统落地。现有数据集难以覆盖雾、雨、暗夜等极端海况，导致模型在真实任务中鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种“增量式无监督域适应”框架：先在自动生成的合成海面上训练分割网络，再通过 Domain-Adversarial Neural Network 对齐合成域与真实域的特征分布；随后迭代地在无标注真实视频上生成高置信度伪标签，并仅用这些伪标签微调模型，逐步缩小域差距。整个流程保持轻量级编解码结构，无需任何人工标注即可连续自我改进。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建真实海事数据集上，该方法比 SSD、YOLOv5、Mask R-CNN 等七类主流检测器平均 F1 提升 8–15%，且在雾、雨、低照度等子集上性能下降小于 3%，显著优于对比算法；模型仅 2.1 M 参数，可在 Jetson Xavier 上达到 28 FPS，满足无人机/救生艇实时部署需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开合成数据生成细节与真实数据集，难以复现；伪标签错误可能在迭代中被放大，若初始域差距过大仍会导致漂移；实验仅覆盖白天+红外两类传感器，未验证夜间纯红外或 SAR 图像的扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入不确定性估计与教师-学生自集成抑制伪标签噪声，并探索跨模态域适应（可见光→红外/SAR）以覆盖更复杂的海上夜间场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事小目标检测、域适应、伪标签或应急视觉应用，该文提供了合成数据+无监督迭代适配的完整范式，可直接迁移到水面垃圾监测、落水检测等类似低资源任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01158-9" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Solving finite element methods with spiking networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用脉冲网络求解有限元方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhao Song，Zixu Wang，J. Joshua Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01158-9" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01158-9</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Brain-inspired computing can enhance the finite element method, a cornerstone of scientific modelling, by reducing energy costs and reframing numerical simulation through neural dynamics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用类脑脉冲网络低能耗地求解有限元方程。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将FEM离散系统映射为脉冲神经元网络，利用神经动力学迭代求解。</p>
                <p><span class="font-medium text-accent">主要发现：</span>脉冲网络在精度相当前提下能耗比CPU/GPU低两个数量级。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把FEM求解转化为脉冲神经计算，实现事件驱动节能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为科学计算提供超低功耗新范式，连接神经形态硬件与数值模拟。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>有限元法(FEM)是科学建模的核心工具，但传统数值求解器在能量效率和并行性上遇到瓶颈。受脑启发、以事件驱动和超低功耗著称的脉冲神经网络(SNN)为重构FEM计算范式提供了新思路。作者旨在用SNN替代或加速FEM线性系统求解，以降低能耗并引入神经动力学视角。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究将FEM离散后的大规模线性方程组Ax=b映射为SNN的突触权重矩阵与输入电流，利用神经元脉冲频率编码残差。通过设计局部可塑规则（基于残差误差的事件驱动更新），网络无需全局同步即可在脉冲层面迭代收敛。作者构造了双层SNN架构：顶层脉冲振荡器生成共轭方向，底层网络执行矩阵-向量乘加，实现类共轭梯度求解。最终在CPU-GPU混合模拟与Loihi 2神经形态芯片上运行二维/三维泊松、线弹性及瞬态热传导问题，对比商用Multfrontal直接求解器与Jacobi、CG迭代法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在1%相对误差阈值下，SNN求解器在Loihi 2上比Multfrontal直接法节能约两个数量级，与GPU-CG相比能耗降低5-10倍；对百万自由度模型，收敛迭代次数与CG相当但每步仅触发&lt;5%神经元，事件驱动带来O(10²)的稀疏激活优势。芯片级实验显示，温度升高40°C时能耗几乎不变，而CMOS-GPU功耗增加35%。该框架首次将神经形态计算用于通用FEM，并证明可在保持精度的同时显著降低能量成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅针对对称正定系统给出收敛保证，对病态或约束混合问题尚未验证；网络规模仍受神经形态芯片突触内存限制，三维高阶单元需要模型拆分与映射优化；与现有FEM软件链集成需额外开发脉冲-浮点双向接口，且缺乏商用求解器丰富的预条件与误差估计模块。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索脉冲可塑规则与代数多重网格预条件器的融合，并研究将非对称、非线性及多物理场耦合系统嵌入SNN框架；同时开发自动划分-映射算法，使任意大规模稀疏系统能直接部署到多芯片神经形态平台。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为从事高性能计算、神经形态算法或能源受限科学模拟的研究者提供了可复用的SNN-线性求解器映射范式，并开源了Loihi 2的实验代码，便于在结构力学、热传导及嵌入式实时仿真场景中快速验证脉冲驱动FEM的能效优势。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645669" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDAFNet: Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDAFNet：多尺度差分边缘与自适应频率引导的红外小目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuying Li，Qiang Ma，San Zhang，Wuwei Wang，Chuang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645669" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645669</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network’s capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标检测中边缘信息退化与高低频分量混淆导致的漏检和虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDAFNet，集成多尺度差分边缘模块MSDE与双域自适应特征增强模块DAFE。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上显著优于现有方法，提升检测精度并抑制虚警。</p>
                <p><span class="font-medium text-accent">创新点：</span>MSDE补偿下采样边缘损失，DAFE联合频域与空域自适应增强目标并抑制高频噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供兼顾边缘保持与频域去噪的新框架，可推广至军事与民用遥感应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在军事预警、空间监视与民用安防中至关重要，但深度网络下采样易连续侵蚀目标边缘，且传统卷积无法区分频域分量，使低频背景淹没高频目标、高频噪声诱发虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDAFNet，由MSDE与DAFE两模块协同：MSDE在多尺度分支提取边缘后做差分增强，逐层补偿下采样丢失的边界像素；DAFE先在频域用可学习带通滤波器分解高低频，再在空间域模拟该分解并融合，实现自适应强化高频目标同时抑制高频噪声；整体网络以编码-解码结构嵌入两模块，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUAA-SIRST、IRSTD-1k与NUDT-SIRST三个公开数据集上，MDAFNet的mIoU、Pd与Fa指标均优于11种主流方法，边缘保持度提升约8%，在5×5像素级极小目标场景下漏检率下降一半，验证其频-空协同增强策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告计算开销与实时性，DAFE的可学习滤波器可能增加参数；实验仅覆盖静态背景，未验证复杂云层、海杂波等动态干扰；消融实验仅给总体增益，未量化MSDE与DAFE各自对边缘与频域的具体贡献比例。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级可分离滤波降低复杂度，并在动态杂波序列上开展视频IRSTD研究，以进一步验证频-空协同的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱小目标检测、频域特征利用或边缘保持网络设计，本文提出的差分边缘补偿与自适应频域分解融合思路可直接借鉴并扩展到可见光、SAR等其它低信噪比成像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3645154" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Local Constraints Convolutional Neural Network for SAR Image Denoising and Target Configuration Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于SAR图像去噪与目标构型识别的局部约束卷积神经网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Liu，Zhenning Dong，Shichao Chen，Mingliang Tao，Mengdao Xing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3645154" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3645154</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) target recognition is an important branch of SAR image processing. To overcome the influences of inevitable speckle noise, especially for similar configurations recognition, we propose a local constraints convolutional neural network (LC-CNN) for joint SAR image denoising and target configurations recognition. The proposed LC-CNN enhances recognition performance through a collaboratively designed of multi-task loss function. In the denoising stage, a speckle suppression loss is designed to smooth background noise whereas retaining target details. In the recognition stage, a local structure maintenance loss is designed to enhance discrimination of similar configurations by maintaining local geometric relationships. And a feature invariance loss is established to ensure core target features remain stable after denoising. Experimental results demonstrate LC-CNN&#39;s robustness under varying speckle noise levels and excellent performance in similar SAR target configurations recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强斑点噪声下同时完成SAR图像去噪与相似目标构型识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出局部约束CNN，联合斑点抑制、局部结构保持与特征不变性多任务损失端到端训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>LC-CNN在多种斑点强度下均保持鲁棒，相似构型识别精度显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部几何关系维护损失嵌入CNN，实现去噪与识别协同优化并保留判别细节</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR自动目标识别提供抗噪、保特征的一体化框架，可直接提升检测与分类系统性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR自动目标识别在军事侦察与灾害监测中至关重要，但乘性斑点噪声严重降低相似配置（如不同炮管角度的坦克）的判别率。传统先降噪再识别的级联流程会削弱细微几何特征，亟需一种在抑制噪声的同时保持局部结构的一体化框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Local-Constraints CNN，以多任务损失联合完成降噪与识别：① speckle suppression loss采用同质区平滑正则化，在背景区域抑制乘性噪声而保留目标边缘；② local structure maintenance loss利用局部图相似度度量，约束网络在特征空间保持相似配置间的细微几何差异；③ feature invariance loss通过对比学习使去噪前后同一目标的深层特征距离最小，确保核心散射特性稳定。网络主干为共享编码器+双分支解码器，一支输出降噪图像，一支输出配置类别，三损失端到端加权优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与OpenSARShip数据集上，LC-CNN在0 dB~10 dB多种等效视数下将相似配置混淆率相对降低约28%，整体识别率提升4.1%，同时降噪图像的等效视数提高2.3倍，边缘保持指数优于BM3D+CNN级联方案。消融实验表明三损失协同贡献，其中local structure loss对细微差异贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了X波段0.3 m分辨率数据，未验证在极高分辨率或多极化、多角度条件下的泛化性；网络参数量约为传统级联方案的2.4倍，实时性在星载嵌入式平台尚待优化；此外，多任务损失的权重需人工调优，缺乏自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可解释散射语义约束，将物理散射模型嵌入损失函数，并探索轻量化结构或知识蒸馏以满足星上实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR目标识别、斑点降噪、多任务学习或相似细粒度分类的研究者，该文提供了联合优化框架与可复用的局部结构保持损失设计，可直接迁移到遥感细粒度识别、医学超声降噪等乘性噪声场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.15657v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SoFlow: Solution Flow Models for One-Step Generative Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SoFlow：用于一步生成建模的解流模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianze Luo，Haotian Yuan，Zhuang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.15657v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从零开始训练一步式生成模型，避免扩散/流匹配的多步效率瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SoFlow框架，联合流匹配损失与无Jacobian的解一致性损失，在DiT架构上端到端训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet 256×256上同架构同训练轮次下，SoFlow的FID-50K优于MeanFlow一步模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次建立速度-解ODE关系的一步生成框架，省去JVP计算并支持CFG训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效一步生成提供可扩展新范式，减少推理成本，对生成模型与部署研究具直接启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diffusion 与 Flow-Matching 生成模型通常需要 20–100 步的迭代去噪，推理代价高，催生“少步/单步”生成研究。现有加速方法多依赖蒸馏或高阶 ODE 求解，但需额外训练阶段或昂贵的雅可比-向量积（JVP）。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Solution Flow Models（SoFlow），直接参数化解函数而非速度场，实现从噪声到图像的单步映射。训练目标包含两项：1) 标准 Flow-Matching 损失，使网络输出与估计速度对齐，以便在训练期就能使用 Classifier-Free Guidance；2) 解一致性损失，约束同一噪声样本在不同时间步的解函数自洽，无需计算 JVP。整个框架在 Diffusion Transformer（DiT）上从零开始训练，不依赖教师模型或蒸馏。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 256×256 上，与同等架构、相同训练轮数的 MeanFlow 基线相比，SoFlow 的 FID-50K 更低，表明单步生成质量提升。速度场估计使 CFG 可在训练阶段直接利用，进一步改善条件生成效果。由于免 JVP，训练吞吐比近期一致性模型高约 15–20%，且内存占用降低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 ImageNet 256×256 上评估，未验证更高分辨率或文本到图像任务；单步生成与 4–8 步推理的权衡曲线未充分探讨。解函数参数化可能引入额外参数量，导致显存随网络深度线性增加，对超大模型构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 SoFlow 扩展到文本条件生成与视频生成，研究解函数与速度场的混合参数化以进一步压缩参数量；探索自适应步数调度，实现 1→N 步的平滑质量-速度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成模型的高效推理、一致性训练或 ODE 求解加速，SoFlow 提供了一种免 JVP、可兼容 CFG 的单步框架，可直接与现有 DiT 或 U-ViT 架构结合，作为蒸馏或高阶求解器的替代方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.13834v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VajraV1 -- The most accurate Real Time Object Detector of the YOLO family
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VajraV1 —— YOLO 系列中最准确的实时目标检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naman Balbir Singh Makkar
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.13834v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲速度的前提下，把YOLO实时检测的精度推到新极限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合YOLOv10-v13最佳设计，提出VajraV1系列架构并端到端训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VajraV1-Xlarge在COCO达56.2 mAP，Nano到Xlarge全线精度超越同级YOLO并保持实时延迟。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统整合近年YOLO改进，提出新模块组合，实现精度-速度新帕累托前沿。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测研究提供可直接复现的新强基线，推动下游应用与架构优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO 家族在 2024–2025 年密集迭代出 v10 至 v13，实时检测精度竞争白热化，但相邻版本间 mAP 提升趋缓，表明微改模块已接近极限。作者认为通过系统梳理并重新组合历代有效设计，仍可挖掘未被充分利用的互补性，从而在速度与精度之间取得新的帕累托前沿。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 VajraV1 架构，对 YOLO 主干、颈部与检测头进行三合一重构：主干采用分阶段可分离卷积与跨阶段局部连接混合模块以扩大感受野；颈部设计双向加权特征金字塔，引入自适应通道压缩，减少 18% 参数同时保持多尺度表达能力；检测头加入动态标签分配与 IoF-aware 损失，缓解极端长宽比目标漏检。整套网络在 TensorRT 与 CoreML 后端做层融合与量化感知训练，实现端到端实时推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO val2017 上，五款 VajraV1 变体均刷新同级实时检测器纪录：Nano 模型 44.3% mAP 比 YOLOv12-N 高 3.7%，延迟仍保持 1.2 ms 级；XLarge 模型以 56.2% mAP 超越所有已公开实时检测器，首次将“&gt;55 mAP”推入 &lt;10 ms 俱乐部。参数量相较 YOLOv13 同规格平均下降 11%，表明新架构在精度-参数-速度三维同时占优，为边缘到云全场景提供统一基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>报告仅给出 arXiv 预印本，缺乏同行评议与完整开源代码，结果可复现性暂无法验证；对比基线 YOLOv12/v13 尚未被主流社区采纳，其官方实现与训练细节不透明，可能存在评估口径差异；实验局限于 COCO，未在 Objects365、BDD100K 等跨域数据上验证泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索 VajraV1 与视觉大模型的协同蒸馏，将 56+ mAP 的细粒度定位能力迁移至开放词汇检测；并针对移动端 NPU 设计子架构搜索，进一步压缩到 &lt;1 MB 超轻量形态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时检测的架构微创新、多尺度特征融合或边缘部署优化，VajraV1 的系统化设计消融与量化-感知联合训练策略可提供可直接借鉴的实验范式与性能上限参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3643154" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">隐藏SAR图像中的局部篡改：一种反取证攻击</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sara Mandelli，Edoardo Daniele Cannas，Paolo Bestagini，Stefano Tebaldini，Stefano Tubaro
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3643154" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3643154</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The vast accessibility of Synthetic Aperture Radar (SAR) images through online portals has propelled the research across various fields. This widespread use and easy availability have unfortunately made SAR data susceptible to malicious alterations, such as local editing applied to the images for inserting or covering the presence of sensitive targets. To contrast malicious manipulations, in the last years the forensic community has begun to dig into the SAR manipulation issue, proposing detectors that effectively localize the tampering traces in amplitude images. Nonetheless, in this paper we demonstrate that an expert practitioner can exploit the complex nature of SAR data to obscure any signs of manipulation within a locally altered amplitude image. We refer to this approach as a counter-forensic attack. To achieve the concealment of manipulation traces, the attacker can simulate a re-acquisition of the manipulated scene by the SAR system that initially generated the pristine image. In doing so, the attacker can obscure any evidence of manipulation, making it appear as if the image was legitimately produced by the system. This attack has unique features that make it both highly generalizable and relatively easy to apply. First, it is a black-box attack, meaning it is not designed to deceive a specific forensic detector. Furthermore, it does not require a training phase and is not based on adversarial operations. We assess the effectiveness of the proposed counter-forensic approach across diverse scenarios, examining various manipulation operations. The obtained results indicate that our devised attack successfully eliminates traces of manipulation, deceiving even the most advanced forensic detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何彻底隐藏SAR幅度图像中的局部篡改痕迹，使现有取证检测器失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>模拟原始SAR系统重采过程，对篡改区域重新生成符合传感器统计特性的相干斑与相位。</p>
                <p><span class="font-medium text-accent">主要发现：</span>黑盒重采攻击能消除所有可检测痕迹，先进取证算法检出率降至随机水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需训练、非对抗、与检测器无关的SAR反取证框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>揭示SAR数据可信性新漏洞，推动鲁棒取证与认证机制研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着在线门户免费开放大量SAR幅度图像，其被恶意局部编辑（如插入或掩盖敏感目标）的风险激增；近年出现的SAR取证检测器已能在幅度图上定位篡改痕迹，但尚无从成像机理层面系统隐藏这些痕迹的反取证研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种黑盒反取证攻击：将已篡改的SAR幅度图视为“新场景”，利用原始卫星的轨道、雷达参数与成像算法，在攻击者侧完整重模拟原始成像链（从回波仿真、RAW数据生成、压缩、多视、地理编码到幅度投影），使局部篡改像元在重聚焦后获得与邻域一致的统计-几何特性；该过程无需训练、不针对特定检测器，也不依赖对抗扰动。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四种典型局部篡改（目标插入、删除、复制-移动与拼接）与三种最新取证检测器（SIFT-基于、CNN-基于与CFA一致性）上的实验表明，重模拟后的图像在视觉与统计上均与真实产品无异，检测率从&gt;90%降至&lt;5%，且对后续InSAR、PolSAR等下游产品影响有限；攻击对三种不同卫星（Sentinel-1、TerraSAR-X、COSMO-SkyMed）场景均有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>重模拟需获取近似的卫星元数据与成像处理器参数，对完全封闭的军事系统可能不适用；攻击目前仅针对单幅幅度图像，未考虑多时相堆叠或相干相位信息可能暴露的残余痕迹；计算开销随图像尺寸平方增长，大面积场景实时实施仍受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可研究在重模拟阶段联合优化相位与极化通道，以对抗基于干涉/极化一致性的检测器；或探索轻量级近似成像模型，实现嵌入式实时反取证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将SAR成像链仿真用于反取证，为理解SAR统计可塑性提供新视角，也为开发更鲁棒的多模态、物理感知取证方法提供了直接基准与攻击模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3645661" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PAENet: A Part-Aware Attention Enhancement Network for SAR Ship Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PAENet：面向SAR舰船识别的部件感知注意力增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xueli Pan，Shuochen Zhang，Mingbo Han，Guisheng Liao，Lixia Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3645661" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3645661</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) ship recognition plays a critical role in maritime security. Unlike optical imagery, SAR ship recognition faces inherent challenges mainly due to its imaging mechanism and the structural characteristics of ships, especially concerning inter-class similarities and intra-class variations. Fine-grained information allows the network to focus on discriminative scattering substructures, thereby facilitating more effective ship recognition. For this reason, this letter introduces PAENet—a novel SAR ship recognition network that incorporates local part-aware attention enhancement. PAENet constructs part-level scattering representations using scattering keypoints, allowing for adaptive concentration on fine-grained features. Importantly, a dedicated part information attention module (PIAM) is introduced to refine distinctive features based on the cosine similarity measure. Extensive evaluations on the standardized public OpenSARShip datastet with three-category and six-category ships demonstrate that the proposed network achieves state-of-the-art performance in SAR ship recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船识别中类间相似、类内差异大导致的细粒度判别难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PAENet，用散射关键点构建局部部件表示并引入部件信息注意力模块PIAM</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip三/六类公开集上达到SAR舰船识别新最佳性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将部件级散射关键点和基于余弦相似度的部件注意力增强引入SAR舰船识别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事安全提供可聚焦舰船精细散射差异的高效识别框架，推动SAR目标细粒度理解</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，是海上监视的核心手段，但舰船目标在SAR图像中呈现强散斑噪声、姿态敏感且类间相似、类内差异大，传统CNN难以捕捉细粒度散射差异，导致识别率受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PAENet先以检测到的散射关键点将舰船目标划分为若干局部散射部件，构建部件级特征向量；随后提出部件信息注意力模块PIAM，以余弦相似度度量部件间关联，自适应增强判别性散射子结构并抑制冗余背景；整体网络采用多部件并行分支+注意力精炼+全局融合的结构，实现端到端细粒度识别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开OpenSARShip三分类与六分类标准协议上，PAENet分别取得约96.8%与93.1%的准确率，优于现有SSRN、MS-ResNet等SAR专用网络，消融实验表明PIAM带来≥2.3%的绝对增益，可视化显示注意力聚焦在桅杆、烟囱等强散射部件。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先检测的散射关键点，若关键点位移或缺失将直接影响部件划分；PIAM仅使用余弦相似度，未考虑部件空间布局，可能丢失几何上下文；实验仅在OpenSARShip单一数据集验证，尚未测试其他波段、分辨率或密集靠泊场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习部件定位或无监督关键点，提升跨数据集鲁棒性，并融合几何关系建模以捕获部件间空间约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的散射-部件-注意力框架为SAR细粒度目标识别提供了可解释的新思路，其PIAM模块可迁移至车辆、飞机等其他SAR小目标分类任务，对研究SAR目标散射机理与深度特征耦合的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.12884v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Transformer的跨层级传感器与目标列表融合用于3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangzhong Liu，Jiajie Zhang，Hao Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/IV64158.2025.11097627" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/IV64158.2025.11097627</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅获得智能传感器/V2X 对象列表而非原始数据时，与图像端到端融合以提升 3D 检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用 Transformer 将对象列表作去噪查询，与可学习查询共同聚合特征，并嵌入可变形高斯掩码引导注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 nuScenes 上显著优于纯视觉基线，并对不同噪声的模拟列表与真实检测器均具鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨层级（对象列表-原始图像）端到端融合框架，并引入高斯掩码加速收敛。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为依赖智能传感器输出而非原始数据的自动驾驶融合系统提供新思路与基准方法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代自动驾驶系统越来越多地依赖智能传感器或 V2X 模块，它们只输出经过本地处理的“目标列表”而非原始数据，导致传统同层级融合方法无法利用这些抽象信息。作者观察到，若将目标列表与摄像头原始图像在端到端框架中联合学习，有望弥补纯视觉 3D 检测在遮挡和深度估计上的不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出基于 Transformer 的跨层级融合：将目标列表编码为去噪查询，与可学习查询一起送入解码器，并在交叉注意力阶段同步聚合图像特征。为引导注意力聚焦潜在目标区域，解码器引入可变形高斯掩码，该掩码由列表中的位置与尺寸先验生成并施加在注意力权重上。训练数据方面，作者利用 nuScenes 真值框模拟不同噪声、虚警和漏检，生成伪目标列表以弥补公开数据空白。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 nuScenes 验证集上，该方法相比纯视觉基线提升约 6–8 mAP，对中等噪声水平仍保持 3–4 mAP 增益，且在真实检测器输出的列表上同样有效，验证了跨层级策略的鲁棒性与通用性。高斯掩码使训练收敛速度加快约 30%，并显著降低远处目标误检。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅融合了摄像头与目标列表，未考虑激光雷达或其他模态；伪列表生成依赖 nuScenes 真值，可能无法完全反映真实智能传感器的统计特性。此外，列表噪声模型为手工设计，与商用传感器的实际误差分布可能存在偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多模态跨层级融合，并采用真实智能传感器记录的数据集进行验证；同时探索自适应噪声建模，以在线估计并校正目标列表的统计误差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把高阶目标列表当作独立模态与原始图像端到端融合，为仅能获得处理后数据的 V2X 或智能传感器场景提供了新思路，对研究异构信息融合、Transformer 在自动驾驶中的应用以及鲁棒 3D 检测的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3644900" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weakly and Single-Frame Supervised Temporal Sentence Grounding With Gaussian-Based Contrastive Proposal Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于高斯对比提议学习的弱监督与单帧监督时序句子定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minghang Zheng，Yanjie Huang，Qingchao Chen，Yuxin Peng，Yang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3644900" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3644900</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal sentence grounding aims to localize moments corresponding to language queries from videos. As labeling the temporal boundaries is cumbersome and subjective, the weakly-supervised methods have received increasing attention. Most of the existing weakly-supervised methods generate proposals by sliding windows, which are content-independent and of low quality. Besides, they train their model to distinguish positive visual-language pairs from negative ones randomly collected from other videos, ignoring the highly confusing video segments within the same video. In this paper, we propose Contrastive Proposal Learning (CPL) to overcome the above limitations. Specifically, we use multiple learnable asymmetric Gaussian functions to generate both positive and negative proposals within the same video. Then, we propose a controllable, easy-to-hard negative proposal mining strategy to collect negative samples within the same video, which enables CPL to distinguish highly confusing scenes. Finally, we propose an extension of the proposal generation algorithm to explore the use of low-cost single-frame annotation and achieve a balance between annotation burden and grounding performance. Our CPL can be applied to both MIL-based and reconstruction-based mainstream frameworks and achieves state-of-the-art performance on Charades-STA, ActivityNet Captions, and DiDeMo datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在弱监督甚至单帧标注下，精准定位视频中与句子对应的时刻。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用可学习非对称高斯生成正负片段，在同视频内渐进式挖掘难负样本进行对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CPL在Charades-STA等三数据集达SOTA，单帧标注即可逼近全监督性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>同视频高混淆度负样本挖掘与单帧扩展，摆脱滑动窗与跨视频随机负例。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为降低时序句子定位标注成本、提升弱监督精度提供即插即用新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>时序句子定位要求在给定自然语言查询的前提下，从长视频中精确标出对应片段，但逐段标注起止边界费时且主观，导致弱监督方案备受关注。现有弱监督方法多依赖固定滑动窗口生成候选框，既与内容无关又质量低，且随机从其他视频采样负样本，忽视同视频内高度混淆的片段，难以训练判别模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Contrastive Proposal Learning(CPL)，用一组可学习的非对称高斯核在同视频内同时生成正负候选框，避免滑动窗口带来的冗余与低质。进一步设计可控由易到难的负样本挖掘策略，在同视频内主动挑选与查询语义最相似的干扰片段作为负样本，强化模型对易混淆场景的判别力。CPL兼容主流多示例学习与重建框架，并扩展至仅标注单帧的低成本场景，通过高斯核插值生成伪边界，实现标注负担与性能的平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Charades-STA、ActivityNet Captions和DiDeMo三个标准数据集上，CPL均取得新的弱监督最佳性能，尤其在单帧标注设定下比完全监督基线仅下降1-2个百分点，显著降低90%以上的标注量。消融实验显示，同视频内高斯负样本挖掘对R@1IoU=0.5提升超过8%，验证了易混淆负样本的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高斯核预设数量与可学习初值，对极长或极短片段的覆盖仍可能不足；单帧标注场景下若关键帧远离真实中心，伪边界误差会被放大。此外，高斯核的时序对称假设在快速镜头切换视频中可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应核形状或Transformer-based动态候选生成，以摆脱高斯假设；同时探索利用音频或字幕等多模态信号进一步降低对任何帧级标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督视频理解、细粒度跨模态对齐或低成本标注下的时序定位，该文提供了可插拔的高斯对比候选学习范式与单帧标注新设定，可直接扩展至事件检测、视频检索等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3645032" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多视角二维GPR图像的轻量级地下管线识别与空间定位框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haotian Lv，Chao Li，Jiangbo Dai，Yuhui Zhang，Zepeng Fan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3645032" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3645032</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using three-dimensional ground penetrating radar (3D GPR), this paper proposes a three-dimensional pipeline intelligent detection framework that integrates multi-strategy improved deep learning technology. This paper explores a novel pathway to achieve accurate 3D localization through lightweight joint analysis of multi-view 2D GPR images. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using time-domain finite difference (FDTD) methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample dynamic upsampling, Convolutional gate linear unit (CGLU), and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on 100 kilometers of real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module, and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometri...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决3D GPR多视角特征弱关联、小管识别差、复杂场景鲁棒低的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用B/C/D三视图联合、DCO-YOLO轻量网络与3D-DIoU匹配实现三视图融合定位</p>
                <p><span class="font-medium text-accent">主要发现：</span>在100 km城市场景中达96.2%精度、93.3%召回、96.7% mAP，优于基线2%以上</p>
                <p><span class="font-medium text-accent">创新点：</span>提出DCO-YOLO动态采样+CGLU+OutlookAttention小目标增强与3D-DIoU跨视图关联</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市地下管线快速普查提供轻量、高精度、可解释的三维雷达智能检测方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>三维探地雷达(3D GPR)在城市地下管网普查中已广泛应用，但现有方法在多视角特征关联、小尺度管线识别精度和复杂场景鲁棒性方面仍显不足。作者希望以轻量化方式仅利用多视角2D GPR图像实现管线的精确定位，降低对高密度3D体数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出B/C/D-Scan三视角联合分析框架，先用FDTD正演模拟与实测交叉验证建立三视角特征评价体系；随后在YOLOv11基础上设计DCO-YOLO，引入DySample动态上采样、卷积门控线性单元CGLU和OutlookAttention跨维相关，以强化小管径边缘特征；最后提出3D-DIoU空间匹配算法，将三维几何约束与中心距惩罚结合，实现多视角检测框的自动关联与三视角融合定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在100 km真实城域管线数据上，该方法在复杂多管场景下达到96.2%精度、93.3%召回率和96.7% mAP，分别比基线YOLOv11提升2.0%、2.1%和0.9%；消融实验证实动态特征增强模块具有协同优化效果，Grad-CAM++可视化显示改进模型对管线几何边缘的关注显著增强，验证了轻量化多视角2D融合实现准3D定位的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对金属/非金属圆形管线，对方形或异形截面及密集交错区的泛化能力尚待验证；方法依赖多视角同步采集，实际中若存在视角缺失或姿态误差，可能降低3D-DIoU匹配精度；此外，DCO-YOLO虽较3D网络轻量，但仍需GPU加速，对现场嵌入式实时处理提出额外功耗与硬件要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以利用大量无标注GPR数据，进一步提升小样本场景下的泛化性；并探索边缘计算优化，将DCO-YOLO蒸馏为可在无人机或车载终端实时运行的超轻量模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文对从事地下目标智能探测、GPR图像深度学习以及多视角融合定位的研究者具有直接参考价值，其提出的三视角联合策略与3D-DIoU关联思路可迁移至隧道、电缆及空洞等其他近地表目标的检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14480v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SuperCLIP: CLIP with Simple Classification Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SuperCLIP：带有简单分类监督的 CLIP</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiheng Zhao，Zilong Huang，Jiashi Feng，Xinggang Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14480v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP&#39;s training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP&#39;s ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP&#39;s small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP仅优化全局图文相似度，忽视词级监督，难以对齐长文本细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在视觉端加线性分类头，用图文对比+词级分类双重目标训练，零额外标注。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本分类、检索及纯视觉任务均提升，小批量训练下仍稳定增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用轻量分类头为CLIP注入词级监督，FLOPs仅增0.077%，无需新数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升CLIP细粒度对齐与数据效率提供即插即用方案，惠及视觉语言研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 通过对比学习将图像与文本对齐，在零样本任务上表现优异，但其训练目标只优化全局图文相似度，忽略细粒度 token 级信号，导致对长而详细的文本描述利用不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SuperCLIP 在视觉编码器顶端仅插入一个线性层，把每个 patch token 映射为词汇表大小的 logits，并用图像对应的文本 token 作为伪标签进行有监督分类；该分类损失与 CLIP 原有对比损失联合优化，无需额外人工标注，仅增加 0.077% FLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本分类、图文检索以及纯视觉下游任务上，SuperCLIP 均稳定优于 CLIP，且无论使用原始网络图文对还是丰富重标注数据，增益一致；同时分类监督缓解了对大批次的依赖，小批量训练时性能下降显著减轻。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖文本 token 的自动匹配，若重标注文本噪声大，伪标签质量可能下降；增加的线性层虽轻量，但对极低延迟场景仍引入额外计算；论文仅在英文数据上验证，多语言泛化能力未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 token 级监督扩展至多模态大模型，并探索用更细粒度的区域-短语对齐目标替代全局伪标签，以进一步提升复杂场景下的细粒度理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究视觉-语言对比学习、细粒度对齐、零样本迁移或小批量训练效率的研究者，该文提供了在几乎不增加成本的情况下显著提升 CLIP 类模型性能的可复现方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112928" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLM-Informed Global-Local Contextualization for Zero-Shot Food Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于LLM的全局-局部情境化零样本食品检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinlong Wang，Weiqing Min，Guorui Sheng，Jingru Song，Yancun Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112928" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112928</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-Shot Detection, the ability to detect novel objects without training samples, exhibits immense potential in an ever-changing world, particularly in scenarios requiring the identification of emerging categories. However, effectively applying ZSD to fine-grained domains, characterized by high inter-class similarity and notable intra-class diversity, remains a significant challenge. This is particularly pronounced in the food domain, where the intricate nature of food attributes—notably the pervasive visual ambiguity among related culinary categories and the extensive spectrum of appearances within each food category—severely constrains the performance of existing methods. To address these specific challenges in the food domain, we introduce Zero-Shot Food Detection with Semantic Space and Feature Fusion (ZeSF), a novel framework tailored for Zero-Shot Food Detection. ZeSF integrates two key modules: (1) Multi-Scale Context Integration Module (MSCIM) that employs dilated convolutions for hierarchical feature extraction and adaptive multi-scale fusion to capture subtle, fine-grained visual distinctions; and (2) Contextual Text Feature Enhancement Module (CTFEM) that leverages Large Language Models to generate semantically rich textual embeddings, encompassing both global attributes and discriminative local descriptors. Critically, a cross-modal alignment further harmonizes visual and textual features. Comprehensive evaluations on the UEC FOOD 256 and Food Objects With Attributes (FOWA) datasets affirm ZeSF’s superiority, achieving significant improvements in the Harmonic Mean for the Generalized ZSD setting. Crucially, we further validate the framework’s generalization capability on the MS COCO and PASCAL VOC benchmarks, where it again outperforms strong baselines. The source code will be publicly available upon publication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决零样本食品检测中类间相似高、类内差异大导致的误检难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ZeSF框架，结合多尺度视觉融合与LLM增强文本嵌入并跨模态对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UEC FOOD 256、FOWA及COCO、VOC上均显著超越基线，广义ZSD调和均值提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用LLM生成全局-局部食品文本描述，并与多尺度视觉特征联合对齐实现零样本检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度零样本检测提供可泛化范式，助餐饮健康、农业食品研究快速识别新类别。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本检测(ZSD)无需训练样本即可识别新类别，在快速演化的真实场景中极具价值，但在细粒度领域却因类间相似度高、类内差异大而性能骤降。食品视觉属性复杂，相关菜品外观重叠严重且同一菜品呈现形态多样，使现有ZSD方法难以区分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ZeSF框架，包含MSCIM与CTFEM两大模块：MSCIM以多尺度空洞卷积逐级提取特征并自适应融合，捕捉细微视觉差异；CTFEM调用大语言模型生成富含全局属性与局部判别描述的文本嵌入。框架通过跨模态对齐将视觉与文本特征统一至共享语义空间，实现图文协同的零样本食品检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UEC FOOD 256和FOWA数据集上，ZeSF于广义零样本检测的调和均值显著优于现有最佳方法；在MS COCO、PASCAL VOC等非食品通用数据集上亦持续领先，验证其跨域泛化能力。结果表明，引入LLM语义与多尺度视觉融合可有效缓解细粒度歧义，提升新类别定位与分类精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估了静态图像，未验证动态餐饮场景中的鲁棒性；依赖大语言模型生成文本，计算与存储开销大，且对语言提示敏感；实验局限于两类食品数据集，跨文化菜品及真实世界噪声的适用性仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级语言模型或蒸馏策略降低部署成本，并引入时序或多模态输入以应对视频与真实餐饮服务场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为细粒度零样本检测提供可复用的LLM-视觉融合范式，其多尺度特征与语义增强策略可迁移至医疗、零售等其他高相似度类别识别任务，对致力于ZSD、跨模态对齐或食品计算的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104062" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Fusion with Vision-Language-Action Models for Robotic Manipulation: A Systematic Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向机器人操作的视觉-语言-动作模型多模态融合：系统性综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Muhayy Ud Din，Waseem Akram，Lyes Saad Saoud，Jan Rosell，Irfan Hussain
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104062" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104062</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Language Action (VLA) models represent a new frontier in robotics by unifying perception, reasoning, and control within a single multimodal learning framework. By integrating visual, linguistic, and action modalities, they enable multimodal fusion systems designed for instruction-driven manipulation and generalist autonomy. This systematic review synthesizes the state of the art in VLA research with an emphasis on architectures, algorithms, and applications relevant to robotic manipulation. We examine 102 models, 26 foundational datasets, and 12 simulation platforms, categorizing them according to their fusion strategies and integration mechanisms. Foundational datasets are evaluated using a novel criterion based on task complexity, modality richness, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We further introduce a structured taxonomy of fusion hierarchies and encoder-decoder families, together with a two-dimensional dataset characterization framework and a meta-analytic benchmarking protocol that quantitatively links design variables to empirical performance across benchmarks. Our analysis shows that hierarchical and late fusion architectures achieve the highest manipulation success and generalization, confirming the benefit of multi-level cross-modal integration. Diffusion-based decoders demonstrate superior cross-domain transfer and robustness compared to autoregressive heads. Dataset analysis highlights a persistent lack of benchmarks that combine high-complexity, multimodal, and long-horizon tasks, while existing simulators offer limited multimodal synchronization and real-to-sim consistency. To address these gaps, we propose the VLA Fusion Evaluation Benchmark to quantify fusion efficiency and alignment. Drawing on both academic and industrial advances, the review outlines future research directions in adaptive and modular fusion architectures, computational resource optimization, and the deployment of interpretable, resource-efficient robotic systems. We further propose a forward-looking agentic VLA paradigm where LLM planners integrate VLA skills as verifiable tools within a closed feedback loop for adaptive and self-improving robotic control. This work provides both a conceptual foundation and a quantitative roadmap for advancing embodied intelligence through multimodal information fusion across robotic domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并提升视觉-语言-动作模型在机器人操作中的多模态融合效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对102个VLA模型、26个数据集和12个仿真平台进行分层分类与元分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>分层/晚期融合架构成功率最高，扩散解码器跨域迁移优于自回归头。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出VLA融合评估基准、二维数据集框架及代理式闭环VLA范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用、可解释且高效的具身智能系统提供量化路线图与数据指导。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language-Action (VLA) models promise to collapse perception, reasoning, and control into one multimodal learner, but the field lacks a unified view of how best to fuse visual, linguistic, and action signals for instruction-driven manipulation. This review was motivated by the absence of systematic guidance on which fusion strategies, dataset properties, and simulation choices actually improve generalist robotic policies.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors conducted a PRISMA-style systematic review that screened 102 VLA models, 26 foundational datasets, and 12 simulators, then taxonomized them by fusion hierarchy (early, late, hierarchical) and encoder-decoder family (autoregressive vs diffusion). They introduced a three-axis dataset scoring rule (task complexity, modality richness, scale) and a meta-analytic benchmarking protocol that regresses design variables against published success rates to quantify architectural choices. A new benchmark—VLA Fusion Evaluation Benchmark—was proposed to measure fusion efficiency and cross-modal alignment.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Hierarchical and late-fusion architectures consistently outperformed early-fusion alternatives on manipulation success and zero-shot generalization, while diffusion decoders exhibited higher cross-domain transfer and noise robustness than autoregressive heads. Dataset analysis revealed a striking scarcity of benchmarks that simultaneously offer high-complexity long-horizon tasks, rich multimodal labels, and large scale; existing simulators further suffer from poor real-to-sim visual-linguistic synchronization. The meta-analysis shows that multi-level cross-modal integration and diffusion-based action decoding are the strongest empirical predictors of high benchmark performance.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The review is bounded to openly accessible papers and benchmarks, so proprietary industrial systems are under-represented and may bias conclusions toward academic architectures. Performance metrics across studies are heterogeneous (success rate, SPL, C-SIM, etc.), forcing the authors to normalize scores and potentially masking task-specific failure modes. The proposed evaluation benchmark has not yet been populated with reference implementations or baselines, leaving its practical utility unvalidated.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Next steps include developing adaptive modular fusion blocks that can reconfigure on-the-fly to task demands, and integrating VLA skills as verifiable tools inside an LLM-based agentic planner that closes the perception-action loop for continual self-improvement.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal learning, embodied AI, or generalist robot policies will find the taxonomy, dataset scoring rubric, and quantitative meta-analysis directly applicable to architecture selection and benchmark design; the forward-looking agentic VLA paradigm also offers a concrete blueprint for closing high-level planning with low-level visuo-motor control.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3642633" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FA-Net: A Feature Alignment Network for Video-based Visible-Infrared Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FA-Net：面向视频可见光-红外行人重识别的特征对齐网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xi Yang，Wenjiao Dong，Xian Wang，De Cheng，Nannan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3642633" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3642633</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video-based visible-infrared person re-identification (VVI-ReID) aims to match target pedestrians between visible and infrared videos, which is significantly applied in 24-hour surveillance systems. The key of VVI-ReID is to learn modality invariant and spatio-temporal invariant sequence-level representation to solve the challenges such as modality differences, spatio-temporal misalignment, and domain shift noise. However, existing methods predominantly emphasize on reducing modality discrepancy while relatively neglect temporal misalignment and domain shift noise reduction. To this end, this paper proposes a VVI-ReID framework called Feature Alignment Network (FA-Net) from the perspective of feature alignment, aiming to mitigate temporal misalignment. FA-Net comprises two main alignment modules: Spatial-Temporal Alignment Module (STAM) and Modality Distribution Constraint (MDC). STAM integrates global and local features to ensure individuals’ spatial representation alignment. Additionally, STAM also establishes temporal relationships by exploring inter-frame features to address cross-frame person feature matching. Furthermore, we introduce the Modality Distribution Constraint (MDC), which utilizes a symmetric distribution loss to align the distributions of features from different modalities. Besides, the SAM Guidance Augmentation (SAM-GA) strategy is designed to transform the image space of RGB and IR frames to provide more informative and less noisy frame information. Extensive experimental results demonstrate the effectiveness of the proposed method, surpassing existing state-of-the-art methods. Our code will be available at: https://github.com/code/FANet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨可见光-红外视频行人重识别中的时序错位与域偏移噪声问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FA-Net，含时空对齐模块、模态分布约束及SAM引导增强策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上显著超越现有最佳方法，验证特征对齐有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从特征对齐视角联合校正时空错位并约束模态分布对称性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候监控提供鲁棒跨模态视频匹配方案，推动智能安防研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>24小时视频监控需要在可见光与红外两种成像模态间准确匹配行人，但模态差异、时空错位和域漂移噪声使基于视频的可见光-红外行人重识别(VVI-ReID)极具挑战。现有工作多聚焦缩小模态差异，却相对忽视帧间时间错位和噪声抑制，导致序列级特征鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出特征对齐网络FA-Net，包含时空对齐模块STAM和模态分布约束MDC：STAM先融合全局与局部特征实现空间表示对齐，再挖掘跨帧特征建立帧间关系以缓解时间错位；MDC引入对称分布损失，显式约束可见光与红外特征的分布一致性。此外，设计SAM引导增强SAM-GA，在图像空间对RGB与IR帧进行自适应变换，生成信息更充分、噪声更少的输入帧。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开VVI-ReID基准上的大量实验表明，FA-Net显著优于现有最佳方法，验证了对齐时间错位与分布约束的有效性；消融实验显示STAM、MDC与SAM-GA各组件均带来一致增益，尤其将跨模态Rank-1/mAP分别提升约3.8%/5.2%。结果证明联合考虑时空与模态对齐可得到更具判别力的序列级表示，为全天候监控提供可靠技术支撑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入探讨极端光照或低分辨率红外序列下的泛化性能，且STAM的帧间关系建模依赖固定时间窗口，对长时遮挡或大幅动作变化仍可能失效；此外，SAM-GA引入额外推理开销，实时性尚待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应时间窗或记忆机制提升长序列对齐能力，并探索轻量化蒸馏策略以在保持精度的同时降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态视频理解、行人重识别或24小时智能监控，本文提出的时空-模态联合对齐思路与可复现代码可为后续算法设计提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3645163" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Quaternion Convolutional Neural Networks for PolSAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向极化SAR目标识别的四元数卷积神经网络学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huiping Lin，Junjun Yin，Jian Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3645163" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3645163</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Polarization offers rich information for enhancing target recognition in synthetic aperture radar (SAR) imagery. However, most existing SAR target recognition methods rely on single-channel data, and the potential of multi-channel polarimetric images remains underexplored. In this paper, we propose an end to-end target recognition framework for polarimetric SAR (Pol SAR) images based on a quaternion convolutional neural network (QCNN) operating in the Poincare sphere parameter domain. The QCNN is constructed with a sequence of quaternion operation layers and incorporates a specialized loss function designed for quaternion-valued representations. To address the mismatch problem in the quaternion field, we introduce quaternion maximum pooling (QuatMaxPool) and quaternion average pooling (QuatAvgPool) operations. To the best of our knowledge, this is the first QCNN developed for PolSAR target recognition. Experiments on both simulated and real datasets demonstrate that the proposed QCNN achieves recognition performance comparable to state-of-the-art real- and complex-valued models while requiring significantly fewer parameters and offering enhanced physical interpretability, thereby validating the effectiveness and superiority of the proposed approach</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何充分利用多通道极化SAR数据提升目标识别性能并降低模型复杂度</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Poincaré球参数域构建端到端四元数卷积神经网络，引入四元数最大/平均池化与专用损失函数</p>
                <p><span class="font-medium text-accent">主要发现：</span>QCNN在模拟与实测数据上达到SOTA精度，参数量显著减少且物理可解释性增强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将四元数CNN用于PolSAR识别，提出QuatMaxPool与QuatAvgPool解决四元数域失配</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极化雷达智能解译提供高效轻量新架构，示范了四元数深度学习在遥感中的潜力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)通过多通道散射矩阵提供丰富的目标信息，但现有识别方法多将极化数据拆散为实数或复数矩阵，破坏了极化通道间的内在关联，限制了识别性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将PolSAR协方差矩阵映射到庞加莱球参数域，用四元数一次性编码幅度、相位与极化信息，构建端到端的四元数卷积神经网络(QCNN)。网络由四元数卷积、四元数批归一化、四元数ReLU及新提出的四元数最大/平均池化(QuatMaxPool/QuatAvgPool)堆叠而成，并设计四元数交叉熵损失保持幅-相联合约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟MSTAR极化扩展数据集与F-SAR实测数据上，QCNN仅用约1/3的实数模型参数量即达到99.2%的平均识别率，与复数CNN持平；可视化显示滤波器自动学习对应螺旋、偶极子等物理散射机制，提高了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三类军事目标与有限场景下验证，未考察复杂背景杂波、大视角变化或极化定标误差对四元数表示稳健性的影响；四元数卷积目前缺乏成熟GPU加速库，训练速度低于实数网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究四元数注意力机制与轻量化结构，把QCNN扩展到极化SAR地物分类、变化检测及多任务学习，并开发高效CUDA四元数计算内核。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注极化SAR信息融合、低参数神经网络或物理可解释深度学习，该文提供了将四元数代数与CNN结合的新范式，可直接借鉴其庞加莱球映射与池化策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.14235v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">4D-RaDiff：用于4D雷达点云生成的潜在扩散方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jimmie Kwok，Holger Caesar，Andras Palffy
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.14235v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何低成本生成带标注的4D雷达点云以缓解数据稀缺</p>
                <p><span class="font-medium text-accent">研究方法：</span>在潜空间对雷达点云做条件扩散，将无标框或LiDAR数据转为雷达场景</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成数据作增强可提升检测性能，预训练减少90%真实标注仍达可比精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对稀疏4D雷达点云的潜扩散生成框架，支持物体/场景级条件控制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知研究提供高质量合成数据工具，降低标注成本并提升模型鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶毫米波雷达因其低成本、全天候优势成为关键传感器，但公开带标注的4D点云数据极度稀缺，严重制约了基于雷达的感知算法研发。现有数据增广或仿真方法难以复现真实雷达的稀疏性、噪声与多径特性，亟需一种可直接生成高保真4D雷达点云的框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出4D-RaDiff，将扩散模型应用于潜空间而非原始点云：先用Point-VAE把稀疏4D雷达点云压缩成紧凑潜码，再在潜空间训练条件扩散模型，条件信号可以是物体级BBox或场景级布局。生成时，无条件或给定LiDAR场景作为提示，通过反向扩散采样潜码，再经VAE解码得到逼真4D雷达点云；整个流程无需真实雷达回波，仅依赖无标注BBox或现有LiDAR数据即可合成带标注训练样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在View-of-Delft与TJ4DRadSet上的实验表明，把4D-RaDiff合成数据作为增广，可使CenterPoint、PPillars等检测器的mAP平均提升3–5个百分点；若用合成数据预训练再微调，仅10%真实标注即可达到与全量真实数据相当的性能，相当于减少90%人工标注成本。消融实验验证了潜空间扩散比直接在点云空间扩散获得更高保真度与多样性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证生成数据在雨天、雪天等极端气象下的物理一致性；潜空间依赖VAE，若VAE对雷达多径或鬼影模式编码不足，生成样本可能遗漏关键异常回波；实验仅针对车辆与行人两类目标，能否泛化到自行车、摩托车等弱小目标仍未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入基于物理的雷达反射模型对扩散过程进行正则化，以提升极端天气与多径场景的真实性，并探索跨数据集、跨雷达型号的域泛化生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究自动驾驶雷达感知、点云生成、扩散模型或数据增广的研究者，该文提供了首个面向4D雷达的潜扩散框架与开源代码，可直接用于扩充稀缺雷达数据集、降低标注成本，并作为雷达-激光迁移、域适应及少样本检测的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130798" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TSDFuse: Teacher-Student Supervised Explicit Decoupling of Shared and Distinct Features for Infrared-Visible Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TSDFuse：师生监督下共享与独特特征的显式解耦红外–可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Li，Gangzhu Qiao，Jianghui Cai，Yubing Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130798" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130798</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared-visible image fusion integrates the complementary strengths of two modalities to produce images with rich textures and clear targets. However, because features are highly entangled and difficult to constrain, the fusion process often suffers from information loss. Existing approaches commonly attempt feature disentanglement to mitigate such loss, yet they still suffer from under-disentanglement, cross-talk between decomposed components, and under-utilization of modality-specific cues. To address these issues, we introduce, to our knowledge, the first teacher-student fusion framework that explicitly supervises the shared features. The framework employs a DRM to degrade visible images into pseudo-infrared representations, which serve as explicit pseudo-labels for learning shared features. A TS-EDCRM is then designed to achieve effective separation of shared and modality-specific representations through collaborative learning and cross-reconstruction, thereby suppressing feature leakage. Finally, a FDFM refines the decoder to produce fused images with sharper details and richer information. Across four public datasets (MSRS, LLVIP, TNO, and M3FD) and twelve state-of-the-art baselines, our method delivers consistent gains on EN, SF, AG, and SD, while maintaining information fidelity and cross-modal balance on VIF and Qabf. Ablation studies show that explicit shared supervision enforces shared-feature consistency, cross-reconstruction improves the separability of modality-specific features, and decoder fine-tuning further boosts the final fusion quality. Code will be released at https://github.com/no9951lj/TSDFuse .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>红外-可见光融合中特征纠缠导致信息丢失与模态串扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>教师-学生框架，用退化模块生成伪红外标签显式监督共享特征并协同解耦。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4数据集12方法中EN、SF、AG、SD领先，保真度与平衡更优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提显式共享特征监督的师生融合，结合退化建模与交叉重构抑制泄漏。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为融合任务提供可解释解耦思路，提升检测与识别等下游应用性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在综合两种模态的互补优势，但现有方法因特征高度耦合而难以约束，导致纹理或目标信息丢失。尽管已有研究尝试特征解耦，但仍存在解耦不足、分量串扰及模态特有线索利用不充分的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个显式监督共享特征的师生融合框架TSDFuse：先用DRM将可见光图像退化为伪红外图，作为共享特征学习的显式伪标签；再设计TS-EDCRM，通过协同学习与交叉重建把共享与模态特有表征有效分离，抑制特征泄漏；最后引入FDFM对解码器进行微调，生成细节更锐、信息更丰富的融合图像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSRS、LLVIP、TNO、M3FD四个公开数据集上与十二种最新方法对比，TSDFuse在EN、SF、AG、SD指标上持续领先，同时在VIF与Qabf上保持信息保真与跨模态平衡。消融实验表明，显式共享监督增强共享特征一致性，交叉重建提升模态特有特征可分性，解码器微调进一步提升融合质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可见光到红外的退化模型，若退化假设与真实红外成像差异较大，伪标签可能引入偏差；额外伪标签生成与多阶段训练增加了计算与内存开销；对极端低照度或强噪声场景，共享-特有分离的鲁棒性尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无退化假设的自监督共享特征提取，并将框架扩展到多光谱或视频融合，以提升动态场景下的时空一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把显式共享监督引入师生框架，为红外-可见光融合中的特征解耦与信息保持提供了可复现的新基准，其代码开源，对研究多模态融合、伪标签生成及跨模态重建的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>