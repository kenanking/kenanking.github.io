<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-25</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-25 10:50 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">968</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感智能处理的交叉问题，核心阅读集中在目标检测、视觉SLAM及模型压缩等方向，同时对大模型与自监督学习保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测领域收藏量最高且持续追踪Kaiming He、Ross Girshick等权威作者，形成从RCNN系列到压缩部署的完整阅读链；合成孔径雷达(SAR)图像理解与遥感目标识别亦形成稳定积累，兼顾IEEE TGRS与《雷达学报》两大阵地。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹明显横跨计算机视觉与遥感科学，既关注CVPR/ICCV前沿算法，又系统吸收SAR成像、旋转目标检测等遥感专用方法，体现出“视觉算法+遥感数据”的交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起收藏量激增且新增关键词聚焦“SAR目标识别、自动驾驶感知”，显示正将视觉检测技术向遥感与自动驾驶场景深化；同时快速跟进大语言模型、扩散模型和DeepSeek，表明对通用基础模型与生成式AI保持同步关注。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步阅读多模态大模型在SAR-光学融合、自动驾驶跨模态感知中的最新应用，以及面向边缘部署的量化/蒸馏框架，巩固视觉-遥感交叉优势并拓展实时智能系统能力。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 942/942 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">48</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-24 10:29 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸对齐', '模型压缩', '对比学习', '重参数化', '卫星导航'],
            datasets: [{
              data: [22, 35, 15, 12, 18, 10, 8, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u63a8\u7406\u4f18\u5316",
            size: 76,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 54,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "\u89c6\u89c9Transformer"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0eTransformer\u8bbe\u8ba1",
            size: 54,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 45,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 5,
            label: "SLAM\u4e0e\u5e95\u5c42\u89c6\u89c9\u7279\u5f81",
            size: 40,
            keywords: ["SIFT", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 6,
            label: "CNN\u53ef\u89e3\u91ca\u6027\u4e0e\u7279\u5f81\u53ef\u89c6\u5316",
            size: 39,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 38,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 38,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5f3a\u5316\u5b66\u4e60\u4e0e\u6301\u7eed\u5b66\u4e60",
            size: 38,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 10,
            label: "SAR\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 37,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 36,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u591a\u6a21\u6001"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 13,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 14,
            label: "\u5c0f\u6837\u672c\u4e0e\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 30,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u7efc\u8ff0"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 27,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 17,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 27,
            keywords: ["\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "cross attention", "edge guidance"]
          },
          
          {
            id: 18,
            label: "SAR\u98de\u673a\u68c0\u6d4b\u8bc6\u522b",
            size: 26,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 19,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e0e\u6d41\u6a21\u578b",
            size: 26,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u8bbe\u8ba1\u6a21\u5f0f"]
          },
          
          {
            id: 20,
            label: "\u667a\u80fd\u96f7\u8fbe\u6297\u5e72\u6270",
            size: 24,
            keywords: ["LaTeX", "\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 24,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 22,
            label: "SAR\u7269\u7406\u53ef\u89e3\u91ca\u5b66\u4e60",
            size: 23,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u76ee\u6807\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0"]
          },
          
          {
            id: 23,
            label: "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u635f\u5931\u8bbe\u8ba1",
            size: 23,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5224\u522b\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 24,
            label: "\u751f\u6210\u5bf9\u6297\u4e0e\u751f\u6210\u6d41",
            size: 20,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 25,
            label: "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b",
            size: 18,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 26,
            label: "\u5206\u5e03\u5916\u6cdb\u5316\u4e0e\u5bf9\u6297\u6837\u672c",
            size: 16,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b"]
          },
          
          {
            id: 27,
            label: "SAR CFAR\u8230\u8239\u68c0\u6d4b",
            size: 11,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u7ea2\u5916\u56fe\u50cf\u53bb\u566a\u589e\u5f3a",
            size: 10,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 29,
            label: "YOLO\u4eba\u8138\u68c0\u6d4b",
            size: 5,
            keywords: []
          }
          
        ];

        const links = [{"source": 7, "target": 23, "value": 0.9418755296469045}, {"source": 18, "target": 20, "value": 0.903822214099914}, {"source": 4, "target": 6, "value": 0.8701394157956346}, {"source": 21, "target": 22, "value": 0.9306437622524354}, {"source": 7, "target": 29, "value": 0.8915406364436496}, {"source": 12, "target": 28, "value": 0.895539338285265}, {"source": 5, "target": 19, "value": 0.9158104145379987}, {"source": 10, "target": 18, "value": 0.9387032763942617}, {"source": 10, "target": 21, "value": 0.9140654957557874}, {"source": 1, "target": 24, "value": 0.893552644910834}, {"source": 18, "target": 22, "value": 0.955419507357583}, {"source": 6, "target": 26, "value": 0.8960315808061123}, {"source": 18, "target": 25, "value": 0.9000477296729186}, {"source": 20, "target": 22, "value": 0.8881913633602737}, {"source": 5, "target": 9, "value": 0.8905220671890401}, {"source": 3, "target": 18, "value": 0.937616808934578}, {"source": 12, "target": 18, "value": 0.9078929561063356}, {"source": 22, "target": 25, "value": 0.8993169328541166}, {"source": 20, "target": 28, "value": 0.8863019020919136}, {"source": 8, "target": 11, "value": 0.9028929323945677}, {"source": 0, "target": 1, "value": 0.8886851062053617}, {"source": 2, "target": 4, "value": 0.8887053384505468}, {"source": 1, "target": 2, "value": 0.9248309955166079}, {"source": 3, "target": 27, "value": 0.9242029487712927}, {"source": 2, "target": 7, "value": 0.9158800285132361}, {"source": 9, "target": 19, "value": 0.9036967750735683}, {"source": 11, "target": 13, "value": 0.8884566325142863}, {"source": 10, "target": 17, "value": 0.9373754443619279}, {"source": 1, "target": 14, "value": 0.9224419064087589}, {"source": 8, "target": 29, "value": 0.8971430819329672}, {"source": 15, "target": 25, "value": 0.8560658963154595}, {"source": 16, "target": 24, "value": 0.9486581175657668}, {"source": 18, "target": 27, "value": 0.9073569512057563}, {"source": 14, "target": 23, "value": 0.9193406940900609}, {"source": 0, "target": 9, "value": 0.8953813747248615}, {"source": 14, "target": 26, "value": 0.886496026663589}, {"source": 17, "target": 22, "value": 0.9257136945841727}, {"source": 1, "target": 13, "value": 0.8937935468968362}, {"source": 2, "target": 6, "value": 0.9378437411176467}, {"source": 11, "target": 15, "value": 0.8517671970119475}, {"source": 1, "target": 16, "value": 0.8868771590606571}, {"source": 10, "target": 22, "value": 0.9606760954720092}, {"source": 7, "target": 11, "value": 0.8909098417545036}, {"source": 6, "target": 9, "value": 0.9155292420871931}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨域检测的论文、2篇关于小样本识别的论文与1篇关于零样本分割的论文。</p>
            
            <p><strong class="text-accent">跨域检测</strong>：《Bridging optical and SAR images via semantic prompt-guided progressive alignment》提出语义提示引导的渐进对齐框架，实现光学-SAR旋转船舶检测的跨域迁移；《OSFGNet: Object Saliency-Driven Feature Aggregation Network》通过显著性驱动特征聚合，联合可见光-红外模态提升全天候目标检测鲁棒性。</p>
            
            <p><strong class="text-accent">小样本识别</strong>：《Consistency-Regularized GAN for Few-Shot SAR Target Recognition》利用一致性正则化GAN合成高质量SAR样本，缓解极端数据稀缺下的目标识别瓶颈；《A Few-Shot Object Detection Framework for Remote Sensing Images》结合自适应决策边界与多尺度特征增强，在遥感图像中实现小样本目标检测。</p>
            
            <p><strong class="text-accent">零样本分割</strong>：《DGL-RSIS: Decoupling global spatial context and local class semantics》通过解耦全局空间背景与局部类别语义，无需任何训练即可完成遥感图像的零样本语义分割。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于跨模态感知的论文、6篇关于大模型优化与泛化的论文、5篇关于小样本与开放世界学习的论文、4篇关于视觉生成与控制的论文、3篇关于小目标检测的论文、2篇关于图像融合的论文、2篇关于数学推理的论文。</p>
            
            <p><strong class="text-text-secondary">跨模态感知</strong>：研究光学-SAR、可见光-红外等不同模态的对齐与融合，如《Bridging optical and SAR images via semantic prompt-guided progressive alignment》提出语义提示渐进对齐实现旋转跨域舰船检测，《A Cross-Modality Feature Adaptive Interaction Approach》设计特征自适应交互提升空中RGB-红外目标检测。</p>
            
            <p><strong class="text-text-secondary">大模型优化</strong>：聚焦大模型后训练、提示学习与质量提升，如《CoScale-RL》通过数据-计算协同缩放稳定大推理模型后训练，《AITQE》提出自适应图文质量增强器 scalable 地提升 MLLM 预训练数据质量。</p>
            
            <p><strong class="text-text-secondary">小样本学习</strong>：解决开放世界下样本稀缺、标签不可靠问题，如《CO⁺₃》构建基础模型协作联盟改进开放世界小样本识别，《PromptMix》利用大模型生成提示增强视觉-语言模型泛化。</p>
            
            <p><strong class="text-text-secondary">视觉生成</strong>：提升扩散模型可控性与生成质量，如《LaCon》提出Late-Constraint控制策略实现精细视觉生成，多篇工作探索文本-图像对齐与语义保真。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对高分辨率特征缺乏语义的问题，《HR-SemNet》引入局部上下文语义增强小目标检测性能，同时保持高分辨率表示。</p>
            
            <p><strong class="text-text-secondary">图像融合</strong>：构建全天候多模态融合统一框架，《All-weather Multi-Modality Image Fusion》提出一体化模型并发布10万基准，提升复杂场景鲁棒性。</p>
            
            <p><strong class="text-text-secondary">数学推理</strong>：通过离线强化学习提升大模型数学能力，《PCL-Reasoner-V1.5》在325亿参数Qwen2.5基础上用SFT+RL训练取得推理突破。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 79%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging optical and SAR images via semantic prompt-guided progressive alignment for rotated cross-domain ship detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语义提示引导的渐进式对齐桥接光学与SAR图像以实现旋转跨域船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Longli Ran，Jiaming Li，Haodong Wu，Anqi Wu，Yi He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in remote sensing imagery is essential for diverse maritime-related tasks, including ocean surveillance, fisheries management, and environmental assessment. In operational scenarios, optical imagery provides rich texture cues under clear conditions, whereas synthetic aperture radar (SAR) enables reliable observation in nighttime and cloudy weather. However, cross-domain ship detection across optical and SAR modalities is still challenging due to discrepancies in imaging mechanisms, speckle noise, and background clutter, particularly in near-shore scenarios with similar reflection characteristics, together with the arbitrariness of ship orientation. To address these issues, we propose RotCD-Ship, a rotated cross-domain ship detection framework that bridges the domain gap between optical and SAR images while enabling accurate detection of arbitrarily oriented ships. Specifically, a domain knowledge-guided semantic prompt (DKSP) strategy based on SAR physical priors is introduced to suppress background clutter such as ship wakes and coastal interference. To handle modal divergence, we design a progressive feature alignment scheme that combines multi-scale local feature alignment (MSL-align) and global feature alignment (GF-align), enabling transfer of both fine-grained textures and high-level semantics across domains. Furthermore, a coarse-to-fine rotated region of interest (CF-RRoI) generator is developed to enhance localization precision of strip-like ships in SAR images by progressively refining orientation-aware proposals. Extensive evaluations on five public ship detection datasets show that RotCD-Ship significantly outperforms state-of-the-art methods in both accuracy and robustness, achieving an average mAP improvement of 7.5% in the horizontal ship detection task and 5.5% in the oriented ship detection task compared to the best existing methods. In addition, large-scale tests on Gaofen-3 SAR images further verify the strong generalization in dense-ship and complex coastal environments, highlighting the practical applicability of our framework for all-weather maritime monitoring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR图像跨域任意方向舰船检测的域差异与精确定位难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RotCD-Ship框架，融合SAR物理先验语义提示、渐进局部-全局特征对齐及粗到精旋转RoI生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五数据集上平均mAP提升7.5%（水平）/5.5%（旋转），高分三号大规模测试验证复杂海岸强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用SAR物理先验语义提示抑制背景杂波，并设计渐进对齐与粗-精旋转RoI提升跨域任意方向检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR全天候海事监测提供高精度跨域检测方案，推动海洋 surveillance 与灾害响应研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与SAR遥感成像机理迥异，导致跨模态舰船检测存在显著域差异，尤其在近岸区域，舰船朝向任意、背景杂波强，传统方法难以兼顾全天候、高精度检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RotCD-Ship框架，以SAR物理先验构建域知识引导的语义提示(DKSP)抑制船尾浪与岸杂波；设计渐进特征对齐(MSL-align+GF-align)在局部纹理与全局语义两级缩小域距；并引入粗到精旋转RoI生成器(CF-RRoI)逐步细化条带舰船的方位感知候选框，实现任意方向舰船检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五套公开数据集上，RotCD-Ship较最佳现有方法平均mAP提升7.5%(水平框)与5.5%(旋转框)；高分三号大尺度SAR密集舰船测试验证其在复杂海岸环境下的强泛化与全天候监测实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖SAR物理先验的准确建模，若成像参数或海况极端可能削弱DKSP效果；渐进对齐增加计算链路，实时性未充分讨论；光学极端条件(强光斑、薄云)下的鲁棒性尚缺系统评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域自适应与在线自监督提示，进一步摆脱对标注SAR先验的依赖，并探索轻量化部署以满足实时舰载/星载需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事跨模态目标检测、旋转框识别、SAR图像去噪与海洋遥感的研究者具有直接参考价值，其提示-对齐-细化范式可迁移至其他遥感跨域小目标检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 65%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本SAR目标识别的一致性正则化GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15681v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少量SAR样本下稳定训练GAN并生成高质量数据以支撑小样本目标识别</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，用双分支判别器解耦对抗与表征学习，并引入通道插值及双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR与SRSDD 8-shot任务上达71.21%与51.64%精度，超越现有方法且参数量仅扩散模型的5%</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，使GAN在极少数据下仍可合成语义保真的SAR图像</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量级高保真数据增强方案，可即插即用于多种自监督框架并显著提升性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别在民用与军事侦察中至关重要，但真实场景往往只能获得极少量标注图像，传统深度模型难以训练。生成式数据增广被视为缓解数据稀缺的有效途径，却陷入“用大数据训练GAN→再用GAN产生数据”的自相矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Consistency-Regularized GAN(Cr-GAN)，通过双分支判别器把对抗学习分支与表示学习分支解耦，使后者可在极少样本下稳定收敛。在表示分支中引入通道级特征插值，直接合成新的潜在特征向量，无需额外真实图像即可扩充训练信号。配合双域循环一致性损失，保证合成图像与真实图像在语义与几何层面保持一致，从而提升样本多样性与保真度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将下游SSL分类准确率提升至71.21%，比现有最佳基线高出约10个百分点；在SRSDD同类任务上亦达51.64%，同时参数量仅为当前先进扩散模型的5%左右。消融实验显示，双分支判别器与循环一致性各自贡献显著，且框架可无缝嵌入StyleGAN2、SNGAN等多种骨干。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开SAR数据集上验证，尚未覆盖更复杂的多视角、多波段或强杂波场景；双分支结构带来额外超参数，极端1-shot条件下稳定性仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将Cr-GAN扩展至多模态遥感数据，或结合神经辐射场(NeRF)实现三维SAR目标生成，以进一步降低对真实样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成或SAR自动目标识别，本文提供的双解耦判别器与一致性正则思路可直接迁移并增强现有方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105113" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGL-RSIS: Decoupling global spatial context and local class semantics for training-free remote sensing image segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGL-RSIS：解耦全局空间上下文与局部类别语义的免训练遥感图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boyi Li，Ce Zhang，Richard M. Timmerman，Wenxuan Bao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105113" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105113</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global–Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual–Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual–Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需训练的条件下，将自然图像预训练视觉-语言模型迁移到遥感开放词汇与指代表达分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGL-RSIS框架，用全局-局部解耦模块分离文本上下文与语义，结合局部对齐和全局Grad-CAM掩膜选择完成分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID与RRSIS-D基准上，DGL-RSIS以零训练方式超越现有无训练方法，消融实验验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现统一的无训练遥感图像分割框架，通过显式分离全局上下文与局部语义提升跨模态对齐效果。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供免训练即可利用大模型语义能力的实用方案，降低标注与计算成本并推动开放词汇应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) have shown great promise in multimodal understanding, but their direct application to remote sensing (RS) image segmentation is hindered by a large domain gap and the heterogeneity of RS tasks such as open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Existing VLMs trained on natural images struggle to generalize to RS imagery without task-specific fine-tuning, motivating a training-free transfer strategy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGL-RSIS introduces a Global–Local Decoupling (GLD) module that splits textual queries into local semantic tokens and global contextual tokens while decomposing images into class-agnostic mask proposals. A Local Visual–Textual Alignment (LVTA) module then enriches text embeddings via knowledge-guided prompt engineering and matches them to context-aware visual features extracted from each mask, enabling OVSS without retraining. For RES, a Global Visual–Textual Alignment (GVTA) module applies a global-enhanced Grad-CAM to capture spatial context, followed by a mask selection stage that converts pixel activations into full mask predictions, integrating both local and global cues in a unified inference pipeline.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the iSAID OVSS benchmark, DGL-RSIS surpasses prior training-free methods, and on the RRSIS-D RES dataset it achieves the best mask AP among zero-shot competitors, demonstrating that decoupled alignment effectively transfers natural-image VLMs to RS scenes. Ablation experiments show that removing either GLD, LVTA, or GVTA causes consistent performance drops, confirming the contribution of each component. The framework operates without any RS-specific training, offering an immediate deployment option for new sensors or vocabularies.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still relies on pretrained VLMs that were never exposed to RS statistics, so performance remains below fully supervised RS specialists, especially for rare land-cover classes. Computational cost scales linearly with the number of mask proposals and textual tokens, leading to longer inference times on large tiles. Grad-CAM heatmaps can be noisy over heterogeneous RS scenes, occasionally propagating errors into the final mask selection step.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporating self-supervised RS adapters that remain training-free at test time could further close the domain gap, while distilling the global–local alignment into a lightweight student network would reduce runtime.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring zero-shot or open-vocabulary segmentation in remote sensing will find a practical, training-free baseline that disentangles semantic and contextual alignment, providing a modular blueprint for extending VLMs to new sensors, tasks, or languages without annotation costs.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030388" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Few-Shot Object Detection Framework for Remote Sensing Images Based on Adaptive Decision Boundary and Multi-Scale Feature Enhancement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应决策边界与多尺度特征增强的遥感图像小样本目标检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lijiale Yang，Bangjie Li，Dongdong Guan，Deliang Xiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030388" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030388</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Given the high cost of acquiring large-scale annotated datasets, few-shot object detection (FSOD) has emerged as an increasingly important research direction. However, existing FSOD methods face two critical challenges in remote sensing images (RSIs): (1) features of small targets within remote sensing images are incompletely represented due to extremely small-scale and cluttered backgrounds, which weakens discriminability and leads to significant detection degradation; (2) unified classification boundaries fail to handle the distinct confidence distributions between well-sampled base classes and sparsely sampled novel classes, leading to ineffective knowledge transfer. To address these issues, we propose TS-FSOD, a Transfer-Stable FSOD framework with two key innovations. First, the proposed detector integrates a Feature Enhancement Module (FEM) leveraging hierarchical attention mechanisms to alleviate small target feature attenuation, and an Adaptive Fusion Unit (AFU) utilizing spatial-channel selection to strengthen target feature representations while mitigating background interference. Second, Dynamic Temperature-scaling Learnable Classifier (DTLC) employs separate learnable temperature parameters for base and novel classes, combined with difficulty-aware weighting and dynamic adjustment, to adaptively calibrate decision boundaries for stable knowledge transfer. Experiments on DIOR and NWPU VHR-10 datasets show that TS-FSOD achieves competitive or superior performance compared to state-of-the-art methods, with improvements up to 4.30% mAP, particularly excelling in 3-shot and 5-shot scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感小样本目标检测中小目标特征缺失与基类/新类决策边界不一致问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TS-FSOD框架，含多尺度特征增强模块与动态温度缩放可学习分类器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR和NWPU VHR-10上mAP提升达4.30%，3-shot/5-shot场景表现最优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层注意力特征增强与类特定动态温度校准引入遥感小样本检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为昂贵标注下的遥感目标检测提供即插即用的高性能小样本解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像目标检测通常依赖大规模标注数据，但获取成本高昂，促使小样本检测(FSOD)成为热点。遥感影像中小目标尺度极小且背景杂乱，现有FSOD方法难以充分提取判别特征，导致性能骤降。此外，基础类与新颖类样本量差异使统一分类边界无法兼顾二者置信度分布，知识迁移不稳定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TS-FSOD框架，核心包括：1) Feature Enhancement Module采用分层注意力在多尺度特征图间强化小目标响应，并通过Adaptive Fusion Unit在通道-空间维度选择性融合，抑制复杂背景干扰；2) Dynamic Temperature-scaling Learnable Classifier为基类与新类分别设置可学习温度参数，结合难度感知加权动态调整决策边界，缓解置信度分布偏移，实现稳定迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR、NWPU VHR-10的3-shot/5-shot设置下，TS-FSOD mAP最高提升4.30%，显著优于现有SOTA，尤其对车辆、船舶等小目标召回率改善明显，证明其特征增强与自适应边界校准策略有效提升小样本遥感检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大尺度差异或更多类别跨域场景验证，计算开销随增强模块增加；温度参数需手动设定初始范围，可能依赖数据集先验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以进一步降低标注需求，并探索在线温度估计实现完全自适应边界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感目标检测或特征增强与分类边界校准技术，本文提供的分层注意力-融合策略与动态温度分类器可直接借鉴并扩展到其他少样本视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3657668" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OSFGNet: Object Saliency-Driven Feature Aggregation Network For Multi-Modal Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OSFGNet：面向多模态遥感目标检测的对象显著性驱动特征聚合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keming Bai，Linyuan He，Shiping Ma，Jiahao Dang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3657668" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3657668</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, object detection in visible (RGB) and infrared (IR) images has garnered extensive attention as a promising solution for achieving robust detection in all-weather scenarios. However, existing methods still face two major challenges: insufficient coordination between multimodal fusion and detection and inadequate feature extraction. To address these issues, this paper proposes a novel end-to-end network architecture for multimodal object detection, termed OSFGNet. Specifically, we introduce an OSM(object saliency-guided mechanism), which realizes adaptive fusion of ROI(region of interest) features from the detection head and fused features. This mechanism achieves collaborative optimization of multimodal features and detection while simplifying the training process, enabling integrated &#34;fusion-detection&#34; functionality. We design a tunable feature correlation factor that introduces learnable focusing parameters into attention mechanisms, dynamically adjusting attention distribution to balance local and global feature capture. Additionally, we embed an SFI(Spatial-Frequency Interaction) module to enhance fine-grained texture extraction through spatial-frequency interaction. Comprehensive experimental results demonstrate that our method achieves outstanding detection performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见光-红外多模态遥感检测中融合与检测脱节、特征提取不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OSFGNet，以目标显著性引导ROI-融合特征自适应聚合，并嵌入可调相关因子与空-频交互模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开多模态数据集上取得领先检测精度，验证融合-检测协同优化的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著性驱动的ROI-融合协同机制引入遥感检测，提出可调特征相关因子与空-频交互增强纹理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感目标检测提供端到端融合-检测一体化方案，可直接提升监测、搜救等应用可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-IR 双模态检测被视为全天候鲁天的关键，但现有网络要么把融合与检测割裂训练，要么在特征层面缺乏协同，导致恶劣天气或低照度下漏检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OSFGNet 以检测头输出的 ROI 特征为锚，提出 Object Saliency-guided Mechanism（OSM），将 ROI 显著性权重广播到融合支路，实现“哪里显著融哪里”的自适应融合；设计可调特征相关系数（TFCF），在通道-空间注意力中引入可学习缩放因子，动态平衡局部纹理与全局语义；嵌入 Spatial-Frequency Interaction（SFI）模块，通过小波高频路径与卷积低频路径并行交互，增强细粒度纹理与边缘；整体端到端训练，融合与检测共享损失，简化流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLVIP、FLIR 及自建全天候数据集上，OSFGNet 比 DA-Faster、ICAN、GAFF 等 SOTA 方法 mAP 提升 2.1–4.3 PP，特别对 &lt;32×32 小目标提升达 5.8 PP；参数量仅增加 5.4%，推理延迟增加 &lt;1 ms，证明显著性引导可在轻量代价下挖掘互补信息。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在地面车载与固定监控场景验证，未评估大幅旋转、侧摆等航空遥感视角；OSM 依赖检测头 ROI，若第一阶段提议缺失，显著性权重可能失效；SFI 的小波分解层数与系数为手工设定，未实现完全可学习。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 OSM 扩展为跨层显著性传播，解决高分辨率航空影像中微小目标提议缺失问题；研究完全可学习的频域卷积，使空间-频率交互与任务联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究多模态融合、全天候检测或轻量级遥感网络，该文提供的显著性引导融合思路、可调注意力机制与频域-空域交互模块均可直接迁移或作为 baseline 比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging optical and SAR images via semantic prompt-guided progressive alignment for rotated cross-domain ship detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语义提示引导的渐进式对齐桥接光学与SAR图像以实现旋转跨域船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Longli Ran，Jiaming Li，Haodong Wu，Anqi Wu，Yi He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in remote sensing imagery is essential for diverse maritime-related tasks, including ocean surveillance, fisheries management, and environmental assessment. In operational scenarios, optical imagery provides rich texture cues under clear conditions, whereas synthetic aperture radar (SAR) enables reliable observation in nighttime and cloudy weather. However, cross-domain ship detection across optical and SAR modalities is still challenging due to discrepancies in imaging mechanisms, speckle noise, and background clutter, particularly in near-shore scenarios with similar reflection characteristics, together with the arbitrariness of ship orientation. To address these issues, we propose RotCD-Ship, a rotated cross-domain ship detection framework that bridges the domain gap between optical and SAR images while enabling accurate detection of arbitrarily oriented ships. Specifically, a domain knowledge-guided semantic prompt (DKSP) strategy based on SAR physical priors is introduced to suppress background clutter such as ship wakes and coastal interference. To handle modal divergence, we design a progressive feature alignment scheme that combines multi-scale local feature alignment (MSL-align) and global feature alignment (GF-align), enabling transfer of both fine-grained textures and high-level semantics across domains. Furthermore, a coarse-to-fine rotated region of interest (CF-RRoI) generator is developed to enhance localization precision of strip-like ships in SAR images by progressively refining orientation-aware proposals. Extensive evaluations on five public ship detection datasets show that RotCD-Ship significantly outperforms state-of-the-art methods in both accuracy and robustness, achieving an average mAP improvement of 7.5% in the horizontal ship detection task and 5.5% in the oriented ship detection task compared to the best existing methods. In addition, large-scale tests on Gaofen-3 SAR images further verify the strong generalization in dense-ship and complex coastal environments, highlighting the practical applicability of our framework for all-weather maritime monitoring.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR图像跨域任意方向舰船检测的域差异与精确定位难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RotCD-Ship框架，融合SAR物理先验语义提示、渐进局部-全局特征对齐及粗到精旋转RoI生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五数据集上平均mAP提升7.5%（水平）/5.5%（旋转），高分三号大规模测试验证复杂海岸强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用SAR物理先验语义提示抑制背景杂波，并设计渐进对齐与粗-精旋转RoI提升跨域任意方向检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR全天候海事监测提供高精度跨域检测方案，推动海洋 surveillance 与灾害响应研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与SAR遥感成像机理迥异，导致跨模态舰船检测存在显著域差异，尤其在近岸区域，舰船朝向任意、背景杂波强，传统方法难以兼顾全天候、高精度检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RotCD-Ship框架，以SAR物理先验构建域知识引导的语义提示(DKSP)抑制船尾浪与岸杂波；设计渐进特征对齐(MSL-align+GF-align)在局部纹理与全局语义两级缩小域距；并引入粗到精旋转RoI生成器(CF-RRoI)逐步细化条带舰船的方位感知候选框，实现任意方向舰船检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五套公开数据集上，RotCD-Ship较最佳现有方法平均mAP提升7.5%(水平框)与5.5%(旋转框)；高分三号大尺度SAR密集舰船测试验证其在复杂海岸环境下的强泛化与全天候监测实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖SAR物理先验的准确建模，若成像参数或海况极端可能削弱DKSP效果；渐进对齐增加计算链路，实时性未充分讨论；光学极端条件(强光斑、薄云)下的鲁棒性尚缺系统评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域自适应与在线自监督提示，进一步摆脱对标注SAR先验的依赖，并探索轻量化部署以满足实时舰载/星载需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事跨模态目标检测、旋转框识别、SAR图像去噪与海洋遥感的研究者具有直接参考价值，其提示-对齐-细化范式可迁移至其他遥感跨域小目标检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104186" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PromptMix: LLM-Aided Prompt Learning for Generalizing Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PromptMix：利用LLM辅助提示学习泛化视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongcai Chen，Qinghua Zhang，Xinfa Shi，Lei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104186" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104186</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决视觉-语言模型在小样本、易混淆场景下泛化性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PromptMix框架，联合语义提示学习、多模态融合与LLM迭代优化提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在7个数据集上显著提升基类到新类及少样本场景下的准确率与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入LLM辅助提示演化与模态无关共享表示，缓解预训练-目标域分布差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工程领域标签稀缺场景提供免重训练、高泛化的视觉-语言模型适配方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在工程任务中落地时，常因数据稀缺或类别易混淆导致性能骤降；视觉-语言模型通过提示学习无需重训主干即可适配新域，但在小样本场景下仍易过拟合且提示表达力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PromptMix提出三模块框架：首先，Modality-Agnostic Shared Representation模块将预训练与目标数据映射到共享潜空间以缓解分布差异；其次，LLM-Aided Prompt Evolution机制利用大语言模型对可学习上下文提示进行语义扩充与迭代精炼；最后，Cross-Attentive Adapter通过跨模态注意力在低样本条件下强化图文信息融合与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六个公开基准与一个自建工业缺陷数据集上，PromptMix在基类到新类泛化及1/5/10-shot设置下均显著优于现有提示学习方法，平均提升5-12%，在工业场景仅用10%标注数据即达到全量数据95%的精度，证明其语义表示与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖大语言模型在线交互，增加推理延迟与计算成本；共享潜空间假设在预训练与目标域极度异构时可能失效；工业数据集仅涵盖缺陷检测，尚需在更多工程任务验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化LLM蒸馏以加速提示演化，并引入物理约束或因果机制提升跨域共享空间的可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本视觉-语言适配、工业检测或提示学习泛化，PromptMix提供的LLM驱动提示演化与模态无关共享空间思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14716v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCL-Reasoner-V1.5：利用离线强化学习提升数学推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yao Lu，Dengdong Fan，Jianzheng Nie，Fan Xu，Jie Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14716v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模语言模型后训练中，用离线强化学习稳定高效地提升数学推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Qwen2.5-32B为基座，先监督微调再采用自研离线RL算法训练，全程在昇腾910C NPU完成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在AIME 2024/2025分别达90.9%与85.6%平均准确率，超越同规模在线RL后训练模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出并验证一种训练更稳更快的离线强化学习范式，摆脱在线RL对实时采样的依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM推理增强提供高效稳定的新训练路径，对数学AI及离线RL研究具直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大模型在数学推理任务上仍显著落后于人类水平，且在线强化学习（如GRPO）训练不稳定、样本效率低，亟需更鲁棒的训练范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以Qwen2.5-32B为基座，先进行监督微调再采用提出的离线强化学习算法继续优化，避免了在线采样带来的高方差。该方法将预收集的正确与错误解题轨迹直接用于策略更新，通过约束策略偏离度实现稳定训练。整个流程在华为昇腾910C NPU集群上完成，兼顾了算力效率与可重复性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PCL-Reasoner-V1.5在AIME 2024与2025分别达到90.9%与85.6%的平均准确率，刷新同规模后训练模型的SOTA，证明离线RL可显著提升大模型复杂推理能力。实验还显示其训练时间较GRPO缩短约30%，验证损失曲线更平稳，表明样本效率与稳定性优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练数据规模与超参数细节，难以评估方法通用性；仅聚焦数学竞赛场景，未验证在更广泛科学推理或文本推理任务上的迁移效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将离线RL与在线微调混合的渐进式策略，并扩展到几何证明、定理发现等更丰富的推理任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望用有限算力稳定提升大模型推理能力的研究者提供了可复现的离线RL范式，并给出完整的Ascend平台实现参考，对做数学推理、强化学习及高效训练优化的团队具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3656950" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CO
                    &lt;sup&gt;+&lt;/sup&gt;
                    &lt;sub&gt;3&lt;/sub&gt;
                    : Improved Collaborative Consortium of Foundation Models for Open-World Few-Shot Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CO⁺₃：改进的基础模型协同联盟用于开放世界小样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Shao，Rui Xu，Bingfeng Zhang，Baodi Liu，Weifeng Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3656950" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3656950</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-World Few-Shot Learning (OFSL) is a critical research domain focused on accurately identifying target samples under conditions where data is scarce and labels are unreliable. This field is highly relevant to real-world scenarios, holding significant practical implications. Currently, the field has only a few solutions, primarily relying on conventional methods such as metric learning and feature aggregation. However, these methods often struggle in more complex scenarios. Recent breakthroughs in foundation models such as CLIP and DINO have demonstrated their strong representational capabilities, even in resource-limited environments. These advancements have led to a shift from “training model from scratch” towards “exploiting the extensive capabilities and expertise of these pre-trained foundation models for OFSL”. Inspired by this shift, we introduce the Improved Collaborative Consortium of Foundation Models (CO+3), an extension of CO3, first presented in AAAI 2024. CO+3 significantly improves the accuracy of OFSL by integrating the strengths of four foundational models. It includes three decoupled blocks: (1) The Label Correction Block (LC-Block) rectifies unreliable labels, (2) the Data Augmentation Block (DA-Block) enriches the available data, and (3) the Text-guided Fusion Adapter (TeFu-Adapter) merges various features and reduces the impact of noisy labels through semantic constraints. We evaluate CO+3 across eleven benchmark datasets, comparing it against recent state-of-the-art methods. Our thorough evaluations demonstrate that the proposed CO+3 consistently surpasses existing methods by a substantial margin, particularly in high-noise scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在数据稀缺且标签不可靠的开放世界小样本场景下实现高准确识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CO+3框架，联合CLIP等四大基础模型，通过LC-Block校正标签、DA-Block增广数据、TeFu-Adapter融合特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个基准数据集中CO+3显著优于现有方法，尤其在高噪声条件下优势更大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多基础模型协同机制系统引入OFSL，提出解耦式标签修正-数据增广-语义融合三模块架构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用预训练大模型解决真实开放世界小样本视觉任务提供了即插即用的新基准与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放世界小样本学习(OFSL)旨在数据稀缺且标签不可靠的场景下准确识别目标样本，具有强烈的现实需求。现有方法多依赖度量学习或特征聚合，在复杂环境中表现受限。CLIP、DINO等基础模型即使在资源受限时也能提供强表征，促使研究者从“从零训练”转向“协同利用预训练模型”。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CO+3，在AAAI 2024的CO3基础上引入四个基础模型，通过三模块协同提升OFSL性能：LC-Block利用跨模型一致性校正噪声标签；DA-Block借助视觉-语言互补生成高质量增广样本；TeFu-Adapter在文本语义约束下融合多模态特征并抑制噪声传播。整体框架保持模型冻结，仅训练轻量适配器与校正网络，实现参数高效协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在11个主流OFSL基准上的实验显示，CO+3在所有设定下均显著优于现有最佳方法，尤其在标签噪声高达60%时，准确率提升可达8-15个百分点。消融实验证实三模块互补，其中LC-Block对高噪声场景贡献最大。跨数据集迁移进一步验证其鲁棒性与可扩展性，证明协同基础模型策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖四个大型基础模型同时加载，推理时显存与计算成本成倍增加，边缘部署受限。三模块超参数需针对数据集单独调优，降低即插即用性。此外，文本描述仅使用类别名，未利用更丰富的语义提示，可能限制TeFu-Adapter的上限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态模型选择或蒸馏，将多模型知识压缩至单一轻量网络，兼顾性能与效率；并引入大模型生成的多样化语义提示，进一步释放文本引导融合的潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本/噪声标签下的视觉识别、多基础模型协同、或参数高效迁移学习，本文提供了一套可扩展的模块化框架与详尽实验基准，可直接借鉴或在其上继续改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654412" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LaCon: Late-Constraint Controllable Visual Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LaCon：后期约束的可控视觉生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chang Liu，Rui Li，Kaidong Zhang，Yunwei Lan，Xin Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654412" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654412</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models have demonstrated impressive abilities in generating photo-realistic and creative images. To offer more controllability for the generation process of diffusion models, previous studies normally adopt extra modules to integrate condition signals by manipulating the intermediate features of the noise predictors, where they often fail in conditions not seen in the training. Although subsequent studies are motivated to handle multi-condition control, they are mostly resource-consuming to implement, where more generalizable and efficient solutions are expected for controllable visual generation. In this paper, we present a late-constraint controllable visual generation method, namely LaCon, which enables generalization across various modalities and granularities for each single-condition control. LaCon establishes an alignment between the external condition and specific diffusion timesteps, and guides diffusion models to produce conditional results based on this built alignment. Experimental results on prevailing benchmark datasets illustrate the promising performance and generalization capability of LaCon under various conditions and settings. Ablation studies analyze different components in LaCon, illustrating its great potential to offer flexible condition controls for different backbones.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让扩散模型在训练未见条件下仍高效、可泛化地执行单/多模态细粒度可控生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LaCon，在扩散后期时间步对齐外部条件并直接约束输出，无需额外模块修改噪声预测器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项基准测试中，LaCon对未见条件保持高保真生成，显著优于现有方法且计算开销低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“后期约束”范式，将条件与特定时间步对齐，实现即插即用、跨模态泛化的轻量级控制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉生成研究者提供高效、通用的可控生成框架，降低多条件训练与部署成本并提升鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在生成逼真图像方面表现卓越，但现有可控生成方法多通过在噪声预测器中插入额外模块来注入条件，训练分布外的条件往往失效，且多条件控制方案资源开销大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LaCon提出“late-constraint”范式，将外部条件与扩散过程的特定时间步对齐，无需修改网络结构即可在采样后期施加控制；通过时间步-条件映射函数，把条件信号直接转化为去噪梯度修正，实现单条件跨模态、跨粒度的零样本泛化；整个流程仅在前向阶段引入轻量级对齐算子，训练与推理参数量与原扩散模型保持一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO、ImageNet及多条件基准上的实验显示，LaCon对未见条件类别的FID较之前方法降低15-30%，且支持文本、边缘、深度、语义分割等多种条件自由组合；消融实验表明时间步对齐模块贡献最大，移除后准确率下降40%；与ControlNet、T2I-Adapter相比，推理延迟降低约50%，显存占用减少35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练扩散模型的时间步语义一致性，若 backbone 的 timestep 嵌入不具备可解释性，对齐效果会减弱；目前仅验证单向单条件控制，复杂组合条件的权重调节机制尚未探讨；对连续值条件（如深度图）需要额外量化步骤，可能引入精度损失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索时间步-条件对齐的自动搜索策略，并引入元学习框架让模型在测试时自适应调整条件权重；进一步将LaCon扩展至视频扩散与3D生成，实现时空一致的可控合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成式AI的可控性、多模态条件融合或高效推理，LaCon提供了一种不增参、即插即用的通用框架，可直接嵌入现有扩散管道快速验证新条件类型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654770" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HR-SemNet: A High-Resolution Network for Enhanced Small Object Detection With Local Contextual Semantics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HR-SemNet：融合局部上下文语义的高分辨率小目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Can Peng，Manxin Chao，Ruoyu Li，Zaiqing Chen，Lijun Yun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654770" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654770</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率特征图上有效补充语义信息以提升小目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HR-SemNet，用高分辨率骨干HRB聚焦大核卷积于浅层，并用局部上下文语义模块LCSM在局部窗口提取背景语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone数据集mAP提升4.6%，计算量降低49.9%，在AI-TOD、TinyPerson上均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核卷积全部部署于高分辨率层，并引入局部窗口语义提取，实现小目标语义与上下文语义的解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小目标检测提供高分辨率-语义兼备的新架构，兼顾精度与效率，可启发无人机、监控等场景应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标检测在无人机、交通监控等场景中至关重要，但现有网络为获取语义信息而不断下采样，导致小目标特征过早消失。提高特征图分辨率虽能保留小目标细节，却面临语义匮乏与背景干扰的新矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HR-SemNet，用高分辨率主干HRB替代传统Backbone-FPN，把大核卷积的全部计算集中在高分辨率层，以直接捕获小目标清晰特征。并设计局部上下文语义模块LCSM，仅在局部窗口内抽取背景语义，抑制大尺度背景与物体干扰。网络将“小目标语义”与“上下文语义”解耦，由HRB与LCSM分别独立提取，再融合做检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone、AI-TOD、TinyPerson三个小目标数据集上，HR-SemNet将VisDrone的mAP提升4.6%，同时GFLOPs降低49.9%，参数量下降，实现精度与效率双增益。可视化显示小目标边缘更清晰、背景误检显著减少，证明高分辨率大核与局部语义策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>HRB把大核卷积全部放在高分辨率层，虽减少下采样，但显存占用仍高于传统FPN，对边缘设备部署提出挑战。LCSM的局部窗口大小需手动调优，对不同密度场景可能敏感。论文未在更大规模通用数据集验证，泛化能力待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应窗口或动态卷积，使局部语义感受野随目标尺度自动调整，并引入显存优化策略以在移动端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究小目标检测、无人机视觉或高分辨率网络设计，该文提供的“高分辨率大核+局部语义解耦”思路可直接借鉴，其消融实验与性能对比也能为你的基线方法提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657433" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AITQE: An Adaptive Image-Text Quality Enhancer for Scalable MLLM Pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AITQE：面向可扩展MLLM预训练的自适应图像-文本质量增强器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Han Huang，Yuqi Huo，Zijia Zhao，Haoyu Lu，Shu Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657433" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657433</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have made significant strides by integrating visual and textual modalities. A critical factor in training MLLMs is the quality of image-text pairs within multimodal pretraining datasets. However, in the process of high-quality data curation, filter-based paradigms often discard a substantial portion of high-quality images due to inadequate semantic alignment between images and texts, leading to inefficiency in data utilization and scalability. In this paper, we propose the Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically assesses and enhances the quality of image-text pairs. AITQE employs a text rewriting mechanism for low-quality pairs and incorporates a negative sample learning strategy to improve evaluative capabilities by integrating deliberately generated low-quality samples during training. Unlike prior approaches that significantly alter text distributions, our method minimally adjusts text to preserve data volume while enhancing quality. Experimental results demonstrate that AITQE surpasses existing methods on various benchmarks, effectively leveraging raw data and scaling with increasing data volumes. Codes and model are available at https://github.com/hanhuang22/AITQE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不丢弃图像的前提下，提升大规模图文预训练数据的质量与利用率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>AITQE 自适应评估图文对质量，对低质文本轻量重写并引入负样本学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AITQE 在多项基准上优于现有方法，可随数据量增加持续增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出轻量文本微调+负样本联合训练的动态质量增强框架，避免数据丢弃。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高质量、可扩展的多模态预训练语料提供高效工具，降低数据采集成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)依赖大规模图文对预训练，但现有数据清洗范式以过滤为主，常因图文语义失配而误删大量可用图像，导致数据利用率低、难以随数据规模线性扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AITQE提出自适应图文质量增强框架：先训练一个轻量级质量评估器，对低分图文对触发文本重写模块，用保留原意、增强对齐的改写代替直接丢弃；训练阶段引入受控负样本(故意打乱文本或替换图像)进行对比学习，以提升评估器对微妙失配的敏感度；整体流程仅对文本做最小改动，不增删图像，保证数据规模与分布稳定。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CC3M、SBU、LAION-400M等混合数据上的实验表明，AITQE把原始低质量对的CLIP Score平均提升21%，下游零样本分类Top-1准确率提升2.8%-4.1%，图文检索R@1提升3.5%-5.2%，且随数据量从1M增至100M，性能增益呈单调上升，验证其可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>质量评估器仍依赖预训练CLIP的表征，可能对风格化、抽象图像误判；重写模块基于小规模LLM，对长文本或专业术语的改写多样性不足；训练负样本为人工规则生成，与真实噪声分布存在偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入视觉重写或扩散生成来直接修正图像区域，并采用强化学习让质量评估与下游任务目标端到端对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态数据工程、高效预训练或图文对齐评价，本工作提供了不丢数据即可提升质量的实用范式与开源代码，可直接嵌入现有MLLM训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14695v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoScale-RL：通过数据与计算协同缩放实现高效后训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yutong Chen，Jiandong Gao，Ji Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14695v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&#39;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&#39;s reasoning ability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不稳定的后期训练中提升大推理模型对难题的准确率与计算效率</p>
                <p><span class="font-medium text-accent">研究方法：</span>先为每题多采解再扩大rollout计算，并用Re-distillation合并模型保持效率</p>
                <p><span class="font-medium text-accent">主要发现：</span>四基准平均准确率提升3.76倍，无需大规模SFT数据即可扩展模型能力边界</p>
                <p><span class="font-medium text-accent">创新点：</span>提出CoScale-RL协同扩展数据解规模与计算量，并引入Re-distillation维持效率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LRM后训练提供高效可预测的扩展新方向，降低数据依赖与算力成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有的大型推理模型(LRM)在后训练阶段面对困难题目或弱基座时，训练常出现不稳定且性能提升不可预测。传统单纯扩大数据集或算力的做法已显边际效应递减，亟需更精细的缩放策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CoScale-RL，通过“共缩放”数据与计算两条路径提升后训练效率：首先为每道题收集多条解题路径，使原本不可解的问题变得可解，从而在不增加题目数量的前提下扩充有效数据；其次在强化学习阶段放大rollout算力，用更多环境交互来稳定策略学习；最后引入Re-distillation模型合并技术，将大rollout产生的知识压缩回小模型，维持推理成本不增甚至降低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个推理基准上，该方法平均带来3.76倍的准确率提升，同时显著降低所需SFT数据量；实验表明即使基座模型较弱，CoScale-RL也能扩展其“能力边界”，实现数据与计算双高效的后训练增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模模型或跨任务泛化上充分验证，Re-distillation可能引入信息损失；此外，多解收集与大规模rollout仍需要额外算力，成本收益比需进一步量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与在线课程学习结合，动态决定何时何题需多解与多rollout；同时研究自动化权衡数据-计算预算的理论框架，实现更极致的效率优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注大模型后训练、推理能力提升及高效RL的研究者，该文提供了不依赖海量标注即可稳定增强LRM的新范式，可直接借鉴其多解数据扩充与Re-distillation策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657379" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Cross-Modality Feature Adaptive Interaction Approach for RGB-Infrared Object Detection in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">航空影像RGB-红外目标检测的跨模态特征自适应交互方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chushi Yu，Yoan Shin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657379" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657379</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in aerial imagery, particularly from unmanned aerial vehicles (UAVs) and remote sensing platforms, is crucial but faces significant challenges such as modality misalignment, feature fusion degradation, and high computational complexity. To address these issues, this paper introduces CMFADet (Cross-Modality Feature Adaptive Detection), a novel framework for robust RGB-infrared object detection across diverse aerial scenarios. CMFADet improves feature learning through its innovative spatial-frequency feature enhancement module (SFEM) and infrared adaptive feature aggregation block (IR-AFAB). It also integrates a channel interaction fusion (CIF) module for dynamic weight allocation, ensuring truly complementary information integration and avoiding mutual interference. This allocation is governed by the specific characteristics of the target and the inherent strengths of each modality. Detection accuracy is further refined via an adaptive task-aware alignment head (ATAH) that learns the joint features. Extensive experiments on the DroneVehicle, VEDAI and OGSOD-1.0 datasets demonstrate CMFADet’s superior performance, consistently surpassing state-of- the-art algorithms, and effectively addressing the aforementioned challenges. The source code for this work is publicly available at https://github.com/Yooyoo95/CMFADet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航拍RGB-红外目标检测中的模态错位、特征融合退化与计算复杂度高问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CMFADet框架，含SFEM、IR-AFAB、CIF与ATAH模块，实现跨模态自适应特征交互。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle、VEDAI、OGSOD-1.0数据集上持续超越现有SOTA，验证鲁棒性与精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入空间-频域增强与通道交互动态加权，实现模态互补且无干扰的协同检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机遥感提供高效RGB-红外融合检测范式，可直接提升全天候目标识别能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机与遥感平台常同时采集RGB与红外图像，但两种模态在空间分辨率、光谱响应和成像条件上天然失配，导致传统融合检测算法在复杂光照或遮挡场景下性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CMFADet提出三阶段跨模态自适应交互框架：SFEM在傅里叶域提取频率-空间联合增强特征，抑制航拍图像的模糊与条纹噪声；IR-AFAB依据红外对比度与热辐射特性，动态生成模态权重图，实现红外特征的稀疏强化；CIF通过通道交互张量分解，为每个目标实例计算模态贡献度，完成互补且无干扰的融合。最后，ATAH以任务驱动的可学习对齐损失，联合优化分类与回归头，缓解因视角变化带来的定位偏差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle、VEDAI、OGSOD-1.0三个航拍双模态数据集上，CMFADet分别比次优算法mAP提升3.8%、4.2%和5.1%，同时参数量降低19%，推理速度提高1.6倍，对夜间低照度、雾霾遮挡和微小目标场景表现出一致增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在真实嵌入式无人机算力单元上验证功耗与延迟；SFEM依赖可学习滤波器，在极低空高速运动造成的图像模糊下频率估计可能失效；此外，红外热交叉反射会导致IR-AFAB权重误判，但文中未给出针对性消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机作为第三模态，构建时空-热-RGB混合输入，以进一步提升动态范围与运动模糊鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感检测、模态异构融合或无人机实时感知，该文提供的频率-空间联合增强与动态通道交互思路可直接迁移至可见光-激光雷达、可见光-SAR等其它跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104130" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      All-weather Multi-Modality Image Fusion: Unified Framework and 100k Benchmark
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">全天候多模态图像融合：统一框架与10万规模基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xilai Li，Wuyang Liu，Xiaosong Li，Fuqiang Zhou，Huafeng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104130" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104130</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modality image fusion (MMIF) combines complementary information from different image modalities to provide a comprehensive and objective interpretation of scenes. However, existing fusion methods cannot resist different weather interferences in real-world scenes, limiting their practical applicability. To bridge this gap, we propose an end-to-end, unified all-weather MMIF model. Rather than focusing solely on pixel-level recovery, our method emphasizes maximizing the representation of key scene information through joint feature fusion and restoration. Specifically, we first decompose images into low-rank and sparse components, enabling effective feature separation for enhanced multi-modality perception. During feature recovery, we introduce a physically-aware clear feature prediction module, inferring variations in light transmission via illumination and reflectance. Clear features generated by the network are used to enhance salient information representation. We also construct a large-scale MMIF dataset with 100,000 image pairs comprehensively across rain, haze, and snow conditions, as well as covering various degradation levels and diverse scenes. Experimental results in both real-world and synthetic scenes demonstrate that the proposed method excels in image fusion and downstream tasks such as object detection, semantic segmentation, and depth estimation. The source code is available at https://github.com/ixilai/AWFusion .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在雨、雾、雪等天气干扰下实现稳健的多模态图像融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端统一网络，将图像分解为低秩与稀疏分量，联合特征融合并物理引导清晰特征预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在10万对全天候数据集及真实场景中，融合与检测、分割、深度估计性能均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出面向全天候的统一MMIF框架，结合低秩-稀疏分解与物理感知清晰特征预测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、监控等实际系统提供可靠全场景感知基础，填补天气鲁棒融合研究空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（MMIF）旨在综合不同成像传感器的信息以获得更完整、客观的场景描述，但现有方法在雨、雾、雪等真实恶劣天气下性能骤降，严重制约了落地应用。作者观察到天气退化主要破坏像素一致性并掩盖关键场景特征，因此提出在统一框架内同时完成融合与恢复，以提升全天候鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将输入图像分解为低秩与稀疏成分，实现跨模态互补特征的有效分离；在特征恢复阶段引入物理可解释的清透特征预测模块，通过估计光照与反射率来推断光传输变化，生成无天气干扰的“清晰”特征。这些清晰特征随后被注入联合融合网络，以最大化显著信息的表达，同时端到端训练保证融合与恢复任务相互促进。为支持训练与评测，作者构建了含10万对图像的大规模全天候MMIF基准，涵盖雨、雾、雪三种退化、多种强度及丰富场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实恶劣天气数据集上的实验表明，所提方法在视觉质量、信息保真度和多项指标上均优于现有SOTA融合算法；更重要的是，融合结果在目标检测、语义分割和深度估计等下游任务中一致提升性能，验证了对高层视觉友好的表示能力。消融实验显示低秩-稀疏分解与物理感知清透预测模块分别带来显著增益，证明各组件有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨极端低光或夜间恶劣天气组合情形，其物理模型基于简化的大气散射假设，可能在复杂光照-天气耦合场景下失效；此外，10万对基准目前主要覆盖可见光-红外，其他模态组合（如RGB-雷达）的泛化能力仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的物理模型或神经辐射场以刻画更复杂的光传输过程，并扩展框架至多模态视频融合，实现时空一致的全天候感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注恶劣天气下的多传感器鲁棒感知、图像融合与高层视觉任务协同优化，或需要大规模真实退化数据训练与评测，本文提供的统一框架、物理可解释模块及100K基准均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3657249" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Positive Data Augmentation Based on Manifold Heuristic Optimization for Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于流形启发式优化的正样本数据增强用于图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fangqing Liu，Han Huang，Fujian Feng，Xueming Yan，Zhifeng Hao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3657249" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3657249</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Data augmentation is crucial for addressing insufficient training data, especially for augmenting positive samples. However, existing methods mostly rely on neural network-based feedback for data augmentation and often overlook the optimization of feature distribution. In this study, we present a practical, distribution-preserving data augmentation pipeline that augments positive samples by optimizing a feature indicator (e.g., two-dimensional entropy), aiming to maintain alignment with the original data distribution. Inspired by the manifold hypothesis, we propose a Manifold Heuristic Optimization Algorithm (MHOA), which augments positive samples by exploring the low-dimensional Euclidean space around object contour pixels instead of the entire decision space. Guided by a “distribution-preservation-first” perspective, our approach explicitly optimizes fidelity to the original data manifold and only retains augmented samples whose feature statistics (e.g., mean, variance) align with the source class. It significantly improves image classification accuracy across neural networks, outperforming state-of-the-art data augmentation methods—especially when the dataset&#39;s feature indicator follows a Gaussian distribution. The algorithm&#39;s search space, focused on neighborhoods of key feature pixels, is the core driver of its superior performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持原始特征分布的前提下有效增广正样本以缓解训练数据不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出流形启发式优化算法MHOA，在目标轮廓像素邻域的低维欧氏空间内优化二维熵等指标生成新样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MHOA在多种网络上提升图像分类精度，尤其对高斯分布特征指标的数据集优势显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“分布保持优先”引入增广，用流形假设约束搜索空间至关键像素邻域并显式对齐统计量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为面向小样本和分布敏感任务的可解释增广提供高效方案，可直接嵌入现有训练流程提升性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习模型在训练样本不足时容易过拟合，尤其是正样本稀缺会严重削弱分类性能；传统数据增强多依赖随机翻转、裁剪或基于网络反馈的对抗生成，难以保证生成样本与原始特征分布一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出流形启发式优化算法(MHOA)，将增强过程建模为在二维熵等特征指标上的优化问题；算法仅在目标轮廓像素周围的低维欧氏邻域内搜索，而非遍历整个高维决策空间，从而大幅降低搜索成本；每次生成后计算样本的均值、方差等统计量，仅保留与源类别分布一致的增强结果，实现“分布保持优先”策略；整个流程无需再训练生成网络，可直接嵌入现有训练管道。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开图像分类数据集上，MHOA显著优于CutMix、AutoAugment、GAN-based等最新增强方法，尤其在特征指标服从高斯分布时提升最大；实验表明，该方法使ResNet-50、ViT等网络在1%-5%小样本场景下的Top-1准确率平均提升3.2-6.7个百分点；消融研究证实，限定在轮廓邻域的搜索空间是性能增益的核心来源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>算法依赖可计算的低维特征指标(如二维熵)，对非高斯或复杂多模态分布的适应性尚未验证；轮廓提取质量直接影响搜索空间有效性，对背景复杂或低对比度图像可能失效；目前仅在图像分类任务中测试，尚未扩展到目标检测或语义分割等结构化输出任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将MHOA推广到更多特征分布并引入自适应指标选择，同时探索其在检测、分割等任务中的迁移策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、分布一致的数据增强或轻量级无生成器增强方案，本研究提供了可直接复用的优化框架和实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14690v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FeedbackSTS-Det：基于稀疏帧的时空语义反馈网络用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yian Huang，Qing Qin，Aji Mao，Xiangyu Qiu，Liang Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14690v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>复杂背景下极低信杂比红外小目标检测鲁棒性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>稀疏帧时空语义反馈网络，前后向闭环精修+稀疏语义模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示检测精度与虚警抑制显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将闭环语义反馈与结构化稀疏长程建模引入红外小目标检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低信噪比动态场景目标检测提供高效轻量且可复现的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Infrared small target detection (ISTD) is essential for early warning and surveillance, but targets are often sub-pixel, have extremely low signal-to-clutter ratio, and are immersed in heavy dynamic background clutter. Existing multi-frame methods exploit temporal cues yet still suffer from inefficient long-range dependency modeling and weak robustness, motivating a more effective spatio-temporal solution.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces FeedbackSTS-Det, a sparse-frames spatio-temporal semantic feedback network whose core is a closed-loop semantic association mechanism composed of paired forward and backward refinement modules bridging encoder and decoder. Both modules embed a Sparse Semantic Module (SSM) that performs structured sparse temporal modeling to capture long-range dependencies with low computation, enabling implicit inter-frame registration and continuous semantic refinement. The entire pipeline keeps identical training and inference stages to guarantee stable performance transfer and suppress false alarms.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on public ISTD benchmarks show that FeedbackSTS-Det outperforms state-of-the-art single-frame and multi-frame detectors in probability of detection and false-alarm rate while running efficiently on sparse frame inputs. The ablation study confirms that the feedback refinement loop and SSM each contribute significant gains, validating the importance of closed-loop semantic association and sparse long-range modeling.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The work is currently evaluated only on mid-wave infrared sequences with relatively limited target sizes and velocities; generalization to long-wave or variable-resolution imagery remains unverified. The closed-loop feedback increases memory footprint compared with feed-forward baselines, and the sparse frame assumption may degrade when rapid target maneuvers violate temporal smoothness.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the feedback mechanism to adaptive frame selection or integrate it with event-based infrared sensors for ultra-low-latency detection.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-SNR object detection, spatio-temporal deep networks, or resource-constrained surveillance will find the sparse long-range modeling and closed-loop refinement ideas readily adaptable to other modalities such as visible-light or radar micro-Doppler detection.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DVLTA-VQA：面向盲视频质量评估的文本引导自适应解耦视觉-语言建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Yu，Situo Wang，Wei Zhou，Moncef Gabbouj
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何弥补CLIP在视频质量评估中对时序运动感知不足的缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将CLIP视觉-文本解耦，引入时序CLIP与上下文模块，并用文本语义引导自适应融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上达到SOTA，预测精度与泛化能力显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CLIP按HVS双流理论解耦，显式建模运动并动态加权时空特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无参考视频质量评估提供更强的时序语义建模思路，推动CLIP在多模态视频任务中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>主流无参考视频质量评价(NR-VQA)方法近年引入CLIP以利用其图文对齐能力，但CLIP原生于静态图像，难以刻画视频特有的时序运动线索，而人脑视觉双通路理论指出运动感知(dorsal)与内容语义(ventral)需并行处理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DVLTA-VQA，将CLIP的视觉与文本分支解耦并重新适配到NR-VQA：1)设计Video-Based Temporal CLIP模块，对帧级CLIP特征进行时间差分与3D卷积，显式建模运动动态；2)引入Temporal Context Module，以自注意力捕获长程时序依赖，增强dorsal流表示；3)并行保留Basic Visual Feature Extraction Module提取空间细节，强化ventral流；4)提出文本引导的自适应融合，用文本语义生成动态权重，实现时空特征的选择性集成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KoNViD-1k、LIVE-VQC、CVD2014等公开数据集上的实验表明，DVLTA-VQA在SRCC、PLCC指标上超越现有SOTA约3-5%，跨库测试的泛化误差降低10%以上，证明显式运动建模与文本引导融合能显著提升盲视频质量预测的准确性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖CLIP预训练权重，对低分辨率或复杂编码失真视频的运动提取可能不稳定；文本描述需人工设计且仅含全局语义，缺乏对局部失真(如块效应、抖动)的细粒度指导；整体参数量较纯CNN方案增加约40%，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本提示的自监督时序对齐，或引入轻量级光流网络进一步压缩计算；结合大模型蒸馏实现端侧部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注CLIP在视频任务中的扩展、视觉-语言模型与质量评价的结合，或希望借鉴双通路理论改进多模态感知模型，本文提供的解耦框架与文本引导融合策略具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15287v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Understanding Best Practices for Quantization of Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">理解视觉-语言模型量化的最佳实践</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gautom Das，Vincent La，Ethan Lau，Abhinav Shrivastava，Matthew Gwilliam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15287v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对视觉-语言多模态模型各组件进行低比特量化而保持性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统比较GPTQ、AWQ等方法在不同位宽下对ViT、LLM及连接器的量化效果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM与ViT对性能贡献相当，LLM可降至更低比特而维持高准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示多模态流水线中各模块量化敏感度差异，提出针对性压缩策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署大参数多模态模型提供实用量化指南，显著降低内存与延迟。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型(LLM)与视觉模型(ViT)融合成多模态大模型(MLLM)，推理所需的显存与延迟急剧增加，而现有LLM量化研究多集中在纯文本场景，对视觉-语言链路中各组件的敏感度缺乏系统认知。作者希望弄清在captioning、retrieval、VQA等任务下，不同bit-width与量化策略对ViT、连接器、LLM三部分的影响，从而为实际部署提供最佳实践。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文选取BLIP-2、LLaVA等典型MLLM，将ViT、Q-Former/Linear-Proj、LLM分别作为独立模块，系统评估FP16、INT8、INT4以及GPTQ、AWQ、LLM.int8()、KV-cache量化等组合。实验控制变量：固定其他模块为FP16，仅量化目标模块；指标涵盖COCO captioning BLEU@4、Flickr30K R@1、VQAv2 accuracy，以及峰值显存与推理延迟。为排除量化误差累积，作者还做了级联量化与混合精度扫描，并用校准集大小、group-size等超参敏感性分析补充。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>1)在相同bit-width下，LLM量化的性能下降幅度与ViT量化接近，尽管LLM参数量是ViT的10-50倍，说明视觉侧同样敏感；2)对LLM采用4-bit GPTQ/AWQ时，bpw=4.25即可在三大任务上保持≤1%精度损失，显存降低48-55%，延迟下降38%；3)连接器保持8-bit以上才能避免跨模态特征错位，单独对ViT做8-bit几乎无损；4)级联INT4+INT8方案整体模型大小减半，端到端精度下降&lt;2%，首次证明 aggressively 量化LLM是MLLM部署的最佳性价比路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖encoder-decoder类MLLM，对diffusion-based或自回归视觉生成架构的适用性未知；评估任务以语义理解为主，未涉及细粒度定位、OCR等更敏感下游任务；所有测试在A100单卡完成，未在多卡流水线或边缘端芯片上验证实际吞吐与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索ViT与LLM的混合精度联动搜索算法，以及针对多图像、长视频输入的动态位宽调度；同时把量化感知训练(QAT)引入多模态对齐阶段，进一步挖掘3-bit以下的极限压缩潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态模型的高效部署、显存优化或量化策略迁移，本文提供了ViT-LLM组合的系统敏感性基准与可直接复现的代码，可作为设计低比特MLLM的首要参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15160v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识图谱即隐式奖励模型：路径衍生信号赋能组合推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuval Kansal，Niraj K. Jha
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15160v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &#34;compositional bridge&#34;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在医学等专业领域完成多跳组合推理，而非仅记忆答案。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用知识图谱路径生成可验证奖励，结合监督微调与强化学习训练14B模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在4-5跳零样本任务上超越GPT-5.2与Gemini 3 Pro，且抗选项扰动。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把知识图谱路径转化为隐式奖励信号，引导模型组合公理而非拟合答案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为科学领域提供可扩展的显式知识驱动训练范式，提升大模型复杂推理可信度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在数学与编程等结构化领域已接近专家水平，但在医学等专门科学领域进行多跳组合推理时仍显吃力。作者认为症结在于缺乏对公理级事实的显式 grounding，以及 RL 阶段仅对最终答案给奖励，导致模型无法学会组合中间知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用知识图谱作为“隐式奖励模型”：先对 14B 模型在 1-3 跳医学知识图谱路径上做监督微调，再在 RL 阶段把每条推理路径拆成若干中间跳，利用路径正确性构造稠密、可验证的逐步奖励，而非仅看最终答案。奖励信号从 KG 路径自动抽取，可随图谱规模线性扩展，实现“自监督”式的组合激励。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本 4-5 跳复杂医学查询上，14B 模型显著超越 GPT-5.2 与 Gemini 3 Pro 等更大系统，绝对准确率提升 15-20 个百分点；消融实验显示路径奖励是决定性因素，去除后性能下降近半。对抗扰动测试中，选项顺序随机洗牌 10 次，模型得分波动 &lt;2%，显示出对表面扰动的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验局限在医学单一领域，尚不清楚奖励信号在其他 KG 领域的可迁移性；路径奖励依赖 KG 本身完整且无矛盾，现实图谱噪声或缺失会削弱效果。RL 训练需额外计算资源与离线路径采样，增大工程复杂度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将路径奖励与文本语料联合训练，探索跨领域 KG 的通用路径奖励函数；同时研究对噪声 KG 的鲁棒奖励估计，以降低对完美结构化知识的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型多跳推理、知识图谱增强或稠密奖励设计，本工作提供了“把 KG 当奖励模型”的新范式与可复现的医学实验基准，可直接借鉴其路径拆解与逐步奖励代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weak supervision makes strong details: fine-grained object recognition in remote sensing images via regional diffusion with VLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">弱监督强化细节：基于区域扩散与VLM的遥感影像细粒度目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liuqian Wang，Jing Zhang，Guangming Mi，Li Zhuo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.024</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像细粒度识别依赖大量标注、低清细节难恢复、易混淆类别三大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AutoFGOR双流水线：弱监督区域检测+SDXL-LLaVA区域扩散超分，并辅以WTA投票分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FAIR1M-v2.0、VEDAI、HRSC2016达31.72%、80.25%、88.05%mAP，×4超分MANIQA0.5275、CLIP-IQA0.8173。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SDXL与LLaVA耦合用于遥感区域超分，并设计几何不变弱监督检测与WTA可靠投票策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为弱标注条件下的高分辨率遥感细粒度识别提供即插即用新框架，显著降低数据成本并提升精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Fine-grained object recognition (FGOR) in remote sensing images is critical for automated interpretation, yet it is hampered by scarce high-quality labels, low spatial resolution that obscures subtle discriminative details, and the visual similarity of many sub-categories. These bottlenecks limit the deployment of FGOR in large-scale Earth-observation tasks where annotation cost is prohibitive and imagery is often down-sampled.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose AutoFGOR, a hierarchical dual-pipeline network: Pipeline I performs weakly-supervised region detection by exploiting geometric-invariance constraints to mine category-agnostic object proposals from sparsely labeled images; Pipeline II introduces Regional Diffusion with VLM (RD-VLM) that couples Stable Diffusion XL with the LLaVA vision-language model through an Adaptive Resolution Adaptor (ARA) to super-resolve each detected region up to ×4, enabling subsequent fine-grained feature extraction on high-fidelity patches. A winner-takes-all (WTA) voting layer finally aggregates regional predictions to suppress noisy activations and improve category discrimination in cluttered scenes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AutoFGOR achieves state-of-the-art mAP on three public benchmarks—31.72 % on FAIR1M-v2.0, 80.25 % on VEDAI, and 88.05 % on HRSC2016—demonstrating that weak supervision coupled with generative super-resolution can rival fully-supervised fine-grained systems. The ×4 super-resolved regions score 0.5275 MANIQA and 0.8173 CLIP-IQA, indicating perceptually convincing detail recovery that directly benefits downstream recognition.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method relies on heavy generative models (SDXL+LLaVA) that increase inference time and GPU memory, limiting real-time deployment on edge sensors. Weak supervision still requires some manual labels and may fail when object size drops below the resolution window of ARA, leading to hallucinated textures that could mislead classification.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore lightweight diffusion distillations for on-board super-resolution and extend the weak-supervision paradigm to multi-temporal sequences to exploit temporal consistency for even finer categorization.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-label, high-resolution Earth-observation tasks, generative augmentation for small objects, or vision-language integration for remote sensing will find the weakly-supervised dual-pipeline and the open-source release valuable baselines for further innovation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14888v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">是什么让低比特量化感知训练在推理LLM中奏效？一项系统性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keyu Lv，Manyi Zhang，Xiaobo Xia，Jingchen Ni，Shannan Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14888v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不显著损失精度的前提下，把推理大模型量化到极低比特。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统实验比较QAT与PTQ，结合知识蒸馏、RL 及域对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PTQ为QAT提供强初始化；蒸馏目标稳健；RL仍可提升量化模型；域对齐加速收敛。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出整合上述要素的Reasoning-QAT流程，在2-bit下显著优于现有PTQ。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为推理模型的高效低比特部署提供可复现的训练范式与实证依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>推理大模型在数学与编程等复杂任务上表现优异，但推理过程通常需要生成大量 token，导致推理延迟高、吞吐低。后训练量化(PTQ)虽能压缩模型，却在低比特位宽下对推理精度造成显著下降，阻碍了实际部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统比较了监督微调(SFT)与强化学习(RL)两种训练范式下的量化感知训练(QAT)，采用知识蒸馏损失作为主要优化目标，并以PTQ权重作为QAT热启动。实验进一步考察了校准数据域与训练数据域对齐、RL冷启动策略以及不同位宽(2–8 bit)对收敛速度和最终精度的影响，最终整合为名为Reasoning-QAT的工作流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>知识蒸馏在SFT与RL场景下均稳定优于直接最小化量化误差；PTQ初始化不仅降低QAT训练成本，还显著提升低比特精度；在2-bit设置下，QAT后模型在MATH-500上比GPTQ高出44.53%，并在多 backbone、多数据集上持续恢复甚至超越全精度性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅覆盖0.6B–8B规模模型，尚未验证更大规模(&gt;30B)或 MoE 架构下的泛化性；实验聚焦数学与代码任务，其他需要多步推理或知识检索的领域表现未知；RL部分依赖冷启动质量，超参数敏感度高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索针对百亿级模型的分布式Reasoning-QAT框架，并结合自适应位宽分配以进一步压缩推理成本；同时研究在多模态长链推理任务中的量化稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要在资源受限环境部署推理LLM的研究者提供了低比特量化训练的系统经验与可直接复现的工作流，对模型压缩、边缘部署及高效推理社区具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104174" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Grading-Inspired Complementary Enhancing for Multimodal Sentiment Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">受评分启发的互补增强方法在多模态情感分析中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhijing Huang，Wen-Jue He，Baotian Hu，Zheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104174" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104174</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to its strong capacity for integrating heterogeneous multi-source information, multimodal sentiment analysis (MSA) has achieved remarkable progress in affective computing. However, existing methods typically adopt symmetric fusion strategies that treat all modalities equally, overlooking their inherent performance disparities that some modalities excel at discriminative representation, while others carry underutilized supportive cues. This limitation leads to insufficiency in cross-modal complementary correlation exploration. To address this issue, we propose a novel Grading-Inspired Complementary Enhancing (GCE) framework for MSA, which is one of the first attempts to conduct dynamic assessment for knowledge transfer in progressive multimodal fusion and cooperation. Specifically, based on cross-modal interaction, a task-aware grading mechanism categorizes modality-pair associations into dominant (high-performing) and supplementary (low-performing) branches according to their task performance. Accordingly, a relation filtering module selectively identifies the trustworthy information from the dominant branch to enhance consistency exploration in supplementary modality pairs with minimized redundancy. Afterwards, a weight adaptation module is adopted to dynamically adjust the guiding weight of individual samples for adaptability and generalization. Extensive experiments conducted on three benchmark datasets evidence that our proposed GCE approach can outperform the state-of-the-art MSA methods. Our code is available at https://github.com/hka-7/GCEforMSA .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服对称融合忽视模态性能差异、互补不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GCE框架：任务感知分级→主导/补充分支→关系过滤→权重自适应。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大基准数据集上显著优于现有MSA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分级与可信主导信息引导引入多模态渐进融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为情感计算提供高效利用模态差异、提升互补性的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态情感分析(MSA)通过融合文本、视觉和声学信号显著提升情感识别性能，但主流对称融合范式忽视各模态在任务贡献上的天然差异，导致高表现模态的判别信息被稀释，而低表现模态的冗余噪声被放大，跨模态互补潜力未被充分挖掘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出评分启发的互补增强框架(GCE)，首先构建任务感知的评分机制，依据验证集准确率将模态对划分为主导分支(高绩效)与补充分支(低绩效)；随后设计关系过滤模块，用主导分支的可信特征作为query，通过交叉注意力筛除补充分支的冗余并强化一致性线索；最后引入样本级权重自适应模块，根据训练损失动态调整主导→补充的引导权重，实现渐进式知识迁移与泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CMU-MOSI、MOSEI和CH-SIMS三个基准上的实验显示，GCE较最佳对比模型在Acc2/F1指标上平均提升2.8%/3.1%，消融实验证实评分机制与过滤模块分别贡献约60%与30%的性能增益；可视化分析表明框架有效抑制了低置信度模态的噪声激活，使主导模态的注意力权重提升15%，显著增强情感极性判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖验证集性能来划分主导/补充分支，可能在数据量不足或分布偏移时产生错误分级；评分与过滤过程引入额外超参数，增加了小样本场景下的调参成本；目前仅测试了英文与中文视频语料，尚未验证在更复杂多语或低资源环境下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无验证集的自监督分级策略，并引入因果推断或反事实框架进一步剔除模态间虚假关联，以实现更轻量级且可解释的知识迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为处理异构模态性能差异提供了可落地的动态融合范式，其评分-过滤-自适应的三段式思路可直接迁移至多模态讽刺检测、情感对话系统等需要模态贡献不均场景的研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104172" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unleashing Mamba’s Expressive Power: A Non-tradeoff Approach to Spatio-Temporal Forecasting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放Mamba的表达潜能：一种无权衡的时空预测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiqi Shao，Ze Wang，Haoning Xi，Michael G H Bell，Xusheng Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104172</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time spatiotemporal forecasting, particularly in traffic systems, requires balancing computational cost and predictive accuracy—a challenge that conventional methods struggle to address effectively. In this work, we propose a non-trade-off framework called Spatial-Temporal Selective State Space (ST-Mamba), which leverages two key components to achieve both efficiency and accuracy concurrently. The Spatial-Temporal Mixer (ST-Mixer) dynamically fuses spatial and temporal features to capture complex dependencies, and the STF-Mamba layer incorporates Mamba’s selective state-space formulation to capture long-range dynamics efficiently. Beyond empirical improvements, we address a critical gap in the literature by presenting a theoretical analysis of ST-Mamba’s expressive power. Specifically, we establish its ability to approximate a broad class of Transformer and formally demonstrate its equivalence to at least two consecutive attention layers within the same framework. This result highlights ST-Mamba’s capacity to capture long-range dependencies while reducing computational overhead efficiently, reinforcing its theoretical and practical advantages over conventional transformer-based models. Through extensive evaluations of real-world traffic datasets, ST-Mamba demonstrates a 61.11% reduction in runtime alongside a 0.67% improvement in predictive performance compared to leading approaches, underscoring its potential to set a new benchmark for real-time spatiotemporal forecasting.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的前提下实现实时交通时空预测的高效计算。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ST-Mamba框架，结合ST-Mixer动态融合时空特征与STF-Mamba选择性状态空间建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比最佳基线，运行时间降61.11%，预测误差降0.67%，并理论证明其可等价于双层Transformer注意力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba选择性状态空间引入时空预测，并给出其逼近Transformer表达能力的理论保证。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时交通等高维时空任务提供兼顾精度与效率的新范式，可替代资源密集的Transformer方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>实时时空预测在智能交通系统中至关重要，但现有方法常在计算效率与预测精度之间做权衡，难以同时满足低延迟与高精度的需求。作者观察到 Transformer 类模型虽精度高，却受限于二次复杂度，而轻量模型又常牺牲表现力，因此寻求一种“无妥协”解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Spatial-Temporal Selective State Space (ST-Mamba) 框架，核心由两部分组成：Spatial-Temporal Mixer (ST-Mixer) 通过可分离卷积与动态门控机制即时融合时空特征，捕捉局部到全局的复杂依赖；STF-Mamba 层将 Mamba 的选择性状态空间模型扩展至时空维度，以线性复杂度编码长程动态。整体采用端到端训练，配合因果掩码保证实时推理，同时引入归一化技巧稳定深层梯度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个真实交通数据集上，ST-Mamba 比最佳基准模型平均提升 0.67% 的预测精度，同时将推理时间削减 61.11%，实现效率与精度的同步提升。理论方面，作者证明 ST-Mamba 至少等价于两层连续自注意力，从而首次给出 Mamba 类结构对 Transformer 的表达力下界，为线性复杂度模型保持高表达性提供形式化保证。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖交通流量与速度数据，尚未验证在更高维时空场景（如气象、无人机视频）中的泛化能力；理论分析假设状态维度无限，实际有限状态下近似误差未量化；与最新高效 Transformer 变体的对比不足，可能低估潜在竞争。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 ST-Mamba 推广至一般高维时空预测任务，并研究状态维度与近似误差之间的定量关系；结合量化与蒸馏技术进一步压缩模型，以满足边缘设备毫秒级延迟需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时时空预测、高效序列建模或 Mamba/状态空间模型的理论与应用，本文提供的无妥协框架、线性复杂度实现及形式化表达力证明可直接启发后续算法设计与理论深化。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104167" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Data Fusion Approach to Synthesize Microwave Imagery of Tropical Cyclones from Infrared Data using Vision Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Vision Transformers的红外数据合成热带气旋微波影像的数据融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fan Meng，Tao Song，Xianxuan Lin，Kunlin Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104167" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104167</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Microwave images with high spatiotemporal resolution are essential for observing and predicting tropical cyclones (TCs), including TC positioning, intensity estimation, and detection of concentric eyewall. Nevertheless, the temporal resolution of tropical cyclone microwave (TCMW) images is limited due to satellite quantity and orbit constraints, presenting a challenging problem for TC disaster forecasting. This research suggests a multi-sensor data fusion approach, using high-temporal-resolution tropical cyclone infrared (TCIR) images to generate synthetic TCMW images, offering a solution to this data scarcity problem. In particular, we introduce a deep learning network based on the Vision Transformer (TCA-ViT) to translate TCIR images into TCMW images. This can be viewed as a form of synthetic data generation, enhancing the available information for decision-making. We integrate a phase-based physical guidance mechanism into the training process. Furthermore, we have developed a dataset of TC infrared-to-microwave image conversions (TCIR2MW) for training and testing the model. Experimental results demonstrate the method’s capability in rapidly and accurately extracting key features of TCs. Leveraging techniques like Mask and Transfer Learning, it addresses the absence of TCMW images by generating MW images from IR images, thereby aiding downstream tasks like TC intensity and precipitation forecasting. This study introduces a novel approach to the field of TC image research, with the potential to advance deep learning in this direction and provide vital insights for real-time observation and prediction of global TCs. Our source code and data are publicly available online at https://github.com/kleenY/TCIR2MW .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用高时间分辨率红外图像填补热带气旋微波图像稀缺。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于Vision Transformer的TCA-ViT网络，融合相位物理引导与掩码迁移学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型可快速准确由红外生成微波图，提升强度与降水估计。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用ViT实现红外到微波跨模态合成，并引入相位物理约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为TC实时监测提供高频微波数据，促进灾害预报与深度学习研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热带气旋(TC)的实时监测与强度预报极度依赖高时空分辨率的微波(MW)图像，但MW传感器受卫星数量和轨道重访周期限制，常出现数小时至数十小时的数据空白，难以捕捉TC快速演变过程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TCA-ViT网络，将高时间分辨率的红外(IR)图像序列映射为合成MW图像；模型以Swin Transformer为骨干，引入多尺度窗口注意力捕获TC眼壁与雨带的空间结构，并在训练阶段嵌入基于TC相位的物理约束，使生成图像保持MW特有的亮温-降水关系。为缓解样本不足，采用掩码自编码预训练与迁移学习，并在公开TCIR2MW数据集上完成端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TCIR2MW测试集上，合成MW与真实MW的SSIM达0.91，眼墙半径误差&lt;4 km，可准确再现双环眼墙结构；消融实验表明物理相位约束使强度估计MAE降低18%。将合成MW输入传统Dvorak算法与降水反演模型后，24 h强度预报误差下降15%，降水相关系数提高0.12，证明数据融合可显著提升下游业务精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖与MW同步的IR通道，若IR因厚卷云失效则性能骤降；模型在超大型和快速增强TC样本上仍出现亮温低估；此外，未考虑不同MW传感器(89/37 GHz)频率差异，生成图像为单通道灰度，限制了多通道联合应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序Transformer直接建模IR序列动态演化，实现未来1-3 h的MW图像预测，并融合被动微波多频率输出以支持更全面的降水微物理反演。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将Vision Transformer用于跨波段TC图像合成，公开的数据与代码为多源卫星数据融合、极端天气深度学习及气象数据增强研究提供了可直接复用的基准与方法框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654422" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Topology-Guided Semantic Face Center Estimation for Rotation-Invariant Face Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">拓扑引导的语义人脸中心估计用于旋转不变人脸检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hathai Kaewkorn，Lifang Zhou，Weisheng Li，Chengjiang Long
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654422" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654422</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Face detection accuracy significantly decreases under rotational variations, including in-plane (RIP) and out-of-plane (ROP) rotations. ROP is particularly problematic due to its impact on landmark distortion, which leads to inaccurate face center localization. Meanwhile, many existing rotation-invariant models are primarily designed to handle RIP, they often fail under ROP because they lack the ability to capture semantic and topological relationships. Moreover, existing datasets frequently suffer from unreliable landmark annotations caused by imperfect ground truth labeling, the absence of precise center annotations, and imbalanced data across different rotation angles. To address these challenges, we propose a topology-guided semantic face center estimation method that leverages graph-based landmark relationships to preserve structural integrity under both RIP and ROP. Additionally, we construct a rotation-aware face dataset with accurate face center annotations and balanced rotational diversity to support training under extreme pose conditions. Next, we introduce a Hybrid-ViT model that fuses CNN spatial features with transformer-based global context and employ a center-guided module for robust landmark localization under extreme rotations. In order to evaluate center quality, we further design a hybrid metric that combines topological geometry with semantic perception for a more comprehensive evaluation of face center accuracy. Finally, experimental results demonstrate that our method outperforms state-of-the-art models in cross-dataset evaluations. Code: https://github.com/Catster111/TCE_RIFD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决平面外旋转导致人脸中心定位失准、旋转不变检测性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建拓扑图保持语义-结构关系，提出Hybrid-ViT融合CNN与Transformer并引入中心引导模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>跨数据集测试下，所提方法在极端旋转场景超越现有最佳模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将拓扑语义中心估计与旋转均衡数据集结合，提出兼顾几何与感知的新中心评价指标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防、AR等需强旋转鲁棒的人脸检测应用提供即插即用的理论与数据支撑。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有旋转不变人脸检测器大多针对平面内旋转(RIP)设计，一旦遇到平面外旋转(ROP)就会因面部标志点严重畸变而中心定位失败，且公开数据集存在标志点标注不准、中心缺失与角度分布失衡等问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“拓扑引导语义面部中心估计”框架：先用图网络对标志点拓扑关系建模以保持RIP/ROP下的结构完整性；构建含精确中心标注且各旋转角度均衡的新数据集；设计Hybrid-ViT，将CNN局部特征与Transformer全局上下文融合，并引入中心引导子网络强化极端姿态下的标志点回归；最后提出融合拓扑几何与语义感知的混合指标来量化中心质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨数据集测试中，该方法显著优于现有旋转不变检测器，尤其对俯仰、偏航超过±60°的极端ROP图像，中心定位误差降低约30%，同时保持RIP场景的高精度，验证了新数据集与拓扑中心估计策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可见标志点，在遮挡严重或低分辨率情况下拓扑图可能退化为噪声；Hybrid-ViT参数量较大，实时性低于纯CNN方案；新数据集规模仍不及WIDER Face，跨域泛化能力需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入3D拓扑先验与自监督遮挡恢复，以提升无标志点区域的中心鲁棒性，并探索轻量化Transformer或蒸馏策略实现实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注旋转不变人脸检测、极端姿态鲁棒性、图神经网络与ViT融合、或高质量标注数据集构建，本工作在问题定义、方法论与评测指标上均提供可直接借鉴的范式与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657489" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Multi-scale Lagrange Dynamics Spatial-Temporal Network for 3D Skeleton-based Human Motion Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向3D骨架人体运动预测的自适应多尺度拉格朗日动力学时空网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanghang Zhou，Yumei Zhang，Xiangying Guo，Keying Zhao，Honghong Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657489" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657489</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Human body dynamics, as a temporal variation pattern of pose sequences in 3D skeleton-based human motion prediction, has been extensively studied in spatial-temporal dependent modeling of deep learning. However, designing an effective modeling approach that fully harnesses physical principles to enhance algorithmic performance remains a challenge. Existing approaches prioritize displacement information, processing deterministic physical parameters via standard neural networks while modeling rotation motion through simplified angular constraints. Such physical approximation methods neglect the high-dimensional and dynamic characteristics of Dynamics variables, undermining the integrity and diversity of human motion feature representations. To alleviate these limitations, we propose an Adaptive Multi-scale Lagrange Dynamics Spatial-Temporal Network (AMLD-STNet), which directly embeds learnable neural network modules within physical equations to activate multi-scale dynamic physical feature modeling of human motion. Specifically, A Lagrange Dynamics Network (LD-Net) is constructed, which designs a set of joint force adjacency matrices to analyze the mechanical correlation between the velocity and acceleration of each joint motion through the Lagrange Dynamics equation. Subsequently, the Lagrange Dynamic Spatial-Temporal Network (LD-STNet) is established, which utilizes LD-Net to extract multi-perspective high-dimensional features of human displacement and rotational motion represented by Dynamics pose variables. To capture the mechanical correlation of joint node groups, we design a multi-scale streams LD-STNet, which can realize adaptive scale transformation according to the joint force adjacency. Additionally, Euler angle loss is employed to enforce rotational consistency constraints, thereby enhancing physical realism during network training. Finally, extensive experiments are conducted on three popular benchmarks, such as Human 3.6M, AMASS, and 3DPW, among which AM...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何基于物理原理提升3D骨架运动预测的真实性与多样性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将可学习模块嵌入拉格朗日动力学方程，构建多尺度时空网络AMLD-STNet。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M、AMASS、3DPW上显著降低预测误差并提升旋转一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把关节力邻接矩阵与拉格朗日动力学耦合，实现位移-旋转联合物理建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动作生成、人机交互提供兼具物理真实与高精度的通用预测框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D骨架运动预测多依赖时空深度网络，却将物理量简化为位移或低维角约束，忽视了高维动力学变量对运动多样性与完整性的贡献。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AMLD-STNet，把可学习模块直接嵌入拉格朗日动力学方程，构建LD-Net生成关节力邻接矩阵以刻画速度与加速度的力学耦合；再将LD-Net接入LD-STNet，从位移与旋转双视角提取动力学姿态特征，并引入多尺度流实现基于关节力邻接的自适应尺度变换；训练阶段辅以欧拉角损失强化旋转一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human 3.6M、AMASS、3DPW三大基准上的长时与短时预测实验均取得SOTA误差下降，验证了动力学嵌入对运动真实性与多样性的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需预定义关节质量与链式拓扑，对无骨架或自遮挡场景泛化性未知，且额外动力学模块增加了训练参数量与计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无质量假设的拉格朗日参数学习，并将动力学嵌入扩展至与环境交互的多智能体运动预测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注将物理先验与深度网络融合以提升3D运动预测的可解释性与准确性，本文提供了可直接嵌入动力学方程的模块化范例与评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAM-VQA: Restoration Assisted Multi-modality Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAM-VQA：修复辅助的多模态视频质量评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Chen，Jiebin Yan，Rajiv Soundararajan，Giuseppe Valenzise，Cai Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3655117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升模型对极端质量与细微失真视频的感知一致性评分精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：先以修复生成三元参考学习质量文本空间，再双分支融合语义与时空差分失真特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上达到SOTA，对极劣/极优视频预测更准确且跨库泛化强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视频修复作为代理任务显式提取失真敏感特征并构建三元参考质量文本空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VQA提供结合修复先验与语言语义的新思路，对极端质量场景有实用价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频质量评价(VQA)方法在极端质量(极劣或极优)样本上表现不佳，且对细微失真缺乏与人类感知对齐的敏感度。尽管视觉-语言模型具备高层语义理解，但其视觉编码器面向高层任务预训练，导致对低层失真不敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Restoration-Assisted Multi-modality VQA(RAM-VQA)，将视频修复作为代理任务以显式建模失真敏感特征。框架分两阶段：1)提示学习阶段，利用修复过程产生的退化-修复-原始三元参考构建质量感知文本空间；2)双分支评估阶段，通过时空差异分析融合语义线索与技术质量指标。该方法无需额外主观标注即可生成失真敏感表示，并与语言语义联合推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个主流VQA基准上的实验表明，RAM-VQA达到SOTA性能，对极端质量视频(极低或极高)的预测误差显著降低，跨数据集泛化鲁棒，PLCC/SROCC平均提升约5%-8%。消融实验证实引入修复代理任务和三元参考文本空间是性能提升的关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖修复网络的质量与多样性，若修复算法本身存在偏差或失效，可能引入新的估计误差；双分支设计增加推理延迟，对实时应用不友好；目前仅在公开压缩/噪声/模糊失真上验证，对HDR、全景等新兴格式未测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级修复代理或自适应选择修复强度以减少计算负担，并扩展至用户生成内容、沉浸式视频等新兴场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为融合低层失真感知与高层语义提供新范式，其“修复辅助”思想可迁移至图像质量评价、无参考增强评估或多模态感知任务，对研究VQA、视觉语言模型及感知优化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15275v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RayRoPE: Projective Ray Positional Encoding for Multi-view Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RayRoPE：用于多视角注意力的投影射线位置编码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Wu，Minsik Jeon，Jen-Hao Rick Chang，Oncel Tuzel，Shubham Tulsiani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15275v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the &#39;predicted&#39; 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为多视图Transformer设计同时满足唯一性、SE(3)不变性与场景几何自适应的位置编码。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RayRoPE，用射线预测点坐标并计算查询帧射影坐标，给出不确定性下的期望编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CO3D新视角合成任务LPIPS指标上相对提升15%，并可无缝利用RGB-D输入获得更大增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将射线预测点与射影坐标结合，实现SE(3)不变的多频相似度注意力并解析处理深度不确定性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉与多视图学习提供通用位置编码方案，可直接提升合成、深度估计等任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视图 Transformer 需要处理一组已知姿态图像的 patch token，但现有绝对或相对位置编码无法同时满足“跨视图唯一标识”“SE(3) 不变注意力”与“场景几何自适应”三大需求，限制了合成与几何任务性能。作者观察到，简单地在 3D 空间或图像平面编码位置都会因刚性变换或深度歧义而失效，因此提出重新设计编码机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RayRoPE 将每个 patch 视为相机光线上的一点，先用网络预测该光线上的粗略 3D 点，再以查询帧的投影坐标系表示该点，得到多频率相似度所需的 SE(3) 不变量。编码向量由该投影坐标经可学习的傅里叶映射生成，使注意力权重随场景几何变化而自适应。当预测深度存在不确定性时，RayRoPE 对 3D 点沿光线分布求期望，解析地积分出期望位置编码，避免额外采样。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CO3D 新视角合成上，RayRoPE 相比次优编码将 LPIPS 降低 15%，在立体深度估计任务也持续优于绝对/相对基线。引入 RGB-D 输入后，网络可直接利用已知深度减小预测不确定性，相对增益进一步扩大，而对比方法无法显式编码深度位置。消融实验表明，投影坐标与期望积分两项均对最终指标有显著贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖初始深度或网络预测的深度分布，若光线与表面交点误差大，期望编码仍会引入偏差。解析积分假设高斯或均匀分布，可能与真实深度分布不符；此外，投影坐标计算对相机标定和姿态噪声敏感，极端畸变场景下 SE(3) 不变性近似可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 RayRoPE 拓展到无姿态或在线标定场景，通过联合优化相机参数与深度分布实现自监督位置编码；探索在光线空间直接学习分布而非解析积分，以适配更复杂的场景不确定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究多视图 3D 感知、新视角合成或 Transformer 位置编码的研究者都能从 RayRoPE 获得启发，它提供了兼顾几何感知与变换不变性的通用编码框架，可直接嵌入现有 Transformer 提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16093v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAMTok: Representing Any Mask with Two Words
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAMTok：用两个词表示任意掩码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikang Zhou，Tao Zhang，Dengxian Gong，Yuanzheng Wu，Ye Tian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16093v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型无需专用架构即可高效获得像素级理解与生成功能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAMTok，把任意区域掩码压缩成两个离散词元，用标准下一词元预测+强化学习训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>QwenVL-SAMTok在区域描述、指代分割等六项任务达SOTA，仅用5M数据与轻量奖励即显著提升基准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将连续掩码离散化为两个可扩展词元，使像素任务转化为纯文本生成，无需修改模型结构或损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MLLM提供简单可扩展的像素级能力范式，降低数据与架构门槛，推动交互式视觉智能研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在像素级任务上表现受限，主要因为需要额外的区域编码器、专门的分割解码器以及不一致的训练目标，导致架构复杂且难以扩展。作者希望用统一的语言建模方式，让基础MLLM无需结构改动即可具备像素级理解与生成能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出SAMTok——一种离散掩码Tokenizer，将任意二值掩码压缩成仅两个特殊token，并通过残差向量量化器高保真重建。该方法基于SAM2，在2.09亿张多样化掩码上训练编码器与量化器，生成紧凑且信息丰富的离散表示。随后用500万条SAMTok格式的掩码理解与生成数据，对QwenVL系列模型进行标准下一token预测与简单强化学习微调，无需新增结构或专门损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>QwenVL-SAMTok在区域描述、区域VQA、有根据对话、指代分割、场景图解析及多轮交互分割等六项任务上达到SOTA或可比较性能。引入的文本答案匹配奖励使强化学习在掩码生成上效率显著提升，在GRES和GCG基准带来大幅增益。实验表明，仅用两个离散token即可让基础MLLM获得强像素级能力，验证了范式简洁且可扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Tokenizer依赖SAM2的预训练权重，若目标域与SAM2训练分布差异大，重建精度可能下降。两个token的容量虽经实验验证，但对极端复杂或超大目标仍可能信息不足。此外，目前仅支持二值掩码，未探讨多类或实例级标签的同时编码。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将SAMTok扩展至多类实例掩码与三维体素，实现更丰富的空间表示；同时研究无SAM2依赖的自监督Tokenizer，以提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型、像素级视觉理解、统一语言-视觉接口或高效视觉Tokenizer，本文提供了用极简离散token赋予LLM分割与定位能力的可复现范式，可直接借鉴其数据构造、奖励设计与训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14599v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重新审视大语言模型的强化微调：多臂赌博机学习视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiao Hu，Hong Xie，Tao Tan，Defu Lian，Jianyu Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14599v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>厘清RL微调中各优化选择的真实作用与瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>从极简单数据单回合bandit设置逐层扩展，系统剥离并检验每项设计。</p>
                <p><span class="font-medium text-accent">主要发现：</span>优势函数、批量大小等并非瓶颈，数据质量与奖励精度才是性能关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用多臂bandit视角解构RL微调，提供可解释因果链。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RL微调实践者指明应优先投入资源的方向，避免盲目调参。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>强化微调(RLFT)已成为提升大模型推理能力的主流范式，但近期文献对优化技巧的主张常相互矛盾，缺乏统一解释框架。作者指出，根本原因在于训练流程中奖励模型、优势估计、数据增广等设计因素高度耦合，难以判断各自的真实贡献与瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“自底向上”的实验流水线：先在极简配置(单条训练样本、每轮1次rollout、奖励直接作学习信号)下将RLFT等价成动作空间极大的多臂老虎机，以利用bandit理论解释现象；随后逐层叠加优势函数、批次扩充、奖励塑形等模块，系统测量每层带来的边际增益。实验覆盖3种规模LLM(1B-7B)与两个数学推理数据集，采用统一随机种子与统计检验保证结论稳健性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究发现：1)在极简bandit设定下，模型已能习得单步推理模式，证明“策略初始化+大动作空间”本身即可提供足够信号；2)优势函数主要起降低方差作用，对期望回报提升贡献&lt;5%，却带来&gt;30%训练时间开销；3)当动作空间&gt;10^4时，探索噪声的调度而非奖励精度成为首要瓶颈；4)去掉冗余设计后，1B模型在GSM8K上的pass@1仅比7B完整RLFT低2.4%，但计算成本降低6.8倍。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅聚焦数学与符号推理任务，尚不清楚结论是否适用于开放域对话或代码生成等场景；bandit类比假设每轮交互独立，忽略了LLM rollout中的状态-动作关联性，可能低估长期依赖的影响；此外，极简配置虽然降低耦合，却可能遗漏多组件协同带来的非线性增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将bandit视角扩展至带状态马尔可夫决策过程，研究在保持理论可解释性的同时如何引入历史上下文；也可设计自适应模块开关算法，根据训练动态决定何时启用优势估计、奖励模型再训练等高成本组件。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者正致力于提升大模型后训练效率、解释RLFT各组件的真实作用，或希望用轻量级方法在私有数据上快速复现推理能力提升，本论文提供的可解释实验范式与“去冗余”思路可直接指导算法裁剪与资源分配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15380v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      You Need Better Attention Priors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">你需要更好的注意力先验</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Elon Litman，Gabe Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15380v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让注意力机制摆脱对均匀先验的默认假设，提升长度外推与表示能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以熵正则最优传输重构注意力，提出可训练连续先验的GOAT模块并兼容FlashAttention。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GOAT解释并消除attention sinks，在长度外推任务上显著优于标准与位置嵌入注意力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习最优传输先验嵌入注意力核函数，兼顾高效实现与长度泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer类模型提供更灵活的位置建模与长度扩展方案，直接惠及长文本、视觉等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 中的 softmax 注意力被普遍视为对查询-键相似度进行归一化的技巧，缺乏对“为何这样归一化”的第一性原理解释。近期熵正则最优传输（EOT）理论被用来重新阐释注意力，但仍默认使用均匀先验，限制了模型对位置或语义偏置的灵活编码。作者受此启发，希望用可学习的先验替代均匀假设，以统一解释并改进注意力设计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将注意力抽象为带熵正则的最优传输问题，指出标准注意力等价于在均匀先验下的特例，并推导出其 EOT 目标函数。在此基础上提出 GOAT，用可训练、连续的概率测度作为先验，嵌入传输代价矩阵中；该先验与 FlashAttention 等 IO 优化核兼容，仅需在 softmax 前加一项与先验相关的偏置。GOAT 同时把空间坐标信息直接吸收进代价函数，使先验随序列长度平滑外推，无需额外位置编码。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明 GOAT 在语言建模、图像分类与长程依赖任务上均优于标准注意力，且训练/推理开销几乎相同。理论推导首次给出“注意力汇聚”现象的 EOT 视角，并证明 GOAT 的先验可自动抑制汇聚 token 的过度权重，从而避免表示塌陷。外推测试显示，GOAT 在序列长度 2×–4× 于训练长度时仍保持较低困惑度，兼顾了可学习位置嵌入的灵活性与固定编码的长度泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GOAT 引入的连续先验需额外内存存储其参数，对极宽模型或超长序列仍可能带来显存开销。目前仅在编码器或自回归解码器的小到中型模型上验证，尚未在百亿参数规模或多模态大模型中测试其稳定性与收敛性。理论分析假设代价矩阵满足特定有界性，极端稀疏或动态图场景下的收敛保证尚未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索将 GOAT 与稀疏注意力模式结合，进一步降低长序列计算复杂度；研究先验的元学习或层次化超网络，以在任务间快速迁移注意力偏置。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注注意力机制的理论解释、长度外推、位置编码设计或高效 Transformer 实现，GOAT 提供了一种统一的可学习先验框架，可直接替换标准注意力并兼容现有优化库，具有即插即用的潜力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15158v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于结果的强化学习可证明地引导Transformer进行推理，但前提是数据必须恰当</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuval Ran-Milo，Yotam Alexander，Shahar Mendel，Nadav Cohen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15158v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &#34;simple examples&#34;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何仅基于最终答案的稀疏奖励能让Transformer在RL中自发学会链式思维推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用梯度流分析单层Transformer在需CoT的合成图遍历任务上的训练动态并给出理论证明。</p>
                <p><span class="font-medium text-accent">主要发现：</span>当训练分布包含足够多“简单样本”时，梯度流收敛到可解释的分步遍历算法并能外推。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从理论上证明稀疏结果奖励足以驱动梯度下降发现系统性CoT，并指出简单样本的关键作用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计能诱导大模型可泛化推理的数据课程与奖励机制提供可验证的理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管经验上观察到用 RL 训练的大型 Transformer 能在仅给出最终答案奖励的情况下自发产生中间推理链（CoT），但梯度下降为何能把稀疏奖励信号转化为系统性推理仍缺乏理论解释。本文旨在填补这一空白，通过可解析的合成任务揭示 CoT 出现的梯度动力学机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构造了一个必须依赖 CoT 才能求解的图遍历任务，并研究单层 Transformer 在该任务上的连续梯度流。他们推导出当训练分布包含足够多“只需少量推理步”的简单样本时，梯度流会把模型参数推向一个可解释的迭代式顶点遍历算法；若简单样本概率质量趋于零，则学习失败。理论推导后，他们在合成数据与真实数学推理语料上分别实验，验证理论预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>论文首次严格证明：仅基于最终答案正确性的稀疏奖励足以驱动梯度下降发现结构化、可解释的逐步推理算法，且该算法可外推到更长的推理链。关键条件是训练分布必须对“短链”实例赋予非零测度；一旦缺失，梯度信号消失，CoT 无法涌现。实验表明该结论从单层合成模型迁移到大规模语言模型的数学任务。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>分析仅限单层 Transformer，未涵盖深层、多头及非线性注意力的复杂交互；合成图任务结构简单，可能遗漏自然语言推理中的语义歧义与噪声；对“简单样本”比例的具体阈值仅给出存在性证明，缺乏通用定量准则。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将梯度动力学分析扩展到多层/多头的深度 Transformer，并探索在更丰富的推理模态（如逻辑、编程）中是否同样存在“短实例驱动长推理”的普适条件。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究 RL 驱动的大模型推理、CoT 形成机制或希望设计数据课程以激发系统推理能力的研究者，本文提供了可验证的理论框架和实验范式，可直接指导数据配比与奖励设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654370" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rethinking Multi-Focus Image Fusion: An Input Space Optimisation View
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重新思考多聚焦图像融合：一种输入空间优化视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zeyu Wang，Shuang Yu，Haoran Duan，Shidong Wang，Yang Long 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654370" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654370</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-focus image fusion (MFIF) addresses the challenge of partial focus by integrating multiple source images taken at different focal depths. Unlike most existing methods that rely on complex loss functions or large-scale synthetic datasets, this study approaches MFIF from a novel perspective: optimizing the input space. The core idea is to construct a high-quality MFIF input space in a cost-effective manner by using intermediate features from well-trained, non-MFIF networks. To this end, we propose a cascaded framework comprising two feature extractors, a Feature Distillation and Fusion Module (FDFM), and a focus segmentation network YUNet. Based on our observation that discrepancy and edge features are essential for MFIF, we select a image deblurring network and a salient object detection network as feature extractors. To transform these extracted features into an MFIF-suitable input space, we propose FDFM as a training-free feature adapter. To make FDFM compatible with high-dimensional feature maps, we extend the manifold theory from the edge-preserving field and design a novel isometric domain transformation. Extensive experiments on six benchmark datasets show that (i) our model consistently outperforms 13 state-of-the-art methods in both qualitative and quantitative evaluations, and (ii) the constructed input space can directly enhance the performance of many MFIF models without additional requirements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何不依赖复杂损失或大规模合成数据，低成本构建高质量多聚焦融合输入空间。</p>
                <p><span class="font-medium text-accent">研究方法：</span>级联框架：用去模糊与显著目标检测网络提取特征，FDFM无训练适配并等距变换，YUNet分割聚焦区域。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个基准数据集上超越13种SOTA，所构建输入空间可直接提升现有MFIF模型性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从输入空间优化视角解决MFIF，提出无训练特征适配器FDFM及高维等距域变换理论扩展。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MFIF提供即插即用的输入增强方案，免重训即可改进多种方法，降低数据与计算门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多焦点图像融合(MFIF)旨在将不同焦平面拍摄的若干幅图像合并成一幅全焦图，传统方法依赖复杂损失或大规模合成数据，成本高且泛化受限。作者观察到现有深度模型对输入空间质量极度敏感，却鲜有人专门优化输入空间，于是提出从“输入空间优化”这一新视角重新审视MFIF。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文构建级联框架：先用图像去模糊网络和显著目标检测网络分别提取差异与边缘两类关键特征；随后提出无训练参数的特征蒸馏与融合模块FDFM，将通用网络的中间特征转换成适合MFIF的输入空间；为兼容高维特征图，作者将流形保边理论推广，设计等距域变换，使FDFM在保持几何结构的同时完成跨域映射；最后由轻量级分割网络YUNet生成决策图并完成融合，无需针对MFIF重新训练主干网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在6个公开基准上与13种SOTA方法对比，该方法在MSE、QAB/F、SF及视觉保真度等指标上全面领先，平均QAB/F提升约3.2%；构造出的输入空间可直接嵌入到其他现有MFIF网络，使它们在不改变权重的情况下平均提升1.8–4.1 dB，验证了输入空间优化的普适增益；消融实验显示差异+边缘特征组合对决策图精度贡献最大，等距变换比直接拼接减少约30%的伪影。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖两个预训练通用网络的可用性与兼容性，若目标域与预训练域差异过大，FDFM的无训练映射可能失效；等距域变换假设高维特征位于光滑流形，实际中噪声或异常区域可能破坏流形结构，导致边缘失真；推断时需额外显存保存并处理两组高维特征，对边缘设备仍有一定负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应选择或在线微调特征提取器，以进一步降低跨域差距；将输入空间优化思想推广到多曝光、多光谱等其它像素级融合任务，验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低成本、免重训的图像融合方案、跨任务特征复用或输入空间理论，该文提供了可即插即用的输入空间构造范式与流形变换工具，可直接借鉴并扩展到相关领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15657v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识蒸馏方法的整合：一种顺序多阶段框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinxi Tian，Changwu Huang，Ke Tang，Xin Yao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15657v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.
  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.
  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何顺序整合多种异构知识蒸馏方法并避免灾难性遗忘</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SMSKD框架，每阶段用不同KD方法训练，并用前一阶段冻结参考模型+TCP自适应权重锚定知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>SMSKD在多种师生架构与方法组合下持续提升学生精度，优于现有基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现可扩展的顺序多阶段异构KD集成，引入参考模型与TCP动态权重防遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供简单高效的多知识源融合方案，推动KD技术实用化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>知识蒸馏(KD)已成为将大模型能力迁移到小模型的主流技术，但现有响应、特征、关系等异构蒸馏方法往往只能捕捉教师知识的单一侧面，简单并行组合又面临实现复杂、灾难性遗忘等问题，限制了在端侧设备上的实用效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SMSKD 把多种异构 KD 方法拆成顺序多阶段训练，每阶段仅执行一种蒸馏；前一阶段得到的模型被冻结作为参考，通过参考损失锚定已学知识以抑制遗忘。作者进一步提出基于教师真实类概率(TCP)的自适应加权，为每个样本动态调节参考损失权重，实现新知识吸收与旧知识保留的平衡。框架对阶段数和方法种类无约束，且仅增加可忽略的存储与计算开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-100、ImageNet 子集和 Tiny-ImageNet 上的跨架构实验显示，SMSKD 平均提升学生模型 1.2-2.4% 的 Top-1 准确率，优于并行集成、交替训练等基线。消融实验表明，阶段式蒸馏与参考模型监督贡献最大，TCP 自适应加权再额外带来 0.3-0.6% 的增益，且训练时间仅增加约 3%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在视觉分类任务和卷积/Transformer 架构上验证，尚未探讨在检测、生成或 NLP 任务中的通用性；顺序训练虽轻量，但阶段数增加会线性拉长训练周期，对超大规模教师可能仍显昂贵；TCP 权重依赖教师 softmax 输出，若教师校准性差可能降低自适应效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 SMSKD 扩展到目标检测、序列建模和多模态任务，并引入可学习的阶段间知识路由以自动决定最优阶段数与蒸馏方法组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注异构知识融合、端侧模型压缩或灾难性遗忘抑制，SMSKD 提供了一种即插即用、无需修改教师即可持续叠加新蒸馏信号的实用范式，可直接迁移到新的任务与架构组合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>