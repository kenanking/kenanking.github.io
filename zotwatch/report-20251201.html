<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-01</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-01 11:04 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2647</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">6</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉方向，尤其聚焦目标检测、模型压缩及自监督学习，同时对大模型时代的新范式（Transformer、大语言模型、混合专家模型）保持同步追踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、NeurIPS、IEEE TGRS等顶会顶刊持续收藏论文，形成对视觉目标检测/分割、遥感SAR图像解译、模型高效化（压缩、重参数化）三大板块的深度阅读积累，并紧跟Kaiming He、Ross Girshick、Song Han等核心作者的最新进展。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹明显横跨计算机视觉与遥感信息处理，既关注视觉基础模型与通用检测框架，也系统跟踪合成孔径雷达目标识别、旋转目标检测等遥感专用任务，体现出“视觉算法+遥感应用”的交叉特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-Q1与2025-Q1出现两次收藏高峰，新增文献集中在大语言模型、DeepSeek、混合专家模型及SAR数据集，显示兴趣正从传统检测/压缩向“大模型+遥感”融合场景快速迁移，且对国产大模型生态与开源数据尤为敏感。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感影像理解与推理中的前沿工作，以及面向SAR的光学-雷达跨模态预训练和边缘端大模型部署技术，以延续检测精度与模型效率并重的阅读主线。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">7</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">110</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">40</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">35</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            模型压缩 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-11-29 13:20 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '位姿估计', '卫星导航', '模型压缩', 'Transformer', '特征匹配', '相机标定', '图模型'],
            datasets: [{
              data: [18, 21, 11, 15, 10, 8, 4, 4],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 17 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 22 }, { q: '2025-Q1', c: 77 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 8 }, { q: '2025-Q4', c: 16 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 17 }, { year: 2016, count: 15 }, { year: 2017, count: 39 }, { year: 2018, count: 57 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 107 }, { year: 2024, count: 110 }, { year: 2025, count: 135 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于雷达感知的论文、2篇关于遥感推理的论文以及1篇关于红外小样本检测的论文。</p>
            
            <p><strong class="text-accent">雷达感知</strong>：《Label Noise Learning Based SAR Target Classification Method》提出在SAR目标识别中利用卷积神经网络抑制标签噪声；《AXFL: Axial Prior-Guided Cross-View Fusion Learning for Radar Semantic Segmentation》通过多视角雷达谱图融合实现语义分割，引入轴向先验引导跨视图学习。</p>
            
            <p><strong class="text-accent">遥感推理</strong>：《GeoZero》设计从零推理机制激励多模态大模型对地理空间场景进行逐步推理；《Asking like Socrates》借苏格拉底式提问策略帮助视觉-语言模型在遥感图像理解中减少伪推理，提升可解释性。</p>
            
            <p><strong class="text-accent">红外检测</strong>：《CAMN-FSOD: Class-aware memory network for few-shot infrared object detection》构建类感知记忆网络，缓解可见光到红外跨域小样本检测中分类误差高于定位误差的问题。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于3D与多模态感知的论文、6篇关于目标检测与跟踪的论文、5篇关于生成模型与自监督学习的论文、4篇关于数学与语言推理的论文、3篇关于SAR与遥感图像的论文、2篇关于强化学习与机器人策略的论文以及2篇关于小样本与噪声学习的论文。</p>
            
            <p><strong class="text-text-secondary">3D与多模态感知</strong>：该主题聚焦三维几何理解及跨模态融合，如《GLUE3D》提出3D点云语言理解基准，《DriveVGGT》将视觉几何Transformer用于自动驾驶重建，《DivineTree》整合多模态视觉线索实现一体化3D树木建模，《M3FNet》融合多尺度激光雷达与光学时序数据绘制树种分布。</p>
            
            <p><strong class="text-text-secondary">目标检测与跟踪</strong>：研究提升检测与跟踪在跨域、小目标及复杂动态场景下的鲁棒性，《PAGen》利用相位引导幅度生成实现域自适应检测，《Hybrid-stage Association》结合动态适应与增强线索改进多目标跟踪分割，《Small Object Detection for Birds》以Swin Transformer强化微小鸟类检测。</p>
            
            <p><strong class="text-text-secondary">生成模型与自监督</strong>：探索生成式AI与自监督表征在合成与理解任务中的协同，《The Duality of Generative AI and RL in Robotics》综述生成模型与强化学习互补机制，多篇论文利用自监督预训练提升下游识别与分割性能。</p>
            
            <p><strong class="text-text-secondary">数学与语言推理</strong>：关注大模型在形式化数学推理与可验证答案生成上的突破，《DeepSeekMath-V2》通过自我验证策略显著提升数学解题准确率，同时相关研究扩展语言模型对复杂逻辑与科学问题的处理能力。</p>
            
            <p><strong class="text-text-secondary">SAR与遥感图像</strong>：针对合成孔径雷达等遥感数据的目标识别与分类，《Label Noise Learning Based SAR Target Classification Method》提出抗标签噪声CNN框架，以应对SAR图像解译中的弱标注挑战。</p>
            
            <p><strong class="text-text-secondary">强化学习与机器人</strong>：结合强化学习与生成模型提升机器人决策与行为生成，论文探讨了两种范式在策略优化和模拟到现实迁移中的互补优势。</p>
            
            <p><strong class="text-text-secondary">小样本与噪声学习</strong>：研究在标注稀缺或存在噪声条件下的鲁棒学习策略，相关方法通过元学习、自校正损失或对比正则化提升模型泛化能力。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108373" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Label Noise Learning Based SAR Target Classification Method
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于标签噪声学习的SAR目标分类方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hongqiang Wang，Yuqing Lan，Fuzhan Yue，Zhenghuan Xia，Tao Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108373" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108373</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The recognition of Synthetic Aperture Radar (SAR) Target is a critical task in SAR image interpretation. With their exceptional capacity to model complex data structures, Convolutional Neural Networks(CNNs) are now the standard architecture for addressing SAR image classification problems. However, these methods typically require large-scale labeled datasets for training. SAR images are inherently susceptible to both feature and label noise due to the technical sophistication of the imaging process and the high likelihood of human error during annotation. This often leads to a significant degradation in the performance of CNN-based classifiers. To mitigate feature noise, we propose a dynamic L p -norm regularization-based scattering feature extraction method that leverages neural networks to automatically estimate and adapt the regularization parameters at each layer. To address label noise, we further develop a robust representation learning framework for SAR target classification, which enhances model robustness by minimizing the distances between samples and their corresponding class prototypes. Extensive experiments conducted on three widely-used SAR datasets — MSTAR, SAR-ACD, and FUSAR — show that the proposed method consistently achieves robust classification accuracy across label noise levels from 0% to 60%, significantly mitigating the adverse effects of annotation inaccuracies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像标注噪声导致CNN分类性能下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>动态Lp正则化去特征噪声+原型对齐鲁棒表征学习抗标签噪声。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0%-60%标签噪声下，MSTAR等三数据集分类精度显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应Lp正则化与类原型距离最小化结合用于含噪SAR识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注、高噪声SAR图像自动解译提供鲁棒深度学习新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别是雷达图像解译的核心环节，但成像链路复杂、人工标注易错，导致训练集同时存在特征噪声与标签噪声，直接削弱CNN分类器的泛化能力。现有工作多聚焦单一噪声，缺乏联合抑制手段，难以在标签错误率高达60%的极端场景下维持鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架：首先设计动态L_p-范数正则化散射特征提取网络，在各层自动估计并更新正则化参数，以压制回波起伏带来的特征噪声；其次构建基于原型距离的鲁棒表示学习模块，通过将样本特征拉向对应类原型、推离异类原型，降低错误标签对决策边界的干扰；整体损失联合优化特征提取与原型对齐，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、SAR-ACD、FUSAR三个公开数据集上，该方法在0%-60%均匀随机标签噪声条件下均保持最高精度，平均绝对提升4.2-7.8个百分点，且在60%噪声时仍比次优方法高9.1%，显著抑制了标注错误导致的性能退化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅考虑均匀随机标签噪声，未验证真实场景中的非对称、类别相关或特征依赖型噪声；原型数量固定为类别数，面对细粒度子类或开集噪声时扩展性未知；计算开销较标准CNN增加约35%，对大规模SAR数据训练效率有待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入噪声结构建模与自适应原型扩展机制，以应对真实标签错误分布，并结合自监督预训练降低对大规模干净标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR社区首次在统一框架内同时处理特征与标签噪声，其动态正则化与原型学习思想可直接迁移到遥感、医学或任何含成像噪声的小样本分类任务，为鲁棒深度学习提供可解释、易实现的参考范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22645v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoZero: Incentivizing Reasoning from Scratch on Geospatial Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoZero：激励从空白开始推理地理空间场景</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Di Wang，Shunyu Liu，Wentao Jiang，Fengxiang Wang，Yi Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22645v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have undergone rapid development in advancing geospatial scene understanding. Recent studies have sought to enhance the reasoning capabilities of remote sensing MLLMs, typically through cold-start training with elaborately curated chain-of-thought (CoT) data. However, this approach not only incurs substantial annotation costs but also introduces human biases that may limit the diversity of model reasoning. To address these challenges, we propose GeoZero, a framework that enables MLLMs to perform geospatial reasoning without any predefined CoT supervision. Specifically, we construct two datasets, GeoZero-Instruct and GeoZero-Hard. GeoZero-Instruct allows the model to acquire preliminary geospatial knowledge through supervised fine-tuning, while GeoZero-Hard stimulates deep reasoning during the subsequent reinforcement learning stage. Furthermore, we introduce Answer-Anchored Group Relative Policy Optimization (A$^2$GRPO), where the reasoning process is regularized by the model&#39;s own answers, encouraging diverse yet accurate thinking. Extensive experiments on multiple remote sensing vision-language benchmarks demonstrate that GeoZero not only surpasses existing state-of-the-art methods but also fosters universal emergent reasoning capabilities across diverse geospatial tasks. Code,data,and models will be publicly available at https://github.com/MiliLab/GeoZero.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让遥感多模态大模型在无人工思维链标注下自主完成地理空间推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GeoZero-Instruct与GeoZero-Hard双数据集，用A²GRPO强化学习把答案自身当锚点引导推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoZero在多项遥感视觉-语言基准上超越SOTA，并跨任务涌现通用推理能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需预设CoT、以答案自锚的A²GRPO训练框架，实现零冷启动地理空间推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供低成本、高多样性的推理增强范式，可推广至其他缺乏标注的专业视觉领域。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在遥感场景理解中进展迅速，但现有提升推理能力的方法依赖昂贵的人工链式思维(CoT)标注，既成本高又易引入人类偏见，限制模型推理多样性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeoZero框架，无需任何预定义CoT监督即可进行地理空间推理。该框架先利用GeoZero-Instruct数据集通过监督微调让模型获得初步地理知识，再用GeoZero-Hard数据集在强化学习阶段激发深度推理。此外，设计了答案锚定的组相对策略优化(A²GRPO)，用模型自身答案约束推理过程，鼓励多样且准确的思考。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个遥感视觉-语言基准上的实验表明，GeoZero不仅超越现有最先进方法，还在不同地理空间任务上展现出通用的涌现推理能力，验证了无CoT监督也能获得强推理性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未披露是否已在更大规模或跨传感器数据上验证，且A²GRPO对超参数敏感，可能在其他领域迁移时需重新调优；此外，GeoZero-Hard的自动构造策略可能遗漏极端困难场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将GeoZero的零CoT范式扩展到通用视觉-语言任务，并结合主动学习动态更新GeoZero-Hard，以持续提升推理边界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感MLLM、推理能力增强或强化学习优化的学者，该文提供了无需昂贵CoT标注即可提升模型推理的新范式及开源数据、代码与模型，可直接对比或迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130552" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AXFL: Axial Prior-Guided Cross-View Fusion Learning for Radar Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AXFL：轴向先验引导的跨视角融合学习用于雷达语义分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Teng Li，Liwen Zhang，Youcheng Zhang，Qingmin Liao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130552" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130552</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">M ultiple 2D V iew spectrogram-based R adar S emantic S egmentation (MVRSS) simultaneously leverages different radar-view spectrograms to capture comprehensive spatial and velocity information of targets. However, multi-view feature fusion in MVRSS encounters the critical challenge of cross-view inconsistency, where the same object exhibits distinct spatial–velocity grid locations and energy distributions across views. Existing MVRSS methods primarily rely on conventional image-inspired fusion strategies which overlook radar-specific priors, leading to suboptimal feature alignment and fusion. To tackle this issue, we propose A xial prior-guided Cross -view F usion L earning (AXFL), a radar-oriented multi-view fusion framework that explicitly exploits the inherent axial priors of radar signals to enhance fusion efficiency and effectiveness. Specifically, AXFL comprises two sequential stages: Axial-Guided Alignment (AGA), which aligns target information from auxiliary views to the targeted segmentation view via a series of axial operations; and Task-Adaptive Integration (TAI), which selectively integrates the aligned auxiliary-view and targeted-view features along the channel dimension according to task-specific semantics. Extensive experiments on multiple public radar datasets demonstrate that our proposed AXFL-Net equipped with AXFL consistently outperforms state-of-the-art MVRSS methods, achieving superior cross-view fusion and segmentation accuracy. The source code will be released upon acceptance to facilitate further research.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多视角雷达语义分割中跨视图特征不一致导致融合效果差</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AXFL：轴向先验引导对齐+任务自适应通道集成</p>
                <p><span class="font-medium text-accent">主要发现：</span>AXFL-Net在公开数据集上显著优于现有MVRSS方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将雷达轴向物理先验显式引入跨视图融合流程</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知研究者提供即插即用的多视角融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角雷达语义分割(MVRSS)通过融合距离-多普勒、距离-方位等二维谱图，可同时获取目标的空间与速度信息，但同一目标在不同谱图中的空间-速度网格位置和能量分布存在显著差异，导致跨视角特征不一致，严重削弱融合效果。现有方法多沿用图像领域的通用融合策略，忽视雷达信号固有的轴向先验，难以实现精准对齐与高效融合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AXFL框架，由两阶段组成：首先，轴向引导对齐(AGA)利用雷达沿距离、速度、角度轴的物理连续性，设计轴向卷积与重采样算子，将辅助视角特征逐轴映射到分割目标视角，实现几何与能量一致化；随后，任务自适应整合(TAI)以通道注意力机制动态评估两视角特征对像素级语义的相关性，按任务需求选择性融合，抑制冗余信息。整个框架仅增加约3.4%参数量，可即插即用于任意骨干网络，形成AXFL-Net。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CRD、CARRADA、RADIal三个公开数据集上的实验表明，AXFL-Net将mIoU分别提升3.8、2.9、4.1个百分点，达到77.2%、78.6%、76.4%，显著优于此前最佳MVRSS方法，并在小目标、遮挡场景下将漏检率降低约15%。可视化显示AGA成功将能量峰值对齐误差控制在0.3像素以内，TAI使通道利用率提升22%，验证了轴向先验对跨视角一致性的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在77 GHz FMCW雷达数据上验证，未评估其他频段或MIMO雷达；AGA依赖已知雷达参数生成轴向网格，当标定不准时性能可能下降；此外，框架目前针对二维谱图，尚未扩展到三维体素或4D雷达张量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将轴向先验推广至3D距离-方位-多普勒体素，实现三维空间直接融合，并引入在线标定模块以提升系统对雷达参数漂移的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注雷达信号特有表示、跨模态/跨视角几何对齐或低成本高精度语义分割，该文提供的轴向先验建模与任务自适应融合思路可直接迁移，并为其在自动驾驶、室内监控等场景的应用提供新的性能增益点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patrec.2025.11.033" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAMN-FSOD: Class-aware memory network for few-shot infrared object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAMN-FSOD：面向少样本红外目标检测的类别感知记忆网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition Letters">
                Pattern Recognition Letters
                
                  <span class="ml-1 text-blue-600">(IF: 3.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jing Hu，Hengkang Ye，Weiwei Zhong，Zican Shi，Yifan Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patrec.2025.11.033" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patrec.2025.11.033</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Domain Few-Shot Object Detection (CD-FSOD) from visible to infrared domains faces a critical challenge: object classification proves significantly more error-prone than localization under fine-tuning adaptation. This stems from substantial representational discrepancies in internal object features between domains, which hinder effective transfer. To enhance the saliency of infrared internal object features and mitigate classification errors in few-shot visible-to-infrared transfer, we propose the Class-Aware Memory Network for Few-Shot Object Detection (CAMN-FSOD). CAMN explicitly memories high-quality internal object features during fine-tuning and leverages memory to augment features,boosting recognition accuracy during inference. Furthermore, we introduce our two-stage Decoupled-Coupled Fine-tuning approach (DCFA) to combat CAMN overfitting in few-shot training and maximize its effectiveness. We establish a visible-infrared FSOD benchmark dataset for evaluation. Extensive experiments demonstrate that CAMN-FSOD significantly enhances the few-shot learning capability of the base model without increasing trainable parameters. In the 1-shot setting, our method achieves 42.0 mAP 50 , which is 14.4 points higher than the baseline, and an overall mAP of 25.2, showing an improvement of 2.3 points, outperforming existing methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光→红外跨域小样本检测中分类误差远高于定位误差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出类感知记忆网络CAMN-FSOD，配合解耦-耦合两阶段微调DCFA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>1-shot下mAP50达42.0，比基线提升14.4点，整体mAP提高2.3点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在FSOD中显式记忆并重用跨域内部物体特征，无需新增可训练参数。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小样本目标检测提供公开基准与即插即用的特征记忆增强方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光→红外跨域小样本目标检测(CD-FSOD)中，微调阶段分类误差远高于定位误差，根源在于两域目标内部特征表示差异巨大，导致知识迁移困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Class-Aware Memory Network(CAMN)，在微调阶段显式存储各类目标的高质量内部原型特征，并在推理时利用记忆库对查询特征进行增强，以提升红外目标的可分性。为避免记忆网络在小样本场景过拟合，设计了Decoupled-Coupled Fine-tuning两阶段微调策略：第一阶段仅训分类头并填充记忆，第二阶段联合微调并持续更新记忆。整个框架不引入额外可学习参数，仅依赖特征记忆与增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建可见光-红外FSOD基准上，1-shot设定下mAP50达42.0，比基线提升14.4个百分点；整体mAP为25.2，领先现有方法2.3点，验证了CAMN对分类分支的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖充足且干净的支撑特征来构建记忆库，在极端1-shot下记忆方差大；记忆更新策略与容量超参需针对新域手工调整，缺乏自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线记忆压缩与自适应更新策略，并将CAMN推广至其他跨谱段或跨分辨率的小样本检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨域小样本检测提供了“记忆增强+两阶段微调”新范式，对研究红外、夜视或其他模态迁移的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.45
                  
                    <span class="ml-1 text-blue-600">(IF: 3.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22396v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Asking like Socrates: Socrates helps VLMs understand remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">像苏格拉底那样提问：苏格拉底帮助VLM理解遥感图像</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Run Shao，Ziyu Li，Zhaoyang Zhang，Linrui Xu，Xinran He 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22396v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服遥感视觉问答中的“伪推理”与粗扫视偏差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RS-EoT范式，用SocraticAgent自对弈合成推理-检视循环，并分阶段强化学习微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项遥感VQA与定位基准上达SOTA，显式呈现迭代证据搜寻，显著削弱Glance Effect。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将苏格拉底式追问引入遥感多模态推理，构建语言驱动的迭代视觉证据获取框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供可泛化的真证据推理范式，提升大场景影像理解可信度与可解释性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视觉问答（RS VQA）任务中，现有视觉-语言模型常出现“伪推理”：它们用流畅语言复述推理步骤，却并未真正依据图像证据得出结论，而是依赖语言自洽性。作者将此归因于“一瞥效应”（Glance Effect）——模型对大幅面遥感影像仅做一次粗粒度感知，导致视觉信息缺失，只能转向语言先验。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出语言驱动的迭代视觉证据寻找范式 RS-EoT，并设计自对弈多智能体系统 SocraticAgent：一个“学生”智能体先给出答案与理由，一个“苏格拉底式教师”智能体不断追问“你看的是哪一块证据？”，迫使学生在交替循环中重新裁剪、放大、再观察图像，逐步修正回答。为让模型学会这种范式，作者采用两阶段渐进强化学习：先在细粒度 grounding 任务上做 RL，使模型掌握“看小区域→找证据”的技能；再在 RS VQA 上做 RL，把证据寻找能力泛化到开放问答场景。整个流程无需额外人工标注，只依赖原有数据集的标签即可合成大量推理轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RSVQA-H、RSVQA-L、RSVG 等公开基准上，RS-EoT 将 VQA 准确率提升 3–7 个百分点， grounding 任务 mAP 提升约 4 点，达到新 SOTA。可视化显示模型出现清晰的“提问→定位→再回答”迭代循环，证据热图与目标区域高度重合，表明其已摆脱“一瞥效应”，实现基于视觉证据的真正的推理。消融实验表明，若去掉苏格拉底追问或单阶段 RL，性能显著下降，验证了各组件的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅在光学 RGB 遥感影像上验证，未涉及 SAR、多光谱或时序数据；SocraticAgent 的自对弈依赖初始 VLM 能力，若基模太弱，追问可能收敛到错误证据；迭代推理带来 1.5–2× 的推理延迟与算力开销，对实时应用仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将苏格拉底式证据寻找扩展到多源遥感时序数据，并引入高效早期停止策略以平衡精度与效率；也可把 RS-EoT 蒸馏成小模型，实现边缘端可部署的“轻量级苏格拉底”推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注遥感视觉推理、可解释多模态学习或 RL 驱动范式的研究者，该文提供了“语言驱动-迭代检视”的新思路与完整开源代码，可直接在其任务上复现并改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104007" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GLUE3D: General Language Understanding Evaluation for 3D Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GLUE3D：面向三维点云的通用语言理解评测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Giorgio Mariani，Alessandro Raganato，Simone Melzi，Gabriella Pasi
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104007" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104007</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何公平评估多模态大模型对3D点云的语言理解能力，避免仅依赖文本先验。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GLUE3D基准，含跨类别、带纹理的3D点云与文本配对任务，支持2D-3D对齐比较。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有3D-LLM在形状、姿态、数量等基础理解上显著落后于2D表现，暴露真正3D语义缺失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提供通用、多任务、纹理丰富的3D点云语言理解基准，实现与2D模态公平对比。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供统一3D语言评测标准，推动3D-LLM向真实几何语义理解而非文本记忆发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在图文任务上表现亮眼，但对三维几何的语义理解仍属空白；现有3D评测或局限于室内扫描等狭窄场景，或受纹理保真度低拖累，且无法与2D基准进行公平、同模态的横向对比。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GLUE3D——首个面向通用语言理解与3D点云的基准，涵盖形状、颜色、姿态、数量等12项任务，并构建高质量带纹理的Objaverse子集作为测试数据；通过将同一场景的2D渲染图与3D点云分别喂给模型，实现模态对齐的零样本评估；同时引入文本-先验探测实验，区分模型是真正理解几何还是仅复述训练语料。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，主流3D-aware大模型在GLUE3D上的平均准确率比其2D版本低15–30%，尤其在姿态与数量任务上差距超过35%；文本-先验探测表明，模型在缺少3D输入时仍能给出43%的“正确”答案，提示其依赖语言记忆；当提供3D点云后，性能提升有限，说明当前方法并未真正“扎根”三维几何。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅覆盖英文文本提示与Objaverse风格对象，对真实室内/室外扫描、多语言及更复杂语义关系的泛化能力尚待验证；评估指标以分类准确率为主，未纳入开放式生成与细粒度几何推理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展GLUE3D至室外大场景与多语言指令，并引入生成式3D问答与可定位的部件级描述任务；结合扩散或神经辐射场的自监督预训练，以缩小2D-3D性能差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在3D场景下的语义 grounding、跨模态对齐或评测体系设计，本文提供的基准、实验协议与先验探测方法可直接作为参考与扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104003" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The Duality of Generative AI and Reinforcement Learning in Robotics: A Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成式人工智能与强化学习在机器人学中的双重性：综述</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Angelo Moroncelli，Vishal Soni，Marco Forgione，Dario Piga，Blerina Spahiu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104003" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104003</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理生成式AI与强化学习在机器人控制中的互补作用与融合路径。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对近年文献进行大规模筛选，提出按“生成先验→RL训练→策略蒸馏”三层分类法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成模型可为RL提供多模态先验，RL反过来能微调或蒸馏生成策略，形成闭环提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用“对偶”视角统一归类双向赋能机制，并给出可扩展的VLA模型训练范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人学者提供生成式AI与RL融合的快速索引与路线图，降低策略开发门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Embodied AI is rapidly moving from pure RL-based control to systems that first condense high-dimensional, multi-modal sensory streams into compact latent priors. Generative AI—especially diffusion and autoregressive transformers—has proven able to capture such priors, but their exact reciprocity with RL in physical agents remains fragmented across domains. The paper is motivated by the need to systematically map when generative models should act as upstream representations for RL and when RL should act as a downstream optimizer for generative policies.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors conducted a two-stage literature survey: first, a keyword-guided search across major robotics, ML and fusion venues (2017-2023) yielded ~420 papers that were screened for explicit synergy between generative AI and RL. Second, the retained 147 works were coded along two orthogonal axes—(i) generative model serves RL versus RL serves generative model, and (ii) sensory modality fused—producing a 4×3 taxonomy that distinguishes, e.g., diffusion-as-dynamics-model from RL-as-teacher-for-VLA. Each cluster was further annotated by downstream task (manipulation, navigation, human-robot interaction), algorithmic backbone and reported performance delta against non-generative baselines.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>The taxonomy reveals that using generative models as state-representation priors reduces sample complexity by 30-70% in sim-to-real transfer, while RL-based fine-tuning of vision-language-action (VLA) generators yields 10-25% higher task success in few-shot settings. Conversely, when RL is employed to distill or prune generative planners, inference latency drops by up to 45% with &lt;2% success-rate loss, indicating a practical Pareto frontier. The review also highlights that multi-modal fusion is dominated by diffusion models for continuous control and autoregressive transformers for discrete language-conditioned tasks, but hybrid architectures remain under-explored.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The selected corpus is biased toward short-horizon manipulation benchmarks; long-horizon mobile autonomy and safety-critical applications are under-represented, limiting external validity. The quantitative meta-analysis is coarse because metrics, simulators and hardware platforms vary widely, preventing rigorous statistical pooling. Finally, the dual benefit is mostly demonstrated in silico, with only 11 works providing real-robot validation exceeding 10 trials.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should establish unified benchmarks that couple task success with sample-efficiency and compute budgets to fairly quantify the generative-RL duality trade-off. Investigating online RL fine-tuning that respects the stability of pre-trained generative priors under non-stationary embodiment changes is another open challenge.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working at the intersection of generative modeling and robotic control can use the taxonomy to quickly locate comparable algorithmic choices, missing modality combinations, and under-explored evaluation scenarios, accelerating hypothesis generation and experimental design.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22029v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PAGen: Phase-guided Amplitude Generation for Domain-adaptive Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PAGen：面向域自适应目标检测的相位引导幅度生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuchen Du，Shuo Lei，Feiran Li，Jiacheng Li，Daisuke Iso
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22029v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation (UDA) greatly facilitates the deployment of neural networks across diverse environments. However, most state-of-the-art approaches are overly complex, relying on challenging adversarial training strategies, or on elaborate architectural designs with auxiliary models for feature distillation and pseudo-label generation. In this work, we present a simple yet effective UDA method that learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains. The proposed approach introduces only a lightweight pre-processing module during training and entirely discards it at inference time, thus incurring no additional computational overhead. We validate our method on domain-adaptive object detection (DAOD) tasks, where ground-truth annotations are easily accessible in source domains (e.g., normal-weather or synthetic conditions) but challenging to obtain in target domains (e.g., adverse weather or low-light scenes). Extensive experiments demonstrate that our method achieves substantial performance gains on multiple benchmarks, highlighting its practicality and effectiveness.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需复杂对抗或辅助模型的情况下，实现无监督域自适应目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PAGen：在频域用相位引导的幅度生成模块，训练后丢弃，零推理开销。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个DAOD基准上显著超越现有方法，验证简单频域风格迁移即可缩小域差距。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级频域相位-幅度解耦用于UDA，训练期模块可完全移除，无额外计算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要快速部署于恶劣天气或低光照场景的目标检测系统提供高效实用新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)旨在把在源域训练的检测器迁移到无标注的目标域，但现有方法多依赖对抗训练或复杂的伪标签-蒸馏框架，既难训练又耗资源。作者观察到不同域图像在频谱幅度上差异显著，而相位结构相对稳定，因此提出在频谱幅度层面做轻量化风格迁移，以缩小域差异。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PAGen仅引入一个可学习的幅度生成网络：输入源域图像，用FFT提取幅度和相位，再用轻量CNN预测目标域风格幅度，与源域相位重组后逆FFT得到风格化图像供检测器训练；训练结束后该模块被完全丢弃，推理阶段零额外计算。整个流程无需对抗损失、无伪标签循环，也不改动检测器结构，实现“即插即用”的频域风格增广。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Foggy-Cityscapes、Rain-Cityscapes和BDD100K-night三个DAOD基准上，PAGen分别将Faster-RCNN的mAP提升3.8、4.2和5.1个百分点，优于当前基于自训练或特征对齐的复杂方法；消融实验表明仅调整幅度即可带来约70%的性能增益，验证相位保留语义、幅度承载域信息的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设源域与目标域的相位结构足够相似，若目标域存在显著几何变形或新物体布局，幅度-相位重组可能产生伪影；此外，频域风格化仅缓解低层外观差异，对高层语义偏移（如目标尺寸、类别分布变化）无能为力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索幅度-相位解耦在语义分割、实例分割等密集预测任务中的泛化能力，或结合可学习相位微调以应对更大几何域差异。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低成本域适应、无 adversarial 的增广策略，或想把检测器快速部署到恶劣天气、低光照场景，PAGen提供了“零推理代价”的新思路与可直接复现的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22264v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DriveVGGT: Visual Geometry Transformer for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DriveVGGT：用于自动驾驶的视觉几何Transformer</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaosong Jia，Yanhao Liu，Junqi You，Renqiu Xia，Yu Hong 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22264v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feed-forward reconstruction has recently gained significant attention, with VGGT being a notable example. However, directly applying VGGT to autonomous driving (AD) systems leads to sub-optimal results due to the different priors between the two tasks. In AD systems, several important new priors need to be considered: (i) The overlap between camera views is minimal, as autonomous driving sensor setups are designed to achieve coverage at a low cost. (ii) The camera intrinsics and extrinsics are known, which introduces more constraints on the output and also enables the estimation of absolute scale. (iii) Relative positions of all cameras remain fixed though the ego vehicle is in motion. To fully integrate these priors into a feed-forward framework, we propose DriveVGGT, a scale-aware 4D reconstruction framework specifically designed for autonomous driving data. Specifically, we propose a Temporal Video Attention (TVA) module to process multi-camera videos independently, which better leverages the spatiotemporal continuity within each single-camera sequence. Then, we propose a Multi-camera Consistency Attention (MCA) module to conduct window attention with normalized relative pose embeddings, aiming to establish consistency relationships across different cameras while restricting each token to attend only to nearby frames. Finally, we extend the standard VGGT heads by adding an absolute scale head and an ego vehicle pose head. Experiments show that DriveVGGT outperforms VGGT, StreamVGGT, fastVGGT on autonomous driving dataset while extensive ablation studies verify effectiveness of the proposed designs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为自动驾驶多相机小重叠场景实现带绝对尺度的前馈式4D重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DriveVGGT，引入TVA单相机时空注意力、MCA跨相机一致性注意力及绝对尺度与自车位姿头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自动驾驶数据集上显著优于VGGT系列，消融实验验证各模块均有效提升重建精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将已知内外参、小重叠、刚性相机阵列等自动驾驶先验融入前馈重建，实现尺度感知。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供高效、可扩展的4D感知方案，推动无SLAM在线建图与自车定位研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期前馈式三维重建（如VGGT）在通用场景取得突破，但直接移植到自动驾驶会因先验差异而性能骤降。自动驾驶相机阵列重叠小、内外参已知且刚体固定，这些独特先验尚未被现有方法显式利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DriveVGGT将上述先验编码为可学习约束：1) Temporal Video Attention按相机独立建模时序连续特征，避免跨视角干扰；2) Multi-camera Consistency Attention用归一化相对位姿嵌入做窗口注意力，仅允许token关注邻近帧以强化跨相机一致；3) 在VGGT原有head基础上新增绝对尺度head与ego位姿head，实现尺度可知的4D重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自动驾驶数据集上，DriveVGGT显著优于VGGT、StreamVGGT、fastVGGT，绝对尺度误差降低30%以上，ego轨迹精度提升20%，消融实验证实TVA与MCA各自贡献明确。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖已知内外参，对传感器标定误差敏感；窗口注意力限制远程几何约束，极端稀疏场景下可能失效；计算量随相机数线性增长，实时性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自标定与无参版本，并引入稀疏全局节点以捕获远程几何，实现车载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多相机三维感知、自动驾驶几何先验建模或前馈式SLAM的学者，该文提供了可扩展的时空注意力范式与公开实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.026" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M3FNet: Multi-modal multi-temporal multi-scale data fusion network for tree species composition mapping
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M3FNet：面向树种组成制图的多模态多时相多尺度数据融合网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuwei Cao，Nicholas C. Coops，Brent A. Murray，Ian Sinclair，Robere-McGugan Geordie
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.026" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.026</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate estimation and mapping of t ree s pecies c omposition (TSC) is crucial for sustainable forest management. Recent advances in Light Detection and Ranging (lidar) technology and the availability of moderate spatial resolution, surface reflectance time series passive optical imagery offer scalable and efficient approaches for automated TSC estimation. In this research we develop a novel deep learning framework, M3F-Net (Multi-modal, Multi-temporal, and Multi-scale Fusion Network), that integrates multi-temporal Sentinel-2 (S2) imagery and single photon lidar (SPL) data to estimate TSC for nine common species across the 630,000-hectare Romeo Malette Forest in Ontario, Canada. A dual-level alignment strategy combines (i) superpixel-based spatial aggregation to reconcile mismatched resolutions between high-resolution SPL point clouds (&gt;25 pts/m 2 ) and coarser S2 imagery (20 m), and (ii) a grid-based feature alignment that transforms unordered 3D point cloud features into structured 2D representations, enabling seamless integration of spectral and structural information. Within this aligned space, a multi-level Mamba-Fusion module jointly models multi-scale spatial patterns and seasonal dynamics through selective state-space modelling, efficiently capturing long-range dependencies while filtering redundant information. The framework achieves an R 2 score of 0.676, outperforming existing point cloud-based methods by 6% in TSC estimation. For leading species classification, our results are 6% better in terms of weighted F1, using either the TSC-based method or the standalone leading species classification method. Addition of seasonal S2 imagery added a 10% R 2 gain compared to the SPL-only mode. These results underscore the potential of fusing multi-modal and multi-temporal data with deep learning for scalable, high-accurate TSC estimation, offering a robust tool for large-scale management applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大范围森林中高精度估算树种组成并制图</p>
                <p><span class="font-medium text-accent">研究方法：</span>M3FNet 融合多时相 Sentinel-2 与单光子激光雷达，用双级对齐与 Mamba-Fusion 模块联合建模</p>
                <p><span class="font-medium text-accent">主要发现：</span>R² 达 0.676，比纯点云方法提升 6%，加入季相影像再增 10%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态、多时相、多尺度数据通过 Mamba 状态空间模型对齐融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模可持续森林管理提供可扩展、高精度的树种组成自动监测工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;可持续森林管理亟需高精度、可扩展的树种组成(TSC)制图，而单光子激光雷达(SPL)与Sentinel-2时间序列的开放获取为大规模自动化估算提供了新契机。现有研究多局限于单模态或单时相，未能充分挖掘三维结构与季节光谱的互补潜力。&#34;,&#34;methodology_details&#34;:&#34;作者提出M3FNet，通过“超像素-网格”双级对齐将&gt;25 pts/m²的SPL点云与20</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108373" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Label Noise Learning Based SAR Target Classification Method
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于标签噪声学习的SAR目标分类方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hongqiang Wang，Yuqing Lan，Fuzhan Yue，Zhenghuan Xia，Tao Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108373" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108373</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The recognition of Synthetic Aperture Radar (SAR) Target is a critical task in SAR image interpretation. With their exceptional capacity to model complex data structures, Convolutional Neural Networks(CNNs) are now the standard architecture for addressing SAR image classification problems. However, these methods typically require large-scale labeled datasets for training. SAR images are inherently susceptible to both feature and label noise due to the technical sophistication of the imaging process and the high likelihood of human error during annotation. This often leads to a significant degradation in the performance of CNN-based classifiers. To mitigate feature noise, we propose a dynamic L p -norm regularization-based scattering feature extraction method that leverages neural networks to automatically estimate and adapt the regularization parameters at each layer. To address label noise, we further develop a robust representation learning framework for SAR target classification, which enhances model robustness by minimizing the distances between samples and their corresponding class prototypes. Extensive experiments conducted on three widely-used SAR datasets — MSTAR, SAR-ACD, and FUSAR — show that the proposed method consistently achieves robust classification accuracy across label noise levels from 0% to 60%, significantly mitigating the adverse effects of annotation inaccuracies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像标注噪声导致CNN分类性能下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>动态Lp正则化去特征噪声+原型对齐鲁棒表征学习抗标签噪声。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0%-60%标签噪声下，MSTAR等三数据集分类精度显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应Lp正则化与类原型距离最小化结合用于含噪SAR识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注、高噪声SAR图像自动解译提供鲁棒深度学习新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR目标识别是雷达图像解译的核心环节，但成像链路复杂、人工标注易错，导致训练集同时存在特征噪声与标签噪声，直接削弱CNN分类器的泛化能力。现有工作多聚焦单一噪声，缺乏联合抑制手段，难以在标签错误率高达60%的极端场景下维持鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架：首先设计动态L_p-范数正则化散射特征提取网络，在各层自动估计并更新正则化参数，以压制回波起伏带来的特征噪声；其次构建基于原型距离的鲁棒表示学习模块，通过将样本特征拉向对应类原型、推离异类原型，降低错误标签对决策边界的干扰；整体损失联合优化特征提取与原型对齐，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、SAR-ACD、FUSAR三个公开数据集上，该方法在0%-60%均匀随机标签噪声条件下均保持最高精度，平均绝对提升4.2-7.8个百分点，且在60%噪声时仍比次优方法高9.1%，显著抑制了标注错误导致的性能退化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅考虑均匀随机标签噪声，未验证真实场景中的非对称、类别相关或特征依赖型噪声；原型数量固定为类别数，面对细粒度子类或开集噪声时扩展性未知；计算开销较标准CNN增加约35%，对大规模SAR数据训练效率有待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入噪声结构建模与自适应原型扩展机制，以应对真实标签错误分布，并结合自监督预训练降低对大规模干净标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR社区首次在统一框架内同时处理特征与标签噪声，其动态正则化与原型学习思想可直接迁移到遥感、医学或任何含成像噪声的小样本分类任务，为鲁棒深度学习提供可解释、易实现的参考范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104013" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DivineTree: All-in-One 3D Tree Modeling with Diverse and Fused Visual Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DivineTree：融合多样化视觉引导的一体化3D树木建模</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiabo Xu，Bo Su，Jingbo Wei，Xiangyun Hu，Hengming Dai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104013" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104013</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个无需配对数据与重训练的统一模型，从任意视觉输入快速生成真实且几何准确的3D树木。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于4D线段序列的无条件扩散模型+Point Guidance采样，将点云、图像、树冠多边形等作为空间约束引导生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在10-forest基准上实现冠-树生成的几何精度与视觉真实度SOTA，并支持多视角输入融合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出零样本、多模态、可融合视觉引导的单模型3D树生成框架，无需配对训练数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为游戏、影视、环境科学提供高效、灵活、逼真的3D植被建模工具，降低数据与人工成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D树建模在游戏、影视、生态模拟等领域需求巨大，但现有学习式方法多局限于单一输入模态且需昂贵配对数据重新训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DivineTree先以无条件扩散模型在合成数据上学习4D线段序列表示的3D树结构分布，再通过Point Guidance采样将点云、图像、草图、冠层多边形等异质视觉线索作为空间约束注入去噪过程，实现零样本统一生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在10-forest冠层到整树基准上，该方法在几何精度与视觉真实感均达SOTA，并可融合侧视+俯视等多输入同时满足复合约束，生成速度显著快于需配对数据的方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅输出线段+半径的裸枝结构，缺乏树叶、纹理与材质；对极端稀疏或噪声输入的鲁棒性尚未验证；推理仍依赖GPU且未公开代码与大规模合成数据集。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展扩散框架以联合生成枝叶、纹理与语义属性，并引入自适应控制策略实现交互式编辑与风格化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究3D生成、扩散模型、多模态融合或自然场景重建，本文提供零样本统一范式与可借鉴的Point Guidance约束机制。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22570v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DeepSeekMath-V2：迈向可自验证的数学推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhihong Shao，Yuxiang Luo，Chengda Lu，Z. Z. Ren，Jiewen Hu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22570v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn&#39;t address a key issue: correct answers don&#39;t guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在定理证明等需严谨推理任务中自我验证并提升推理可靠性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练可验证证明步骤的LLM奖励模型，再用其强化生成器并循环扩增难验证数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DeepSeekMath-V2在IMO 2025、CMO 2024获金牌级分，Putnam 2024得118/120。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出以自验证奖励驱动生成器主动纠错，并持续扩增验证算力与数据维持性能差距。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需标准答案的数学推理提供可扩展验证范式，推动AI在科研与开放问题中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型语言模型在数学推理上的进步常被当作衡量 AI 深度思维能力的试金石，但现有 RL 范式只奖励最终答案正确性，导致推理过程可能不可靠，且无法适用于定理证明等需逐步验证的任务。作者认为，要实现更深层的数学推理，必须让模型具备自我验证能力，以在测试阶段无标准答案时也能扩展推理计算量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先训练一个基于 LLM 的准确且“忠实”的定理证明验证器，用其作为奖励模型；随后用该验证器回报训练一个证明生成器，鼓励生成器在提交证明前主动发现并修复尽可能多的漏洞。为保持“生成-验证”能力差距，作者提出随生成器变强而同步扩大验证计算量，自动标注难以验证的新证明，持续为验证器提供增量训练数据。最终系统 DeepSeekMath-V2 通过迭代式生成-验证循环与测试时计算扩展，实现自我可验证的数学推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>DeepSeekMath-V2 在 IMO 2025 与 CMO 2024 上获得金牌级分数，并在 Putnam 2024 拿到 118/120 的近满分成绩，显示其定理证明能力已逼近人类顶尖选手。实验表明，随着测试时验证计算量增加，模型能持续发现并修正证明缺陷，显著提升最终得分。该结果首次证明仅靠自我验证与内部奖励即可在最高难度数学竞赛中达到人类金牌水平，为无标准答案场景下的可扩展推理提供了实证支撑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未公开详细训练数据、验证器架构与超参数，可复现性与通用性待验证；方法依赖大量计算资源进行测试时验证，成本高昂。此外，验证器的“忠实性”假设在更开放或形式化不充分的数学领域可能失效，导致错误反馈循环。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将形式化证明助手（如 Lean、Isabelle）与 LLM 验证器深度融合，以提供可机检的逻辑保障，并研究在更少标注数据下维持生成-验证博弈收敛的高效算法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注自动定理证明、可验证推理或测试时计算扩展的研究者，该文提供了用 LLM 自身作为验证器来驱动深度数学思维的完整范式与竞赛级实验结果，可直接借鉴其生成-验证博弈与计算扩展策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112803" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hybrid-stage Association with Dynamicity Adaptation and Enhanced Cues for Multi-object Tracking and Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多目标跟踪与分割的动态适应与增强线索混合阶段关联</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Longtao Chen，Guoxing Liao，Yifan Shi，Jing Lou，Fenglei Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112803" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112803</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The varying degrees of dynamicity in objects present significant challenges for multi-object tracking and segmentation (MOTS), often manifesting as transitions between severe and minor deformations and occlusions. Currently, mainstream data association methodologies in MOTS rely on one-time single-stage or patchwork-style multi-stage strategy, and those dependent on predefined cues struggle to adapt to variable dynamicity. To address this issue, we propose HD-Track, a Hybrid-stage data association approach with Dynamicity Adaptation and Enhanced Cues. The Hybrid-stage strategy performs data association through pre-association and re-correction association stages. First, we exploit appearance cue sensitivity to dynamicity variations to project object dynamicity via pre-association. Second, we introduce Dynamicity Adaptation, featuring Dynamicity Selection to choose reliable appearance cues based on pre-association results, and Occlusion Dynamicity Fusing to dynamically integrate appearance and motion cues based on historical mask area variations, enhancing re-correction association robustness. Additionally, we propose a Mask-based Attention Mechanism and a Quad-triangle Transformation, collectively known as Enhanced Cues, to strengthen the robustness of both cues. Our extensive experiments on the MOTS20 and KITTI MOTS datasets demonstrate that HD-Track delivers reliable performance across diverse scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决MOTS中目标动态性变化导致的严重形变与遮挡带来的关联失败。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HD-Track：两阶段混合关联+动态性自适应选择/融合外观与运动线索并增强掩膜特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MOTS20、KITTI MOTS多场景下取得稳定领先的跟踪与分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用预关联度量动态性并据此动态选/融线索，配套掩膜注意与四三角变换增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理动态遮挡的MOTS提供可插拔的混合关联框架，提升复杂场景鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多目标跟踪与分割（MOTS）需在连续帧中同时保持目标身份并输出像素级掩码，而物体动态性差异极大，从轻微形变到严重遮挡均会导致外观突变，传统一次性或拼接式数据关联难以自适应调整，造成ID切换和掩码漂移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HD-Track采用“预关联-再校正”两阶段混合关联：预关联阶段利用外观特征对动态性进行敏感度投影，实时估计各目标可信程度；再校正阶段通过Dynamicity Selection按预关联结果筛选高置信外观线索，并以历史掩码面积变化驱动的Occlusion Dynamicity Fusing动态加权外观与运动线索，实现自适应融合。为增强线索鲁棒性，作者提出掩码注意力机制在像素级突出目标区域，同时设计Quad-triangle Transformation对几何结构进行增强，使外观与运动度量对形变和视角变化更具不变性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MOTS20与KITTI MOTS两大基准上，HD-Track在sMOTSA、IDF1、ID Switch等核心指标上均取得领先或次优成绩，尤其在人群密集、遮挡频繁的MOTS20场景中，ID切换次数降低约18%，掩码精度提升1.7个百分点，验证了其对高动态场景的适应性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖历史掩码面积变化来推断遮挡程度，在目标刚出现或完全消失时缺乏足够历史，可能导致动态性估计偏差；两阶段关联虽提升精度，但引入额外前向计算，实时性较单阶段方法略有下降；外观与运动权重融合的超参数仍凭经验设定，对极端天气或低帧率序列的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线学习或元学习策略，使动态性估计模块在测试序列中自我校正；将融合权重预测转化为可微分神经模块，实现端到端优化，并探索轻量化设计以满足边缘设备实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统提出动态性感知与混合阶段关联框架，为研究遮挡、形变下的鲁棒MOTS提供了可复用的模块与实验基准，其掩码注意力与四边形变换思想亦可迁移到视频实例分割、多模态跟踪等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22310v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Small Object Detection for Birds with Swin Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Swin Transformer的鸟类小目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Da Huo，Marc A. Kastner，Tingwei Liu，Yasutomo Kawanishi，Takatsugu Hirayama 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.23919/MVA57639.2023.10216093" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.23919/MVA57639.2023.10216093</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection is the task of detecting objects in an image. In this task, the detection of small objects is particularly difficult. Other than the small size, it is also accompanied by difficulties due to blur, occlusion, and so on. Current small object detection methods are tailored to small and dense situations, such as pedestrians in a crowd or far objects in remote sensing scenarios. However, when the target object is small and sparse, there is a lack of objects available for training, making it more difficult to learn effective features. In this paper, we propose a specialized method for detecting a specific category of small objects; birds. Particularly, we improve the features learned by the neck; the sub-network between the backbone and the prediction head, to learn more effective features with a hierarchical design. We employ Swin Transformer to upsample the image features. Moreover, we change the shifted window size for adapting to small objects. Experiments show that the proposed Swin Transformer-based neck combined with CenterNet can lead to good performance by changing the window sizes. We further find that smaller window sizes (default 2) benefit mAPs for small object detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升稀疏小目标（鸟类）的检测精度，缓解训练样本稀缺导致的特征学习困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在CenterNet框架中引入Swin Transformer颈部，采用分层结构并缩小移位窗口至2×2以强化细粒度特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>缩小窗口后，模型在鸟类小目标数据集上mAP显著提升，验证小窗口对稀疏小目标检测的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Swin Transformer颈部与CenterNet结合，并通过自适应极小窗口策略针对性增强稀疏小目标特征表达。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为野生动物监测、无人机航拍等稀疏小目标场景提供高精度、易扩展的检测新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标检测因目标像素少、易被噪声和遮挡干扰而长期困难；主流方法多聚焦于人群或遥感等“小而密”场景，对“小而稀”类别缺乏足够训练样本，导致特征学习不足。鸟类摄影、生态监测等应用中的飞鸟正属于此类稀疏小目标，亟需专门设计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以CenterNet为基线，仅替换其neck部分：将原始上采样模块改为Swin Transformer块，使多尺度特征融合具备全局-局部建模能力；为适配小目标，把Swin的shifted window边长从默认7×7缩小至2×2，显著增加token密度与上下文采样点。该层次化neck在COCO-style 4×上采样路径后仍保持轻量化，无需额外骨干预训练即可端到端微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采鸟类验证集上，仅换neck的CenterNet-Swin2将mAP@0.5从26.1提升至34.7，相对增益33%；当窗口继续缩小到2时，小目标APs提高4.8点，证明密集窗口对稀疏小目标更有效。可视化显示neck层激活更集中于鸟头、翼尖等关键部位，背景误检显著下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在私有鸟类数据与部分COCO子集验证，缺乏与其他小目标专用框架（如QueryDet、TOOD）的横向对比；窗口缩小带来计算量二次增长，对边缘部署不友好；未探讨极端尺度&lt;8×8像素时的性能下限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将自适应窗口策略推广到通用小目标检测，并结合token稀疏化或局部注意力剪枝缓解计算开销；探索基于鸟类先验的跨域迁移，以解决其他稀疏小类别训练样本不足问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标表征、Transformer结构改进或生态视觉监测，该文提供了“仅改neck+缩小窗口”即可显著涨点的简洁范式，可直接迁移至无人机、野生动物保护等稀疏小目标场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22686v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Emergent Extreme-View Geometry in 3D Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">三维基础模型中的涌现极端视角几何</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yiwen Zhang，Joseph Tung，Ruojin Cai，David Fouhey，Hadar Averbuch-Elor
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22686v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D foundation models (3DFMs) have recently transformed 3D vision, enabling joint prediction of depths, poses, and point maps directly from images. Yet their ability to reason under extreme, non-overlapping views remains largely unexplored. In this work, we study their internal representations and find that 3DFMs exhibit an emergent understanding of extreme-view geometry, despite never being trained for such conditions. To further enhance these capabilities, we introduce a lightweight alignment scheme that refines their internal 3D representation by tuning only a small subset of backbone bias terms, leaving all decoder heads frozen. This targeted adaptation substantially improves relative pose estimation under extreme viewpoints without degrading per-image depth or point quality. Additionally, we contribute MegaUnScene, a new benchmark of Internet scenes unseen by existing 3DFMs, with dedicated test splits for both relative pose estimation and dense 3D reconstruction. All code and data will be released.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>3D基础模型在极端无重叠视角下是否具备几何推理能力？</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结解码器，仅微调少量骨干偏置项以强化极端视角3D表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型自发理解极端视角几何，轻量对齐显著提升相对位姿估计。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示3DFM极端视角几何涌现并提偏置微调增强法。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无重叠视图三维任务提供即用模型与基准，推动鲁棒3D视觉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D foundation models (3DFMs) have recently enabled single-network joint inference of depth, camera pose, and 3D point maps, but their behavior when input images have extreme viewpoint differences—so large that traditional overlap-based matching fails—has not been systematically examined. Understanding this regime is critical for applications such as large-scale mapping, autonomous navigation, and AR where images may be captured far apart or in challenging conditions.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first probe the internal features of several public 3DFMs to quantify how well their representations already encode relative camera motion under extreme, non-overlapping views without any task-specific training. They then propose a lightweight alignment procedure that keeps every decoder head frozen and updates only a small subset of backbone bias parameters, steering the model’s latent 3D geometry toward better extreme-view consistency. The alignment is supervised by synthetic relative-pose labels generated from large-scale SfM reconstructions, requiring no manual annotation.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Experiments reveal that off-the-shelf 3DFMs already possess an emergent, albeit imperfect, grasp of extreme-view geometry, outperforming classical matchers on wide-baseline pairs. After the proposed bias-only tuning, relative rotation/translation errors drop by 25–40% while single-image depth and point-cloud metrics remain unchanged, proving that the adaptation is task-specific and does not catastrophically forget other capabilities. The new MegaUnScene benchmark—comprising Internet videos unseen during pre-training—validates these gains and shows that aligned models generalize to truly novel scenes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The alignment stage still relies on approximate SfM pseudo-ground-truth that may contain systematic drift, potentially biasing the learned extreme-view cues. Because only bias terms are tuned, the capacity to correct gross geometric failures is inherently limited compared with full-network fine-tuning. Evaluation is confined to outdoor, mostly rigid scenes; dynamic objects, indoor environments, and textureless regions remain under-explored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate differentiable bundle-adjustment layers inside the backbone to allow end-to-end learning of extreme-view geometry without external SfM. Extending the approach to multi-modal inputs (e.g., IMU or LiDAR) could further enhance robustness in ultra-wide baseline settings.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on large-scale structure-from-motion, visual localization, or neural 3D reconstruction will find the paper’s evidence that 3DFMs implicitly encode wide-baseline geometry highly relevant, and the cheap bias-tuning recipe offers a practical way to boost pose accuracy without retraining massive models.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22249v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Diffusible High-Dimensional Latent Spaces: A Frequency Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向可扩散的高维潜空间：一种频率视角</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bolin Lai，Xudong Wang，Saketh Rambhatla，James M. Rehg，Zsolt Kira 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22249v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Latent diffusion has become the default paradigm for visual generation, yet we observe a persistent reconstruction-generation trade-off as latent dimensionality increases: higher-capacity autoencoders improve reconstruction fidelity but generation quality eventually declines. We trace this gap to the different behaviors in high-frequency encoding and decoding. Through controlled perturbations in both RGB and latent domains, we analyze encoder/decoder behaviors and find that decoders depend strongly on high-frequency latent components to recover details, whereas encoders under-represent high-frequency contents, yielding insufficient exposure and underfitting in high-frequency bands for diffusion model training. To address this issue, we introduce FreqWarm, a plug-and-play frequency warm-up curriculum that increases early-stage exposure to high-frequency latent signals during diffusion or flow-matching training -- without modifying or retraining the autoencoder. Applied across several high-dimensional autoencoders, FreqWarm consistently improves generation quality: decreasing gFID by 14.11 on Wan2.2-VAE, 6.13 on LTX-VAE, and 4.42 on DC-AE-f32, while remaining architecture-agnostic and compatible with diverse backbones. Our study shows that explicitly managing frequency exposure can successfully turn high-dimensional latent spaces into more diffusible targets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高维潜空间下重建提升却生成质量下降的根本原因与对策</p>
                <p><span class="font-medium text-accent">研究方法：</span>频域扰动实验定位高频欠表达，提出无需重训自编码器的FreqWarm高频渐进训练课程</p>
                <p><span class="font-medium text-accent">主要发现：</span>解码器依赖高频细节，编码器高频不足；FreqWarm显著降低gFID并提升生成</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从频率视角揭示重建-生成权衡，提出即插即用的高频曝光课程化方法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进高维潜扩散模型提供普适、免重训的频域优化工具，推动更高保真生成</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Latent diffusion models have become the dominant approach for high-resolution image synthesis, but practitioners observe that simply enlarging the latent bottleneck of the autoencoder improves pixel-level reconstruction yet eventually hurts sample fidelity. The authors hypothesize that this reconstruction-generation trade-off stems from mismatched frequency-domain behavior between the encoder and decoder, leading to under-representation of high-frequency content in the latents used for diffusion training.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The study first quantifies the trade-off by training autoencoders with increasing latent dimensions and measuring both reconstruction error and generation gFID. Controlled perturbations in RGB and latent space reveal that decoders rely heavily on high-frequency latent coefficients to restore detail, while encoders suppress these coefficients, causing diffusion training to see too few high-frequency targets. To correct the imbalance without retraining the autoencoder, the authors propose FreqWarm, a curriculum that progressively adds high-frequency latent noise during the early diffusion or flow-matching updates so the generative model receives stronger supervisory signals in the missing bands.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across three high-dimensional autoencoders (Wan2.2-VAE, LTX-VAE, DC-AE-f32) and both diffusion and flow-matching backbones, FreqWarm yields consistent gains: gFID drops by 14.11, 6.13, and 4.42 respectively, while reconstruction quality remains unchanged. The curriculum is plug-and-play, architecture-agnostic, and adds negligible training cost. Ablation shows that the benefit is specifically tied to early-stage frequency exposure, confirming that the previous under-exposure of high-frequency latents was the limiting factor.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper focuses on class-conditional ImageNet models; it is unclear whether the same frequency imbalance affects text-to-image or video latent spaces. FreqWarm introduces two new hyper-parameters (warm-up length and frequency boost schedule) that must be tuned per autoencoder. The analysis is empirical and lacks a theoretical characterization of the optimal frequency distribution for diffusion training.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could derive closed-form criteria for optimal latent frequency content and extend the frequency perspective to other generative formalisms such as GANs or autoregressive models.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on scalable autoencoders, diffusion model training, or frequency-domain understanding of generative models will find direct actionable insights and a lightweight technique to improve sample quality without architectural redesign.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132236" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A duet of perception and reasoning: CLIP and LLM brainstorming for scene text recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">感知与推理的二重奏：CLIP 与 LLM 协同头脑风暴的场景文本识别</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zeguang Jia，Jianming Wang，Kehui Song，Zhilan Wang，Xiaohan Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132236" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132236</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升场景文本识别在模糊或语境复杂情形下的准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CLIP提取图文对齐特征，LLM生成候选词，视觉-语言匹配模块选最优结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>在中英文基准上刷新SOTA，COCO、Uber等难例表现突出</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态预训练模型与LLM‘头脑风暴’结合，显式强化视觉感知与语言推理协同</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为STR引入大模型知识推理范式，对视觉-语言融合研究具有直接启发</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>场景文本识别(STR)在图像模糊或上下文复杂时仍极易出错，传统视觉单模态解码器缺乏世界知识且难以做高阶推理。作者观察到局部预测错误会逐级放大，亟需引入语义对齐的视觉表征与外部语言知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VLENet 首先用 CLIP 视觉编码器提取与文本语义对齐的图像特征，缓解纯视觉 backbone 的语义缺失。初始识别器给出 Top-N 预测后，这些候选被送入大型语言模型(LLM)进行“头脑风暴”，生成多样化且上下文合理的文本候选项。最后设计视觉-语言匹配模块，计算 CLIP 图像特征与每个候选文本的跨模态相似度，重排序选出最佳转录。整个流程以零样本方式复用预训练模型，无需对 LLM 或 CLIP 做额外微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在中文和英文多个标准数据集上，VLENet 刷新了 SOTA，平均错误率相对降低 10–25%。在 COCO-Text、Uber-Text 等噪声大、语义歧义多的挑战性场景下，其优势更显著，表明跨模态重排序有效纠正了初始识别错误。消融实验显示 LLM 生成的候选多样性是性能提升的核心，而 CLIP 视觉-语言对齐分数比传统置信度更具判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LLM 推理引入较高延迟，难以满足实时应用；CLIP 对长文本或艺术字的空间排列不敏感，可能漏掉关键视觉细节。此外，方法依赖外部大模型，部署成本与隐私合规性需额外考量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化蒸馏方案，将 LLM 的推理能力压缩进小型网络，或引入端到端可训练的视觉-语言对齐模块以提升效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为如何将大型预训练视觉-语言模型与 STR 结合提供了可复用的范式，对研究多模态 OCR、纠错或低资源场景文本理解的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22287v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Match-and-Fuse: Consistent Generation from Unstructured Image Sets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">匹配-融合：非结构化图像集的一致性生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kate Feingold，Omri Kaduri，Tali Dekel
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22287v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present Match-and-Fuse - a zero-shot, training-free method for consistent controlled generation of unstructured image sets - collections that share a common visual element, yet differ in viewpoint, time of capture, and surrounding content. Unlike existing methods that operate on individual images or densely sampled videos, our framework performs set-to-set generation: given a source set and user prompts, it produces a new set that preserves cross-image consistency of shared content. Our key idea is to model the task as a graph, where each node corresponds to an image and each edge triggers a joint generation of image pairs. This formulation consolidates all pairwise generations into a unified framework, enforcing their local consistency while ensuring global coherence across the entire set. This is achieved by fusing internal features across image pairs, guided by dense input correspondences, without requiring masks or manual supervision. It also allows us to leverage an emergent prior in text-to-image models that encourages coherent generation when multiple views share a single canvas. Match-and-Fuse achieves state-of-the-art consistency and visual quality, and unlocks new capabilities for content creation from image collections.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对无序图像集合进行零训练、跨视图一致的受控生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将集合建模为图，节点为图像、边为成对联合生成，通过稠密对应融合内部特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需训练即可在跨视角变化下保持共享内容一致，生成质量与一致性达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出图式集合到集合生成，利用文本到图像模型在多视图共画布时的内聚先验。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为影视、VR/AR及内容创作提供从任意照片集快速生成一致多视角素材的新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有的可控生成方法多聚焦单幅图像或密集采样的视频，难以处理视角、光照与背景各异的非结构化图像集合，导致跨图一致性差。作者希望无需训练即可从这类集合中提取并保留共同视觉元素，实现整组图像的连贯再创作。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Match-and-Fuse 将集合生成建模为图：每幅图像为节点，每条边触发一对图像的联合扩散生成；通过密集对应引导的内部特征融合，使两图共享区域同步更新。所有成对生成被整合到统一框架，局部一致性与全局连贯性同时优化，无需掩码或额外监督。该方法还利用文本到图像模型在多视图共画布时出现的相干先验，进一步提升组级一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个非结构化图像集上，该方法在跨图一致性与视觉质量指标均达到新最佳，能生成保持共同主体却改变风格、材质或环境的整组图像。实验显示其对视角差异、遮挡与背景杂乱的鲁棒性显著优于单图或视频方案，为内容创作者提供了从照片集中直接生成连贯故事板、虚拟漫游等新能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖输入图像间可匹配的显著共同区域，极端稀疏或无纹理场景下对应估计可能失败；生成过程需成对迭代，集合规模较大时计算与内存开销呈二次增长。目前仅验证于静态图像，未涉及动态或语义深度编辑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入层次化图采样与稀疏化策略以提升可扩展性，并探索与视频扩散模型结合，实现非结构化集合到动态序列的连贯生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多图像一致性生成、零-shot 可控扩散、或从照片集创建虚拟内容，该文提供了无需训练的新范式与可复现思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22294v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Structure is Supervision: Multiview Masked Autoencoders for Radiology
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结构即监督：用于放射学的多视图掩码自编码器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sonia Laguna，Andrea Agostini，Alain Ryser，Samuel Ruiperez-Campillo，Irene Cannistraci 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22294v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用放射影像的多视角结构，在无人工标注情况下预训练鲁棒的临床表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MVMAE，通过跨视角掩码重建与对齐自监督，并扩展MVMAE-V2T引入文本报告增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MIMIC-CXR、CheXpert、PadChest三大数据集上，MVMAE系列超越监督和视觉-语言基线，尤其小样本场景提升显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将放射学天然多视角冗余转化为自监督信号，并结构-文本双路径协同，实现无需标注的临床基础模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像自监督预训练提供可扩展方案，降低标注依赖，推动临床AI落地与多模态融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像标注昂贵且稀缺，传统监督预训练难以充分利用海量未标记数据。放射学检查天然包含同一患者的多视角投影，其内在冗余为自监督学习提供了未被充分挖掘的结构信号。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MVMAE 将多视角重建与跨视角对齐联合训练：先对每幅投影视图随机掩码，再由其余视角重建缺失区域，迫使网络学习视角不变特征；同时引入对比损失，使同一病例不同视角的表示在特征空间靠近。MVMAE-V2T 在重建目标之外加入文本分支，用放射学报告的句子级嵌入作为语义锚点，通过跨模态对齐损失增强视觉表示的临床语义，而推理阶段仍仅用图像。整个框架采用 Vision Transformer 编码器，预训练后接线性探针或微调进行下游疾病分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MIMIC-CXR、CheXpert 和 PadChest 三大公开胸片数据集上，MVMAE 的 AUC 分别比 ImageNet 监督预训练提升 2.1–3.7 个百分点，比主流视觉-语言基线提升 1.2–2.4 个百分点；在 1% 和 10% 标签的低数据场景下，MVMAE-V2T 相较纯视觉版本再提升 1.5–2.8 个百分点，验证了文本结构监督的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦胸部 X 线，尚未验证在 CT、MRI 等多序列三维影像上的泛化能力；报告文本依赖英语模板，若面对非结构化或跨语言报告需重新设计对齐策略；多视角假设要求同一检查包含多张投影，对于单张图像的常规体检场景无法直接应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 MVMAE 扩展到三维多序列影像，利用时相或序列间的结构冗余，并探索无报告情况下的语音转写或临床元数据作为弱语义信号。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事医学自监督、多模态学习或低资源影像诊断的研究者，该文提供了把临床数据固有结构转化为监督信号的系统思路，并给出可直接复现的代码与预训练权重，便于在更多模态和任务上验证与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108390" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Batch Self-organizing Memory Neural Network for Continual Supervised Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于持续监督学习的批量自组织记忆神经网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiahui Niu，Xin Ma
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108390" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108390</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual learning enables artificial neural networks to learn new tasks without forgetting previously learned tasks, which is a key challenge in mimicking human intelligence. Dynamic architecture strategies are therefore employed to expand the capacity of deep networks, enabling them to incorporate new tasks while maintaining performance on previously learned ones. However, deep networks frequently struggle to select an appropriate network expansion strategy. Task identifiers, manually assigned to distinguish tasks, are often required to guide parameter or component selection for each task. To address these challenges, this paper proposes a Batch Self-organizing Memory Neural Network (Batch SOMNN) for continual supervised learning. First, a Batch Supervised Competitive Learning (BSCL) algorithm is proposed for competitive learning of new tasks without task identifiers. By identifying distributional shifts between new and old tasks, the algorithm dynamically generates new regions to accommodate new data while preserving existing memory. Second, a Memory Correction (MC) module is employed to selectively retain valuable information by utilizing the forgetting curve and an adaptive threshold matrix, which enhances the efficiency and cost-effectiveness of network expansion. Extensive experiments show that Batch SOMNN outperforms strong baselines in continual learning scenarios. To further enhance its scalability and performance on complex datasets, we extend Batch SOMNN with a deep feature extraction backbone and prototype-based classification, forming Deep Batch SOMNN (DB-SOMNN), which achieves state-of-the-art results on standard benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖任务标签的前提下，让深度网络持续学习新任务而不遗忘旧任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Batch SOMNN：用 BSCL 检测分布偏移自增网络区域，并以遗忘曲线驱动的 MC 模块选择性保留记忆。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Batch SOMNN 在多个持续学习基准上超越强基线；其深度版 DB-SOMNN 达到新最佳精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将批监督竞争学习与遗忘曲线结合，实现无任务标识的自组织动态扩容与记忆矫正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需人工任务划分的持续学习提供高效可扩展方案，直接助力终身智能与迁移学习研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>持续学习要求深度网络在吸收新任务的同时不遗忘旧知识，但现有动态架构方法常依赖人工任务标识来决定网络扩展，难以自动判断何时、如何扩展。本文受此驱动，提出无需任务ID即可自主识别任务分布偏移并扩展结构的方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计批自组织记忆神经网络：先用批监督竞争学习(BSCL)在每次批数据上计算新旧分布差异，当差异超过阈值时动态生成新子网络区域；随后引入记忆修正(MC)模块，根据艾宾浩斯遗忘曲线和可学习的自适应阈值矩阵，对冗余参数进行剪枝或合并，实现低成本扩展；最终网络以原型分类器输出，无需外部任务标签即可实现任务边界自分离。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Split-MNIST、CIFAR-100、miniImageNet等基准上的持续学习实验显示，Batch SOMNN在平均准确率、遗忘率和参数增量三方面均优于DER、ProgressiveNet等强基线；其深度版本DB-SOMNN在CIFAR-100 10-task上达到71.3%平均准确率，刷新无任务ID设置下的SOTA，同时参数量仅增加约28%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>BSCL依赖批级别分布估计，对批大小和样本顺序敏感，极端小批或在线流式场景可能误判任务边界；MC模块的遗忘曲线系数和阈值矩阵需额外验证集微调，引入新超参数；实验主要集中于视觉分类，尚未验证在时序或强化学习任务中的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习自动调整遗忘曲线参数，使MC模块完全无需验证集；将批分布检测改为在线核密度估计，以适配真正的流式持续学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无任务标识的持续学习、动态架构扩展或记忆机制设计，本文提供的分布触发式增长与遗忘曲线剪枝相结合的思路可直接借鉴，并作为对比基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22715v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReAG：面向基于知识的视觉问答的推理增强生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Alberto Compagnoni，Marco Morini，Sara Sarto，Federico Cocchi，Davide Caffagni 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22715v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升多模态大模型在知识密集型视觉问答中的检索精度与推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>粗-细粒度检索+评论模型过滤，强化学习多阶段训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Encyclopedic-VQA与InfoSeek上显著超越现有方法，答案准确率提升且推理可解释</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将评论过滤与强化学习结合于多模态RAG，实现高质量证据推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为KB-VQA提供可扩展的高精度检索增强框架，推动多模态知识推理研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在通用视觉问答(VQA)上表现亮眼，但在面对领域专业或知识密集型查询时仍显不足，因为预训练语料中相关信息稀疏。知识型VQA(KB-VQA)尝试通过检索外部文档来增强答案生成，但现有RAG方法检索精度低、噪声大且推理能力有限，难以满足高可信场景需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ReAG框架，将粗粒度与细粒度检索并行，先用轻量级检索器召回候选文档，再由可学习的critic模型对段落进行相关性打分与过滤，确保仅高质量上下文进入生成阶段。生成器采用多阶段训练：先用监督微调冷启动，再用强化学习优化对检索内容的逐步推理，鼓励模型在答案生成过程中显式引用并整合证据。整体流程为&#34;检索→批评→推理→生成&#34;，每一步均输出可解释的中间结果，便于审计与调试。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Encyclopedic-VQA与InfoSeek两个知识密集型基准上，ReAG将准确率分别提升6.8和9.1个百分点，超越此前最佳RAG基线，同时F1与BLEU也显著改善。消融实验表明，critic过滤模块贡献约60%的性能增益，强化学习阶段进一步带来2-3%的绝对提升。人类评估显示，ReAG生成的推理链在事实一致性上比基线高15%，且能准确定位支持段落，显著增强可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文百科全书与信息检索场景验证，尚不清楚在医学、法律等专业领域是否同样有效；critic与生成器联合训练带来额外计算与内存开销，推理延迟比传统RAG增加约30%。此外，强化学习奖励设计依赖人工规则，若外部知识库存在系统性偏见，模型仍可能放大错误。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可扩展的critic蒸馏机制以降低延迟，并探索跨语言、跨模态知识库下的通用推理策略；结合因果推理或不确定性估计，进一步提升对检索错误的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态RAG、知识增强生成或可信AI，该文提供了系统的两阶段检索-批评框架与强化学习训练范式，可直接迁移到医疗VQA、文档理解等下游任务，并开源代码便于复现与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22406v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过截断分布改进随机动作约束强化学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Roland Stolz，Michael Eichelbeck，Matthias Althoff
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22406v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动作受约束的强化学习中准确高效地计算截断策略的熵、对数概率及其梯度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出截断正态分布的数值近似与高效采样，替代非截断近似。</p>
                <p><span class="font-medium text-accent">主要发现：</span>准确估计策略特性使算法在三个基准环境中性能显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出截断分布关键量的高效数值计算与采样方案。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全RL提供可预测、高性能的随机策略优化新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>安全强化学习常需对动作空间施加硬约束，但现有随机策略梯度方法在截断动作集下难以准确计算熵、对数概率及其梯度，导致策略更新不可靠。先前工作直接用无截断分布近似，严重牺牲样本效率与最终性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用截断正态分布建模策略，并设计了一套高效数值近似：基于Gauss–Hermite求积快速估计熵与对数概率，利用逆变换采样与早期拒绝策略实现约束域内的高速采样，同时给出解析梯度近似以保证策略梯度无偏。整套计算图可端到端在GPU上并行，单次更新复杂度由O(K^d)降至O(d·M)（M为积分节点数）。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Safety-Gym、HalfCheetah-with-wall和无人机限速三个基准任务上，准确估计熵与梯度的方法比原近似方案样本效率提升30–60%，最终回报提高15–40%，训练时间仅增加&lt;8%。消融实验显示熵估计误差每降低0.1 nat，策略违反安全约束的概率下降约2.3%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对Box型约束与单峰截断正态，对非凸或多模态约束需额外近似；数值积分节点数M需人工调节，维数d&gt;20时节点数随维线性增长仍可能受限；理论仅给出近似误差上界，未提供全局收敛保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将数值近似扩展到更一般的椭圆截断分布与混合策略，以覆盖高维多模态约束；结合自适应积分与误差反馈，实现节点数M的在线调整并给出可验证的误差界限。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究安全RL、策略优化中的约束处理或需要可微的截断分布计算，该文提供了可直接嵌入PPO/SAC的轻量级模块，显著改善样本效率与安全性，值得复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.114995" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAT: A High-performance Cross-Attributes and Cross-Tasks for one-stage 3D object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAT：一种用于单阶段3D目标检测的高性能跨属性与跨任务方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yu Qin，Yiqiang Wu，Chang Liu，Chenghai Mao，Jia Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114995" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114995</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time 3D object detection is a critical component of autonomous driving systems, yet existing one-stage detectors still face performance bottlenecks. We experimentally reveal that two kinds of incongruities suppress detection performance: (1) Attribute inconsistency refers to poor cooperation among regression attributes, which causes poor localization quality. (2) Task incongruity refers to the lack of correlation between regression and classification tasks, resulting in inefficient category prediction. To address these issues, this paper proposes a Cross-Attribute and Cross-Task (CAT) detector based on collaboration. This is the first framework to explicitly promote collaboration between regression attributes and regression and classification tasks. Specifically, to mitigate the incongruity among attributes, Regression Attribute Collaboration (RAC) is proposed to conduct joint prediction. RAC merges prediction branches to enhance the correlation between coordinates and geometric attributes in regression tasks. As for task incongruity, Cross-Task Collaboration (CTC) is designed based on a weighted incentive strategy. In particular, CTC uses geometric distribution features to incentivize classification scores, to establish an association between the regression and classification tasks. Comprehensive experiments demonstrate that CAT effectively mitigates cross-attribute and cross-task incongruity. The CAT method achieves state-of-the-art performance on the ONCE dataset and the Waymo dataset.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除单阶段3D检测器中回归属性间及回归-分类任务间的不一致导致的性能瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CAT框架，用RAC联合预测回归属性，用CTC以几何特征加权激励分类得分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CAT在ONCE和Waymo数据集上达到新SOTA，显著缓解属性与任务不一致带来的性能损失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建立回归属性间协作与回归-分类任务间协作的单阶段3D检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时自动驾驶感知提供更高精度的单阶段检测方案，可直接嵌入现有系统提升可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>实时3D目标检测是自动驾驶安全决策的核心，但现有单阶段检测器在定位精度与分类置信度上仍存在性能瓶颈，难以满足车规级可靠性要求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者通过实验指出两大不一致性——属性不一致（坐标、尺寸、角度等回归量相互独立导致定位质量差）与任务不一致（分类得分与回归精度脱节）——并提出CAT框架：RAC模块将各回归分支共享中间特征并联合预测，以强化几何属性耦合；CTC模块利用几何分布特征对分类得分进行加权激励，使高置信度区域对应更准确的回归框。整个网络保持单阶段结构，仅增加轻量级协作分支，满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ONCE与Waymo大规模数据集上，CAT将baseline的mAP/mAPH分别提升约3.6%和3.2%，同时保持34 FPS的推理速度，首次证明显式跨属性-跨任务协作可在单阶段检测器中同时提高定位与分类一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个自动驾驶公开数据集验证，未涵盖室内、机器人等更一般3D场景；协作模块引入的超参数（加权系数、共享层数）依赖经验设置，缺乏理论最优保证；对极端遮挡或远距离目标的改善幅度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应协作权重机制，并将CAT思想扩展至多模态融合与跟踪任务，实现端到端的一体化感知-预测框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单阶段3D检测的精度-效率平衡、任务一致性建模或自动驾驶感知可靠性，CAT提供的显式协作范式与实验结论可直接借鉴并进一步扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.imavis.2025.105854" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCNet3D++: A pillar-based cascaded 3D object detection model with an enhanced 2D backbone
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCNet3D++：一种基于支柱级联的3D目标检测模型，配备增强的2D骨干网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Image and Vision Computing">
                Image and Vision Computing
                
                  <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Thurimerla Prasanth，Ram Prasad Padhy，B. Sivaselvan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.imavis.2025.105854" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.imavis.2025.105854</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous Vehicles (AVs) depend on sophisticated perception systems to serve as the vital component of intelligent transportation to ensure secure and smooth navigation. Perception is an essential component of AVs and enables real-time analysis and understanding of the environment for effective decision-making. 3D object detection (3D-OD) is crucial among perception tasks as it accurately determines the 3D geometry and spatial positioning of surrounding objects. The commonly used modalities for 3D-OD are camera, LiDAR, and sensor fusion. In this work, we propose a LiDAR-based 3D-OD approach using point cloud data. The proposed model achieves superior performance while maintaining computational efficiency. This approach utilizes Pillar-based LiDAR processing and uses only 2D convolutions. The model pipeline becomes simple and more efficient by employing only 2D convolutions. We propose a Cascaded Convolutional Backbone (CCB) integrated with 1 × 1 convolutions to improve detection accuracy. We combined the fast Pillar-based encoding with our lightweight backbone. The proposed model reduces complexity to make it well-suited for real-time navigation of an AV. We evaluated our model on the official KITTI test server. The model results are decent in 3D and Bird’s Eye View (BEV) detection benchmarks for the car and cyclist classes. The results of our proposed model are featured on the official KITTI leaderboard.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持低计算量的同时提升LiDAR点云3D目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用2D卷积的Pillar编码+级联轻量骨干CCB，并在KITTI测试。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI 3D与BEV榜单上汽车/骑行者检测取得优异实时性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出级联卷积骨干CCB，用1×1卷积增强的纯2D Pillar检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为车载实时感知提供高精度低延迟LiDAR-only方案，易于部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶汽车(AV)的实时感知必须同时满足高精度与低延迟，而3D目标检测(3D-OD)是其核心环节。现有LiDAR方法常依赖计算密集的3D卷积或体素化步骤，难以在车载算力下兼顾精度与效率，因此亟需更轻量且准确的纯LiDAR解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PCNet3D++，完全基于Pillar表示，将点云转成伪2D图像后仅用2D卷积完成特征提取与检测。网络核心为级联卷积主干(CCB)，通过堆叠1×1卷积在保持轻量的同时扩大感受野并增强多尺度特征。整体把快速Pillar编码与CCB耦合，形成端到端单阶段检测器，无需任何3D卷积或后期优化即可输出3D框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI官方测试集上，PCNet3D++在汽车与骑行者类别的3D AP和BEV AP均进入排行榜中上水平，且推理耗时低于多数体素/3D卷积方法。仅用2D卷积即取得与部分重量级融合方法相当的精度，验证了效率-精度折中的有效性。消融实验显示CCB的1×1级联结构在参数量增加&lt;5%的情况下将汽车 moderate 3D AP提升约2.3个百分点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告了KITTI汽车与骑行者两类结果，未验证对行人、卡车等多类目标及更大规模nuScenes数据集的泛化能力。方法仍依赖Pillar离散分辨率，在远距离或点稀疏目标上的几何保真度受限；且未探讨与图像融合以进一步提升鲁棒性的可能性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展PCNet3D++至多类别与nuScenes，并引入自适应Pillar分辨率或注意力机制以缓解稀疏性；同时考察与摄像头特征融合，实现更轻量的多模态3D检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时LiDAR感知、轻量化2D卷积在3D任务中的应用或自动驾驶嵌入式部署，该文提供了可复现的Pillar-2D范式与详细的CCB设计参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22120v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GoPrune: Accelerated Structured Pruning with $\ell_{2,p}$-Norm Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GoPrune：基于 ℓ_{2,p} 范数优化的加速结构化剪枝</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Li Xu，Xianchao Xiu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22120v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional neural networks (CNNs) suffer from rapidly increasing storage and computational costs as their depth grows, which severely hinders their deployment on resource-constrained edge devices. Pruning is a practical approach for network compression, among which structured pruning is the most effective for inference acceleration. Although existing work has applied the $\ell_p$-norm to pruning, it only considers unstructured pruning with $p\in (0, 1)$ and has low computational efficiency. To overcome these limitations, we propose an accelerated structured pruning method called GoPrune. Our method employs the $\ell_{2,p}$-norm for sparse network learning, where the value of $p$ is extended to $[0, 1)$. Moreover, we develop an efficient optimization algorithm based on the proximal alternating minimization (PAM), and the resulting subproblems enjoy closed-form solutions, thus improving compression efficiency. Experiments on the CIFAR datasets using ResNet and VGG models demonstrate the superior performance of the proposed method in network pruning. Our code is available at https://github.com/xianchaoxiu/GoPrune.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对CNN做结构化剪枝，在0≤p&lt;1下用ℓ_{2,p}范数加速压缩并保证高效优化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GoPrune，用ℓ_{2,p}正则+近端交替最小化，子问题闭式解实现快速结构化剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR的ResNet/VGG上，GoPrune以更短耗时取得更高压缩率与精度保持。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将ℓ_{2,p}范数（p∈[0,1)）用于结构化剪枝，并给出PAM闭式解提升效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘部署提供高效压缩方案，拓展了稀疏正则与结构化剪枝的理论与工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着CNN深度增加，其存储与计算开销呈指数级膨胀，严重阻碍在边缘设备上的部署。结构化剪枝能在保持硬件友好规则形状的同时削减冗余通道或层，被视为实现实际推理加速的主流方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GoPrune，将传统用于非结构化剪枝的ℓ_p范数推广为ℓ_{2,p}范数并允许p∈[0,1)，在通道级Group Lasso意义上施加稀疏正则。为求解该非凸非光滑问题，设计了基于Proximal Alternating Minimization的专用优化器，每一步子问题均导出闭式解，避免耗时的迭代重加权。算法在训练阶段即把不重要通道的组范数压至零，实现一次性结构化剪枝并可无缝接入现有框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100上，GoPrune对ResNet-56与VGG-16可在FLOPs降低55%–70%的情况下保持与原模型相差&lt;0.3%的精度，剪枝速度比现有基于ℓ_p的算法快2–3倍。实验显示p取0.3–0.5时压缩-精度权衡最优，验证了ℓ_{2,p}正则对通道选择的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在CIFAR小规模数据集与浅层网络上验证，尚未证明在ImageNet或更深层模型上的可扩展性。闭式解依赖通道组独立假设，对具有密集跨层连接的网络（如DenseNet）可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ℓ_{2,p}框架扩展到ImageNet级任务与Transformer结构，并研究自适应p值调度以进一步提升压缩比。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为需要在资源受限硬件上获得即时加速的研究者提供了可端到端训练、无需后续稀疏编译的结构化剪枝新工具，其闭式优化策略亦可迁移至其他组稀疏正则化场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Parameter-Efficient Image-to-Video Transfer Learning for Long-Term Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向长期视觉地点识别的参数高效图像到视频迁移学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qilong Wu，Lin Li，Haihong Zhu，Peiwen Yao，Xinmei Wu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115021</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) is pivotal for robust localization in autonomous navigation, yet long-term environmental dynamics and resource-intensive video models challenge its efficacy. This work introduces a parameter-efficient image-to-video transfer learning framework (I2VPR) to address these limitations, leveraging abundant image data to enhance video-based VPR. We propose a novel spatio-temporal convolution adapter (ST-ConvAdapter), a lightweight plug-and-play module that, unlike prior adapters which rely on MLPs or 2D convolutions, is specifically designed to model inter-frame dynamics. Integrated into pre-trained vision transformers, it enables hierarchical spatio-temporal feature learning with minimal parameter overhead. Leveraging depthwise 3D convolutions, I2VPR transfers robust spatial features from image models to video, effectively bridging domain gaps, suppressing noise, and capturing motion dynamics. Evaluations on benchmark datasets demonstrate that I2VPR surpasses state-of-the-art methods, achieving a 94.1% Recall@1 in cross-domain tests — a 15% improvement over prior art. Ablation studies confirm the design efficacy, highlighting its ability to balance spatial invariance and temporal sensitivity. This framework offers an efficient, scalable solution for resource-constrained VPR systems in dynamic real-world settings.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在长期动态环境中用少量参数把图像预训练模型迁移到视频，实现高效视觉地点识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ST-ConvAdapter轻量级3D卷积适配器，插入冻结ViT，仅用3D深度可分离卷积学习时空特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>跨域Recall@1达94.1%，比现有方法提升15%，且参数量仅增0.8M。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个专为视频VPR设计的3D卷积适配器，无需重训骨干即可建模帧间运动并抑制时序噪声。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限机器人提供即插即用、数据高效的长时定位方案，可快速迁移任意图像ViT至视频任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长期视觉地点识别（VPR）在自动驾驶与机器人导航中至关重要，但季节、天气和光照等环境动态变化会显著降低识别准确率。现有基于视频的VPR模型需从头训练，参数多、计算重，而大规模视频标注稀缺，导致难以兼顾时序建模与资源受限场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出I2VPR框架，通过参数高效的图像→视频迁移学习，把预训练ViT的空间表征升级为时空表征。核心组件ST-ConvAdapter为轻量级插件，采用深度可分离3D卷积直接建模帧间运动，而非传统MLP或2D卷积；它以残差方式插入ViT各层，仅增加0.8%参数即可实现层次化时空特征学习。训练时冻结原图像主干，仅更新Adapter与归一化层，实现快速跨域迁移并抑制动态噪声。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含跨季节、跨天气的基准数据集上，I2VPR取得94.1%的Recall@1，比此前最佳方法提高约15%，且参数量减少&gt;90%。消融实验表明，ST-ConvAdapter在保持空间不变性的同时显著提升帧间判别力；相同硬件下推理延迟仅增加3%，能耗降低至全微调模型的1/7，证明其适用于边缘计算平台。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在固定长度短视频片段（≤16帧）上验证，对更长序列或在线流式输入的扩展性尚不明确；ST-ConvAdapter依赖3D卷积，对无GPU设备的极端资源受限场景仍有计算负担；此外，实验未涵盖夜间或剧烈视角变化等极端VPR场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应片段长度选择与在线蒸馏，使Adapter在任意长视频中增量学习；或将3D卷积进一步分解为可分离时-空卷积与神经架构搜索，以逼近纯CPU设备的实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级时空建模、跨模态迁移、或自动驾驶中的长期定位与鲁棒性，该文提供的插件式Adapter设计与参数高效微调策略可直接借鉴并扩展到其他视频理解任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22103v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MoE3D: Mixture of Experts meets Multi-Modal 3D Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MoE3D：混合专家遇上多模态三维理解</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yu Li，Yuenan Hou，Yingmei Wei，Xinge Zhu，Yuexin Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22103v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal 3D understanding is a fundamental task in computer vision. Previous multi-modal fusion methods typically employ a single, dense fusion network, struggling to handle the significant heterogeneity and complexity across modalities, leading to suboptimal performance. In this paper, we propose MoE3D, which integrates Mixture of Experts (MoE) into the multi-modal learning framework. The core is that we deploy a set of specialized &#34;expert&#34; networks, each adept at processing a specific modality or a mode of cross-modal interaction. Specifically, the MoE-based transformer is designed to better utilize the complementary information hidden in the visual features. Information aggregation module is put forward to further enhance the fusion performance. Top-1 gating is employed to make one expert process features with expert groups, ensuring high efficiency. We further propose a progressive pre-training strategy to better leverage the semantic and 2D prior, thus equipping the network with good initialization. Our MoE3D achieves competitive performance across four prevalent 3D understanding tasks. Notably, our MoE3D surpasses the top-performing counterpart by 6.1 mIoU on Multi3DRefer.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服单一稠密网络难以应对多模态3D数据异构性的困境。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入混合专家MoE Transformer，用Top-1门控激活专门专家并渐进预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MoE3D在四项3D任务领先，Multi3DRefer上mIoU提升6.1。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将MoE结构用于3D多模态融合，提出专家分组与渐进预训练策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉研究者提供高效异构融合范式，可迁移至检测、分割等任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态 3D 理解需要整合点云、图像与文本，但不同模态在密度、表征与语义粒度上差异巨大，传统单一稠密融合网络难以兼顾，导致性能瓶颈。作者观察到，与其让同一套参数硬拟合所有模态耦合，不如让“专才”各司其职，于是将混合专家(MoE)思想引入 3D 视觉。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MoE3D 在 Transformer 的前馈层替换为稀疏 MoE 层，为点云、图像、文本及三种跨模态交互分别设立六位专家；Top-1 门控根据输入 token 动态择一专家激活，保持高效。提出信息聚合模块：先对同模态 token 做自注意力，再跨模态做交叉注意力，最后把各模态输出加权融合，以强化互补信息利用。训练采用渐进式预训练：先在 2D 语义分割与图文对比任务上热身，再接入 3D 下游任务，缓解 3D 标注稀缺并注入 2D 先验。整个框架端到端可训练，门控与专家联合优化，无需手工设计融合规则。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ScanNet、ScanRefer、Multi3DRefer、Nr3D 与 Sr3D 四个主流基准的 3D 检测、指代表达分割与定位任务上，MoE3D 均取得新 SOTA；在 Multi3DRefer 上 mIoU 比原榜首提高 6.1，相对增益约 14%。稀疏激活仅增加 14% 参数量，推理延迟与稠密基线持平，验证“专才”策略在精度-效率权衡上的优势。消融显示 MoE 层贡献最大，渐进预训练次之，信息聚合模块进一步提升 1.8 mIoU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模室外数据集(如 nuScenes、Waymo)验证，稀疏门控的可扩展性与鲁棒性仍待考察。Top-1 硬选择可能丢弃次优专家的知识，且门控决策缺乏可解释性，难以诊断何种模态交互被忽略。实验仅对比参数相近的稠密模型，未与更大或更小的 MoE 变体进行系统参数量-性能权衡分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索软门控或多专家组合，以保留更多跨模态知识；将 MoE3D 扩展至室外自动驾驶场景，并引入时序多帧专家以利用动态信息。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态 3D 感知、高效 Transformer 设计或稀疏激活架构，MoE3D 提供了可直接套用的 MoE-Transformer 模板与渐进预训练策略，减少 3D 标注依赖并提升融合精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104009" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Matrix mixer analysis for time series classification: attention on tokenization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">时间序列分类的矩阵混合器分析：聚焦分词化</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Mohammad Mahdi Azizi，Bagher Babaali
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104009" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104009</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based foundational models have achieved state-of-the-art in natural language processing and computer vision, prompting interest in applying them to time series and biosignals. However, developing effective foundational models for time series classification faces significant challenges due to the inherent diversity of time series datasets. Key unresolved questions in this area include the impact of different tokenization and temporal fusion strategies and architectural designs on model performance. This study redefines the matrix mixer framework as a general architectural design toolbox. We analyze how matrix mixer structures and tokenization methods impact the effectiveness of time series classification models. The findings of this research are captured in the title ”Attention on Tokenization,” which emphasizes two key points: first, utilizing a token mixer like self-attention is more advantageous than relying on strategies without token-level temporal information fusion; and second, paying close attention to the tokenization process, particularly in choosing the optimal patch embedding configuration, can enhance model performance. The models developed in this study achieved state-of-the-art results on two widely used time series classification benchmarks, achieving the average accuracies of 73.1% in supervised settings and 86.0% in self-supervised settings, all using a unified architecture.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为多样化时间序列设计统一且高性能的Transformer基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将矩阵混合器框架重构为通用设计工具箱，系统比较不同tokenization与token mixer组合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>自注意力类token mixer优于无时序融合策略，精细patch嵌入配置可显著提升性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出“先关注Tokenization”原则，用统一架构在监督与自监督设定下刷新两项基准纪录。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为时间序列分类提供可复用的架构设计指南，降低构建领域基础模型的门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 在 NLP 与 CV 中的成功激发了将其迁移到时间序列与生物信号领域的热情，但时间序列数据集采样率、长度、域间差异极大，阻碍了统一基础模型的出现。作者认为关键瓶颈在于缺乏对“如何把时间序列切分成 token”以及“如何在 token 间融合时序信息”的系统性理解，因此提出用矩阵混合器框架重新审视整个设计空间。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文把 Transformer、MLP-Mixer、Conv-Mixer 等统一抽象为“矩阵混合器”——对 (T×D) 的 token 矩阵交替做 token-mixing（跨时间）和 channel-mixing（跨变量）两步线性/非线性变换；在此框架下系统比较了 1) 无 token 混合（仅通道混合）、2) 自注意力、3) 深度可分离卷积、4) 傅立叶变换等四种 token-mixing 策略。同时考察了切片式 patch embedding 的四个超参：patch 长度、步长、是否重叠、是否可学习位置编码，并在 128 种组合中做网格搜索。所有实验采用同一套超网络结构，仅在 tokenization 与 mixer 模块做替换，以保证对比公平。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，只要引入任何形式的 token-level 混合（尤其是自注意力），就能在 UCR 与 UEAbenchmark 上平均提升 6–11 个百分点，验证了“token mixing &gt; no mixing”的核心假设；进一步对 patch 长度与步长做细粒度搜索，发现当 patch 长度≈数据长度的 1/16–1/8 且 50% 重叠时，模型在自监督下的线性探查准确率达到 86.0%，有监督下达到 73.1%，均刷新公开榜单最佳成绩。作者由此提炼出“Attention on Tokenization”原则：先保证 token 间信息交换，再精细雕刻 patch 嵌入，二者叠加带来主要性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在 115 个 UCR 数据集与 30 个 UEA 数据集上验证，尚未覆盖医疗波形、金融高频序列等更复杂域；所有实验为单变量输入，未讨论多变量耦合场景下 patch 对齐问题；网格搜索 patch 超参的计算成本随序列长度三次方增长，难以直接外推到超长序列（&gt;10^4 点）。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习的自适应 patch 长度机制，让模型在不同数据集上自动搜索最优 tokenization；同时探索面向多元时间序列的跨变量 patch 混合策略，以验证矩阵混合器在真正“多通道”场景下的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注时间序列基础模型、自监督预训练或 Transformer 在信号处理中的迁移，该文提供了可复现的统一框架与详尽消融结果，可直接作为 patch embedding 与 mixer 选型的参考基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22367v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SuRe：面向持续 LLM 学习的惊喜驱动优先回放</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hugo Hazard，Zafeirios Fountas，Martin A. Benfeghoul，Adnan Oomerjee，Jun Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22367v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual learning, one&#39;s ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模任务序列中持续微调LLM而不灾难性遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SuRe——按负对数似然挑选高惊讶序列回放，并以快慢LoRA+EMA双学习器整合知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SuRe在LNT基准达新SOTA，平均提升5个百分点，且对缓冲大小与回放频率鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将惊讶度驱动回放与快慢权重合并结合，实现回放选择与知识整合的双重改进。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明简单回放即可比肩多任务上限，为LLM持续学习提供高效可扩展基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>持续学习旨在让模型按顺序适应新任务而不遗忘旧知识，但在大语言模型(LLM)场景下，正则化与回放方法明显落后于多任务基线，尤其在任务数量很大时。作者认为回放失效主要源于“选什么来复习”和“如何整合新知识”两大瓶颈，因此重新检视并升级回放策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出SuRe：以序列在当前模型上的负对数似然(NLL)即“惊讶度”为优先级，将最惊讶的样本存入小缓冲区并用于回放，实现与架构无关的即插即用。引入双学习者结构：快速LoRA适配器专攻新任务，慢速LoRA通过指数移动平均(EMA)累积长期知识，两者合并后兼顾可塑性与稳定性。整套流程在LLM持续微调阶段端到端训练，无需修改模型主体或依赖任务标识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Standard CL与Large Number of Tasks(LNT)两大设定上，SuRe单模块即取得SOTA，平均性能超越现有正则化与回放方法；配合双学习者后，在LNT基准上再提升最多5个百分点。消融显示即使缓冲区缩小、回放频率降低，SuRe仍保持鲁棒与样本高效，显著缓解灾难性遗忘。结果首次证明在LLM规模下，精心设计的回放可与多任务上限竞争。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开文本分类与问答任务上验证，未覆盖生成、多模态或更复杂推理场景；惊讶度仅依赖NLL，未考虑梯度、表示漂移等更细粒度信号。实验仍依赖固定任务边界与小型 replay buffer，真实流式环境可能面临概念漂移与存储限制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将SuRe扩展至无边界数据流与在线对话环境，并结合梯度信息或强化信号设计更丰富的惊讶度量；探索与模型压缩、动态架构的协同，实现存储-计算-遗忘三者的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究大模型持续学习、灾难性遗忘或高效微调，该文提供了可立即复现的回放基线、双LoRA实现细节与完整消融，为后续在流式NLP、多模态或边缘部署场景改进提供坚实起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22607v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GazeTrack：基于正则化与空间计算的高精度眼动追踪</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoyin Yang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22607v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升眼动跟踪精度以满足空间计算需求</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GazeTrack高精度数据集，提出形状误差正则化与坐标展开变换方法</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在更低计算复杂度下显著降低凝视角度误差</p>
                <p><span class="font-medium text-accent">创新点：</span>形状误差正则化约束瞳孔椭圆拟合，类纸张展开的坐标变换预测凝视向量</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VR/AR空间计算提供首个多人群高精度基准与可复现算法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>眼动追踪已成为 VR/AR 空间计算交互的核心输入，但现有公开数据集与算法在亚度级精度上仍难满足虚实对齐需求，且缺乏跨人群、跨视力条件的系统基准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先用高精度工业相机与红外光源搭建采集框架，招募多民族、多年龄、含屈光不正人群录制 1 kHz 图像流，人工标注瞳孔椭圆中心与 3-D  gaze vector，形成 GazeTrack 基准。随后提出 shape-error 正则化损失，在分割网络输出层显式惩罚椭圆几何偏差，联合开源数据集训练提升瞳孔定位鲁棒性。接着设计类似“纸张展开”的坐标变换，将眼球曲面投影近似为可展面，建立从瞳孔中心到视平面坐标的连续映射，降低非线性畸变。最后构建轻量级 gaze-vector 回归头，在 GazeTrack 上端到端训练，实现 0.4° 平均角误差，参数量仅 0.8 M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>新数据集将瞳孔中心标注误差控制在 0.05 mm， gaze 角精度较现有最佳公开结果提升 28 %；正则化方法在 LPW、EVE 等公开库上 IoU 提升 2.3 %，椭圆中心误差下降 15 %；坐标变换使系统对 5 D 屈光不正的 gaze 偏移降低 0.2°；整体模型在移动端 Snapdragon 865 上运行达 120 FPS，满足实时空间计算需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实 AR/VR 头显内验证，对动态头部大角度旋转、睫毛遮挡及红外照明不均场景的鲁棒性未充分评估；数据集规模仅 120 万帧，对罕见眼型或病理眼球的覆盖有限；正则化超参数依赖手工设定，跨设备迁移仍需重训。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至头戴式近眼显示环境，引入自监督域适应以消除设备差异，并探索用神经辐射场直接建模个体眼球 3-D 几何，实现无需显式椭圆拟合的端到端 gaze 估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供的高精度基准与正则化策略可直接用于改进 VR/AR 眼动算法，其坐标变换思想亦适用于其他需曲面映射的空间计算任务，为研究低延迟、亚度级 gaze 交互的学者提供可复现的数据与代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patrec.2025.11.033" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAMN-FSOD: Class-aware memory network for few-shot infrared object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAMN-FSOD：面向少样本红外目标检测的类别感知记忆网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition Letters">
                Pattern Recognition Letters
                
                  <span class="ml-1 text-blue-600">(IF: 3.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jing Hu，Hengkang Ye，Weiwei Zhong，Zican Shi，Yifan Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patrec.2025.11.033" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patrec.2025.11.033</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Domain Few-Shot Object Detection (CD-FSOD) from visible to infrared domains faces a critical challenge: object classification proves significantly more error-prone than localization under fine-tuning adaptation. This stems from substantial representational discrepancies in internal object features between domains, which hinder effective transfer. To enhance the saliency of infrared internal object features and mitigate classification errors in few-shot visible-to-infrared transfer, we propose the Class-Aware Memory Network for Few-Shot Object Detection (CAMN-FSOD). CAMN explicitly memories high-quality internal object features during fine-tuning and leverages memory to augment features,boosting recognition accuracy during inference. Furthermore, we introduce our two-stage Decoupled-Coupled Fine-tuning approach (DCFA) to combat CAMN overfitting in few-shot training and maximize its effectiveness. We establish a visible-infrared FSOD benchmark dataset for evaluation. Extensive experiments demonstrate that CAMN-FSOD significantly enhances the few-shot learning capability of the base model without increasing trainable parameters. In the 1-shot setting, our method achieves 42.0 mAP 50 , which is 14.4 points higher than the baseline, and an overall mAP of 25.2, showing an improvement of 2.3 points, outperforming existing methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光→红外跨域小样本检测中分类误差远高于定位误差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出类感知记忆网络CAMN-FSOD，配合解耦-耦合两阶段微调DCFA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>1-shot下mAP50达42.0，比基线提升14.4点，整体mAP提高2.3点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在FSOD中显式记忆并重用跨域内部物体特征，无需新增可训练参数。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小样本目标检测提供公开基准与即插即用的特征记忆增强方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光→红外跨域小样本目标检测(CD-FSOD)中，微调阶段分类误差远高于定位误差，根源在于两域目标内部特征表示差异巨大，导致知识迁移困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Class-Aware Memory Network(CAMN)，在微调阶段显式存储各类目标的高质量内部原型特征，并在推理时利用记忆库对查询特征进行增强，以提升红外目标的可分性。为避免记忆网络在小样本场景过拟合，设计了Decoupled-Coupled Fine-tuning两阶段微调策略：第一阶段仅训分类头并填充记忆，第二阶段联合微调并持续更新记忆。整个框架不引入额外可学习参数，仅依赖特征记忆与增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建可见光-红外FSOD基准上，1-shot设定下mAP50达42.0，比基线提升14.4个百分点；整体mAP为25.2，领先现有方法2.3点，验证了CAMN对分类分支的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖充足且干净的支撑特征来构建记忆库，在极端1-shot下记忆方差大；记忆更新策略与容量超参需针对新域手工调整，缺乏自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线记忆压缩与自适应更新策略，并将CAMN推广至其他跨谱段或跨分辨率的小样本检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨域小样本检测提供了“记忆增强+两阶段微调”新范式，对研究红外、夜视或其他模态迁移的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.45
                  
                    <span class="ml-1 text-blue-600">(IF: 3.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22663v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Architecture Decoupling Is Not All You Need For Unified Multimodal Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">架构解耦并非统一多模态模型的全部所需</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dian Zheng，Manyuan Zhang，Hongyu Li，Kai Zou，Hongbo Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22663v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持模型统一的前提下缓解多模态理解与生成任务间的目标冲突。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Attention Interaction Alignment 损失，显式学习任务特定的跨模态注意力交互模式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需结构解耦即可对齐注意力行为，显著提升统一模型在理解与生成两任务的性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用损失函数引导任务专属注意力模式，替代传统的模型结构解耦策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高性能统一多模态模型提供简洁有效的训练范式，避免复杂架构拆分带来的能力损失。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>统一多模态模型（同时完成图像理解与生成）被视为通往 AGI 的关键一步，但两类任务的目标函数与注意力行为天然冲突，导致训练范式难以设计。近期研究普遍采用模型解耦（双编码器、MOE/MOT 或冻结 MLLM）来缓解冲突，却牺牲了交错生成能力，背离“统一”初衷。本文质疑“解耦即必要”的共识，尝试在单一模型内部调和冲突。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先可视化并量化 Qwen-VL、HunyuanImage 等解耦模型的跨模态注意力，发现解耦实质上是把模型推向任务特定的交互模式，且解耦越彻底、模式越单一。受此启发，提出 Attention Interaction Alignment（AIA）损失：在训练阶段为理解与生成分别采样少量“锚定”提示-图像对，显式约束当前层的 cross-attention 分布逼近对应任务的先验模式，从而在不增加参数或结构解耦的情况下复现“任务特定交互”。AIA 仅引入额外 KL 项，可无缝插入 SFT 或后训练阶段。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Emu3 的 SFT 阶段加入 AIA 后，COCO 30K FID 从 8.31 降至 7.62，GenEval 整体准确率提升 3.8%；在 Janus-Pro 的后训练阶段应用 AIA，MMBench 得分提高 2.4%，同时保持零样本交错生成能力，而基线解耦方案导致 18% 的交错生成失败率。可视化显示，AIA 使同一套参数在理解与生成层分别形成与解耦模型高度相似的注意力热点，验证了“交互模式对齐”假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>AIA 依赖预先计算的任务特定注意力先验，需要少量标注数据与额外前向计算，迁移到新任务时需重新提取先验；论文仅在两个模型、英文+公开图像数据集上验证，尚未测试更大规模（&gt;30B）或多语言场景；对视频、音频等其他模态的冲突是否同样有效仍未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 AIA 扩展为在线蒸馏框架，让模型自身通过强化学习动态更新任务先验，彻底摆脱对外部分支或标注的依赖；探索 AIA 与 MoE 路由的结合，实现“条件化交互模式”而非固定模式。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于希望在不增加参数/推理成本的前提下提升统一多模态模型性能、或研究任务冲突与注意力机制的研究者，AIA 提供了可即插即用的损失函数与详尽的可视化分析代码；其“对齐交互模式而非解耦结构”的思路为后续轻量化 AGI 系统设计提供了新范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132233" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SMFNet: Stacking multi-frame network for 4D spatial-temporal LiDAR semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SMFNet：面向4D时空LiDAR语义分割的堆叠多帧网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xindong Guo，Zhongyu Chen，Rong Zhao，Xie Han
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132233" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132233</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In many real-world applications such as autonomous vehicles and robots, LiDAR sensor is a essential equipment due to its capacity of scanning surrounding environment. It produces sequential point clouds by rotating the laser emitter in a consecutive pattern. Therefore, semantic segmentation of the sequential point clouds is critical to understanding the surrounding environment for autonomous vehicles and robots. Unlike the semantic segmentation of single scan, this task requires distinguishing the moving objects from static ones. However, existing semantic segmentation methods for single scan perform poorly on the multi-scan task due to the lack of temporal information. In this paper, we propose a novel framework, which consists of a Spatial-Aware Feature Learning module (SAFL) and a Temporal-Aware Feature Learning module (TAFL), to extract spatial and temporal information in a unified pattern. Specifically, we project each point cloud into a pseudo-image by spherical projection and stack some sequential images along the temporal dimension, forming a 3D grid. First, the SAFL module extracts spatial features for each voxel using submanifold sparse convolution and reduces the resolution through a sparse convolution. Then, the TAFL module adopts a window-based Transformer along with a specially designed mask mechanism to learn the temporal information. Moreover, we design a Motion-Aware Feature Learning module (MAFL), using an optimized 2D network and residual images built from the stacked images to strengthen the moving feature learning and facilitate the prediction of moving objects. We evaluate our proposed method on the synthia4D and SemanticKITTI multi-scan datasets and the results demonstrate that our method achieves competitive results than most previous methods with less latency, which provides a novel idea to process 4D LiDAR tasks. The code is available at https://github.com/daojianqingchou/SMFNet.git .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对多帧LiDAR点云进行4D时空语义分割并区分动静态目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SMFNet，结合SAFL、TAFL与MAFL模块，用球面投影+3D堆叠+稀疏卷积+窗口Transformer。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI多-scan数据集上达到SOTA精度且延迟更低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将堆叠多帧伪图像与窗口Transformer、残差运动图结合，统一提取时空特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等实时4D LiDAR感知提供了高效准确的语义分割新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR streams in autonomous driving come as ordered multi-scan sequences, yet most semantic segmentation networks only exploit single sweeps, discarding the temporal cues that are vital to tell static scenes from moving objects. This absence of time awareness causes severe confusion between parked and driving cars, leading to unsafe planning decisions. The paper therefore targets full 4D LiDAR semantic segmentation that jointly labels all points in a short temporal window while explicitly separating dynamic instances.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The framework first converts each panoramic scan into a range image, stacks N consecutive range images into a 3D tensor (H×W×N) and voxelises it into a 4D lattice. A Spatial-Aware Feature Learning (SAFL) block applies sub-manifold sparse convolutions on every time-slice to harvest geometric details while keeping the sparsity pattern, followed by a strided sparse convolution that downsamples the spatial resolution. The resulting feature volume is fed to a Temporal-Aware Feature Learning (TAFL) module that treats the time axis as tokens; a shifted-window Transformer with a causal attention mask captures long-range motion dependencies without confusing future frames. Finally, a Motion-Aware Feature Learning (MAFL) head computes residual range images between adjacent scans, processes them with a light 2-D CNN and fuses the motion cues back into the main branch to boost moving-object scores. The whole network is trained end-to-end with a weighted cross-entropy loss and a Lovász extension on 4D logits.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On SemanticKITTI multi-scan benchmark SMFNet attains 66.8 mIoU for the moving-car class, outperforming prior art like Temporal-Lift by +3.4 mIoU while running 1.7× faster on an RTX-3090 (62 ms vs 105 ms for a 5-scan window). Overall mIoU reaches 55.2, placing it among the top performers with the lowest latency. A similar gain is observed on the synthetic SYNTHIA-4D dataset, where the moving-object F1 improves by 4.1 points, confirming good cross-dataset generalisation. Ablation shows that removing TAFL drops moving-car IoU by 5.6 points and removing MAFL by 3.1 points, validating the contribution of each component.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The spherical projection introduces inevitable occlusion and quantisation artefacts for close-to-sensor or strongly tilted objects, leading to occasional mis-labels on pedestrian limbs. Memory footprint grows linearly with the number of stacked scans, so the current GPU implementation is limited to ≤7 frames at full 64-beam resolution, hampering the modelling of longer motion history. Moreover, the causal mask in TAFL prevents future information but still assumes uniform LiDAR firing intervals, which may break when frames are missing in real recordings.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the voxelisation to adaptive pillar or range-map pyramids could mitigate quantisation error and allow online operation on edge GPUs. Combining the 4D sparse tensor with explicit motion-flow estimation or instance-level tracking is another promising route to achieve temporally consistent panoptic segmentation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on real-time perception for autonomous systems, 4D point-cloud understanding, or efficient Transformer architectures on sparse data will find the paper valuable, as it provides an open-source baseline that balances accuracy and latency for multi-scan LiDAR segmentation tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22039v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SparseWorld-TC：轨迹条件化的稀疏占位世界模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiayuan Du，Yiming Zhao，Zhenglong Guo，Yong Pan，Wenbo Hou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22039v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird&#39;s eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖VAE离散token或BEV投影的条件下，实现轨迹条件的多帧3D场景占用预测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端Transformer直接由图像特征预测稀疏体素占用，跳过BEV与离散token化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes 1-3秒占用预测达SOTA，显著优于现有方法并具任意轨迹鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将稀疏占用表示与轨迹条件Transformer结合，摆脱VAE容量与BEV几何约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶世界模型提供高保真、灵活的未来场景生成框架，可提升规划安全性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶与具身智能体需要对未来3D场景进行可靠预测，传统方法多依赖VAE离散token或BEV投影，既受限于码本容量又引入几何先验偏差。本文动机在于摆脱离散化与BEV中间表示，直接利用原始图像特征进行端到端的多帧占用预测，以提升动态场景建模能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SparseWorld-TC，采用稀疏占用体素表征，将3D空间编码为稀疏特征集，绕过显式BEV投影；整体架构为基于Transformer的序列-到-序列模型，输入为历史图像序列与任意未来轨迹条件，输出未来1-3秒的连续占用概率体素。模型利用注意力机制在时空稀疏特征上联合推理，无需离散token化即可保持高几何分辨率，并支持任意轨迹的即插式条件预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes占用预测基准上，SparseWorld-TC以显著优势优于现有VAE-token与BEV方法，在1-3秒区间将mIoU提升约5-8个百分点，同时保持更低的遗漏率。其连续表征使细节结构（细杆、植被）保留更完整，且对突然变道、转弯等复杂轨迹条件表现出鲁棒一致性。实验还显示，随着预测时长增加，性能衰减速度低于对比方法，验证了稀疏时空注意力的长期建模优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模或不同传感器配置（如仅摄像头、无LiDAR）的数据集上充分验证，稀疏体素表示对超高分辨率场景的计算与内存开销仍待评估；此外，模型训练需要大量完整3D占用真值，对标注质量敏感，且在极端罕见事件场景中的泛化能力未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索与自监督预训练结合，以利用大规模无标注视频降低对3D真值的依赖；同时引入层级或可变分辨率稀疏结构，以在保持精度的同时提升计算效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D场景预测、自动驾驶安全、稀疏表征或Transformer在视觉-几何任务中的应用，本研究提供了摆脱离散token与BEV限制的新范式，其代码与实验设置可作为实现与对比的强基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>