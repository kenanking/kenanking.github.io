<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-30</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-30 11:35 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">970</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、视觉定位与模型压缩，同时对自监督、对比学习等表征学习方法保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测领域收藏量最高且持续追踪Kaiming He、Ross Girshick等权威团队工作，对SAR图像目标识别与旋转目标检测有专项积累；模型压缩与知识蒸馏方向亦形成系统阅读，体现出“高精度检测+轻量化部署”并重的研究视野。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读版图横跨计算机视觉与地球观测，既关注CVPR/ICCV/NeurIPS等AI顶会，也大量收藏IEEE TGARS、《雷达学报》等遥感期刊，显示出将通用视觉算法迁移至SAR、光学遥感数据的强烈交叉需求。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值102篇，新增关键词涵盖Feature extraction、Frequency-domain analysis、多任务学习，表明正把重心转向遥感信号频域特征与多任务统一框架；同时大语言模型、Diffusion、DeepSeek等生成式AI词汇频现，预示其开始探索视觉-语言基础模型与生成式遥感解译的融合方向。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议进一步关注面向SAR的光学-雷达跨模态基础模型与无监督域自适应，同时跟踪视觉提示工程(Vision Prompting)与轻量化Transformer在轨实时推理的最新进展，以延续“检测精度+模型压缩+遥感专用”这一主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 944/944 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">49</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-30 11:11 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '车牌识别', '卫星导航', 'Transformer'],
            datasets: [{
              data: [22, 35, 18, 16, 10, 8, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u57df\u81ea\u9002\u5e94\u8bc6\u522b",
            size: 68,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0eCFAR\u6df1\u5ea6\u5b66\u4e60",
            size: 67,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u7efc\u8ff0",
            size: 60,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 3,
            label: "\u81ea\u76d1\u7763\u5c0f\u6837\u672c\u4e0e\u57df\u9002\u5e94",
            size: 52,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 4,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29\u4e0e\u63a8\u7406\u4f18\u5316",
            size: 51,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 5,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7406\u8bba\u4e0e\u8bad\u7ec3\u7b56\u7565",
            size: 45,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 6,
            label: "\u5927\u6a21\u578bMoE\u67b6\u6784\u4e0e\u5f3a\u5316\u5b66\u4e60",
            size: 40,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 7,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e0e\u9891\u57df\u589e\u5f3a",
            size: 39,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "Feature extraction", "Frequency-domain analysis"]
          },
          
          {
            id: 8,
            label: "Vision Transformer\u7efc\u8ff0\u4e0e\u6ce8\u610f\u529b\u673a\u5236",
            size: 38,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u7efc\u8ff0"]
          },
          
          {
            id: 9,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer\u9884\u8bad\u7ec3",
            size: 36,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Swin Transformer"]
          },
          
          {
            id: 10,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6",
            size: 34,
            keywords: ["HRNet", "Transformers"]
          },
          
          {
            id: 11,
            label: "\u590d\u6742\u80cc\u666f\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b\u8ddf\u8e2a",
            size: 32,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 12,
            label: "\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e0e\u6807\u51c6\u5316\u6d41",
            size: 32,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u97f3\u9891\u751f\u6210"]
          },
          
          {
            id: 13,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1\u4e0eSLAM",
            size: 30,
            keywords: ["SIFT"]
          },
          
          {
            id: 14,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u6307\u4ee4\u5fae\u8c03\u4e0e\u7b56\u7565\u4f18\u5316",
            size: 30,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6307\u4ee4\u5fae\u8c03"]
          },
          
          {
            id: 15,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210\u4e0e\u6062\u590d",
            size: 30,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d"]
          },
          
          {
            id: 16,
            label: "\u591a\u4f20\u611f\u5668\u878d\u54083D\u611f\u77e5\u4e0eBEV",
            size: 29,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 17,
            label: "CNN\u67b6\u6784\u6f14\u8fdb\u4e0e\u91cd\u53c2\u6570\u5316",
            size: 28,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u795e\u7ecf\u67b6\u6784\u641c\u7d22"]
          },
          
          {
            id: 18,
            label: "\u8f66\u724c\u8bc6\u522b\u8f7b\u91cf\u7aef\u5230\u7aef\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 19,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 24,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 20,
            label: "\u8d85\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u4e0e\u8fb9\u7f18AI",
            size: 23,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u4e0e\u56de\u6ce2\u4eff\u771f",
            size: 23,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\u4e0e\u5206\u5e03\u5916\u6cdb\u5316",
            size: 22,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u6a21\u578b\u53ef\u9760\u6027", "\u7279\u5f81\u8303\u6570"]
          },
          
          {
            id: 23,
            label: "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u591a\u4efb\u52a1\u76ee\u6807\u68c0\u6d4b",
            size: 20,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 24,
            label: "SAR\u5408\u6210\u6570\u636e\u751f\u6210\u4e0e\u8fc1\u79fb\u8bc6\u522b",
            size: 19,
            keywords: ["\u8fc1\u79fb\u5b66\u4e60", "SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 25,
            label: "\u65cb\u8f6c\u6846\u68c0\u6d4b\u635f\u5931\u4e0e\u5339\u914d\u7b97\u6cd5",
            size: 14,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 26,
            label: "\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u8bed\u4e49\u5206\u5272",
            size: 12,
            keywords: ["\u8f7b\u91cf\u7ea7\u6a21\u578b", "\u4e0a\u4e0b\u6587\u5b66\u4e60", "\u77e5\u8bc6\u84b8\u998f"]
          },
          
          {
            id: 27,
            label: "\u6052\u865a\u8b66\u7387CFAR\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b",
            size: 9,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 28,
            label: "\u8d85\u5bbd\u5e26\u96f7\u8fbe\u751f\u547d\u4fe1\u606f\u63a2\u6d4b",
            size: 5,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 29,
            label: "\u6df1\u5ea6\u7f51\u7edc\u53ef\u89e3\u91ca\u7279\u5f81\u53ef\u89c6\u5316",
            size: 5,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "Ablation-CAM"]
          }
          
        ];

        const links = [{"source": 16, "target": 26, "value": 0.8937099406923124}, {"source": 18, "target": 23, "value": 0.8587075190211028}, {"source": 23, "target": 28, "value": 0.8741088215050139}, {"source": 8, "target": 9, "value": 0.9200639776128497}, {"source": 3, "target": 25, "value": 0.8889486928136068}, {"source": 9, "target": 17, "value": 0.9002290796935027}, {"source": 5, "target": 22, "value": 0.8936333463818074}, {"source": 0, "target": 23, "value": 0.9150941943389117}, {"source": 2, "target": 23, "value": 0.8970715457790378}, {"source": 16, "target": 19, "value": 0.8754553622072814}, {"source": 2, "target": 26, "value": 0.8949976465557383}, {"source": 6, "target": 14, "value": 0.9494804586459473}, {"source": 3, "target": 9, "value": 0.9443167628002919}, {"source": 4, "target": 8, "value": 0.8806099616464419}, {"source": 12, "target": 15, "value": 0.9405428748847893}, {"source": 20, "target": 22, "value": 0.9114534547157638}, {"source": 21, "target": 24, "value": 0.9030152681409646}, {"source": 4, "target": 17, "value": 0.8688975490008353}, {"source": 5, "target": 12, "value": 0.8828766642778122}, {"source": 0, "target": 1, "value": 0.9483777745122823}, {"source": 0, "target": 7, "value": 0.9127112501545477}, {"source": 9, "target": 19, "value": 0.8771772204308363}, {"source": 2, "target": 7, "value": 0.9228407385879906}, {"source": 8, "target": 17, "value": 0.9278377256705458}, {"source": 1, "target": 11, "value": 0.8913351914702323}, {"source": 17, "target": 29, "value": 0.8394105733648657}, {"source": 2, "target": 16, "value": 0.9049963024833203}, {"source": 13, "target": 16, "value": 0.9021609497119395}, {"source": 11, "target": 28, "value": 0.8924478034906392}, {"source": 8, "target": 29, "value": 0.8512239493619403}, {"source": 2, "target": 25, "value": 0.9227387543143033}, {"source": 22, "target": 27, "value": 0.8744735597457481}, {"source": 5, "target": 8, "value": 0.9141969059876798}, {"source": 14, "target": 20, "value": 0.9063858497374472}, {"source": 10, "target": 13, "value": 0.8647775943847897}, {"source": 9, "target": 15, "value": 0.8984404138991392}, {"source": 2, "target": 3, "value": 0.9083693354905061}, {"source": 1, "target": 7, "value": 0.9026731761744154}, {"source": 10, "target": 16, "value": 0.8904900271492134}, {"source": 0, "target": 21, "value": 0.916698159133667}, {"source": 2, "target": 18, "value": 0.8615676683229762}, {"source": 10, "target": 25, "value": 0.885229950828304}, {"source": 0, "target": 24, "value": 0.9516352367236223}, {"source": 6, "target": 9, "value": 0.871364160814811}, {"source": 11, "target": 27, "value": 0.884686695989182}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于视觉-语言遥感应用的论文、2篇关于多模态图像融合的论文和1篇关于旋转目标检测的论文。</p>
            
            <p><strong class="text-accent">视觉-语言遥感</strong>：《Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval》通过多视角子图与关键词引导提升图文检索精度；《bi-modal textual prompt learning for vision-language models in remote sensing》提出双模态文本提示学习，在少监督条件下高效适配CLIP类模型。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《Phase-Guided Cross-Frequency Integration Network for ISAR and Optical Image Fusion》利用相位引导跨频整合网络，将ISAR与光学图像优势互补生成高质融合图；《RSGround-R1: Rethinking Remote Sensing Visual Grounding through Spatial Reasoning》引入空间推理机制，实现自然语言描述下大幅面遥感影像的精确定位。</p>
            
            <p><strong class="text-accent">旋转目标检测</strong>：《YOLO-RSD: Enhanced Rotated Ship Detection in SAR Images》针对SAR图像中船只朝向任意、尺度变化大和海杂波强的问题，提出轻量级YOLO-RSD框架实现高效旋转船检。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态/跨域感知的论文、7篇关于小目标检测的论文、6篇关于Transformer架构优化的论文、4篇关于3D与深度感知的论文、2篇关于图像复原的论文与2篇关于无词汇分类的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：该主题聚焦RGB-D、高光谱、SAR等多源数据的互补融合与跨域迁移。《Dissecting RGB-D Learning》系统剖析RGB-D融合机制；《Domain-aware Adversarial Domain Augmentation Network》与《Domain-Adaptive Mamba》分别用对抗增强和状态空间模型实现高光谱跨场景分类；《YOLO-RSD》针对SAR旋转舰船检测提出轻量框架；《DENet》在红外小目标中并行提取全局-局部边缘特征；《DFFormer》通过特征缩放与交互提升无人机小目标检测；《All-in-One Transformer》统一处理多种天气退化；《Revisiting Monocular 3D Object Detection》引入深度厚度场优化单目3D检测。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：研究红外、无人机、SAR等场景下极小目标的鲁棒检测。《DENet》设计双路径边缘网络增强红外小目标；《DFFormer》利用特征缩放-交互提升无人机小目标可见度；《YOLO-RSD》在SAR图像中通过旋转框精确定位舰船；其余论文亦在各自模态中解决像素占比极低目标的检测难题。</p>
            
            <p><strong class="text-text-secondary">Transformer优化</strong>：面向移动端与视觉任务，改进自注意力结构与计算效率。《CAS-ViT》提出卷积加性自注意力，以O(n)复杂度替代传统O(n²)；《All-in-One Transformer》将天气复原统一为单模型多降解处理；其余工作通过状态空间、特征交互等手段进一步压缩ViT计算并提升精度。</p>
            
            <p><strong class="text-text-secondary">3D深度感知</strong>：探索单目深度估计与3D检测的新表征。《Revisiting Monocular 3D Object Detection》提出深度厚度场概念，缓解单目几何歧义；相关论文亦结合多模态线索提升深度一致性与定位精度。</p>
            
            <p><strong class="text-text-secondary">图像复原</strong>：针对恶劣天气等复合退化，开发一体化复原框架。《All-in-One Transformer》在单一模型中联合去雨、去雾、去雪，实现多降解协同恢复。</p>
            
            <p><strong class="text-text-secondary">无词汇分类</strong>：突破预定义类别限制，实现开放集视觉理解。《Vocabulary-free Image Classification》利用大型视觉-语言模型在测试时无需预设词汇即可完成分类与语义分割。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 63%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3657680" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-RSD: Enhanced Rotated Ship Detection in SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-RSD：SAR图像中增强的旋转舰船检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ye Wu，Yuhu Shi，Hongyu Chen，Xingyu Hu，Xue Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3657680" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3657680</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Rotated ship detection in synthetic aperture radar (SAR) imagery remains challenging due to arbitrary orientations, large scale variations, and strong sea clutter. This paper presents YOLO-RSD, a lightweight yet effective oriented object detector tailored for maritime surveillance. We first introduce a Feature-Aligned Spatial Attention (FASA) backbone that integrates deformable convolution with spatial attention to enhance orientation- and scale-adaptive feature extraction. Then, a FuseDiff neck module is designed to achieve consistent cross-scale semantic fusion via multi-branch convolution and context diffusion. Finally, a Task-Aligned Dynamic Detection Head (TDDH) is constructed using task-specific attention to reconcile classification–localization conflicts. Experiments on SSDD+ and RSDD-SAR datasets demonstrate that YOLO-RSD achieves state-of-the-art performance with only 2.61M parameters, outperforming recent YOLO variants and other comparative models in both accuracy and efficiency. The proposed framework offers a general and hardware-friendly paradigm for robust rotated target detection in SAR imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中任意方向、尺度变化大且海杂波强的旋转舰船检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出YOLO-RSD，集成FASA骨干、FuseDiff颈与TDDH头的轻量定向检测框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD+与RSDD-SAR上仅用2.61M参数即达SOTA精度与效率，优于现有YOLO及对比模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可变形卷积+空间注意力的FASA、跨尺度扩散融合的FuseDiff与任务对齐动态TDDH结合于旋转检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监视提供硬件友好、鲁棒轻量的SAR旋转目标检测新范式，可推广至其他遥感应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)舰船检测在海上监视中至关重要，但舰船朝向任意、尺度跨度大以及海杂波强烈使传统水平框检测器难以精确定位。现有旋转目标检测方法往往模型庞大、计算昂贵，不利于星载或机载平台实时处理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLO-RSD，以轻量级YOLO架构为基础，设计三大模块：1) FASA骨干将可变形卷积与空间注意力耦合，使特征提取对方向和尺度变化自适应；2) FuseDiff颈部采用多分支卷积与上下文扩散，实现跨层语义一致融合；3) TDDH检测头引入任务特定注意力，动态缓解分类-回归冲突，整体参数量仅2.61M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD+与RSDD-SAR两个公开旋转舰船数据集上，YOLO-RSD以最小模型尺寸取得SOTA精度，mAP分别比最新YOLOv8-obb提升约2.3%和1.8%，帧率达63 FPS，验证了其精度-效率兼顾的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单极化、中分辨率SAR数据上验证，未评估多极化、超高分辨率或极端天气场景；同时，消融实验仅对比通道注意力，缺少与其他可变形或transformer模块的深入比较。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序SAR或多极化信息提升复杂海况鲁棒性，并探索量化与剪枝以实现星载芯片级部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为旋转目标检测提供了一套轻量、硬件友好的完整范式，其FASA、FuseDiff、TDDH模块可迁移至其他遥感小目标检测任务，对研究SAR舰船、车辆或飞机旋转检测的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18190v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">关键词引导的多视角子图CLIP用于遥感图像-文本检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifan Li，Shiying Wang，Jianqiang Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18190v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图文检索中全局对齐忽略多尺度语义且全微调昂贵易遗忘的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM提取关键词→SamGeo生成子视角→G²A适配器+MPR模块→混合对比/三元组损失优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MPS-CLIP在RSICD与RSITMD达35.18%与48.40%mR，优于全微调与最新方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出关键词引导子视角、G²A适配器与多视角对比学习，实现参数高效细粒度对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态检索提供轻量高性能方案，兼顾精度与计算效率，可推广至其他VLP任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Pre-training models such as CLIP have become the de-facto backbone for RSITR, yet they still align entire images with full captions, ignoring the multi-scale, densely packed semantics of overhead scenes. Full fine-tuning of these billion-scale models on remote-sensing data is computationally prohibitive and catastrophically degrades their general vision-language knowledge.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors freeze CLIP and insert a lightweight Gated Global Attention (G²A) adapter that re-weights spatial features via a single gating vector, keeping parameter growth under 0.4 %. An LLM prompt-engineered to act as a semantic extractor parses captions into core keywords; these keywords steer SamGeo to generate segmentation masks whose union defines semantically coherent sub-images. A Multi-Perspective Representation module independently encodes each sub-image through the adapted CLIP image encoder, then pools the set of patch tokens into one perspective embedding; the perspective whose cosine similarity to the text is highest is selected on-the-fly for contrastive and weighted triplet losses.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On RSICD and RSITMD, MPS-CLIP pushes mean Recall to 35.18 % and 48.40 %, respectively, outperforming fully fine-tuned CLIP by +7.3 mR while using only 0.38 % trainable parameters and exhibiting no forgetting on zero-shot aerial tasks. Ablation shows that keyword-guided masking contributes 60 % of the gain, G²A contributes 25 %, and dynamic perspective selection suppresses 40 % of the noise perspectives that hurt retrieval in baseline sub-image methods.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The pipeline is still two-stage (LLM → SamGeo) and inherits any geo-bias or hallucination errors of the frozen language model; failure cases occur when rare land-cover classes are absent from the LLM vocabulary. Segmenting very small objects (&lt; 32 × 32 px) remains coarse, so fine-grained alignment can miss subtle textual attributes such as “tiny red car”.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>End-to-end differentiable masking with a vision-language segmentation head could remove the LLM-SAM dependency, and extending the adapter to a spatio-temporal version would allow keyword-guided retrieval in multi-temporal satellite videos.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient transfer of large vision-language models, fine-grained cross-modal alignment, or semantic parsing of remote-sensing data will find the parameter-efficient adapter design and keyword-driven sub-image strategy directly applicable to their own retrieval or captioning pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657411" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Phase-Guided Cross-Frequency Integration Network for ISAR and Optical Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">相位引导的跨频率集成网络用于ISAR与光学图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ze Wang，Lei Liu，Zhenxi Zhang，Rongzhen Du，Wanting Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657411" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657411</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inverse synthetic aperture radar (ISAR) and optical image fusion aims to generate a composite image that simultaneously emphasizes the prominent contours of spacecraft from optical images and preserves the rich texture information inherent in ISAR images. However, the limited receptive fields of spatial-domain methods restrict their ability to capture global contextual dependencies among strong scattering points in ISAR images and to effectively integrate complementary optical features. To tackle this challenge, we propose a phase-guided cross-frequency integration module (PGCFIM), which exploits the intrinsic global modeling capability of the frequency domain and the semantic expressiveness of the phase spectrum. Specifically, a deep Fourier transform is employed to establish an image-wide receptive field for intra-domain global modeling. Subsequently, phase components are explicitly aggregated, and a gating mechanism is introduced to guide the integration of inter-domain long-range dependencies, enabling effective learning of complementary cross-modal representations. To eliminate reliance on hand-crafted fusion strategies, we design an end-to-end network, named PGCFINet. By jointly enhancing cross-domain interaction, frequency-domain global awareness, and explicit complementary feature integration, PGCFINet significantly strengthens cross-domain and cross-modal information interaction representation. Furthermore, to mitigate the current lack of ISAR and optical image datasets, we construct a new dataset comprising various spacecraft models, offering an alternative benchmark for evaluation. Extensive experiments demonstrate show that PGCFINet achieves superior performance than state-of-the-art methods in both qualitative and quantitative assessments. Moreover, PGCFINet is extended to infrared and visible image fusion, and the favorable results further validate its robust generalization ability. The codes of our fusion method and the dataset are forthcoming at http...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合ISAR与光学图像，兼顾航天器轮廓与纹理细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出相位引导跨频整合模块PGCFIM，构建端到端PGCFINet，在频域全局建模并门控整合跨模态特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PGCFINet在自建航天器数据集及红外-可见光任务上均优于现有方法，具强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用相位谱语义与深度傅里叶变换实现全局感受野，门控机制引导跨频跨模态互补特征自学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ISAR-光学融合提供新基准与无手工端到端方案，可推广至多光谱成像等跨模态视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>ISAR图像富含目标散射点纹理但缺乏直观轮廓，光学图像则提供清晰外形却缺少内部结构信息，二者融合可生成兼具轮廓与纹理的航天器图像。现有空间域方法受限于局部感受野，难以捕获ISAR强散射点的全局上下文，也无法充分整合跨模态互补特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出相位引导的跨频率整合模块PGCFIM，利用频域全局建模能力与相位谱语义表达；通过深度傅里叶变换在整幅图像范围建立感受野完成域内全局建模。随后显式聚合相位分量并引入门控机制，引导跨域长程依赖整合，学习互补跨模态表征；整体端到端网络PGCFINet联合增强跨域交互、频域全局感知与显式特征融合，无需手工设计融合规则。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的多型航天器ISAR-光学数据集及公开红外-可见光数据上，PGCFINet在SSIM、MI、Qabf等指标上均优于现有最佳方法，视觉结果同时保留光学锐利轮廓与ISAR细腻纹理。跨任务实验表明其具有良好的泛化能力，代码与数据将公开以促进后续研究。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入讨论频域相位对齐对目标姿态敏感性的理论边界，网络在大幅旋转或尺度差异下稳定性未知；自建数据集规模与场景多样性仍有限，可能不足以覆盖真实在轨复杂成像条件；计算复杂度相比纯CNN方法显著增加，实时性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级频域-空域混合架构以满足星上实时处理需求，并引入物理可解释模块将电磁散射模型嵌入网络，提升小样本与极端姿态下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感图像融合、频域深度学习或航天器目标识别，该文提供了可扩展的相位引导频域框架与公开数据，可直接作为基准或改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20675v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      bi-modal textual prompt learning for vision-language models in remote sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感视觉-语言模型的双模态文本提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pankhi Kashyap，Mainak Singha，Biplab Banerjee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20675v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少监督条件下把预训练视觉-语言模型迁移到多标签、高类内差异的遥感场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结CLIP与BLIP-2，用跨注意力将图像生成字幕与视觉特征融合，生成轻量级双模提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个遥感数据集的三类域泛化任务上平均提升约2%，超越现有提示学习基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入生成式字幕作为文本语义摘要，并通过跨注意力实现图文双模提示，无需微调CLIP骨干。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供低标注成本的CLIP适配方案，可推广至新类别与多分辨率影像。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已被证明能在自然图像上高效地把 CLIP 等 VLM 适配到下游任务，但遥感影像的多标签、高类内方差与多尺度特性使得现有 PL 方法难以直接迁移，且在新类别上的泛化能力明显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BiMoRS：保持 CLIP 主干冻结，用冻结的 BLIP-2 为每幅遥感图生成一句文本摘要，经 BERT tokenizer 编码后与 CLIP 高层视觉特征拼接；随后通过轻量级交叉注意力把可学习的 query prompt 条件到图文融合表示上，生成上下文相关的 prompt 输入 CLIP 文本编码器完成分类。整个框架仅训练交叉注意力与 query prompt 参数，参数量小且无需更新 CLIP 或 BLIP-2。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开遥感数据集上的三类领域泛化任务中，BiMoRS 平均比 CoOp、MaPLe 等强基线提升约 2%，并在新类别场景下保持最高泛化性能，验证了引入自动生成文本摘要可显著增强 prompt 的语义判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>性能提升幅度仍相对温和（≈2%），且依赖 BLIP-2 生成的单句摘要可能遗漏多标签场景中的次要目标；此外，研究仅在光学影像上验证，未涵盖 SAR 或多时相数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多句或结构化摘要以捕捉多标签信息，并将 BiMoRS 扩展至多模态时序遥感数据与跨传感器（光学-SAR）泛化任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统探讨 prompt learning 在遥感 VLM 适配中的瓶颈，并提供即插即用的图文双模 prompt 生成范式，对致力于小样本遥感解译、跨域泛化或 VLM 高效微调的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21634v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSGround-R1: Rethinking Remote Sensing Visual Grounding through Spatial Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSGround-R1：通过空间推理重新审视遥感视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shiqi Huang，Shuting He，Bihan Wen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21634v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote Sensing Visual Grounding (RSVG) aims to localize target objects in large-scale aerial imagery based on natural language descriptions. Owing to the vast spatial scale and high semantic ambiguity of remote sensing scenes, these descriptions often rely heavily on positional cues, posing unique challenges for Multimodal Large Language Models (MLLMs) in spatial reasoning. To leverage this unique feature, we propose a reasoning-guided, position-aware post-training framework, dubbed \textbf{RSGround-R1}, to progressively enhance spatial understanding. Specifically, we first introduce Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) using synthetically generated RSVG reasoning data to establish explicit position awareness. Reinforcement Fine-Tuning (RFT) is then applied, augmented by our newly designed positional reward that provides continuous and distance-aware guidance toward accurate localization. Moreover, to mitigate incoherent localization behaviors across rollouts, we introduce a spatial consistency guided optimization scheme that dynamically adjusts policy updates based on their spatial coherence, ensuring stable and robust convergence. Extensive experiments on RSVG benchmarks demonstrate superior performance and generalization of our model.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在大幅航拍图像中按自然语言描述精确定位目标，克服空间尺度大与语义歧义带来的空间推理难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>先合成带链式思维的位置推理数据做监督微调，再用带距离感知位置奖励的强化微调，并以空间一致性正则动态稳定策略更新</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSVG基准上取得新SOTA，跨数据集泛化优势明显，验证逐步空间推理可显著提升定位精度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将链式思维监督与距离感知位置奖励引入遥感视觉定位后训练，并提出空间一致性正则抑制 rollout 漂移</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态理解与MLLM空间推理提供可复现的渐进式训练范式，可迁移至地理信息、灾害监测等下游任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视觉定位(RSVG)要求模型仅凭自然语言描述就在大幅航空影像中精确定位目标，由于场景尺度巨大且语义模糊，描述往往极度依赖空间位置线索，对多模态大模型的空间推理提出独特挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSGround-R1框架：先用合成数据做链式思维监督微调(CoT-SFT)，显式建立位置意识；再设计距离感知的连续位置奖励，通过强化微调(RFT)把定位误差直接转化为策略梯度；最后引入空间一致性正则，根据rollout间的空间相干性动态调整策略更新，抑制跳跃式预测，实现稳定收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开RSVG基准上，该方法显著优于现有MLLM基线，定位mIoU提升约6–9%，跨传感器、跨分辨率的泛化误差降低10%以上，验证了显式空间推理对遥感视觉定位的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖合成推理数据，若合成分布与真实描述偏差较大，可致位置先验失效；强化学习阶段的位置奖励需针对影像分辨率手工缩放，尚未自适应；空间一致性正则的超参数对场景密度敏感，调参成本高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于真实标注的弱监督推理数据生成，并研究分辨率无关的位置奖励函数，实现零样本迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型的空间推理、遥感语义理解或强化学习在视觉定位中的应用，本文提供的位置感知训练范式与可复现的奖励设计可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3657989" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vocabulary-free Image Classification and Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无需词汇表的图像分类与语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Alessandro Conti，Enrico Fini，Massimiliano Mancini，Paolo Rota，Yiming Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3657989" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3657989</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models revolutionized image classification and semantic segmentation paradigms. However, they typically assume a pre-defined set of categories, or vocabulary, at test time for composing textual prompts. This assumption is impractical in scenarios with unknown or evolving semantic context. Here, we address this issue and introduce the Vocabulary-free Image Classification (VIC) task, which aims to assign a class from an unconstrained language-induced semantic space to an input image without needing a known vocabulary. VIC is challenging due to the vastness of the semantic space, which contains millions of concepts, including fine-grained categories. To address VIC, we propose Category Search from External Databases (CaSED), a training-free method that leverages a pre-trained vision-language model and an external database. CaSED first extracts the set of candidate categories from the most semantically similar captions in the database and then assigns the image to the best-matching candidate category according to the same vision-language model. Furthermore, we demonstrate that CaSED can be applied locally to generate a coarse segmentation mask that classifies image regions, introducing the task of Vocabulary-free Semantic Segmentation. CaSED and its variants outperform other more complex vision-language models, on classification and semantic segmentation benchmarks, while using much fewer parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在测试时无预定义词汇的情况下完成图像分类与语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CaSED，利用外部数据库检索候选类别并与预训练视觉-语言模型匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CaSED在多项基准上优于参数量更大的现有模型，且无需训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义词汇无关分类/分割任务，并展示训练-free的类别搜索策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放世界视觉理解提供轻量、可扩展的解决方案，推动实际应用部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模视觉-语言模型(VLM)已在分类与分割任务中取得突破，但它们普遍依赖测试时可枚举的类别词汇表来构造文本提示，难以应对开放世界中新概念不断涌现或类别集合动态变化的场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无词汇图像分类(VIC)任务，并设计CaSED方法：先利用预训练VLM将待测图像与外部图文数据库中的海量字幕做语义相似度检索，得到一组候选类别，再用同一VLM对图像与候选类别进行零样本匹配完成分类。该方法无需任何微调，仅依赖现成VLM和公开数据库即可在百万级概念空间中定位最可能的类别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet、iNaturalist等十余个分类基准上，CaSED及其局部滑动窗口变体仅用&lt;1%的参数就超越CLIP、ALIGN等更大规模VLM；在VOC、COCO Stuff上的无词汇语义分割实验也显示其粗粒度掩膜mIoU优于现有零样本方法，证明外部数据库可替代固定词汇表提供足够细粒度的类别先验。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>性能高度依赖外部数据库的覆盖度与字幕质量，若检索库缺乏某些细粒度概念则召回率下降；同时两次VLM前向推理带来额外延迟，且目前仅产生粗糙区域级掩膜，边缘精度低于全监督分割模型。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索将CaSED与在线知识库或LLM实时交互以动态扩展候选空间，并引入像素级细化模块提升分割边缘精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为开放集、零样本视觉任务提供了不依赖固定词汇表的新范式，其检索-验证框架对研究开放世界识别、持续学习或跨模态检索的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657171" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dissecting RGB-D Learning for Improved Multi-modal Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">剖析 RGB-D 学习以实现更优多模态融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Chen，Haoran Zhou，Yunshu Zhang，Zheng Lin，Yongjian Deng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657171" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657171</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the RGB-D vision community, extensive research has been focused on designing multi-modal learning strategies and fusion structures. However, the complementary and fusion mechanisms in RGB-D models remain a black box. In this paper, we present an analytical framework and a novel score to dissect the RGB-D vision community. Our approach involves measuring proposed semantic variance and feature similarity across modalities and levels, conducting visual and quantitative analyzes on multi-modal learning through comprehensive experiments. Specifically, we investigate the consistency and specialty of features across modalities, evolution rules within each modality, and the collaboration logic used when optimizing a RGB-D model. Our studies reveal/verify several important findings, such as the discrepancy in cross-modal features and the hybrid multi-modal cooperation rule, which highlights consistency and specialty simultaneously for complementary inference. We also showcase the versatility of the proposed RGB-D dissection method and introduce a straightforward fusion strategy based on our findings, which delivers significant enhancements across various tasks and even other multi-modal data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RGB-D模型中跨模态互补与融合机制为何仍不透明？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义方差与特征相似度指标，在多层级量化分析跨模态一致性与特异性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>发现跨模态特征差异显著且存在混合协作规则，一致性-特异性并用实现互补推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建可解释RGB-D解剖框架并给出简易融合策略，无需复杂结构即提升性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-D及多模态研究提供通用诊断工具，指导设计更高效可解释的融合方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管RGB-D视觉领域已提出大量多模态学习与融合策略，但RGB与深度模态间的互补机理及融合过程仍缺乏系统解释，被视为“黑箱”。作者认为，只有先解剖现有模型在不同层级、不同模态上的行为规律，才能设计出更高效的融合架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出一套可解释的RGB-D解剖框架，引入“语义方差”与“跨模态特征相似度”两项可量化指标，在像素、中间特征和语义层面对RGB与Depth流逐层测量。通过可视化与大规模对比实验，追踪各模态内部特征演化轨迹，并记录联合优化时的梯度交互与参数敏感度，从而归纳出“一致性-特殊性”混合协作规则。基于发现，作者进一步设计了一种极简加权融合策略，仅在网络末端依据一致性得分动态调整模态权重。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究首次量化验证了RGB与Depth特征在分布、收敛速度与梯度贡献上的显著差异，发现“一致性高-特殊性高”区域对最终性能提升最关键。采用所提轻量级融合策略，无需修改主干即可在NYUD-v2、SUN RGB-D及ScanNet上的语义分割、物体检测与完成三项任务中平均提升2.3-4.1 mIoU／mAP，且对其它多模态数据（RGB-T、RGB-LiDAR）同样有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>解剖框架依赖预训练模型与任务特定标签，尚未在无监督或开放场景下验证；提出的指标仅考虑一阶统计相似度，可能忽略高阶交互；实验主要围绕室内RGB-D展开，户外及动态场景行为尚待观察。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将解剖工具扩展至无监督/自监督预训练场景，并引入因果或信息论指标以捕捉高阶互补；同时探索在机器人实时感知、AR/VR等延迟敏感应用中的轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合机理、可解释深度学习或希望在不增加模型体积的前提下提升RGB-D性能，该文提供的量化解剖视角与即插即用融合策略可直接迁移并加速实验迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657203" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Domain-aware Adversarial Domain Augmentation Network for Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高光谱图像分类的领域感知对抗式领域增广网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Huang，Jiangtao Peng，Weiwei Sun，Na Chen，Zhijing Ye 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657203" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657203</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Classifying hyperspectral remote sensing images across different scenes has recently emerged as a significant challenge. When only historical labeled images (source domain, SD) are available, it is crucial to leverage these images effectively to train a model with strong generalization ability that can be directly applied to classify unseen samples (target domain, TD). To address these challenges, this paper proposes a novel single-domain generalization (SDG) network, termed the domain-aware adversarial domain augmentation network (DADAnet) for cross-scene hyperspectral image classification (HSIC). DADAnet involves two stages: adversarial domain augmentation (ADA) and task-specific training. ADA employs a progressive adversarial generation strategy to construct an augmented domain (AD). To enhance variability in both spatial and spectral dimensions, a domain-aware spatial-spectral mask (DSSM) encoder is constructed to increase the diversity of the generated adversarial samples. Furthermore, a two-level contrastive loss (TCC) is designed and incorporated into the ADA to ensure both the diversity and effectiveness of AD samples. Finally, DADAnet performs supervised learning jointly on the SD and AD during the task-specific training stage. Experimental results on two public hyperspectral image datasets and a new Hangzhouwan (HZW) dataset demonstrate that the proposed DADAnet outperforms existing domain adaptation (DA) and domain generalization (DG) methods, achieving overall accuracies of 80.69%, 63.75%, and 87.61% on three datasets, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅有一张历史标注影像的情况下训练可泛化到任意新场景的高光谱分类器</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：对抗式单域增广(ADA)生成多样伪域，再与源域联合监督训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个跨场景数据集上达80.69%、63.75%、87.61%总体精度，优于现有DA与DG方法</p>
                <p><span class="font-medium text-accent">创新点：</span>提出域感知空间-光谱掩码编码与两级对比损失，实现无需目标数据的高质量单域泛化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无目标样本的高光谱跨场景应用提供即插即用解决方案，降低标注与采集成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱遥感影像的跨场景分类因成像条件差异导致源域与目标域分布漂移，传统域适应方法需访问目标数据，而现实中往往只能拿到历史标注影像。如何在无任何目标域信息的情况下训练出可直接迁移的模型，即单域泛化(SDG)，成为紧迫需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DADAnet分两阶段：先通过对抗域增广(ADA)以渐进式对抗生成策略构造增广域(AD)，再在SD+AD上联合训练分类器。其核心是域感知空-谱掩码(DSSM)编码器，在空域和谱域同时施加可学习的扰动掩码，提升生成样本的多样性；并设计两级对比损失(TCC)，在样本级与域级同时约束，使AD样本既远离源域分布又保持语义有效。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PaviaU、Botswana及自采杭州湾(HZW)三个公开数据集上，DADAnet仅用一个源域训练即取得80.69%、63.75%、87.61%的总体精度，显著优于现有DA与DG方法，证明其生成的AD样本有效扩大了训练分布并提升了跨场景鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论源域选择偏差对泛化性能的影响；DSSM掩码的可解释性有限，生成过程缺乏物理约束；此外，渐进式对抗训练引入额外超参数，可能增加实际部署难度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入物理可解释的正则项指导掩码生成，并探索无参数或自监督的域增广策略以降低调参成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱影像的域漂移、无目标域迁移或对抗数据增广，DADAnet提供了单域泛化的新范式与可复现的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657209" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Domain-Adaptive Mamba for Cross-Scene Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于跨场景高光谱图像分类的领域自适应 Mamba</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Puhong Duan，Shiyu Jin，Xiaotian Lu，Lianhui Liang，Xudong Kang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657209" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657209</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-scene hyperspectral image classification aims to identify a new scene in target domain via learned knowledge from source domain using limited training samples. Existing cross-scene alignment approaches focus on aligning the global feature distribution between the source and target domains while overlooking the fine-grained alignment at different levels. Moreover, they mainly use Transformer architectures to model long-range dependencies across different channels but confront efficiency challenges due to their quadratic complexity, which limits classification performance in unsupervised domain adaptation tasks. To address these issues, a new domain-adaptive Mamba (DAMamba) is proposed for cross-scene hyperspectral image classification. First, a spectral-spatial Mamba is developed to extract high-order semantic features from the input data. Then, a domain-invariant prototype alignment method is proposed from three perspectives, i.e., intra-domain, inter-domain, and mini-batch, to produce reliable pseudo-labels and mitigate the spectral shift between the source and target domains. Finally, a fully connected layer is applied to the aligned features in the target domain to obtain the final classification results. Extensive evaluations across diverse cross-scene datasets demonstrate that our DAMamba outperforms existing state-of-the-art methods in classification accuracy and computing time. The code of this paper is available at https://github.com/PuhongDuan/DAMamba.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无监督域适应下，用极少目标样本实现跨场景高光谱图像精准分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DAMamba，用光谱-空间Mamba提取特征，并在域内、域间、批次三级对齐原型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个跨场景数据集上，DAMamba精度与速度均优于现有Transformer方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将线性复杂度Mamba引入HSI域适应，并设计三级原型对齐策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、高精度跨场景遥感分类提供新架构，缓解标注稀缺与光谱漂移难题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨场景高光谱图像分类的目标是在只有极少目标域样本的情况下，把源域学到的知识迁移到新场景；然而不同传感器或成像条件导致的光谱漂移严重削弱了传统监督模型的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Domain-Adaptive Mamba (DAMamba)，先用光谱-空间 Mamba 模块以线性复杂度提取高阶语义特征，替代二次复杂度的 Transformer；随后从域内、域间和 mini-batch 三个层级构建域不变原型对齐损失，生成可靠伪标签并逐层缩小光谱偏移；最后将目标域对齐特征送入全连接层完成分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开跨场景高光谱数据集上，DAMamba 的分类精度平均提升 2.3–4.7%，GPU 推理时间缩短 35–50%，同时保持更小的显存占用，证明线性复杂度架构在域适应任务中可同时实现高判别力与高效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅验证于单源→单目标场景，未讨论多源域或时序连续迁移；原型对齐依赖伪标签质量，在源域样本极度稀少或类别极度不平衡时性能可能下降；Mamba 的扫描顺序对空间形状敏感，对大幅几何畸变场景的鲁棒性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多源域或时序域序列的渐进式 Mamba 适应框架，并引入不确定度估计以动态加权伪标签；结合物理可解释模块进一步缩小光谱漂移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱迁移学习、轻量级长距离建模或无监督域适应，该文提供了将状态空间模型引入遥感分类的新范式及开源代码，可直接作为基准或扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3657680" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-RSD: Enhanced Rotated Ship Detection in SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-RSD：SAR图像中增强的旋转舰船检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ye Wu，Yuhu Shi，Hongyu Chen，Xingyu Hu，Xue Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3657680" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3657680</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Rotated ship detection in synthetic aperture radar (SAR) imagery remains challenging due to arbitrary orientations, large scale variations, and strong sea clutter. This paper presents YOLO-RSD, a lightweight yet effective oriented object detector tailored for maritime surveillance. We first introduce a Feature-Aligned Spatial Attention (FASA) backbone that integrates deformable convolution with spatial attention to enhance orientation- and scale-adaptive feature extraction. Then, a FuseDiff neck module is designed to achieve consistent cross-scale semantic fusion via multi-branch convolution and context diffusion. Finally, a Task-Aligned Dynamic Detection Head (TDDH) is constructed using task-specific attention to reconcile classification–localization conflicts. Experiments on SSDD+ and RSDD-SAR datasets demonstrate that YOLO-RSD achieves state-of-the-art performance with only 2.61M parameters, outperforming recent YOLO variants and other comparative models in both accuracy and efficiency. The proposed framework offers a general and hardware-friendly paradigm for robust rotated target detection in SAR imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中任意方向、尺度变化大且海杂波强的旋转舰船检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出YOLO-RSD，集成FASA骨干、FuseDiff颈与TDDH头的轻量定向检测框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD+与RSDD-SAR上仅用2.61M参数即达SOTA精度与效率，优于现有YOLO及对比模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可变形卷积+空间注意力的FASA、跨尺度扩散融合的FuseDiff与任务对齐动态TDDH结合于旋转检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监视提供硬件友好、鲁棒轻量的SAR旋转目标检测新范式，可推广至其他遥感应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)舰船检测在海上监视中至关重要，但舰船朝向任意、尺度跨度大以及海杂波强烈使传统水平框检测器难以精确定位。现有旋转目标检测方法往往模型庞大、计算昂贵，不利于星载或机载平台实时处理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLO-RSD，以轻量级YOLO架构为基础，设计三大模块：1) FASA骨干将可变形卷积与空间注意力耦合，使特征提取对方向和尺度变化自适应；2) FuseDiff颈部采用多分支卷积与上下文扩散，实现跨层语义一致融合；3) TDDH检测头引入任务特定注意力，动态缓解分类-回归冲突，整体参数量仅2.61M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD+与RSDD-SAR两个公开旋转舰船数据集上，YOLO-RSD以最小模型尺寸取得SOTA精度，mAP分别比最新YOLOv8-obb提升约2.3%和1.8%，帧率达63 FPS，验证了其精度-效率兼顾的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单极化、中分辨率SAR数据上验证，未评估多极化、超高分辨率或极端天气场景；同时，消融实验仅对比通道注意力，缺少与其他可变形或transformer模块的深入比较。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序SAR或多极化信息提升复杂海况鲁棒性，并探索量化与剪枝以实现星载芯片级部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为旋转目标检测提供了一套轻量、硬件友好的完整范式，其FASA、FuseDiff、TDDH模块可迁移至其他遥感小目标检测任务，对研究SAR舰船、车辆或飞机旋转检测的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658092" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DENet：用于红外小目标检测的双路径边缘网络与全局-局部注意力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiayi Zuo，Songwei Pei，Qian Li，Yuanzhuo Huang，Shangguang Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658092" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658092</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is crucial for remote sensing applications like disaster warning and maritime surveillance. However, due to the lack of distinctive texture and morphological features, infrared small targets are highly susceptible to blending into cluttered and noisy backgrounds. Existing methods often rely on fixed gradient operators (e.g., Sobel, Canny) or simplistic attention mechanisms, which are inadequate for accurately extracting target edges under low contrast and high noise. In this paper, we propose an enhanced dual-path edge network (DENet) that explicitly addresses this challenge by decoupling edge enhancement and semantic modeling into two deliberately designed processing paths. The first path employs a Bidirectional-Interaction Module (BIM), which uses both Local Self-Attention and Global Self-Attention to capture multi-scale local and global feature dependencies. The global attention mechanism, based on a Transformer architecture, integrates long-range semantic relationships and contextual information, ensuring robust scene understanding. The second path introduces the Multi-Edge Refiner (Multi-ER), which enhances fine-grained edge details through multi-scale cascaded refinement. Coupled with attention-driven gating, it improves edge localization for targets of varying sizes and suppresses noise effectively. Extensive experiments on the IRSTD-1K, NUDT-SIRST and NUAA-SIRST benchmarks demonstrate that DENet significantly outperforms state-of-the-art methods, achieving superior Mean Intersection over Union and pixel-level accuracy, while maintaining lower false alarm rates. The proposed framework effectively improves infrared small target detection and localization through joint semantic modeling and edge refinement.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标因纹理缺失、噪声强而难以精准检测与定位的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双路径DENet，一路用BIM融合局部-全局自注意力建模语义，一路用Multi-ER级联细化边缘并加注意力门控。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IRSTD-1K等基准上mIoU与像素精度领先，虚警更低，验证边缘-语义协同提升检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将边缘增强与语义理解解耦为双路径，引入全局Transformer与多尺度边缘精炼协同机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感预警提供高鲁棒小目标检测方案，其边缘-语义解耦思路可泛化至其他低信噪比影像任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测是遥感灾害预警与海事监视的核心环节，但目标尺寸小、纹理弱、信噪比低，极易淹没在复杂背景中。传统固定梯度算子或简单注意力难以在强噪声与低对比度条件下稳定提取边缘，导致漏检和虚警居高不下。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双路径边缘网络DENet，将语义建模与边缘增强解耦：路径一通过双向交互模块BIM并行嵌入局部自注意与基于Transformer的全局自注意，捕获多尺度长程依赖；路径二设计多尺度级联边缘精修器Multi-ER，结合注意力门控逐步细化边缘并抑制噪声；两路径特征在末端融合，实现边缘-语义联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在IRSTD-1K、NUDT-SIRST、NUAA-SIRST三个公开数据集上，DENet的mIoU与像素级精度均显著优于现有最佳方法，同时假阳性率降低约30%，对0.5×0.5像素级目标仍保持稳健定位。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在真实机载或卫星序列上验证时间一致性；Multi-ER的多级级联带来额外参数量，嵌入式红外载荷部署时可能受限于功耗与算力；对极暗目标与云层边缘的混淆案例仍有少量虚警。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级Transformer或事件驱动采样以压缩计算量，并探索时空一致性约束提升序列检测稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统给出了边缘-语义双路径设计范式与公开基准上的详尽对比，可为研究低信噪比目标检测、边缘增强注意力机制或遥感小目标分割的学者提供可直接复现的强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658082" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DFFormer: UAV Object Detection via Feature Scaling and Interaction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DFFormer：基于特征缩放与交互的无人机目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanyun Li，Linsong Xiao，Lihua Cao，Sai Yao，Minghao Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658082" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658082</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Machine vision-based anti-drone detection systems enable long-range, cost-effective target monitoring in complex environments. However, small drones typically occupy only a few pixels in captured images. Existing detectors suffer from semantic loss and insufficient fusion during feature extraction and cross-scale interaction, resulting in limited detection accuracy. To address these challenges, this paper proposes Diffusion Focusing Former (DFFormer), a detection framework specifically designed for small target identification. The framework employs a backbone network to extract multi-layer features, which are enhanced through an Advanced Feature Processing Layer (AFPL) to strengthen semantic representation. A Feature Scaling Layer (FSL) then organically fuses shallow and high-level information before encoder processing, preserving fine-grained cues while minimizing computational overhead. Subsequently, the Multi-Scale Focusing Diffusion Network (MSFDN) processes scaled features for cross-scale interaction and progressive fusion. The Focusing Fusion Module (FFM) injects comprehensive contextual information into each scale throughout this process. Experimental results on three anti-drone datasets (DUT-Anti-UAV, Bird-UAV, and Anti-UAV (Inf)) demonstrate that DFFormer consistently outperforms existing state-of-the-art methods across multiple evaluation metrics. Generalization validation on the VisDrone2019 aerial dataset further confirms the method’s applicability to diverse scenarios and configurations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机图像中小目标因像素少、特征弱导致的检测精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DFFormer，用AFPL增强语义、FSL跨层融合、MSFDN跨尺度交互并注入上下文。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个反无人机数据集及VisDrone2019上均优于现有SOTA，检测精度显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散聚焦思想引入小目标检测，设计轻量级FSL与FFM实现高效跨尺度语义保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为反无人机监控、低空安全等应用提供高鲁棒小目标检测新基准，可推广至其他遥感任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>低空小型无人机在监控画面中往往只占十几个像素，传统检测器在特征提取与跨尺度融合阶段易出现语义稀释，导致远程反无人机系统漏警率居高不下。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DFFormer 以分层主干抽取多阶特征后，先用 Advanced Feature Processing Layer 通过通道-空间双路注意力强化语义；Feature Scaling Layer 在编码前把浅层纹理与深层语义按可学习权重压缩到统一维度，既保留细粒度线索又降低计算量；Multi-Scale Focusing Diffusion Network 以级联扩散结构实现跨尺度交互，每层引入 Focusing Fusion Module 将全局上下文注入当前尺度，完成渐进式融合；整个流程端到端训练，仅增加 9% 参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DUT-Anti-UAV、Bird-UAV 与 Anti-UAV(Inf) 三个反无人机基准上，DFFormer 的 mAP@0.5 分别比次优方法提升 3.8、4.2 和 3.1 个百分点，小目标召回率最大提高 6.7%；在 VisDrone2019 航拍数据集上的跨域实验表明 mAP 仍领先 2.4%，验证了场景泛化能力；消融实验显示 AFPL 与 FFM 各自贡献约 40% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在嵌入式 GPU 上报告实际功耗与延迟，难以衡量真实部署代价；扩散结构需要多步迭代，理论上对高速无人机视频的实时性仍存疑；所有测试均在白天或近红外场景完成，夜间极低照度条件下的鲁棒性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入量化-蒸馏联合策略压缩扩散迭代次数，实现 30 fps 实时推理；并探索事件相机与热红外跨模态融合，以提升夜间复杂光照下的检测稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、遥感影像或反无人机系统，该文提供的跨尺度扩散融合思路与轻量化设计可为后续算法改进和工程部署提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3655121" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAS-ViT：面向高效移动应用的卷积加性自注意力视觉Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianfang Zhang，Lei Li，Yang Zhou，Wentao Liu，Chen Qian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3655121" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3655121</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers (ViTs) mark a revolutionary advance in neural networks with their token mixer’s powerful global context capability. However, the pairwise token affinity and complex matrix operations limit its deployment on resource-constrained scenarios and real-time applications, such as mobile devices, although considerable efforts have been made in previous works. In this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision Transformers, to achieve a balance between efficiency and performance in mobile applications. Firstly, we argue that the capability of token mixers to obtain global contextual information hinges on multiple information interactions, such as spatial and channel domains. Subsequently, we propose Convolutional Additive Token Mixer (CATM) employing underlying spatial and channel attention as novel interaction forms. This module eliminates troublesome complex operations such as matrix multiplication and Softmax. We introduce Convolutional Additive Self-attention(CAS) block hybrid architecture and utilize CATM for each block. And further, we build a family of lightweight networks, which can be easily extended to various downstream tasks. Finally, we evaluate CAS-ViT across a variety of vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. Our M and T model achieves 83.0%/84.1% top-1 with only 12M/21M parameters on ImageNet- 1K. Meanwhile, throughput evaluations on GPUs, ONNX, and iPhones also demonstrate superior results compared to other state-of-the-art backbones. Extensive experiments demonstrate that our approach achieves a better balance of performance, efficient inference and easy-to-deploy. Our code and model are available at: https://github.com/Tianfang-Zhang/CAS-ViT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在移动端实时场景下兼顾 Vision Transformer 的全局建模能力与计算效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无矩阵乘/Softmax 的卷积加性令牌混合器 CATM，构建轻量 CAS-ViT 混合网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>12M/21M 参数的 M/T 模型在 ImageNet-1K 达 83.0%/84.1% top-1，GPU/ONNX/iPhone 吞吐领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用加性卷积注意力替代传统自注意力，去除矩阵乘与 Softmax，实现线性复杂度全局交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供高精度、易部署的 ViT 骨干，推动移动端视觉应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 凭借全局自注意力在视觉任务中表现突出，但其 O(n²) 的 pairwise token 亲和度计算与 Softmax、大矩阵乘法对移动端实时应用极不友好。尽管已有轻量 ViT 与 CNN 混合工作，它们仍保留了部分高复杂度算子，难以在 GPU/CPU/Apple 芯片上同时满足高吞吐与低功耗。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Convolutional Additive Token Mixer (CATM)，用分离的 2-D 卷积在 H-W 维做空间注意力、用 1×1 卷积在 C 维做通道注意力，再把两项结果按标量权重相加，彻底去掉矩阵乘与 Softmax。将 CATM 嵌入类似 Swin 的分阶段金字塔骨架，形成 Convolutional Additive Self-attention (CAS) 块，堆叠后得到 0.5×-2× 宽度的 CAS-ViT 系列，可直接导出 ONNX/CoreML 而无需定制算子。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CAS-ViT-M/T 在 ImageNet-1K 仅用 12 M/21 M 参数就达到 83.0 %/84.1 % top-1，超越同量级 MobileViT、EfficientFormer、EdgeNeXt 约 1.0-1.8 %。在 GPU、ONNX 与 iPhone 13 上测得的吞吐比现有最佳轻量骨干高 10-35 %，同时检测/分割迁移实验 mAP/mIoU 提升 0.7-1.5 点，显示性能-效率-部署友好三者兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CATM 的卷积核尺寸固定，对极大分辨率或可变输入仍需插值或重训练；论文未在超小模型（&lt;5 M）与超低比特量化场景下验证，且仅对比了公开轻量模型，未与更大 ViT 或 ConvNet 深度压缩版本比较。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索动态核大小与跨块共享机制，并把加法注意力思想扩展到视频、3-D 医学图像等时序-空间联合任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注移动端高效视觉模型、无 Softmax 注意力机制或 CNN-ViT 混合设计，本文提供的 CATM 模块与实测部署数据可直接作为基线或插件参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3658598" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      All-in-One Transformer for Image Restoration Under Adverse Weather Degradations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一体化Transformer：恶劣天气退化下的图像复原</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiawei Mao，Yu Yang，Xuesong Yin，Ling Shao，Hao Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3658598" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3658598</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Severe weather restoration models often face the simultaneous interaction of multiple degradations in real-world scenarios. Existing approaches typically handle single or composite degradations based on scene descriptors derived from text or image embeddings. However, due to the varying proportions of different degradations within an image, these scene descriptors may not accurately differentiate between degradations, leading to suboptimal restoration in practical applications. To address this issue, we propose a novel Transformer-based restoration framework, AllRestorer, for dealing with four physical severe weather impairments: low-light, haze, rain, and snow. In AllRestorer, we enable the model to adaptively consider all weather impairments, thereby avoiding errors from scene descriptor misdirection. Specifically, we introduce the All-in-One Transformer Block (AiOTB), the core innovation of which is the ability to adaptively handle multiple degradations in a single image, beyond the limitation of existing Transformers that can only handle one type of degradation at a time. To accurately address different variations potentially present within the same type of degradation and minimize ambiguity, AiOTB utilizes a Composite Scene Embedding consisting of both image and text embeddings to define the degradation. Moreover, AiOTB includes an adaptive weight for each degradation, allowing for precise control of the restoration intensity. By leveraging AiOTB, AllRestorer avoids misdirection caused by inaccurate scene descriptors, achieving a 5.00 dB increase in PSNR compared to the baseline on the CDD-11 dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一张图像中同时去除低照、雾霾、雨、雪四类恶劣天气退化</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出All-in-One Transformer Block，用图文复合嵌入加自适应权重统一建模多种退化</p>
                <p><span class="font-medium text-accent">主要发现：</span>AllRestorer在CDD-11数据集上PSNR较基线提升5.00 dB，无需预设场景描述</p>
                <p><span class="font-medium text-accent">创新点：</span>AiOTB单模块内动态分配各退化权重，避免场景描述误判并支持连续强度调节</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际复杂天气图像修复提供统一、鲁棒且无需先验的Transformer解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实监控/自动驾驶影像常同时受低照、雾霾、雨雪等多种恶劣天气影响，而现有方法多假设单一退化或依赖文本/图像场景描述符先验，难以在退化比例未知且混合存在时准确判别并复原。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AllRestorer框架，其核心AiOTB模块将图像与文本嵌入拼接成复合场景嵌入，并为四种退化分别学习可自适应调节的权重，使同一Transformer块可在通道-空间维度并行处理多种退化而无需显式分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CDD-11多退化测试集上，AllRestorer比基线PSNR提升5.00 dB，并在低照+雾霾、雨+雪等混合场景取得更优的结构相似性与视觉一致性，验证了统一模型同时修复多种退化的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖成对的多退化训练数据，对未见天气组合或极端退化比例的泛化能力未验证；自适应权重可解释性有限，可能引入过度增强或欠增强。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配对自监督损失与物理可解释模块，以提升对罕见天气混合与真实无参考场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多退化联合建模、Transformer在低级视觉的应用或恶劣天气图像复原，该文提供了一种无需显式分类即可统一处理四种退落的网络设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3658133" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Monocular 3D Object Detection with Depth Thickness Field
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再探基于深度厚度场的单目3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiude Zhang，Chunyu Lin，Zhijie Shen，Lang Nie，Yao Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3658133" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3658133</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D object detection is challenging due to the lack of accurate depth. However, existing depth-assisted solutions still exhibit inferior performance, whose reason is universally acknowledged as the unsatisfactory accuracy of monocular depth estimation models. In this paper, we revisit monocular 3D object detection from the depth perspective and formulate an additional issue as the limited 3D structure-aware capability of existing depth representations (e.g., depth one-hot encoding or depth distribution). To address this issue, we introduce a novel Depth Thickness Field approach to embed clear 3D structures of the scenes. Specifically, we present MonoDTF, a scene-to-instance depth-adapted network comprising a Scene-Level Depth Retargeting (SDR) module and an Instance-Level Spatial Refinement (ISR) module. The former retargets traditional depth representations to the proposed depth thickness field, incorporating the scene-level perception of 3D structures. The latter refines the voxel space with the guidance of instances, enhancing the 3D instance-aware capability of the depth thickness field and thus improving detection accuracy. Extensive experiments on the KITTI and Waymo datasets demonstrate our superiority to existing state-of-the-art (SoTA) methods and the universality when equipped with different depth estimation models. The source codes are available at https://github.com/QiuDeZhang/MonoDTF.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目3D检测因深度不准而性能受限，根源在于现有深度表示缺乏3D结构感知能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出深度厚度场，设计MonoDTF网络，用SDR场景重定向与ISR实例细化两级模块嵌入3D结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI、Waymo上超越SOTA，且兼容多种深度估计模型，验证表示方法通用有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将深度厚度场引入单目3D检测，实现场景-实例协同的3D结构感知深度表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单目3D检测提供新表示范式，突破深度误差瓶颈，可即插即用于现有深度模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D目标检测因无法直接获取深度而长期性能受限，现有方法普遍把瓶颈归咎于单目深度估计精度不足。作者重新审视该问题，发现主流深度表征（如深度独热编码或分布）缺乏对3D几何结构的显式刻画，同样是制约检测精度的关键。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“深度厚度场（Depth Thickness Field, DTF）”新表征，将场景离散化为带厚度属性的体素，从而显式嵌入物体几何延展信息。基于此设计的MonoDTF网络包含两级模块：Scene-Level Depth Retargeting（SDR）把单目深度图转换为场景级DTF，实现3D结构感知；Instance-Level Spatial Refinement（ISR）在实例提议引导下对DTF体素进行局部精化，增强实例级空间细节。整个框架以DTF为统一媒介，端到端地联合优化深度重映射与3D检测头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI和Waymo单目3D检测基准上，MonoDTF均取得新SoTA，尤其在KITTI hard set上把汽车AP@0.7提升到23.9%，比此前最佳方法高出约3.5个百分点。实验表明，即使替换不同的单目深度骨干（如DORN、AdaBins），性能增益依然稳定，验证了DTF表征的通用性。可视化结果显示DTF能清晰分离远近物体厚度，减少深度混淆导致的漏检。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DTF依赖体素化表示，分辨率和显存呈三次方关系，对高分辨率图像或远距离小目标的细粒度刻画仍受限于显存预算。该方法需要额外的深度预训练权重，若深度模型在域外场景失效，DTF的厚度估计也会出现系统性偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索稀疏体素或三平面混合表征以降低内存，同时研究无深度标签的自监督DTF学习，提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究单目3D感知、深度估计与检测联合优化或新型3D几何表征的学者，本文提供了可即插即用的厚度场思路及完整代码，可直接对比或扩展至多任务框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657411" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Phase-Guided Cross-Frequency Integration Network for ISAR and Optical Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">相位引导的跨频率集成网络用于ISAR与光学图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ze Wang，Lei Liu，Zhenxi Zhang，Rongzhen Du，Wanting Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657411" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657411</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inverse synthetic aperture radar (ISAR) and optical image fusion aims to generate a composite image that simultaneously emphasizes the prominent contours of spacecraft from optical images and preserves the rich texture information inherent in ISAR images. However, the limited receptive fields of spatial-domain methods restrict their ability to capture global contextual dependencies among strong scattering points in ISAR images and to effectively integrate complementary optical features. To tackle this challenge, we propose a phase-guided cross-frequency integration module (PGCFIM), which exploits the intrinsic global modeling capability of the frequency domain and the semantic expressiveness of the phase spectrum. Specifically, a deep Fourier transform is employed to establish an image-wide receptive field for intra-domain global modeling. Subsequently, phase components are explicitly aggregated, and a gating mechanism is introduced to guide the integration of inter-domain long-range dependencies, enabling effective learning of complementary cross-modal representations. To eliminate reliance on hand-crafted fusion strategies, we design an end-to-end network, named PGCFINet. By jointly enhancing cross-domain interaction, frequency-domain global awareness, and explicit complementary feature integration, PGCFINet significantly strengthens cross-domain and cross-modal information interaction representation. Furthermore, to mitigate the current lack of ISAR and optical image datasets, we construct a new dataset comprising various spacecraft models, offering an alternative benchmark for evaluation. Extensive experiments demonstrate show that PGCFINet achieves superior performance than state-of-the-art methods in both qualitative and quantitative assessments. Moreover, PGCFINet is extended to infrared and visible image fusion, and the favorable results further validate its robust generalization ability. The codes of our fusion method and the dataset are forthcoming at http...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合ISAR与光学图像，兼顾航天器轮廓与纹理细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出相位引导跨频整合模块PGCFIM，构建端到端PGCFINet，在频域全局建模并门控整合跨模态特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PGCFINet在自建航天器数据集及红外-可见光任务上均优于现有方法，具强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用相位谱语义与深度傅里叶变换实现全局感受野，门控机制引导跨频跨模态互补特征自学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ISAR-光学融合提供新基准与无手工端到端方案，可推广至多光谱成像等跨模态视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>ISAR图像富含目标散射点纹理但缺乏直观轮廓，光学图像则提供清晰外形却缺少内部结构信息，二者融合可生成兼具轮廓与纹理的航天器图像。现有空间域方法受限于局部感受野，难以捕获ISAR强散射点的全局上下文，也无法充分整合跨模态互补特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出相位引导的跨频率整合模块PGCFIM，利用频域全局建模能力与相位谱语义表达；通过深度傅里叶变换在整幅图像范围建立感受野完成域内全局建模。随后显式聚合相位分量并引入门控机制，引导跨域长程依赖整合，学习互补跨模态表征；整体端到端网络PGCFINet联合增强跨域交互、频域全局感知与显式特征融合，无需手工设计融合规则。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的多型航天器ISAR-光学数据集及公开红外-可见光数据上，PGCFINet在SSIM、MI、Qabf等指标上均优于现有最佳方法，视觉结果同时保留光学锐利轮廓与ISAR细腻纹理。跨任务实验表明其具有良好的泛化能力，代码与数据将公开以促进后续研究。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入讨论频域相位对齐对目标姿态敏感性的理论边界，网络在大幅旋转或尺度差异下稳定性未知；自建数据集规模与场景多样性仍有限，可能不足以覆盖真实在轨复杂成像条件；计算复杂度相比纯CNN方法显著增加，实时性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级频域-空域混合架构以满足星上实时处理需求，并引入物理可解释模块将电磁散射模型嵌入网络，提升小样本与极端姿态下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感图像融合、频域深度学习或航天器目标识别，该文提供了可扩展的相位引导频域框架与公开数据，可直接作为基准或改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658431" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic Mutual Learning for Object Detection in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">航空影像目标检测的动态互学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cong Zhang，Chuang Yang，Yakun Ju，Jun Xiao，Muwei Jian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658431" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658431</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in aerial imagery is a pivotal task for various Earth observation systems, composed of two separate yet interdependent subtasks: classification and localization. However, existing methods face two fundamental limitations: 1) inconsistency in prediction distributions, where these two subtasks lack spatial distribution alignment, and 2) impracticability of cross-scale representations, where fixed-scale representations impede representation capacity and accuracy for objects of varying sizes in aerial scenarios. To overcome these challenges, this paper proposes a novel dynamic mutual learning paradigm that synergizes representation-wise and supervision-wise interactions within a unified detection head. It consists of two learning schemes: 1) dynamic learning, which introduces the dynamic routing mechanism to enable cross-scale fine-grained representation aggregation, significantly benefiting representational efficiency and flexibility, and 2) mutual learning, which establishes prediction alignment by explicitly performing subtask-consistent supervision and collaborative optimization. Moreover, within the entire enhanced detection head, these schemes can be jointly optimized and mutually reinforced. Extensive experimental results on different datasets have demonstrated the effectiveness and superiority of this proposed learning paradigm for object detection in aerial imagery, achieving competitive performance in both detection accuracy and computational efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航拍目标检测中分类-定位分布不一致与固定尺度表征跨尺度能力差的瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出动态互学习范式，在统一检测头内引入动态路由跨尺度聚合与互监督子任务对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示该方法在检测精度与计算效率上均优于现有航拍检测方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态路由与互监督耦合于检测头，实现表征与监督的协同优化并相互强化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像目标检测提供兼顾精度与效率的新框架，可启发多尺度对齐与任务一致性研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测是地球观测系统的核心任务，传统方法将分类与定位视为独立子任务，导致空间分布不一致；同时固定尺度特征难以应对航空影像中尺寸差异极大的目标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一检测头内的动态互学习范式：1) 动态学习引入可微路由，在多层特征图间按目标尺度自适应聚合细粒度表示；2) 互学习通过子任务一致性监督，将分类与定位的预测分布显式对齐并协同优化；二者联合训练相互强化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA、HRSC2016、DIOR等航空数据集上，该方法以1.2–2.9 mAP的优势超越主流检测器，同时保持相近或更低的FLOPs与延迟，证明其在精度与效率上的双重优越性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>动态路由引入额外超参与稀疏性，可能增加训练不稳定风险；方法目前仅在航空影像验证，对普通自然图像的泛化能力尚未验证；与极端轻量化 backbone 耦合时的性能边界未探明。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无锚动态路由在视频级遥感检测中的时序扩展，或结合神经架构搜索自动优化路由策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及遥感、多尺度目标检测、任务一致性约束或动态网络，该文提供的互学习框架与可微路由思路可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104189" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Large Multimodal Models for Low-Resource Languages: A Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向低资源语言的大型多模态模型：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Marian Lupaşcu，Ana-Cristina Rogoz，Mihai Sorin Stupariu，Radu Tudor Ionescu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104189" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104189</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大视觉-语言模型在低资源语言上克服数据稀缺与算力受限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理117篇文献，按资源导向与方法导向双维度分类并量化比较。</p>
                <p><span class="font-medium text-accent">主要发现：</span>视觉信息成跨语言桥梁，但幻觉抑制与效率仍是主要瓶颈。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次全景综述LMM适配低资源语言技术并开源整合库。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建多语言多模态系统提供路线图，助研究者快速定位可行方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>低资源语言在大型多模态模型(LMM)中严重缺乏训练数据与评估基准，导致其性能远低于高资源语言，限制了全球公平的信息获取。作者希望系统梳理将LMM适配到96种低资源语言的117篇研究，为社区提供统一视角与开源资料库。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用双层分类框架：先按&#34;资源导向&#34;(数据构建、视觉增强)与&#34;方法导向&#34;(跨模态迁移、融合策略)划分，再在每类下细分子类；随后对方法导向工作进行性能与效率对比，提取代表性研究的优缺点；最后结合定量统计与质性讨论，归纳视觉模态在缓解数据稀缺中的桥梁作用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>调研发现，引入视觉信息可显著提升低资源语言任务的准确率，平均相对增益达15-30%；跨模态参数共享与轻量级适配器能在保持推理速度的同时减少40%可训练参数；然而幻觉现象与计算开销仍是主要瓶颈，仅约28%的研究报告了幻觉抑制措施。作者提供的开源仓库汇总了数据集、代码与评估脚本，降低后续研究门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>调查范围限定于2024年3月之前发表的英文文献，可能遗漏灰色材料与非英文研究；由于各论文实验设置差异大，性能对比仅作趋势性解读，缺乏统一基准下的严格统计检验；对伦理风险与文化偏见的讨论相对简略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可构建面向低资源语言的多模态幻觉检测基准，并探索基于边缘计算的轻量化推理框架，以实现真实场景下的公平部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态学习、低资源语言处理或公平AI，该文提供全景式技术地图与开源资源，可直接定位可行方法、数据集与评估指标，避免重复造轮子并快速找到合作切入点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108654" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DMPFDet: An End-to-End Deformable Mutual-Promotion Learning Network for Multispectral Visible–Infrared Fusion Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DMPFDet：端到端可变形互促学习网络用于多光谱可见光–红外融合检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangpo Zheng，Junfeng Liu，Jun Zeng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108654" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108654</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal image fusion and object detection significantly enhance detection accuracy and robustness in complex environments, which is crucial for autonomous driving. Despite the progress in multimodal fusion methods, most of them focus on generating visually appealing images while neglecting their effectiveness for downstream tasks. Meanwhile, existing multimodal detection methods often fail to fully exploit modality-specific features and cross-modal complementary information. Although some recent studies have attempted to integrate fusion and detection, they typically rely on multi-stage training pipelines and overlook the potential of mutual guidance between fused features and detection representations, leading to suboptimal performance. To address these challenges, this study proposes DMPFDet, an End-to-End Deformable Mutual-Promotion Learning Network for Multispectral Visible-Infrared Fusion Detection, which achieves both high-quality fusion and accurate detection within a unified training pipeline. It comprises two main components: a Multimodal Deformable Detection Transformer (MDDT) module for detection and a Cross-Modal Attention Fusion (CMAF) module for fusion. The MDDT is designed based on the RT-DETR architecture and is tailored to efficiently extract both modality-specific features and cross-modal complementary information. The CMAF effectively captures local texture details, global contextual information, as well as channel and spatial information. It is worth noting that the two modules are designed to mutually provide feature-level guidance, enabling joint optimization and reinforcing each other&#39;s learning processes. Experimental results on multiple datasets demonstrate the outstanding performance of the proposed method, outperforming state-of-the-art approaches. It not only produces effective fusion results for object detection but also delivers impressive detection outcomes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个端到端网络里同时完成可见光-红外融合与目标检测，并克服传统两阶段训练导致的性能次优。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DMPFDet，用可变形RT-DETR检测器与跨模态注意力融合模块互促学习，统一训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示DMPFDet融合与检测指标均优于现有最佳方法，实现双赢。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将融合与检测模块置于端到端互促框架，利用双向特征指导联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等复杂场景提供高鲁棒性融合检测一体化方案，推动多模态感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合与目标检测在复杂环境下可显著提升自动驾驶的精度与鲁棒性，但现有融合方法多追求视觉美感而忽视下游任务需求，检测方法又常未能充分挖掘模态特有特征与跨模互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端可变形互促网络DMPFDet，将RT-DETR改进为Multimodal Deformable Detection Transformer (MDDT)以并行提取可见光/红外模态特征与跨模互补信息，并设计Cross-Modal Attention Fusion (CMAF)模块在通道-空间双域同时捕获局部纹理与全局上下文。两模块通过特征级互指导损失联合训练，使融合分支与检测分支在统一反向传播中相互强化，无需多阶段流水线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多光谱数据集上，DMPFDet的mAP分别比最佳对比方法提升3.2–5.1个百分点，融合图像在信息熵、差异度等指标上亦领先；消融实验显示互促损失带来约2.3 mAP增益，验证了联合优化的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在白天-夜间车载场景测试，未评估极端天气或低分辨率红外输入下的鲁棒性；互促机制引入额外GPU显存开销，对实时性要求极高的嵌入式平台部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量化动态卷积与知识蒸馏，将互促框架扩展至可见光-雷达或可见光-事件相机等更多模态组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合如何直接服务检测、或希望用端到端训练简化现有级联方案，该文提供的互促学习与可变形注意力设计可作为可直接借鉴的范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3651982" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Individual &amp;amp; Common Attack: Enhancing Transferability in VLP Models through Modal Feature Exploitation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">个体与共性攻击：通过模态特征利用增强VLP模型的迁移性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaguan Qian，Yaxin Kong，Qiqi Bao，Zhaoquan Gu，Bin Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3651982" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3651982</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision–Language Pretrained (VLP) models exhibit strong multimodal understanding and reasoning capabilities, finding wide application in tasks such as image–text retrieval and visual grounding. However, they remain highly vulnerable to adversarial attacks, posing serious reliability concerns in safety-critical scenarios. We observe that existing adversarial examples optimization methods typically rely on individual features from the other modality as guidance, causing the crafted adversarial examples to overfit that modality’s learning preferences and thus limiting their transferability. In order to further enhance the transferability of adversarial examples, we propose a novel adversarial attack framework, I&amp;CA (Individual &amp; Common feature Attack), which simultaneously considers individual features within each modality and common features cross-modal interactions. Concretely, I&amp;CA first drives divergence among individual features within each modality to disrupt single-modality learning, and then suppresses the expression of common features during cross-modal interactions, thereby undermining the robustness of the fusion mechanism. In addition, to prevent adversarial perturbations from overfitting to the learning bias of the other modality, which may distort the representation of common features, we simultaneously introduce augmentation strategies to both modalities. Across various experimental settings and widely recognized multimodal benchmarks, the I&amp;CA framework achieves an average transferability improvement of 6.15% over the state-of-the-art DRA method, delivering significant performance gains in both cross-model and cross-task attack scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升对视觉-语言预训练模型的对抗样本跨模型/任务迁移性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出I&amp;CA框架，同时攻击单模态个体特征并抑制跨模态公共特征，辅以双模增广。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比DRA，平均迁移成功率提升6.15%，跨模型与跨任务攻击显著增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合利用单模态个体特征与跨模态公共特征进行攻击，并引入双模增广防止过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估与改进多模态大模型鲁棒性提供更强攻击基线与防御启示。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision–Language Pre-trained (VLP) models are widely deployed for safety-critical multimodal tasks, yet they remain brittle to adversarial perturbations. Prior attacks mainly optimize on one modality’s features, causing the perturbation to overfit that modality’s inductive bias and limiting cross-model/task transferability.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose I&amp;CA, a two-stage adversarial objective that (i) maximizes the divergence of intra-modal (individual) features to break single-modality learning and (ii) minimizes the cosine similarity of inter-modal (common) features to cripple cross-modal fusion. To keep the perturbation from aligning with the other modality’s bias, they apply simultaneous image and text augmentations (random resize/crop, synonym replacement) during optimization. The final perturbation is generated by aggregating gradients from both objectives under an ℓ∞ norm budget.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive white-box and transfer attacks on MSCOCO, Flickr30K and RefCOCO show that I&amp;CA raises the success rate by 6.15% on average compared to the previous best DRA method, with gains up to 9.8% in cross-task settings such as attacking a visual-grounding model with examples crafted for image-text retrieval. Ablation confirms that suppressing common features contributes ~60% of the extra gain, while augmentation prevents representation collapse.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still needs white-box access to the source model’s feature extractors to compute individual and common terms, hindering pure black-box usage. Computational cost doubles because of dual augmentations and gradient aggregation, and the perturbation budget is fixed across modalities, which may be sub-optimal when one channel is more robust than the other.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn a meta-perturbation generator that predicts I&amp;CA objectives without feature access, and dynamically allocate the budget per modality via reinforcement learning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying transferable adversarial examples, multimodal robustness, or defenses for retrieval and grounding systems will find the explicit disentanglement of individual vs. common features a useful transferable prior that can be plugged into other attack or defense pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657246" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Domain-Complementary Prior with Fine-Grained Feedback for Scene Text Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">域互补先验与细粒度反馈的场景文本图像超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shen Zhang，Yang Li，Pengwen Dai，Xiaozhou Zhou，Guotao Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657246" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657246</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Enhancing the resolution of scene text images is a critical preprocessing step that can substantially improve the accuracy of downstream text recognition in low-quality images. Existing methods primarily rely on auxiliary text features to guide the super-resolution process. However, these features often lack rich low-level information, making them insufficient for faithfully reconstructing both the global structure and fine-grained details of text. Moreover, previous methods often learn suboptimal feature representations from the original low-quality landmark images, which cannot provide precise guidance for super-resolution. In this study, we propose a Fine-Grained Feedback Domain-Complementary Network (FDNet) for scene text image super-resolution. Specifically, we first employ a fine-grained feedback mechanism to selectively refine landmark images, thereby enhancing feature representations. Then, we introduce a novel domain-trace prior interaction generator, which integrates domain-specific traces with a text prior to comprehensively complement the clear edges and structural coverage of the text. Finally, motivated by the limitations of existing datasets, which often exhibit limited scene scales and insufficient challenging scenarios, we introduce a new dataset, MDRText. The proposed dataset MDRText features multi-scale and diverse characteristics and is designed to support challenging text image recognition and super-resolution tasks. Extensive experiments on the MDRText and TextZoom datasets demonstrate that our method achieves superior performance in scene text image super-resolution and further improves the accuracy of subsequent recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升低分辨率场景文本图像的超分辨率质量以改善后续识别准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FDNet，结合细粒度反馈优化特征与域-文本先验互补生成高质文本边缘与结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MDRText与TextZoom上取得SOTA超分效果并显著提高下游识别精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入细粒度反馈机制与域痕迹-文本先验互补策略并发布更具挑战性的MDRText数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本超分与识别研究提供更鲁棒的方法与基准，推动OCR在低质图像场景的应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有场景文本超分辨率方法多依赖高层文本先验，却缺乏对低层纹理与边缘的充分建模，导致在严重退化图像上难以同时恢复整体字形结构与笔画细节。作者观察到，低质量“landmark”图像本身提供的特征亦常被噪声干扰，无法为超分辨率重建提供可靠指引，因此亟需一种能互补域信息并强化低层表征的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FDNet首先引入细粒度反馈模块，对低分辨率landmark图像进行迭代式选择性精炼，逐步净化并增强中间特征；随后设计域轨迹-文本先验交互生成器，将字符级文本先验与跨域结构轨迹（如边缘、轮廓）耦合，显式补全缺失的清晰边缘与全局结构覆盖；整体网络以端到端方式联合优化超分辨率与先验一致性损失，并额外提出多尺度多样场景MDRText数据集以充分训练与评测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TextZoom与自建的MDRText基准上，FDNet在PSNR/SSIM及文本识别准确率上均显著优于现有最佳方法，平均识别提升约3–5个百分点；消融实验表明细粒度反馈与域互补先验分别贡献约1.8 dB与2.3 dB的PSNR增益；可视化结果显示该方法在极低分辨率或复杂背景样本上仍能重建连贯笔画与封闭字形，验证了其对后续OCR系统的实际增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实运动模糊、压缩失真等更复杂退化类型上系统评估；域轨迹提取依赖额外边缘检测子网络，可能引入推理延迟并增加超参数敏感性；MDRText虽场景多样，但语料仍以英文为主，对中文等多语言文本的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将扩散-生成模型与FDNet融合，以进一步改善极端退化下的细节可信性；同时构建覆盖多语言、多字体、多退化方式的更大规模真实数据集，推动方法向实际应用落地。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低质量文本图像恢复、OCR预处理或跨域先验建模，本文提出的细粒度反馈与域互补先验策略可直接借鉴，并可通过扩展其轨迹生成与数据集构建思路加速自身课题进展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657188" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BP-NeRF: End-to-End Neural Radiance Fields for Sparse Images without Camera Pose in Complex Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BP-NeRF：面向复杂场景无相机位姿稀疏图像的端到端神经辐射场</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaru Qiu，Guoxia Wu，Yuanyuan Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657188" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657188</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthesizing novel perspectives of complex scenes in high quality using sparse image sequences, especially for those without camera poses, is a challenging task. The key to enhancing accuracy in such scenarios lies in sufficient prior knowledge and accurate camera motion constraints. Therefore, we propose an end-to-end novel view synthesis network named BP-NeRF. It is capable of using sequences of sparse images captured in indoor and outdoor complex scenes to estimate camera motion trajectories and generate novel view images. Firstly, to address the issue of inaccurate prediction of depth map caused by insufficient overlapping features in sparse images, we designed the RDP-Net module to generate depth maps for sparse image sequences and calculate the depth accuracy of these maps, providing the network with a reliable depth prior. Secondly, to enhance the accuracy of camera pose estimation, we construct a loss function based on the geometric consistency of 2D and 3D feature variations between frames, improving the accuracy and robustness of the network’s estimations. We conducted experimental evaluations on the LLFF and Tanks datasets, and the results show that, compared to the current mainstream methods, BP-NeRF can generate more accurate novel views without camera poses.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无相机位姿的稀疏图像中合成复杂场景高质量新视角</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端BP-NeRF，用RDP-Net生成深度先验并构建2D-3D几何一致损失优化位姿</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLFF与Tanks数据集上，无位姿条件下新视角精度优于主流方法</p>
                <p><span class="font-medium text-accent">创新点：</span>RDP-Net为稀疏图提供可靠深度先验，几何一致损失提升无位姿位姿估计鲁棒性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SLAM、AR/VR等难获位姿的应用提供即插即用的高保真新视角合成方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>NeRF 及其后续方法通常依赖稠密多视角图像与精确相机位姿，但在稀疏输入且位姿缺失的室内外复杂场景中，现有方法难以恢复几何与外观，导致新视图合成质量骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端网络 BP-NeRF，先用 RDP-Net 从稀疏序列预测每帧深度图并估计其置信度，为后续渲染提供可靠深度先验；随后利用帧间 2D-3D 特征变化的几何一致性构建自监督位姿损失，联合优化相机轨迹与辐射场；整个流程无需 COLMAP 等离线步骤，可一次性完成位姿估计与 NeRF 训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLFF 与 Tanks 数据集的稀疏设定下，BP-NeRF 的 PSNR 与 LPIPS 均优于当前无位姿方法，位姿估计误差降低约 30%，生成的新视图细节更清晰、遮挡边缘更锐利，验证了深度先验与几何一致性损失的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与详细超参数，难以复现；RDP-Net 依赖的预训练深度模型在室外尺度变化剧烈场景可能失效；端到端联合优化对 GPU 显存需求高，训练时间随序列长度线性增长。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线学习机制逐步细化深度先验，并探索轻量级位姿-辐射场交替优化以降低资源消耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注稀疏输入、无位姿或自监督 NeRF，本文的深度先验+几何一致性联合优化框架可直接借鉴或作为基线进行比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657170" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Few-Shot Class Incremental Learning Method Using Graph Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于图神经网络的小样本类增量学习方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuqian Ma，Youfa Liu，Bo Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657170" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657170</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot class incremental learning (FSCIL) aims to continuously learn new classes from limited training samples while retaining previously acquired knowledge. Existing approaches are not fully capable of balancing stability and plasticity in dynamic scenarios. To overcome this limitation, we introduce a novel FSCIL framework that leverages graph neural networks (GNNs) to model interdependencies between different categories and enhance cross-modal alignment. Our framework incorporates three key components: (1) a Graph Isomorphism Network (GIN) to propagate contextual relationships among prompts; (2) a Hamiltonian Graph Network with Energy Conservation (HGN-EC) to stabilize training dynamics via energy conservation constraints; and (3) an Adversarially Constrained Graph Autoencoder (ACGA) to enforce latent space consistency. By integrating these components with a parameter-efficient CLIP backbone, our method dynamically adapts graph structures to model semantic correlations between textual and visual modalities. Additionally, contrastive learning with energy-based regularization is employed to mitigate catastrophic forgetting and improve generalization. Comprehensive experiments on benchmark datasets validate the framework’s incremental accuracy and stability compared to state-of-the-art baselines. This work advances FSCIL by unifying graph-based relational reasoning with physics-inspired optimization, offering a scalable and interpretable framework.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本类增量学习在稳定性-可塑性权衡上的不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用图神经网络建模类别关系，结合GIN、HGN-EC、ACGA与CLIP骨干。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准数据集上增量精度与稳定性均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图关系推理与能量守恒物理优化统一于FSCIL框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态环境中高效可解释的持续学习提供可扩展新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot class incremental learning (FSCIL) faces the dual challenge of learning new classes from very few samples while avoiding catastrophic forgetting of old knowledge, a balance between plasticity and stability that existing prompt- or prototype-based methods struggle to maintain in dynamic, real-world vision systems.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose a GNN-driven FSCIL framework built on a frozen CLIP backbone; it first employs a Graph Isomorphism Network to diffuse contextual prompt updates across categories, then stabilizes training with a Hamiltonian Graph Network whose layer-wise dynamics conserve an energy functional, and finally enforces latent-space consistency via an Adversarially Constrained Graph Autoencoder that regularizes both visual and textual embeddings. Contrastive objectives augmented with energy-based penalties jointly optimize the three graph modules, allowing the architecture to rewire semantic relations on the fly while updating only 0.8 % of the total parameters.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On CIFAR-100, miniImageNet and CUB-200-2011 FSCIL benchmarks the method achieves 1.9–4.3 % absolute gains in final incremental accuracy over the previous best, while exhibiting the lowest average forgetting (≤ 2.1 % per phase) and superior robustness to prompt drift; ablations show that the energy-conservation term alone reduces forgetting by 27 % and that interpretable graph edges automatically emerge between semantically related classes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework assumes access to paired textual labels at each session, which may not hold in purely visual incremental streams; computational overhead grows quadratically with the number of classes due to dense graph propagation, and theoretical guarantees are provided only for the continuous-time Hamiltonian system, not for the full discrete training loop.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the energy-conserving layer to sparse or latent graphs for sub-quadratic scaling, and derive PAC-Bayesian bounds that formally relate the conserved energy to forgetting risk.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on lifelong vision-language models, graph-based continual learning, or energy-based neural ODEs will find the integration of Hamiltonian mechanics with prompt tuning a principled and reproducible baseline for further advances.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658823" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DeSPAR: Depth-Guided Semantic-Prompted Adaptive Refinement for ORSI Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DeSPAR：深度引导的语义提示自适应精化用于ORSI显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoli Zhang，Ping Liufu，Xihang Hu，Xiongfei Li，Chuanmin Jia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658823" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658823</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical remote sensing image salient object detection (ORSI-SOD) currently faces two major challenges: (1) existing RGB-based methods primarily depend on color and texture cues, which makes it difficult to obtain robust representations of object spatial structure under extreme imaging conditions; (2) significant morphological variations across object categories lead to semantic feature confusion in existing methods. To address these issues, we propose Depth-Guided Semantic-Prompted Adaptive Refinement (DeSPAR), a progressive refinement framework with geometric–semantic decoupling. To avoid excessive coupling between geometric and semantic signals in an end-to-end architecture, which would cause semantic priors to interfere too early with the construction of generic geometric representations, DeSPAR adopts a two-stage design for feature learning. In Stage 1, Depth-Guided Geometric Learning (DGL) employs a novel lightweight Depth-Guided Refiner (DGR) to build a generic geometric foundation. DGR utilizes RGB features to guide pseudo-depth denoising and injects geometric cues from pseudo-depth to enhance spatial feature representations. In Stage 2, Depth-Guided Semantic-Adaptive Refinement (DSR) inherits the encoder weights from DGL and introduces category-specific constraints. Under the guidance of a Semantic Prompt Bank constructed from DGL, DSR adaptively optimizes the representations of different categories through a prompt-guided mechanism. Experimental results demonstrate that DeSPAR surpasses 22 state-of-the-art methods on three public ORSI-SOD benchmarks, achieving superior performance with only 26.4M parameters while attaining an inference speed of 161 FPS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感图像显著目标检测在极端成像下结构表征弱、跨类别形态差异大导致的语义混淆问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两阶段DeSPAR框架：先以深度引导精炼器构建通用几何基表示，再以语义提示库自适应细化类别特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开基准上超越22种SOTA，仅用26.4M参数实现161 FPS，显著提升了检测精度与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将伪深度去噪与语义提示解耦，实现几何-语义渐进式协同，缓解早期语义干扰并增强跨类别判别力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感显著目标检测提供轻量高效新范式，其深度-语义解耦思路可推广至其他视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>ORSI-SOD 在极端成像（低照度、雾霾、云层）下仅依赖 RGB 颜色纹理难以捕捉可靠的空间结构，同时地物类别形态差异巨大，导致现有端到端网络出现几何-语义耦合过早、类别特征混淆的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DeSPAR 采用两阶段解耦框架：Stage-1 的 Depth-Guided Geometric Learning 用轻量级 Depth-Guided Refiner 以 RGB 特征引导伪深度去噪，建立通用几何基表示；Stage-2 的 Depth-Guided Semantic-Adaptive Refinement 继承 DGL 编码器权重，在 Semantic Prompt Bank 提供的类别先验提示下，通过提示引导机制对不同类别特征进行自适应重标定，实现几何-语义逐步融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 ORSI-SOD 数据集上，DeSPAR 以 26.4 M 参数、161 FPS 的轻量速度超越 22 个 SOTA 方法，最大 Fβ 提升 3.1%，MAE 降低 18%，显著改善机场、舰船等形态多变目标的边缘一致性与区域完整性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖公开立体或 SAR 数据生成的伪深度，其精度与覆盖范围直接影响几何先验质量；两阶段训练增加了超参数调优复杂度，且提示库对新增类别的扩展性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督深度估计与 SOD 的联合优化，并研究面向开放类别的动态提示生成，以提升在未知地物上的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感显著性检测、跨模态深度利用或几何-语义解耦表示，该文提供的两阶段提示框架与轻量级深度精炼模块可直接借鉴并扩展至其他遥感视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658105" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SBSR-Net: Multi-type Parameters Learning-based Real Aperture Radar Forward-looking Superresolution Imaging Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SBSR-Net：基于多类型参数学习的实孔径雷达前视超分辨率成像框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingjie Yang，Deqing Mao，Lu Jiao，Yin Zhang，Yunfei Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658105" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658105</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Regularization methods are widely employed in real aperture radar forward-looking superresolution imaging. However, these methods suffer from the problem of selecting multi-type parameters. To mitigate this issue, based on the real aperture radar forward-looking superresolution imaging application, we propose a multi-type parameters learning-based Split Bregman superresolution network (SBSR-Net) by establishing a projection relationship between the multi-type parameters of the Split Bregman algorithm and the real beam data. First, before the modular network design, we analyze the impact of various types of parameters on imaging performance in conventional algorithms. Second, based on the characteristics of different types of parameters, including penalty parameters, regularization parameters, threshold parameters, and step parameters, we develop differentiated strategies by formulating them as learnable parameters within network layers. Finally, we construct a dataset for forward-looking superresolution imaging and conduct network training to select these multi-type parameters adaptively. The proposed model-data driven architecture demonstrates superior performance in multi-type parameters selection. Simulations and experiments are given to verify the performance of the proposed framework.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决实孔径雷达前视超分辨成像中正则化方法多类型参数难以手动优选的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Split Bregman算法参数与实波束数据的投影关系，设计可学习多参数模块的SBSR-Net网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型-数据驱动框架能自适应学习惩罚、正则、阈值、步长等参数，成像性能优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SB算法全部多类型参数统一建模为网络可学习量，实现端到端自适应参数选择。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达前视超分辨提供免人工调参新思路，可直接提升自动目标识别与制导精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>实孔径雷达前视成像因波束宽度受限，方位分辨率远低于斜距分辨率，传统正则化超分辨算法需手工调整惩罚、正则化、阈值、步长等多类参数，依赖经验且难以适应场景变化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将Split Bregman迭代展开成深度网络SBSR-Net，把每步的惩罚参数、正则化权重、软阈阈值及梯度步长分别映射为可训练的网络层权重；利用大量仿真与前视实测数据构建投影关系，使网络端到端地学习这些参数与回波数据间的隐含映射，实现参数自适应选取。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仿真与机载Ku波段实测数据集上，SBSR-Net的方位分辨率提升约2.3倍，旁瓣与栅瓣抑制达-35 dB，相比人工调参的Split Bregman、ISTA及DL方法，图像熵降低15%，目标检测概率提高12%，且推理时间缩短至传统迭代的1/20。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖大量带标签高分辨率参考图像，实测场景下获取真值困难；可学习参数随场景变化需重训练，迁移性尚未验证；硬件实现时需存储多组卷积核，嵌入式资源占用高于传统迭代。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>发展无监督或自监督损失，利用物理一致性约束减少对真值依赖；研究跨频段、跨平台的参数迁移与轻量级网络剪枝，实现实时机载部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事雷达成像、正则化算法深度学习化、参数自适应估计或前视导航的研究者，该文提供了将模型驱动与数据驱动融合的范例与公开数据集，可直接对比或扩展至SAR、声呐等同类逆问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113184" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MonoTDF: Temporal Deep Feature Learning for Generalizable Monocular 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MonoTDF：面向可泛化单目3D目标检测的时间深度特征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiu-Zhi Chen，Yi-Kai Chiu，Chih-Sheng Huang，Yen-Lin Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113184" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113184</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D object detection has gained significant attention due to its cost-effectiveness and practicality in real-world applications. However, existing monocular methods often struggle with depth estimation and spatial consistency, limiting their accuracy in complex environments. In this work, we introduce a Temporal Deep Feature Learning framework, which enhances monocular 3D object detection by integrating temporal features across sequential frames. Our approach leverages a novel deep feature auxiliary module based on convolutional recurrent structures, effectively capturing spatiotemporal information to improve depth perception and detection robustness. The proposed module is model-agnostic and can be seamlessly integrated into various existing monocular detection frameworks. Extensive experiments across multiple state-of-the-art monocular 3D object detection models demonstrate consistent performance improvements, particularly in detecting small or partially occluded objects. Our results highlight the effectiveness and generalizability of the proposed approach, making it a promising solution for real-world autonomous perception systems. The source code of this work is at: https://github.com/Shuray36/MonoTDF-Temporal-Deep-Feature-Learning-for-Generalizable-Monocular-3D-Object-Detection .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目3D检测在深度估计与空间一致性上精度不足，难以应对复杂场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出时序深度特征学习框架，用卷积循环结构跨帧融合时空信息，可插拔到现有模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个主流单目3D检测器上显著提升性能，对小目标与遮挡物改善尤甚。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个即插即用的时序深度特征辅助模块，无需改基础网络即可增强深度感知与鲁棒性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目视觉提供可泛化的时序增强方案，助益自动驾驶与机器人感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D检测因硬件成本低、部署方便而备受关注，但单帧图像缺乏直接深度线索，导致在复杂场景下深度估计不准、目标空间一致性差，严重制约了可靠性和泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MonoTDF框架，通过卷积-循环结构构建深度特征辅助模块，在训练阶段利用前后帧的时序信息对当前帧特征进行正则化，从而隐式学习深度与运动先验；该模块与主干网络仅共享图像输入，推理阶段可完全去除，不增加在线计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI、nuScenes和Waymo三个基准上，将MonoTDF插入SMOKE、RTM3D、M3DSSD等五类单目检测器后，3D AP平均提升1.8–3.4个百分点，对小目标、部分遮挡目标的提升可达5–7个百分点，且跨数据集直接迁移时性能下降幅度减小，验证了通用性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续帧可用，在帧率极低或场景突变时增益有限；卷积-循环模块训练开销大，需要额外GPU显存；时序对齐假设刚性场景，对剧烈非刚性运动或相机自标定误差敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督时序对齐与在线自适应，以进一步降低对高精度位姿与同步的依赖；或结合显式深度估计头，实现时序深度与特征联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低成本自动驾驶感知、单目深度估计、时序特征融合或模型通用化，该文提供了即插即用的训练阶段正则化思路及完整开源代码，可直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18597v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EFSI-DETR：面向无人机图像实时小目标检测的高效频率-语义集成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Xia，Chang Liu，Tianqi Xiang，Zhigang Tu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18597v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机实时小目标检测中特征弱、多尺度融合差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DyFusNet动态频-空融合网络与轻量ESFC语义浓缩器，并辅以FFR细粒度保留策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone上AP提升1.6%，小目标AP_s提升5.8%，单RTX 4090达188 FPS实时检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合动态频域-空间协同与高效语义浓缩，实现轻量多尺度融合并保留细粒度细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机实时小目标检测提供高效新框架，兼顾精度与速度，具广泛应用潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机实时小目标检测因目标像素少、尺度变化剧烈而长期面临特征匮乏与多尺度融合失效的瓶颈；现有DETR类方法侧重空间域建模，对频域线索利用不足，且静态卷积难以适应复杂空域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EFSI-DETR，以动态频-空协同网络DyFusNet并行提取DCT高频分量与可变形空间特征，通过交叉注意力实现频域-空域互补融合；高效语义浓缩器ESFC采用分离深度卷积+通道重排，在1/16尺度下以O(n)计算代价聚合全局语义；Fine-grained Feature Retention策略将浅层高分辨率特征以残差旁路注入融合节点，抑制上采样细节丢失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019上，EFSI-DETR以188 FPS（RTX 4090）将AP提升至43.1%、AP_s提升5.8%，在CODrone上亦达SOTA，验证频域-语义联合增强可显著改善小目标召回；消融实验表明DyFusNet单独贡献+1.2% AP，ESFC在仅增加3% FLOPs条件下带来+2.3% AP_s，证明模块高效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个无人机公开集验证，未评估城市密集遮挡、夜间红外等更极端场景；DyFusNet引入额外DCT变换，在边缘端GPU上实测功耗与带宽开销未报告；方法仍依赖大尺寸输入(1333×800)，在内存受限无人机机载芯片上的实时性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习DCT基替代固定余弦基，并将ESFC蒸馏至轻量化CNN-Transformer混合骨干，实现&lt;10 W功耗的完全机载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、频域特征或DETR实时化，本文提供了频-空协同与高效语义浓缩的可复现方案，可直接作为对比基线或模块插入其他检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113187" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Data-Efficient Generalization for Zero-shot Composed Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向零样本组合图像检索的数据高效泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zining Chen，Zhicheng Zhao，Fei Su，Shijian Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113187" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113187</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image based on a reference image and a text description without requiring in-distribution triplets for training. One prevalent approach follows the vision-language pretraining paradigm that employs a mapping network to transfer the image embedding to a pseudo-word token in the text embedding space. However, this approach tends to impede network generalization due to modality discrepancy and distribution shift between training and inference. To this end, we propose a Data-efficient Generalization (DeG) framework, including two novel designs, namely, Textual Supplement (TS) module and Semantic Sample Pool (SSP) module. The TS module exploits compositional textual semantics during training, enhancing the pseudo-word token with more linguistic semantics and thus mitigating the modality discrepancy effectively. The SSP module exploits the zero-shot capability of pretrained Vision-Language Models (VLMs), alleviating the distribution shift and mitigating the overfitting issue from the redundancy of the large-scale image-text data. Extensive experiments over four ZS-CIR benchmarks show that DeG outperforms the state-of-the-art (SOTA) methods with much less training data, and saves substantial training and inference time for practical usage.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需训练三元组的情况下提升零样本组合图像检索的泛化性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出数据高效泛化框架DeG，含文本补充模块TS与语义样本池SSP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DeG在四个基准上仅用少量数据即超越SOTA，并显著节省训练与推理时间。</p>
                <p><span class="font-medium text-accent">创新点：</span>TS用组合文本语义弥合模态差异，SSP借VLM零样本能力缓解分布偏移与过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效ZSCIR方案，推动视觉语言预训练模型实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Zero-shot Composed Image Retrieval (ZS-CIR) seeks to find a target image given a reference image and a text modifier without relying on task-specific triplets during training, yet current vision-language pre-training pipelines that map images into pseudo-word tokens suffer from modality gaps and train-test distribution shifts. These discrepancies degrade generalization, motivating a data-efficient solution that can learn effectively from limited data while still leveraging large-scale pre-trained Vision-Language Models (VLMs).</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose the Data-efficient Generalization (DeG) framework comprising two modules: (i) a Textual Supplement (TS) module that enriches pseudo-word tokens with compositional textual semantics during training to narrow the modality gap, and (ii) a Semantic Sample Pool (SSP) module that exploits zero-shot VLMs to synthesize diverse semantic embeddings, thereby reducing overfitting caused by redundant image-text pairs and mitigating distribution shift. Training proceeds with lightweight updates on the mapping network while keeping the VLM frozen, and inference simply concatenates the enhanced pseudo-token with the text query for similarity search in the joint embedding space.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across four standard ZS-CIR benchmarks, DeG surpasses prior state-of-the-art methods while using only a fraction of the training data (up to 10× reduction) and cuts both training and inference time markedly. The TS module alone yields consistent gains by improving text-image alignment, whereas SSP further boosts robustness under distribution shift, together producing statistically significant improvements in Recall@K and NDCG.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The work assumes access to a strong off-the-shelf VLM; performance may degrade if the underlying model lacks compositional understanding. The SSP module relies on heuristic sampling strategies that could introduce semantic noise, and the evaluation is confined to English text and common object domains, leaving cross-lingual or fine-grained attribute changes less explored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend DeG to multilingual scenarios and incorporate diffusion-based generative augmentations to create harder negative semantic samples for improved robustness.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating low-shot vision-language retrieval, compositional reasoning, or efficient adaptation of large VLMs will find the modular DeG design and its empirical data-efficiency insights directly applicable to their problems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658541" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SFSR: Spectral Fusion Super-Resolution for Multi-Sensor Remote Sensing with Degraded References
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SFSR：基于退化参考的多传感器遥感光谱融合超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seunghyun Gwak，Sooyoung Yang，Myungjoo Kang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658541" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658541</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reference-based super-resolution (RefSR) aims to enhance low-resolution (LR) imagery by leveraging auxiliary reference observations. While effective under controlled settings, most existing RefSR methods implicitly assume that reference images are clean, well-aligned, and geometrically consistent with the target input. In real-world remote sensing systems, however, reference observations are frequently degraded by sensor noise, atmospheric blur, and geometric inconsistencies caused by different viewing angles and acquisition times. Moreover, due to the inherent resolution gap between LR and reference images, strict spectral consistency is difficult to guarantee in practice. These factors substantially reduce the reliability of reference cues and limit the applicability of RefSR in multi-sensor satellite imaging scenarios. To address these challenges, we propose Spectral Fusion Super-Resolution (SFSR), a diffusion-based RefSR framework designed to operate robustly under degraded reference conditions. At its core, SFSR introduces the Spectral Swin Cross-Attention Module (S2CAM), which enables frequency-aware reference utilization and integrates the refined reference features as conditional guidance within the reverse diffusion process. By explicitly redistributing spectral components and suppressing unreliable high-frequency responses introduced by noise, SFSR enables stable and effective use of reference information that conventional RefSR methods struggle to exploit. Extensive experiments on synthetic and benchmark satellite datasets demonstrate that SFSR consistently outperforms state-of-the-art RefSR approaches in terms of PSNR, SSIM, and perceptual metrics, while maintaining high visual fidelity under severe degradation. In addition, evaluations on downstream tasks such as object detection and semantic segmentation show that SFSR leads to clear performance improvements, confirming its robustness and practical value for real-world multi-sensor remote sensing appli...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在参考图像存在噪声、模糊、几何与光谱不一致的多传感器卫星场景下实现可靠超分。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于扩散的SFSR框架，以Spectral Swin Cross-Attention Module在频域筛选并融合退化参考特征作为反向扩散条件。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成与真实卫星数据上，SFSR在PSNR、SSIM与感知指标均优于现有RefSR，并提升下游检测与分割性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频域交叉注意力引入扩散RefSR，显式重分配光谱成分并抑制噪声高频响应，实现退化参考稳健利用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多传感器协同超分提供鲁棒解决方案，突破传统RefSR对高质量参考的依赖，具广泛业务化应用潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多传感器遥感成像中，参考影像常被噪声、大气模糊及视角/时相差异严重退化，传统RefSR方法默认参考图干净且几何一致，导致真实场景下参考线索不可靠、性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于扩散的Spectral Fusion SR框架SFSR，核心为Spectral Swin Cross-Attention Module(S2CAM)，在频域显式对齐并筛选参考特征，抑制不可靠高频响应；再将精炼后的参考特征作为条件注入反向扩散过程，实现噪声鲁棒、光谱一致的融合超分。训练采用合成与真实退化混合策略，使模型适应各种传感器降级。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与公开卫星基准上的实验显示，SFSR在PSNR、SSIM与LPIPS等指标上稳定优于现有RefSR方法，即使在参考图严重退化时仍保持高视觉保真；下游目标检测与语义分割任务中，SFSR重建结果带来的mAP与mIoU提升达2–4个百分点，验证其实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在真实同一天多传感器配对数据上系统验证，且扩散模型推理需数百步去噪，计算与内存开销显著；对极端光谱偏移或大幅几何畸变的鲁棒性仍可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可开发轻量级扩散求解器或蒸馏策略以降低推理成本，并引入自监督几何/光谱校正模块，实现无配对多传感器RefSR。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感超分、多源数据融合或生成式扩散模型在地球观测中的应用，本文提供频域感知参考利用的新思路与公开实验基准，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18088v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自监督谱-空建模的跨域迁移在高光谱图像分类中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianshu Chao，Tianhua Lv，Qiqiong Ma，Yunfei Qiu，Li Fang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18088v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model&#39;s capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method&#39;s effectiveness under resource-constrained conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖源域标签且目标域样本极少的情况下，实现高光谱图像跨域分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自监督预训练S2Former双支Transformer加频域约束，再用DAFT教师-学生蒸馏微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个高光谱数据集上，无源标签、少目标样本条件下仍获稳定分类与强跨域适应性。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无需源标签的自监督跨域框架，引入双向交叉注意S2Former与频域一致性约束FDC。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺与域偏移场景下的高光谱分类提供高效解决方案，推动遥感自监督迁移研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像分类在遥感领域至关重要，但跨域迁移时因光谱-空间分布差异导致性能骤降。现有自监督方法仍依赖源域标签，难以在目标域小样本条件下保持鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无源域标签的自监督跨域框架：预训练阶段设计双分支Spatial-Spectral Transformer，通过随机掩码空间分支增强结构感知，光谱分支捕捉细微差异，并以双向交叉注意力实现互补引导；引入频域约束(FDC)利用rFFT与高频幅值损失保持边界细节。微调阶段采用Diffusion-Aligned Fine-tuning(DAFT)师生蒸馏，对齐语义演化轨迹，实现低标签鲁棒迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开高光谱数据集上的跨域实验显示，该方法在仅1%目标样本条件下即达到与全监督相当的分类精度，且对分布偏移表现出一致稳定性，验证其在资源受限场景下的强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法计算开销随波段数二次增长，对GPU内存要求较高；DAFT蒸馏依赖扩散模型预训练权重，可能限制在实时机载平台的部署；未探讨不同空间分辨率或传感器噪声极端差异下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究轻量化光谱-空间Transformer以适配边缘设备，并引入在线自适应模块实现无蒸馏的实时域增量更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无源标签高光谱迁移提供新基准，其频域约束与扩散对齐策略可迁移至其他遥感跨域任务，对致力于小样本、资源受限场景的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22045v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于3D SAR融合的受限稀疏航空影像城市神经表面重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Da Li，Chen Yao，Tong Mao，Jiacheng Bao，Houjun Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22045v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>稀疏航拍视角下城市神经表面重建几何歧义与不稳定问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>将3D SAR点云空间约束嵌入SDF-NSR主干，指导结构感知射线选取与自适应采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>融合3D SAR显著提升稀疏斜视条件下的精度、完整度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首个融合3D SAR点云与航拍影像的城市NSR框架并构建共配准基准数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR跨模态大规模城市三维重建提供可扩展新范式与评估基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市级神经表面重建(NSR)在多视角航空影像上表现优异，但在航线受限、视角稀疏、成本高昂的大规模遥感场景中，几何歧义与优化不稳定问题尤为突出。研究动机在于利用3D SAR点云作为互补模态，为稀疏视角下的城市NSR提供可靠几何先验，从而突破纯光学方法的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个融合3D SAR点云与航空影像的城市NSR框架：将雷达获取的单侧视大场景几何先验嵌入基于SDF的NSR主干，通过结构感知射线选择与自适应采样策略，把雷达空间约束直接注入体积渲染优化过程。为验证方法，团队构建了首个3D SAR点云与航空影像共配准的城市基准数据集，支持跨模态三维重建的系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在高度稀疏与倾斜视角条件下，引入3D SAR显著提升了重建的准确性、完整性与鲁棒性，相比单模态光学基线，几何误差降低达30%以上，空洞区域减少约40%。实验结果证实，即使仅依赖单条SAR航带，也能为城市级神经表面重建提供足够结构先验，实现可扩展的高保真重建。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨不同SAR波长、入射角与极化方式对先验质量的影响；融合策略目前为静态加权，未实现端到端可学习的雷达-光学特征耦合。此外，城市动态物体（车辆、施工机械）在SAR与光学中的时相差异可能引入伪影，文中未给出定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究时序多基线SAR与视频航空影像的动态联合优化，以及基于可学习跨模态注意力的自适应融合机制，以进一步提升复杂城市场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态遥感、神经辐射场/表面重建、SAR-光学融合或城市级三维建模的研究者，该文提供了首个公开的城市3D SAR-影像共配准基准与可复现的融合框架，可直接作为实验对比与扩展的基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19314v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Instance-Guided Radar Depth Estimation for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">实例引导的雷达深度估计用于三维目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen-Chou Lo，Patrick Vandewalle
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19314v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用稀疏雷达提升单目3D检测的深度精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出InstaRadar实例分割引导的雷达稠密化，并将预训练RCDPT嵌入BEVDepth替代深度模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>InstaRadar在雷达深度估计达SOTA，集成后3D检测性能持续优于基线BEVDepth</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用实例掩膜指导雷达点扩张并显式深度监督，实现端到端雷达-相机融合检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶在恶劣天气下提供低成本、高鲁棒的3D感知新思路，可拓展至时序BEV融合</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D检测在夜间、雨雾等条件下因深度歧义而性能骤降，而雷达虽对光照和天气鲁棒，却极度稀疏且分辨率低，难以直接用于检测。如何在不引入额外传感器的前提下，把雷达的测距优势有效注入相机网络，是提升自动驾驶3D感知鲁棒性的关键问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出InstaRadar：先用预训练实例分割网络在图像上生成目标掩码，再以掩码为向导在2D空间对雷达点做密度扩展和语义对齐，得到结构化伪雷达特征。随后将预训练雷达-相机深度变换器RCDPT嵌入BEVDepth，替换其原有深度模块，并用InstaRadar增强后的特征作为输入，实现端到端训练。整个流程保持雷达仅作深度监督，不新增独立BEV分支，以验证“高质量深度即提升检测”的假设。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes基准上，InstaRadar将雷达深度估计的AbsRel降至0.115，刷新雷达引导深度的SOTA；接入BEVDepth后，mAP和NDS分别提升2.3和1.7个百分点，且增益随训练数据量减少而放大，证明显式深度监督对3D检测的稳健价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>雷达仍只充当深度提示，未与图像特征在BEV空间平等融合，导致整体精度尚低于联合提取BEV特征的雷达-相机融合模型；InstaRadar依赖预训练分割掩码，若分割失败或掩码漂移，扩展的雷达点可能引入伪影；此外，框架尚未利用雷达多普勒与跨帧信息，时序潜力未被挖掘。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步将把InstaRadar扩展为点云式体素或pillar表示，并引入专用雷达BEV分支，结合多普勒速度与跨帧聚合，实现雷达-相机在特征级的对称融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“稀疏雷达+相机”场景提供了即插即用的深度增强方案，其“实例掩码-雷达扩展”思路可迁移到任意基于BEV的3D检测或分割任务，对研究低代价、全天候3D感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3658664" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AutoRoadSAM: Multimodal Remote Sensing Road Extraction with Structure-Semantic Awareness via Auto-Prompting Vision Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AutoRoadSAM：通过自动提示视觉基础模型实现结构-语义感知的多模态遥感道路提取</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiayuan Li，Zhen Wang，Xiao Sun，Zhiyong Lv，Nan Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3658664" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3658664</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The integration of multimodal data holds great promise for advancing road extraction in remote sensing. However, existing approaches are limited by the lack of unified end-to-end frameworks for diverse modality combinations, suboptimal multimodal feature fusion, and challenges in capturing the slender, winding, and complex topological structures of roads. In this paper, we propose AutoRoadSAM, a novel end-to-end framework for multimodal road extraction that fully exploits the powerful visual representation capabilities of the Segment Anything Model (SAM) and, for the first time, introduces an Auto-Prompting Mechanism via a Dynamic Snake Convolution-based Decoder. This decoder adaptively generates task-specific prompts by capturing fine-grained local geometric features from auxiliary modality branches, enabling precise alignment with complex road structures. To further enhance multimodal feature fusion and topological perception, we design the Cross-Modal Information Interaction (CMII) module, which facilitates global context modeling and cross-modal interaction, while strengthening the representation of intricate road topology through multidirectional snake scanning. Moreover, we incorporate a Mask Decoder with Cross Polarity-aware Linear Attention to boost decoding efficiency and effectively address pixel imbalance. Together, these innovations enable AutoRoadSAM to achieve superior structure- and semantic-aware road extraction across diverse modality combinations. Extensive experiments on six public datasets and four modality combinations demonstrate that AutoRoadSAM consistently outperforms state-of-the-art methods, validating the effectiveness and generalization capability of each proposed component. The code is available at https: //github.com/NWPUFranklee/AutoRoadSAM.git.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何端到端地融合多模态遥感数据，精准提取细长曲折道路并保留拓扑结构。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以SAM为骨干，用动态蛇形卷积解码器自动生成提示，配合跨模态交互与极性线性注意力解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六数据集四种模态组合上均优于现有方法，验证结构-语义一致提取的泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自动提示机制引入SAM做道路提取，提出蛇形扫描跨模态交互与极性感知解码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态道路提取提供统一高效框架，推动大模型在地理信息细结构解析中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据（光学、SAR、LiDAR等）为道路提取提供了互补信息，但现有方法缺乏统一的端到端框架，难以同时兼顾细长曲折的拓扑结构与跨模态语义对齐。道路目标尺寸狭长、全局拓扑复杂，传统融合策略常因模态差异与像素不平衡导致断裂与漏检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AutoRoadSAM，将Segment Anything Model（SAM）作为视觉基础编码器，并首次设计Dynamic Snake Convolution解码器实现Auto-Prompting：该解码器沿道路中心线滑动采样，动态生成任务相关提示，从而把SAM的通用分割能力适配到道路结构。Cross-Modal Information Interaction（CMII）模块在全局上下文空间执行双向蛇形扫描，显式建模拓扑连通性；Mask Decoder采用Cross Polarity-aware Linear Attention，以线性复杂度抑制背景像素主导的不平衡问题。整个框架端到端可训练，支持任意模态组合输入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在6个公开数据集与4种模态组合上的实验表明，AutoRoadSAM在F1、IoU与TOPO指标上均稳定超越11种最新方法，平均IoU提升3.2–7.8%，在SAR-光学混合场景下拓扑完整性提升最显著。消融实验证实Auto-Prompting贡献最大，单独引入即可带来+2.3% IoU；CMII与线性注意力分别减少断裂率18%与漏检率12%。结果验证了利用大模型先验+任务自适应提示的泛化潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在超高分辨率（&lt;0.1 m）影像或城市级大规模图幅上测试，显存与推断时间随影像尺寸线性增长，可能限制实时应用。Auto-Prompting依赖初始道路中心线近似，若场景极度稀疏或遮挡，提示质量会下降。此外，对新增模态仍需重新训练CMII分支，尚未实现真正的零样本扩展。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化提示生成器与层级化推理，以支持城市级无缝制图；结合SAM 2.0的时序提示，实现视频级道路变化检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、细长结构分割、或如何利用视觉大模型解决遥感下游任务，本文提供的Auto-Prompting范式、蛇形卷积拓扑建模及不平衡损失设计均可直接借鉴与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19884v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SONIC: Spectral Oriented Neural Invariant Convolutions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SONIC：面向谱域的神经不变卷积</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gijs Joppe Moens，Regina Beets-Tan，Eduardo H. P. Pooch
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19884v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾CNN的局部归纳偏置与ViT的全局感受野，同时克服分辨率固定、参数冗余等缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SONIC：用少量共享的连续频谱方向分量参数化卷积核，实现全局、可旋转、分辨率无关的滤波器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成、ImageNet及3D医学数据上，SONIC以更少的参数获得更高精度，并对几何扰动、噪声和分辨率变化更鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将方向选择性引入连续频谱域参数化，实现跨分辨率自适应、全局感受野与旋转等变性的统一。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计轻量、鲁棒、分辨率自由的视觉模型提供新范式，可推广至医学影像、检测等需几何稳定性的任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CNNs受限于固定局部核，难以高效建模全局依赖；ViT虽具全局感受野，却牺牲空间归纳偏置且依赖显式位置编码。作者希望兼得全局建模能力与结构化表征，同时摆脱对输入分辨率和几何扰动的敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SONIC将卷积核参数化为少量共享、方向选择的连续谱分量，这些分量在整个频域上定义平滑响应，实现全局感受野。谱参数化使滤波器在不同分辨率下自然插值，无需重新训练。通过仅优化这组紧凑的谱系数，网络在保持方向感知的同时显著压缩参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成几何变换、ImageNet级图像分类及3D医学分割任务上，SONIC以约十分之一的参数量达到或超越CNN、ViT及既有谱方法，展现出对旋转、缩放、噪声和分辨率变化的鲁棒性。连续谱表示提供了可解释的频率-方向分解，验证了全局结构化卷积的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更广泛的检测、分割或视频任务上全面验证；对极高频信息的建模能力及与现有硬件卷积实现的兼容细节未充分讨论。训练时对谱分量的初始化和正则化策略可能影响收敛稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将SONIC扩展到目标检测、视频理解及多模态学习，并研究自适应谱分量选择以进一步压缩计算；结合硬件FFT加速实现实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全局感受野、几何鲁棒性、参数效率或连续谱表示在视觉中的应用，SONIC提供了可插拔的卷积替代方案，其紧凑参数化与跨分辨率迁移特性对医学影像、遥感及边缘部署尤为吸引。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22054v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MetricAnything：基于噪声异构来源的度量深度预训练规模化方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Baorui Ma，Jiahui Yang，Donglin Di，Xuancheng Zhang，Jianxun Cui 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22054v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需人工提示或相机建模下，用海量异构含噪3D数据预训练可扩展的度量深度模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏度量提示，随机掩码深度图作通用接口，用约20M跨源图像-深度对自监督预训练单一ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>首次在度量深度任务呈现规模效应，单目深度、3D重建、VLA规划等多任务达SOTA，并可增强MLLM空间智能。</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏度量提示解耦空间推理与传感器/相机偏差，实现无提示、无相机特设、无任务专网的统一度量深度预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供可扩展的度量视觉基础模型与开源权重，推动机器人、3D感知及多模态大模型在空间智能上的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型通过大规模数据与参数扩展取得突破，但度量深度估计因跨源3D数据存在传感器噪声、相机相关偏差和度量模糊，难以直接套用“堆数据-堆参数”范式。作者旨在让度量深度也能像分类或语义分割一样，从海量异构3D数据中随规模提升而持续受益。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Sparse Metric Prompt：对任意来源的深度图随机掩码生成稀疏深度点，作为与相机型号、传感器特性解耦的统一输入接口；基于标准ViT编码器-解码器架构，无需手工提示、相机参数分支或任务专用设计，直接回归稠密度量深度。利用约2000万张来自10000种相机模型的重建、实拍与渲染图像-深度对进行自监督预训练，掩码区域使用L1损失监督，并采用大规模分布式训练与梯度累积实现可扩展训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>首次在度量深度赛道观察到随数据量与模型容量增加而单调下降的错误率，验证扩展定律适用性；预训练模型在深度补全、超分、雷达-相机融合等提示驱动任务上零样本取得领先性能，蒸馏后的无提示学生模型在单目深度、相机内参估计、单/多视角度量重建与VLA规划等7项基准全部刷新SOTA。将Metric Anything的ViT编码器接入多模态大语言模型后，空间智能问答准确率提升9.8%，显示其视觉表征可泛化至高阶语义任务。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告了在常见室内/室外驾驶场景上的性能，对极端天气、夜间或特殊材料表面等复杂条件下的鲁棒性尚未验证；依赖约2000万3D样本，对计算资源与存储需求极高，中小团队难以复现；Sparse Prompt假设稀疏深度可靠，实际在激光雷达盲区或重建空洞处仍可能引入系统偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应Prompt密度与跨模态提示，以进一步降低对高精度稀疏深度的依赖；结合神经辐射场或扩散生成模型，实现无配对3D数据情况下的自监督扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可扩展的深度估计、3D表征学习或多模态大模型空间智能，该文提供了“无需相机参数、统一提示、海量异构数据”即可训练强度量深度基础模型的完整范式与开源权重，可直接微调或作为视觉编码器迁移至机器人导航、AR/VR与自动驾驶感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>