<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>深入理解 PyTorch SGD 优化器参数 | Yan Tang</title><meta name=keywords content="PyTorch,SGD,优化器,梯度下降,机器学习,损失函数,动量"><meta name=description content="本文通过可视化损失函数和梯度下降路径，深入探讨了 PyTorch SGD 优化器中的各种参数（如动量、权重衰减、Nesterov 动量、动量抑制因子等）对简单线性回归问题的影响，以深入理解神经网络训练中的复杂性。"><meta name=author content="Yan Tang"><link rel=canonical href=https://ehehe.cn/posts/2025/03-pytorch-sgd-optimizer/><link crossorigin=anonymous href=/assets/css/stylesheet.16688c28815fab2857aba83927b88054dc9027b8a2b37dd2c695d9111db01da3.css integrity="sha256-FmiMKIFfqyhXq6g5J7iAVNyQJ7iis33SxpXZER2wHaM=" rel="preload stylesheet" as=style><link rel=icon href=https://ehehe.cn/assets/images/favicon-16x16.ico><link rel=icon type=image/png sizes=16x16 href=https://ehehe.cn/assets/images/favicon-16x16.ico><link rel=icon type=image/png sizes=32x32 href=https://ehehe.cn/assets/images/favicon-32x32.ico><link rel=apple-touch-icon href=https://ehehe.cn/assets/images/apple-touch-icon.png><link rel=mask-icon href=https://ehehe.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ehehe.cn/posts/2025/03-pytorch-sgd-optimizer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css integrity=sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js integrity=sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ crossorigin=anonymous></script><meta property="og:url" content="https://ehehe.cn/posts/2025/03-pytorch-sgd-optimizer/"><meta property="og:site_name" content="Yan Tang"><meta property="og:title" content="深入理解 PyTorch SGD 优化器参数"><meta property="og:description" content="本文通过可视化损失函数和梯度下降路径，深入探讨了 PyTorch SGD 优化器中的各种参数（如动量、权重衰减、Nesterov 动量、动量抑制因子等）对简单线性回归问题的影响，以深入理解神经网络训练中的复杂性。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-05T00:00:00+00:00"><meta property="article:tag" content="PyTorch"><meta property="article:tag" content="SGD"><meta property="article:tag" content="优化器"><meta property="article:tag" content="梯度下降"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="损失函数"><meta name=twitter:card content="summary"><meta name=twitter:title content="深入理解 PyTorch SGD 优化器参数"><meta name=twitter:description content="本文通过可视化损失函数和梯度下降路径，深入探讨了 PyTorch SGD 优化器中的各种参数（如动量、权重衰减、Nesterov 动量、动量抑制因子等）对简单线性回归问题的影响，以深入理解神经网络训练中的复杂性。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ehehe.cn/posts/"},{"@type":"ListItem","position":2,"name":"深入理解 PyTorch SGD 优化器参数","item":"https://ehehe.cn/posts/2025/03-pytorch-sgd-optimizer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"深入理解 PyTorch SGD 优化器参数","name":"深入理解 PyTorch SGD 优化器参数","description":"本文通过可视化损失函数和梯度下降路径，深入探讨了 PyTorch SGD 优化器中的各种参数（如动量、权重衰减、Nesterov 动量、动量抑制因子等）对简单线性回归问题的影响，以深入理解神经网络训练中的复杂性。","keywords":["PyTorch","SGD","优化器","梯度下降","机器学习","损失函数","动量"],"articleBody":"梯度下降是优化神经网络最流行也是最常用的方法，每个深度学习库都包含了用于优化梯度下降的各种算法实现。在现代机器学习框架其简洁的接口背后，隐藏着大量的复杂性。这些框架为我们提供了许多可以调节的选项，如果不深入理解其中的机理，我们可能会陷入误区，即：抽象泄露法则 | The Law of Leaky Abstractions。\n我们看到，PyTorch 中的 随机梯度下降（SGD）优化器 包含了众多参数：\n1 2 3 4 5 6 7 8 9 10 11 12 torch.optim.SGD( params, lr=0.001, momentum=0, weight_decay=0, dampening=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False, ) 除了我们熟悉的学习率 lr 和动量 momentum 参数，还有一些其他参数，对神经网络训练有着重要影响。本文将通过一系列损失函数来展示这些参数对一个简单的机器学习问题的影响。\n在深入探讨理论之前，让我们先通过一个交互式可视化来直观感受 SGD 优化器在不同参数下的表现。你可以点击等高线图上的任意位置开始优化，也可以调整下方的参数来观察不同配置对优化路径的影响。\n起点 终点 优化路径 损失函数 损失函数： MSE（均方误差） L1（绝对误差） Huber 损失 Smooth L1 损失 SGD 参数 学习率： 0.1000 动量： 0.000 动量抑制因子： 0.000 Nesterov： 启用 Nesterov 动量 权重衰减： 0.0000 重置 清除路径 使用说明：点击等高线图上的任意位置开始优化。绿色圆圈表示起点，蓝色圆圈表示收敛点，红色路径表示优化轨迹。调整上方参数可观察不同配置对优化过程的影响。 简单的机器学习问题 我们首先构建一个简单的机器学习问题，即使用一次函数（y=wx+by = wx +by=wx+b）对一组数据点进行拟合。这里，原函数我们使用了一个带噪声的二次函数，这样神经网络就需要在多个因素之间做出权衡。同时，我们也能观察到不同损失函数的影响：\ny=0.75x2+x+2+ noise y=0.75x^2+x+2+\\text{ noise } y=0.75x2+x+2+ noise 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import numpy as np import matplotlib.pyplot as plt np.random.seed(0) # Generate some data n = 50 x = np.array(np.random.randn(n), dtype=np.float32) y = np.array(0.75 * x**2 + 1.0 * x + 2.0 + 0.3 * np.random.randn(n), dtype=np.float32) # Plot the data plt.figure() plt.scatter(x, y, facecolor=\"r\", edgecolor=\"k\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.title(\"Toy Data\") plt.show() 简易数据集。\n下面，我们用 PyTorch 来构建一次函数（y=wx+by = wx +by=wx+b）模型，即一个单神经元网络，并通过 SGD 的方式训练我们的模型。为了保证每次实验的一致性，我们设定 w=6w=6w=6 且 b=−3b=-3b=−3。这样每次训练都从同样的起点出发。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import torch model = torch.nn.Linear(1, 1) model.weight.data.fill_(6.0) model.bias.data.fill_(-3.0) loss_fn = torch.nn.MSELoss() epochs = 100 learning_rate = 0.1 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) for epoch in range(epochs): inputs = torch.from_numpy(x).requires_grad_().reshape(-1, 1) labels = torch.from_numpy(y).reshape(-1, 1) optimizer.zero_grad() outputs = model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() print(\"epoch {}, loss {}\".format(epoch, loss.item())) 梯度下降沿着目标函数 J(θ)J(\\theta)J(θ) 的关于模型参数 θ∈Rd\\theta \\in \\mathbb{R}^dθ∈Rd 的负梯度方向进行更新，从而最小化该目标函数，即：\nθ←θ−η∇J(θ) \\theta \\leftarrow \\theta - \\eta \\nabla J(\\theta) θ←θ−η∇J(θ)学习率 η\\etaη 决定了我们为达到（局部）最小值所采取的步长。\n经过 100 次迭代后，损失值接近稳定。为了直观展示拟合效果，我们绘制拟合结果与数据点的对比图：\n1 2 3 4 5 6 7 8 9 10 weight = model.weight.item() bias = model.bias.item() plt.figure() plt.scatter(x, y, facecolors=\"r\", edgecolors=\"k\") plt.plot([x.min(), x.max()], [weight * x.min() + bias, weight * x.max() + bias], c=\"b\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.title(\"Linear Regression\") plt.show() 用 L2 损失学到的一次函数。\n损失函数可视化 虽然上面的拟合结果看起来合理，但到目前为止，一切都是通过高级 PyTorch 函数，例如 optimizer.zero_grad()、loss.backward() 和 optimizer.step() 来处理的。为了更好地理解我们接下来的步骤，我们需要可视化模型在损失函数中的路径。为此，我们在 101×101101 \\times 101101×101 的点网格上采样损失函数，然后用 imshow 来绘制：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_loss_map(loss_fn, x, y): losses = [[0.0] * 101 for _ in range(101)] x = torch.from_numpy(x) y = torch.from_numpy(y) for wi in range(101): for wb in range(101): w = -5.0 + 13.0 * wi / 100.0 b = -5.0 + 13.0 * wb / 100.0 ywb = w * x + b losses[wi][wb] = loss_fn(ywb, y).item() return list(reversed(losses)) loss_fn = torch.nn.MSELoss() losses = get_loss_map(loss_fn, x, y) fig, ax = plt.subplots() cax = ax.imshow(losses, interpolation=\"nearest\", cmap=\"terrain\", extent=[-5, 8, -5, 8]) fig.colorbar(cax) plt.xlabel(\"Bias\") plt.ylabel(\"Weight\") plt.title(\"Loss Map\") plt.show() L2 损失函数。\n现在，我们可以在运行梯度下降的过程中记录模型的参数变化，来展示优化器的表现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 model = torch.nn.Linear(1, 1) ... models = [[model.weight.item(), model.bias.item()]] for epoch in range(epochs): ... print(\"epoch {}, loss {}\".format(epoch, loss.item())) models.append([model.weight.item(), model.bias.item()]) fig, ax = plt.subplots() plt.xlabel(\"Bias\") plt.ylabel(\"Weight\") i = ax.imshow(losses, cmap=\"terrain\", interpolation=\"nearest\", extent=[-5, 8, -5, 8]) model_weights, model_biases = zip(*models) ax.scatter(model_biases, model_weights, c=\"r\", marker=\"+\") ax.plot(model_biases, model_weights, c=\"r\") fig.colorbar(i) plt.show() 沿损失函数路径的梯度下降可视化。\n通过观察可以看出，这与我们的预期完全一致：模型从我们预设的初始参数 (-3, 6) 开始，沿着梯度方向逐渐减小步长，并最终达到全局最小值。\n重要参数的可视化 损失函数 现在，我们来探讨其他因素对梯度下降的影响。首先是损失函数，我们这里使用了标准的 L2 损失：\nℓn=(xn−yn)2. \\ell_n = (x_n - y_n)^2. ℓn​=(xn​−yn​)2.L2 损失（torch.nn.MSELoss）会累计平方误差。\n我们还可以尝试其他几种损失函数：\nℓn=∣xn−yn∣. \\ell_n = \\lvert x_n - y_n \\rvert. ℓn​=∣xn​−yn​∣.L1 损失（torch.nn.L1Loss）累积绝对误差。\nℓn={12 (xn−yn)2,if ∣xn−yn∣≤δ,δ(∣xn−yn∣−12 δ),otherwise. \\ell_n = \\begin{cases} \\tfrac{1}{2}\\,(x_n - y_n)^2, \u0026 \\text{if } \\lvert x_n - y_n \\rvert \\le \\delta, \\\\ \\delta\\big(\\lvert x_n - y_n \\rvert - \\tfrac{1}{2}\\,\\delta\\big), \u0026 \\text{otherwise}. \\end{cases} ℓn​={21​(xn​−yn​)2,δ(∣xn​−yn​∣−21​δ),​if ∣xn​−yn​∣≤δ,otherwise.​Huber 损失（torch.nn.HuberLoss）在小误差时采用 L2，在大误差时采用 L1。\nℓn={12β (xn−yn)2,if ∣xn−yn∣≤β,∣xn−yn∣−12 β,otherwise. \\ell_n = \\begin{cases} \\tfrac{1}{2\\beta}\\,(x_n - y_n)^2, \u0026 \\text{if } \\lvert x_n - y_n \\rvert \\le \\beta, \\\\ \\lvert x_n - y_n \\rvert - \\tfrac{1}{2}\\,\\beta, \u0026 \\text{otherwise}. \\end{cases} ℓn​={2β1​(xn​−yn​)2,∣xn​−yn​∣−21​β,​if ∣xn​−yn​∣≤β,otherwise.​Smooth L1 损失（torch.nn.SmoothL1Loss）大体上等同于 Huber 损失，但多了一个额外的 $\\beta$ 参数。\n我们将之前的所有操作封装在一个循环中，尝试所有的损失函数，并将它们一同绘制出来：\n通过各种损失函数的梯度下降过程可视化。\n我们可以看到非 L2 损失函数的有趣特征。尽管 L2 损失函数较为平滑，数值可达到 100，但其他损失函数的数值较小，因为它们仅反映绝对误差。L2 损失的陡峭梯度使得优化器更快接近全局最小值，这从它早期点之间的较大间距中可以看出。与此同时，L1 损失则显示出对最小值的更缓慢接近。\n动量 下一个重要的参数是动量（Momentum）。在具有“峡谷形”几何的目标函数上（某一维度的曲率远大于另一维度），普通 SGD 会在陡峭方向来回“之”字形振荡，而沿着谷底方向推进缓慢。动量旨在沿“共识方向”加速、在相反方向抑振，从而允许更大的稳定步长并加快收敛。\n标准的动量形式（Polyak, 1964）为：\nvt=γvt−1+∇θJ(θt)θt+1=θt−η vt. \\begin{aligned} v_t \u0026= \\gamma v_{t-1} + \\nabla_\\theta J(\\theta_t)\\\\ \\theta_{t+1} \u0026= \\theta_t - \\eta\\, v_t. \\end{aligned} vt​θt+1​​=γvt−1​+∇θ​J(θt​)=θt​−ηvt​.​本文采用把学习率 η\\etaη 放在参数更新中的写法；一些实现会把 η\\etaη 合并进 vtv_tvt​ 或对梯度使用相反符号，但本质等价。直观地可以把参数看作在损失地形上滚动的“重球”：当连续多步梯度方向一致时，速度项 vtv_tvt​ 会累加，使有效步长被放大；当梯度方向频繁改变时，γvt−1\\gamma v_{t-1}γvt−1​ 的惯性会抵消来回摆动。\n在一维恒定梯度 ggg 的简化情形下，速度收敛到 v∞=11−γ gv_\\infty = \\tfrac{1}{1-\\gamma}\\, gv∞​=1−γ1​g，于是单步参数更新的有效幅度约为 η1−γ\\tfrac{\\eta}{1-\\gamma}1−γη​。例如 γ=0.9\\gamma=0.9γ=0.9 时，有效步长约为普通 SGD 的 10×10\\times10×。为便于观察本文的可视化效果，我们将 γ\\gammaγ 设为较大的值 0.90.90.9：\n通过各种损失函数的梯度下降过程可视化，展示高动量的影响。\n可以清楚看到高动量带来的影响：在峡谷的陡峭方向，惯性项 γvt−1\\gamma v_{t-1}γvt−1​ 抑制了“之”字形振荡；但在接近全局最小值时，由于累积的动量，轨迹会出现“超调”，需要若干步才能衰减回稳，这在 L2 损失（曲率更大）下最为明显。若同时把学习率 η\\etaη 取过大，甚至可能出现持续振荡或接近发散。实际使用中常见的取值是 γ∈[0.9,0.99]\\gamma\\in[0.9, 0.99]γ∈[0.9,0.99]。\n建议在交互式可视化中，调整动量参数时，以更明显地观察动量对梯度下降的影响。 Nesterov 加速梯度法 Nesterov 加速梯度法（Nesterov Accelerated Gradient, NAG）在“前瞻”位置计算梯度：不是在当前参数 θt\\theta_tθt​ 处，而是在仅由惯性项带来的预估位置处评估梯度，从而实现“先大步、后修正”。与上文约定一致，我们将学习率 η\\etaη 放在参数更新中：\nθlook=θt−η γ vt−1vt=γvt−1+∇θJ(θlook)θt+1=θt−η vt. \\begin{aligned} \\theta_{\\text{look}} \u0026= \\theta_t - \\eta\\,\\gamma\\, v_{t-1} \\\\ v_t \u0026= \\gamma v_{t-1} + \\nabla_\\theta J\\big(\\theta_{\\text{look}}\\big) \\\\ \\theta_{t+1} \u0026= \\theta_t - \\eta\\, v_t. \\end{aligned} θlook​vt​θt+1​​=θt​−ηγvt−1​=γvt−1​+∇θ​J(θlook​)=θt​−ηvt​.​其中：第一行给出仅由动量项产生的前瞻位置；第二行在前瞻点处评估梯度并更新速度；第三行用更新后的速度完成一步参数更新。\n直观理解：普通动量先用“旧位置”的梯度叠加速度再更新；而 Nesterov 先按惯性“冲到前瞻点”，再在该点测量梯度并微调方向。这种预判能在“峡谷形”几何下减少超调并提升收敛响应性；在凸问题上也有更强的收敛结果报道（实践中常较标准动量略优）。一些实现会把 η\\etaη 合并进 vtv_tvt​ 中，写作：\nvt=γvt−1+η ∇θJ(θt−γvt−1),θt+1=θt−vt, v_t = \\gamma v_{t-1} + \\eta\\, \\nabla_\\theta J\\big(\\theta_t - \\gamma v_{t-1}\\big),\\quad \\theta_{t+1} = \\theta_t - v_t,vt​=γvt−1​+η∇θ​J(θt​−γvt−1​),θt+1​=θt​−vt​,两种写法在含义上等价。\n（a）动量与（b）Nesterov 动量的比较。\n通过各种损失函数的梯度下降过程可视化，展示高 Nesterov 动量的影响。\n从图中可以看出，Nesterov 动量相对普通动量更少超调：它先沿 γvt−1\\gamma v_{t-1}γvt−1​ 的方向做大步前瞻，再用前瞻点的梯度做修正，整体轨迹更“稳”。在 L2 损失（曲率更大）下这一差异更明显。\n权重衰减 权重衰减（weight decay）通过在目标函数上添加 L2 正则项来抑制过大的参数，鼓励更简单的模型：\nJλ(θ) = J(θ) + λ2 ∥θ∥22, J_\\lambda(\\theta) \\;=\\; J(\\theta) \\;+\\; \\tfrac{\\lambda}{2}\\,\\lVert \\theta \\rVert_2^2, Jλ​(θ)=J(θ)+2λ​∥θ∥22​,其中 θ\\thetaθ 表示所有可训练参数，λ≥0\\lambda \\ge 0λ≥0 为正则化强度。在标准（带或不带动量/Nesterov）的 SGD 中，其对应的更新可写为\nθt+1 = θt − η(∇θJ(θt) + λ θt). \\theta_{t+1} \\;=\\; \\theta_t \\;-\\; \\eta\\big(\\nabla_\\theta J(\\theta_t) \\;+\\; \\lambda\\,\\theta_t\\big). θt+1​=θt​−η(∇θ​J(θt​)+λθt​).这等价于在每一步对参数做一次“收缩”（向零靠拢）并沿负梯度方向前进。对于本文的二维示例 (w,b)(w,b)(w,b)，正则项会把解从理想全局最小值拉向原点 (0,0)(0,0)(0,0)。\n通过各种损失函数的梯度下降过程可视化，展示高 Nesterov 动量和权重衰减的影响。\n与其他损失相比，L2 损失下这种“拉回”最不显著，因为其更大的曲率使得梯度项相对更强。对于自适应优化器，常见做法是使用“解耦”权重衰减（AdamW；Decoupled Weight Decay Regularization），但对纯 SGD，以上写法与在目标中加入 L2 正则是等价的。\n实践中通常不对 bias（以及如 BatchNorm 的缩放/偏移参数）使用 weight decay，以免无谓惩罚平移项。下面给出仅对权重衰减、不对偏置衰减的简易伪代码：\n1 2 3 4 5 6 7 8 9 10 11 12 # Split parameters: apply weight decay only to weights (not to biases) weight_params = [p for name, p in model.named_parameters() if \"bias\" not in name] bias_params = [p for name, p in model.named_parameters() if \"bias\" in name] optimizer = SGD( [ {\"params\": weight_params}, {\"params\": bias_params, \"weight_decay\": 0.0}, ], weight_decay=1e-2, lr=1e-2, ) 动量抑制因子 动量抑制因子（dampening）用来降低“当前梯度”注入到动量项中的权重，从而减弱瞬时梯度对速度项 vtv_tvt​ 的驱动。令动量系数为 γ∈[0,1)\\gamma \\in [0,1)γ∈[0,1)，抑制因子为 α∈[0,1]\\alpha\\in[0,1]α∈[0,1]。带抑制因子的更新可写为：\nvt=γ vt−1+(1−α) ∇θJ(θt),θt+1=θt−η vt. \\begin{aligned} v_t \u0026= \\gamma\\, v_{t-1} + (1-\\alpha)\\,\\nabla_\\theta J(\\theta_t),\\\\ \\theta_{t+1} \u0026= \\theta_t - \\eta\\, v_t. \\end{aligned} vt​θt+1​​=γvt−1​+(1−α)∇θ​J(θt​),=θt​−ηvt​.​当 α=0\\alpha=0α=0 时退化为标准动量；α\\alphaα 越大，当前梯度的“即时贡献”越被抑制，轨迹更平滑但响应更慢。在一维恒定梯度 ggg 的简化情形下，稳态速度\nv∞=1−α1−γ g⇒有效步长≈η(1−α)1−γ, v_\\infty=\\frac{1-\\alpha}{1-\\gamma}\\,g \\quad\\Rightarrow\\quad \\text{有效步长}\\approx \\frac{\\eta(1-\\alpha)}{1-\\gamma}, v∞​=1−γ1−α​g⇒有效步长≈1−γη(1−α)​,表明抑制因子本质上缩小了有效步长与动量累积强度，可缓解超调（overshoot），但可能降低收敛速度。本文可视化中把 α=0.8\\alpha=0.8α=0.8 仅用于放大这种效应，便于观察轨迹变化。\n与 Nesterov 搭配时需注意：在 PyTorch 中启用 nesterov=True 时要求 dampening=0 才对应经典 NAG（见 SGD 文档）。实践中通常保持 dampening=0；若需减弱过强动量或减少超调，优先调低动量系数 γ\\gammaγ 或学习率 η\\etaη。 通过各种损失函数的梯度下降过程可视化，展示高动量和高动量抑制因子的影响。\n参考文献 G. Goh, “Why Momentum Really Works”, Distill, 2017. https://distill.pub/2017/momentum S. Ruder, “An overview of gradient descent optimization algorithms”, 2016. https://www.ruder.io/optimizing-gradient-descent ","wordCount":"949","inLanguage":"en","datePublished":"2025-11-05T00:00:00Z","dateModified":"2025-11-05T00:00:00Z","author":{"@type":"Person","name":"Yan Tang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ehehe.cn/posts/2025/03-pytorch-sgd-optimizer/"},"publisher":{"@type":"Organization","name":"Yan Tang","logo":{"@type":"ImageObject","url":"https://ehehe.cn/assets/images/favicon-16x16.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://ehehe.cn/ accesskey=h title="Yan Tang (Alt + H)">Yan Tang</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ehehe.cn/ title=Posts><span>Posts</span></a></li><li><a href=https://ehehe.cn/about/ title=About><span>About</span></a></li><li><a href=https://ehehe.cn/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://ehehe.cn/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">深入理解 PyTorch SGD 优化器参数</h1><div class=post-meta><span title='2025-11-05 00:00:00 +0000 UTC'>November 5, 2025</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>Yan Tang</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=# aria-label=损失函数>损失函数</a></li><li><a href=# aria-label="SGD 参数">SGD 参数</a></li></ul><li><a href=#%e7%ae%80%e5%8d%95%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e9%97%ae%e9%a2%98 aria-label=简单的机器学习问题>简单的机器学习问题</a></li><li><a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e5%8f%af%e8%a7%86%e5%8c%96 aria-label=损失函数可视化>损失函数可视化</a></li><li><a href=#%e9%87%8d%e8%a6%81%e5%8f%82%e6%95%b0%e7%9a%84%e5%8f%af%e8%a7%86%e5%8c%96 aria-label=重要参数的可视化>重要参数的可视化</a><ul><li><a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label=损失函数>损失函数</a></li><li><a href=#%e5%8a%a8%e9%87%8f aria-label=动量>动量</a></li><li><a href=#nesterov-%e5%8a%a0%e9%80%9f%e6%a2%af%e5%ba%a6%e6%b3%95 aria-label="Nesterov 加速梯度法">Nesterov 加速梯度法</a></li><li><a href=#%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8f aria-label=权重衰减>权重衰减</a></li><li><a href=#%e5%8a%a8%e9%87%8f%e6%8a%91%e5%88%b6%e5%9b%a0%e5%ad%90 aria-label=动量抑制因子>动量抑制因子</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></div></details></div><div class=post-content><p>梯度下降是优化神经网络最流行也是最常用的方法，每个深度学习库都包含了用于优化梯度下降的各种算法实现。在现代机器学习框架其简洁的接口背后，隐藏着大量的复杂性。这些框架为我们提供了许多可以调节的选项，如果不深入理解其中的机理，我们可能会陷入误区，即：<a href=https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/>抽象泄露法则 | The Law of Leaky Abstractions</a>。</p><p>我们看到，PyTorch 中的 <a href=https://pytorch.org/docs/stable/generated/torch.optim.SGD.html>随机梯度下降（SGD）优化器</a> 包含了众多参数：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>params</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>momentum</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dampening</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>nesterov</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=o>*</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>maximize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>foreach</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>differentiable</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>除了我们熟悉的学习率 <code>lr</code> 和动量 <code>momentum</code> 参数，还有一些其他参数，对神经网络训练有着重要影响。本文将通过一系列损失函数来展示这些参数对一个简单的机器学习问题的影响。</p><p>在深入探讨理论之前，让我们先通过一个交互式可视化来直观感受 SGD 优化器在不同参数下的表现。你可以点击等高线图上的任意位置开始优化，也可以调整下方的参数来观察不同配置对优化路径的影响。</p><div id=sgd-visualization-container><div id=sgd-canvas><div class=legend><div class=legend-item><span class="legend-marker start"></span>
<span>起点</span></div><div class=legend-item><span class="legend-marker end"></span>
<span>终点</span></div><div class=legend-item><span class="legend-marker path"></span>
<span>优化路径</span></div></div></div><div id=sgd-controls><div class=control-section><h3>损失函数</h3><div class=control-row><label class=control-label for=loss-function>损失函数：</label><div class=control-input><select id=loss-function><option value=MSE selected>MSE（均方误差）</option><option value=L1>L1（绝对误差）</option><option value=Huber>Huber 损失</option><option value=SmoothL1>Smooth L1 损失</option></select></div></div></div><div class=control-section><h3>SGD 参数</h3><div class=control-rows-grid><div class=control-row><label class=control-label for=learning-rate>学习率：</label><div class=control-input><input type=range id=learning-rate min=0.001 max=0.2 step=0.001 value=0.1>
<span class=control-value id=lr-value>0.1000</span></div></div><div class=control-row><label class=control-label for=momentum>动量：</label><div class=control-input><input type=range id=momentum min=0 max=0.99 step=0.01 value=0>
<span class=control-value id=momentum-value>0.000</span></div></div><div class=control-row><label class=control-label for=dampening>动量抑制因子：</label><div class=control-input><input type=range id=dampening min=0 max=0.99 step=0.01 value=0>
<span class=control-value id=dampening-value>0.000</span></div></div><div class=control-row><label class=control-label for=nesterov>Nesterov：</label><div class=control-input><div class=checkbox-container><input type=checkbox id=nesterov>
<label for=nesterov>启用 Nesterov 动量</label></div></div></div><div class=control-row><label class=control-label for=weight-decay>权重衰减：</label><div class=control-input><input type=range id=weight-decay min=0 max=0.1 step=0.001 value=0>
<span class=control-value id=wd-value>0.0000</span></div></div></div></div><div class=button-group><button id=reset-btn>重置</button>
<button id=clear-btn>清除路径</button></div><div class=info-text><strong>使用说明：</strong>点击等高线图上的任意位置开始优化。绿色圆圈表示起点，蓝色圆圈表示收敛点，红色路径表示优化轨迹。调整上方参数可观察不同配置对优化过程的影响。</div></div></div><link rel=stylesheet href=css/sgd-visualization.css><script type=module src=js/sgd-visualization.js></script><h2 id=简单的机器学习问题>简单的机器学习问题<a hidden class=anchor aria-hidden=true href=#简单的机器学习问题>#</a></h2><p>我们首先构建一个简单的机器学习问题，即使用一次函数（<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = wx +b</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6667em;vertical-align:-.0833em></span><span class="mord mathnormal" style=margin-right:.02691em>w</span><span class="mord mathnormal">x</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6944em></span><span class="mord mathnormal">b</span></span></span></span>）对一组数据点进行拟合。这里，原函数我们使用了一个带噪声的二次函数，这样神经网络就需要在多个因素之间做出权衡。同时，我们也能观察到不同损失函数的影响：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0.75</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mi>x</mi><mo>+</mo><mn>2</mn><mo>+</mo><mtext> noise </mtext></mrow><annotation encoding="application/x-tex">
y=0.75x^2+x+2+\text{ noise }
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.9474em;vertical-align:-.0833em></span><span class=mord>0.75</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8641em><span style=top:-3.113em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6667em;vertical-align:-.0833em></span><span class="mord mathnormal">x</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>2</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6679em></span><span class="mord text"><span class=mord> noise </span></span></span></span></span></span><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Generate some data</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=mf>0.75</span> <span class=o>*</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=mf>1.0</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=mf>2.0</span> <span class=o>+</span> <span class=mf>0.3</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>facecolor</span><span class=o>=</span><span class=s2>&#34;r&#34;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s2>&#34;k&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;y&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Toy Data&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><figure class=align-center><img loading=lazy src=images/fig01_toy_dataset.svg#center alt=简易数据集。 width=450><figcaption><p>简易数据集。</p></figcaption></figure><p>下面，我们用 PyTorch 来构建一次函数（<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = wx +b</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6667em;vertical-align:-.0833em></span><span class="mord mathnormal" style=margin-right:.02691em>w</span><span class="mord mathnormal">x</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6944em></span><span class="mord mathnormal">b</span></span></span></span>）模型，即一个单神经元网络，并通过 SGD 的方式训练我们的模型。为了保证每次实验的一致性，我们设定 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">w=6</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.4306em></span><span class="mord mathnormal" style=margin-right:.02691em>w</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>6</span></span></span></span> 且 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mo>−</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">b=-3</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6944em></span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>−</span><span class=mord>3</span></span></span></span>。这样每次训练都从同样的起点出发。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>fill_</span><span class=p>(</span><span class=mf>6.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>fill_</span><span class=p>(</span><span class=o>-</span><span class=mf>3.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss_fn</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>inputs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;epoch </span><span class=si>{}</span><span class=s2>, loss </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()))</span>
</span></span></code></pre></td></tr></table></div></div><p>梯度下降沿着目标函数 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=mclose>)</span></span></span></span> 的关于模型参数 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\theta \in \mathbb{R}^d</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7335em;vertical-align:-.0391em></span><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.8491em></span><span class=mord><span class="mord mathbb">R</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8491em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span> 的负梯度方向进行更新，从而最小化该目标函数，即：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>−</mo><mi>η</mi><mi mathvariant="normal">∇</mi><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
\theta \leftarrow \theta - \eta \nabla J(\theta)
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6944em></span><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>←</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.7778em;vertical-align:-.0833em></span><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span><span class=mord>∇</span><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=mclose>)</span></span></span></span></span><p>学习率 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span></span></span></span> 决定了我们为达到（局部）最小值所采取的步长。</p><p>经过 100 次迭代后，损失值接近稳定。为了直观展示拟合效果，我们绘制拟合结果与数据点的对比图：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>weight</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bias</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>facecolors</span><span class=o>=</span><span class=s2>&#34;r&#34;</span><span class=p>,</span> <span class=n>edgecolors</span><span class=o>=</span><span class=s2>&#34;k&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=n>x</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>x</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span> <span class=p>[</span><span class=n>weight</span> <span class=o>*</span> <span class=n>x</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>+</span> <span class=n>bias</span><span class=p>,</span> <span class=n>weight</span> <span class=o>*</span> <span class=n>x</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=n>bias</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=s2>&#34;b&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;y&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Linear Regression&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><figure class=align-center><img loading=lazy src=images/fig02_linear_regression.svg#center alt="用 L2 损失学到的一次函数。" width=450><figcaption><p>用 L2 损失学到的一次函数。</p></figcaption></figure><h2 id=损失函数可视化>损失函数可视化<a hidden class=anchor aria-hidden=true href=#损失函数可视化>#</a></h2><p>虽然上面的拟合结果看起来合理，但到目前为止，一切都是通过高级 PyTorch 函数，例如 <code>optimizer.zero_grad()</code>、<code>loss.backward()</code> 和 <code>optimizer.step()</code> 来处理的。为了更好地理解我们接下来的步骤，我们需要可视化模型在损失函数中的路径。为此，我们在 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>101</mn><mo>×</mo><mn>101</mn></mrow><annotation encoding="application/x-tex">101 \times 101</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>101</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>101</span></span></span></span> 的点网格上采样损失函数，然后用 <code>imshow</code> 来绘制：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_loss_map</span><span class=p>(</span><span class=n>loss_fn</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>losses</span> <span class=o>=</span> <span class=p>[[</span><span class=mf>0.0</span><span class=p>]</span> <span class=o>*</span> <span class=mi>101</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>101</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>wi</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>101</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>wb</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>101</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>w</span> <span class=o>=</span> <span class=o>-</span><span class=mf>5.0</span> <span class=o>+</span> <span class=mf>13.0</span> <span class=o>*</span> <span class=n>wi</span> <span class=o>/</span> <span class=mf>100.0</span>
</span></span><span class=line><span class=cl>            <span class=n>b</span> <span class=o>=</span> <span class=o>-</span><span class=mf>5.0</span> <span class=o>+</span> <span class=mf>13.0</span> <span class=o>*</span> <span class=n>wb</span> <span class=o>/</span> <span class=mf>100.0</span>
</span></span><span class=line><span class=cl>            <span class=n>ywb</span> <span class=o>=</span> <span class=n>w</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>            <span class=n>losses</span><span class=p>[</span><span class=n>wi</span><span class=p>][</span><span class=n>wb</span><span class=p>]</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>ywb</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>list</span><span class=p>(</span><span class=nb>reversed</span><span class=p>(</span><span class=n>losses</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss_fn</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=n>get_loss_map</span><span class=p>(</span><span class=n>loss_fn</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>cax</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>losses</span><span class=p>,</span> <span class=n>interpolation</span><span class=o>=</span><span class=s2>&#34;nearest&#34;</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;terrain&#34;</span><span class=p>,</span> <span class=n>extent</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=mi>8</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>cax</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Bias&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Weight&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Loss Map&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><figure class=align-center><img loading=lazy src=images/fig03_loss_map.svg#center alt="L2 损失函数。" width=450><figcaption><p>L2 损失函数。</p></figcaption></figure><p>现在，我们可以在运行梯度下降的过程中记录模型的参数变化，来展示优化器的表现：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>models</span> <span class=o>=</span> <span class=p>[[</span><span class=n>model</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>model</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>item</span><span class=p>()]]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;epoch </span><span class=si>{}</span><span class=s2>, loss </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>    <span class=n>models</span><span class=o>.</span><span class=n>append</span><span class=p>([</span><span class=n>model</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>model</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>item</span><span class=p>()])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Bias&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Weight&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>i</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>losses</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;terrain&#34;</span><span class=p>,</span> <span class=n>interpolation</span><span class=o>=</span><span class=s2>&#34;nearest&#34;</span><span class=p>,</span> <span class=n>extent</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=mi>8</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>model_weights</span><span class=p>,</span> <span class=n>model_biases</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>models</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>model_biases</span><span class=p>,</span> <span class=n>model_weights</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s2>&#34;r&#34;</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s2>&#34;+&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>model_biases</span><span class=p>,</span> <span class=n>model_weights</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=s2>&#34;r&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>i</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><figure class=align-center><img loading=lazy src=images/fig04_gradient_descent_visualization.svg#center alt=沿损失函数路径的梯度下降可视化。 width=450><figcaption><p>沿损失函数路径的梯度下降可视化。</p></figcaption></figure><p>通过观察可以看出，这与我们的预期完全一致：模型从我们预设的初始参数 <code>(-3, 6)</code> 开始，沿着梯度方向逐渐减小步长，并最终达到全局最小值。</p><h2 id=重要参数的可视化>重要参数的可视化<a hidden class=anchor aria-hidden=true href=#重要参数的可视化>#</a></h2><h3 id=损失函数>损失函数<a hidden class=anchor aria-hidden=true href=#损失函数>#</a></h3><p>现在，我们来探讨其他因素对梯度下降的影响。首先是损失函数，我们这里使用了标准的 L2 损失：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mi>n</mi></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">
\ell_n = (x_n - y_n)^2.
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1.1141em;vertical-align:-.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose><span class=mclose>)</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8641em><span style=top:-3.113em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mord>.</span></span></span></span></span><p>L2 损失（<code>torch.nn.MSELoss</code>）会累计平方误差。</p><p>我们还可以尝试其他几种损失函数：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mi>n</mi></msub><mo>=</mo><mo stretchy="false">∣</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">∣</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">
\ell_n = \lvert x_n - y_n \rvert.
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>∣</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>∣</span><span class=mord>.</span></span></span></span></span><p>L1 损失（<code>torch.nn.L1Loss</code>）累积绝对误差。</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mi>n</mi></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mtext> </mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mo stretchy="false">∣</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">∣</mo><mo>≤</mo><mi>δ</mi><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>δ</mi><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><mo stretchy="false">∣</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">∣</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mtext> </mtext><mi>δ</mi><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>otherwise</mtext><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">
\ell_n =
\begin{cases}
\tfrac{1}{2}\,(x_n - y_n)^2, &amp; \text{if } \lvert x_n - y_n \rvert \le \delta, \\
\delta\big(\lvert x_n - y_n \rvert - \tfrac{1}{2}\,\delta\big), &amp; \text{otherwise}.
\end{cases}
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:3em;vertical-align:-1.25em></span><span class=minner><span class="mopen delimcenter" style=top:0><span class="delimsizing size4">{</span></span><span class=mord><span class=mtable><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.69em><span style=top:-3.69em><span class=pstrut style=height:3.008em></span><span class=mord><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8451em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.345em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:.1667em></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose><span class=mclose>)</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mpunct>,</span></span></span><span style=top:-2.25em><span class=pstrut style=height:3.008em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03785em>δ</span><span class=mord><span class="delimsizing size1">(</span></span><span class=mopen>∣</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>∣</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8451em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.345em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.03785em>δ</span><span class=mord><span class="delimsizing size1">)</span></span><span class=mpunct>,</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.19em><span></span></span></span></span></span><span class=arraycolsep style=width:1em></span><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.69em><span style=top:-3.69em><span class=pstrut style=height:3.008em></span><span class=mord><span class="mord text"><span class=mord>if </span></span><span class=mopen>∣</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>∣</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>≤</span><span class=mspace style=margin-right:.2778em></span><span class="mord mathnormal" style=margin-right:.03785em>δ</span><span class=mpunct>,</span></span></span><span style=top:-2.25em><span class=pstrut style=height:3.008em></span><span class=mord><span class="mord text"><span class=mord>otherwise</span></span><span class=mord>.</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.19em><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span><p>Huber 损失（<code>torch.nn.HuberLoss</code>）在小误差时采用 L2，在大误差时采用 L1。</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mi>n</mi></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mi>β</mi></mrow></mfrac><mtext> </mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mo stretchy="false">∣</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">∣</mo><mo>≤</mo><mi>β</mi><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo stretchy="false">∣</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">∣</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mtext> </mtext><mi>β</mi><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>otherwise</mtext><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">
\ell_n =
\begin{cases}
\tfrac{1}{2\beta}\,(x_n - y_n)^2, &amp; \text{if } \lvert x_n - y_n \rvert \le \beta, \\
\lvert x_n - y_n \rvert - \tfrac{1}{2}\,\beta, &amp; \text{otherwise}.
\end{cases}
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:3em;vertical-align:-1.25em></span><span class=minner><span class="mopen delimcenter" style=top:0><span class="delimsizing size4">{</span></span><span class=mord><span class=mtable><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.7146em><span style=top:-3.7146em><span class=pstrut style=height:3.008em></span><span class=mord><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8451em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight" style=margin-right:.05278em>β</span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4811em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:.1667em></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose><span class=mclose>)</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mpunct>,</span></span></span><span style=top:-2.2254em><span class=pstrut style=height:3.008em></span><span class=mord><span class=mopen>∣</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>∣</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8451em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.345em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.05278em>β</span><span class=mpunct>,</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.2146em><span></span></span></span></span></span><span class=arraycolsep style=width:1em></span><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.7146em><span style=top:-3.7146em><span class=pstrut style=height:3.008em></span><span class=mord><span class="mord text"><span class=mord>if </span></span><span class=mopen>∣</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>∣</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>≤</span><span class=mspace style=margin-right:.2778em></span><span class="mord mathnormal" style=margin-right:.05278em>β</span><span class=mpunct>,</span></span></span><span style=top:-2.2254em><span class=pstrut style=height:3.008em></span><span class=mord><span class="mord text"><span class=mord>otherwise</span></span><span class=mord>.</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.2146em><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span><p>Smooth L1 损失（<code>torch.nn.SmoothL1Loss</code>）大体上等同于 Huber 损失，但多了一个额外的 $\beta$ 参数。</p><p>我们将之前的所有操作封装在一个循环中，尝试所有的损失函数，并将它们一同绘制出来：</p><figure class=align-center><img loading=lazy src=images/fig05_gradient_descent_all_losses.svg#center alt=通过各种损失函数的梯度下降过程可视化。><figcaption><p>通过各种损失函数的梯度下降过程可视化。</p></figcaption></figure><p>我们可以看到非 L2 损失函数的有趣特征。尽管 L2 损失函数较为平滑，数值可达到 100，但其他损失函数的数值较小，因为它们仅反映绝对误差。L2 损失的陡峭梯度使得优化器更快接近全局最小值，这从它早期点之间的较大间距中可以看出。与此同时，L1 损失则显示出对最小值的更缓慢接近。</p><h3 id=动量>动量<a hidden class=anchor aria-hidden=true href=#动量>#</a></h3><p>下一个重要的参数是动量（Momentum）。在具有“峡谷形”几何的目标函数上（某一维度的曲率远大于另一维度），普通 SGD 会在陡峭方向来回“之”字形振荡，而沿着谷底方向推进缓慢。动量旨在沿“共识方向”加速、在相反方向抑振，从而允许更大的稳定步长并加快收敛。</p><p>标准的动量形式（<a href=https://www.sciencedirect.com/science/article/abs/pii/0041555364901375>Polyak, 1964</a>）为：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>v</mi><mi>t</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>γ</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mtext> </mtext><msub><mi>v</mi><mi>t</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
v_t &amp;= \gamma v_{t-1} + \nabla_\theta J(\theta_t)\\
\theta_{t+1} &amp;= \theta_t - \eta\, v_t.
\end{aligned}
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:3em;vertical-align:-1.25em></span><span class=mord><span class=mtable><span class=col-align-r><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.75em><span style=top:-3.91em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span><span style=top:-2.41em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.25em><span></span></span></span></span></span><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.75em><span style=top:-3.91em><span class=pstrut style=height:3em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class=mord>∇</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.02778em>θ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>)</span></span></span><span style=top:-2.41em><span class=pstrut style=height:3em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mord>.</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.25em><span></span></span></span></span></span></span></span></span></span></span></span><p>本文采用把学习率 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span></span></span></span> 放在参数更新中的写法；一些实现会把 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span></span></span></span> 合并进 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">v_t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5806em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span> 或对梯度使用相反符号，但本质等价。直观地可以把参数看作在损失地形上滚动的“重球”：当连续多步梯度方向一致时，速度项 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">v_t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5806em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span> 会累加，使有效步长被放大；当梯度方向频繁改变时，<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\gamma v_{t-1}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6389em;vertical-align:-.2083em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span></span></span></span> 的惯性会抵消来回摆动。</p><p>在一维恒定梯度 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>g</span></span></span></span> 的简化情形下，速度收敛到 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi mathvariant="normal">∞</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow></mfrac><mtext> </mtext><mi>g</mi></mrow><annotation encoding="application/x-tex">v_\infty = \tfrac{1}{1-\gamma}\, g</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5806em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1.3262em;vertical-align:-.4811em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8451em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style=margin-right:.05556em>γ</span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4811em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.03588em>g</span></span></span></span>，于是单步参数更新的有效幅度约为 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>η</mi><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\tfrac{\eta}{1-\gamma}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.2286em;vertical-align:-.4811em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.7475em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style=margin-right:.05556em>γ</span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.4461em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:.03588em>η</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4811em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。例如 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\gamma=0.9</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>0.9</span></span></span></span> 时，有效步长约为普通 SGD 的 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>10</span><span class=mord>×</span></span></span></span>。为便于观察本文的可视化效果，我们将 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span></span></span></span> 设为较大的值 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.9</mn></mrow><annotation encoding="application/x-tex">0.9</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6444em></span><span class=mord>0.9</span></span></span></span>：</p><figure class=align-center><img loading=lazy src=images/fig06_gradient_descent_all_losses_momentum.svg#center alt=通过各种损失函数的梯度下降过程可视化，展示高动量的影响。><figcaption><p>通过各种损失函数的梯度下降过程可视化，展示高动量的影响。</p></figcaption></figure><p>可以清楚看到高动量带来的影响：在峡谷的陡峭方向，惯性项 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\gamma v_{t-1}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6389em;vertical-align:-.2083em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span></span></span></span> 抑制了“之”字形振荡；但在接近全局最小值时，由于累积的动量，轨迹会出现“超调”，需要若干步才能衰减回稳，这在 L2 损失（曲率更大）下最为明显。若同时把学习率 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span></span></span></span> 取过大，甚至可能出现持续振荡或接近发散。实际使用中常见的取值是 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0.9</mn><mo separator="true">,</mo><mn>0.99</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\gamma\in[0.9, 0.99]</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7335em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>[</span><span class=mord>0.9</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord>0.99</span><span class=mclose>]</span></span></span></span>。</p><div class="notice notice-tip" role=note><span class=notice-icon aria-hidden=true><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2v2m0 16v2M4.93 4.93l1.41 1.41M17.66 17.66l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41"/><circle cx="12" cy="12" r="4"/><path d="M12 16v1a2 2 0 002 2h0a2 2 0 002-2v-1"/></svg></span><div class=notice-content>建议在交互式可视化中，调整动量参数时，以更明显地观察动量对梯度下降的影响。</div></div><h3 id=nesterov-加速梯度法>Nesterov 加速梯度法<a hidden class=anchor aria-hidden=true href=#nesterov-加速梯度法>#</a></h3><p>Nesterov 加速梯度法（Nesterov Accelerated Gradient, NAG）在“前瞻”位置计算梯度：不是在当前参数 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\theta_t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span> 处，而是在仅由惯性项带来的预估位置处评估梯度，从而实现“先大步、后修正”。与上文约定一致，我们将学习率 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span></span></span></span> 放在参数更新中：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>θ</mi><mtext>look</mtext></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mtext> </mtext><mi>γ</mi><mtext> </mtext><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>v</mi><mi>t</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>γ</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><msub><mi>θ</mi><mtext>look</mtext></msub><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mtext> </mtext><msub><mi>v</mi><mi>t</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\theta_{\text{look}} &amp;= \theta_t - \eta\,\gamma\, v_{t-1} \\
v_t &amp;= \gamma v_{t-1} + \nabla_\theta J\big(\theta_{\text{look}}\big) \\
\theta_{t+1} &amp;= \theta_t - \eta\, v_t.
\end{aligned}
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:4.51em;vertical-align:-2.005em></span><span class=mord><span class=mtable><span class=col-align-r><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.505em><span style=top:-4.665em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">look</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span><span style=top:-3.155em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span><span style=top:-1.655em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.005em><span></span></span></span></span></span><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.505em><span style=top:-4.665em><span class=pstrut style=height:3em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span></span></span><span style=top:-3.155em><span class=pstrut style=height:3em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class=mord>∇</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.02778em>θ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=mord><span class="delimsizing size1">(</span></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">look</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mord><span class="delimsizing size1">)</span></span></span></span><span style=top:-1.655em><span class=pstrut style=height:3em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mord>.</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.005em><span></span></span></span></span></span></span></span></span></span></span></span><p>其中：第一行给出仅由动量项产生的前瞻位置；第二行在前瞻点处评估梯度并更新速度；第三行用更新后的速度完成一步参数更新。</p><p>直观理解：普通动量先用“旧位置”的梯度叠加速度再更新；而 Nesterov 先按惯性“冲到前瞻点”，再在该点测量梯度并微调方向。这种预判能在“峡谷形”几何下减少超调并提升收敛响应性；在凸问题上也有更强的收敛结果报道（实践中常较标准动量略优）。一些实现会把 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span></span></span></span> 合并进 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">v_t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5806em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span> 中，写作：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><mi>γ</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>η</mi><mtext> </mtext><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>γ</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo><mo separator="true">,</mo><mspace width="1em"/><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><msub><mi>v</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">
v_t = \gamma v_{t-1} + \eta\, \nabla_\theta J\big(\theta_t - \gamma v_{t-1}\big),\quad \theta_{t+1} = \theta_t - v_t,</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5806em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.7917em;vertical-align:-.2083em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1.2em;vertical-align:-.35em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class=mord>∇</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.02778em>θ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=mord><span class="delimsizing size1">(</span></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1.2em;vertical-align:-.35em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span><span class=mord><span class="delimsizing size1">)</span></span><span class=mpunct>,</span><span class=mspace style=margin-right:1em></span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mpunct>,</span></span></span></span></span><p>两种写法在含义上等价。</p><figure class=align-center><img loading=lazy src=images/fig07_momentum_vs_nesterov.svg#center alt="（a）动量与（b）Nesterov 动量的比较。" width=600><figcaption><p>（a）动量与（b）Nesterov 动量的比较。</p></figcaption></figure><figure class=align-center><img loading=lazy src=images/fig08_gradient_descent_all_losses_nesterov.svg#center alt="通过各种损失函数的梯度下降过程可视化，展示高 Nesterov 动量的影响。"><figcaption><p>通过各种损失函数的梯度下降过程可视化，展示高 Nesterov 动量的影响。</p></figcaption></figure><p>从图中可以看出，Nesterov 动量相对普通动量更少超调：它先沿 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\gamma v_{t-1}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6389em;vertical-align:-.2083em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span></span></span></span> 的方向做大步前瞻，再用前瞻点的梯度做修正，整体轨迹更“稳”。在 L2 损失（曲率更大）下这一差异更明显。</p><h3 id=权重衰减>权重衰减<a hidden class=anchor aria-hidden=true href=#权重衰减>#</a></h3><p>权重衰减（weight decay）通过在目标函数上添加 L2 正则项来抑制过大的参数，鼓励更简单的模型：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>J</mi><mi>λ</mi></msub><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mtext>  </mtext><mo>=</mo><mtext>  </mtext><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mtext>  </mtext><mo>+</mo><mtext>  </mtext><mstyle displaystyle="false" scriptlevel="0"><mfrac><mi>λ</mi><mn>2</mn></mfrac></mstyle><mtext> </mtext><mo stretchy="false">∥</mo><mi>θ</mi><msubsup><mo stretchy="false">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">
J_\lambda(\theta) \;=\; J(\theta) \;+\; \tfrac{\lambda}{2}\,\lVert \theta \rVert_2^2,
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:-.0962em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">λ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1.2251em;vertical-align:-.345em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8801em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">λ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.345em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:.1667em></span><span class=mopen>∥</span><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=mclose><span class=mclose>∥</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8641em><span style=top:-2.453em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style=top:-3.113em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.247em><span></span></span></span></span></span></span><span class=mpunct>,</span></span></span></span></span><p>其中 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6944em></span><span class="mord mathnormal" style=margin-right:.02778em>θ</span></span></span></span> 表示所有可训练参数，<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda \ge 0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8304em;vertical-align:-.136em></span><span class="mord mathnormal">λ</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>≥</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>0</span></span></span></span> 为正则化强度。在标准（带或不带动量/Nesterov）的 SGD 中，其对应的更新可写为</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext>  </mtext><mo>=</mo><mtext>  </mtext><msub><mi>θ</mi><mi>t</mi></msub><mtext>  </mtext><mo>−</mo><mtext>  </mtext><mi>η</mi><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mtext>  </mtext><mo>+</mo><mtext>  </mtext><mi>λ</mi><mtext> </mtext><msub><mi>θ</mi><mi>t</mi></msub><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">
\theta_{t+1} \;=\; \theta_t \;-\; \eta\big(\nabla_\theta J(\theta_t) \;+\; \lambda\,\theta_t\big).
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.9028em;vertical-align:-.2083em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1.2em;vertical-align:-.35em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span><span class=mord><span class="delimsizing size1">(</span></span><span class=mord><span class=mord>∇</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.02778em>θ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2778em></span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1.2em;vertical-align:-.35em></span><span class="mord mathnormal">λ</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mord><span class="delimsizing size1">)</span></span><span class=mord>.</span></span></span></span></span><p>这等价于在每一步对参数做一次“收缩”（向零靠拢）并沿负梯度方向前进。对于本文的二维示例 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(w,b)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.02691em>w</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal">b</span><span class=mclose>)</span></span></span></span>，正则项会把解从理想全局最小值拉向原点 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0,0)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>(</span><span class=mord>0</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord>0</span><span class=mclose>)</span></span></span></span>。</p><figure class=align-center><img loading=lazy src=images/fig09_gradient_descent_all_losses_nesterov_weight_decay.svg#center alt="通过各种损失函数的梯度下降过程可视化，展示高 Nesterov 动量和权重衰减的影响。"><figcaption><p>通过各种损失函数的梯度下降过程可视化，展示高 Nesterov 动量和权重衰减的影响。</p></figcaption></figure><p>与其他损失相比，L2 损失下这种“拉回”最不显著，因为其更大的曲率使得梯度项相对更强。对于自适应优化器，常见做法是使用“解耦”权重衰减（AdamW；<a href=https://arxiv.org/abs/1711.05101>Decoupled Weight Decay Regularization</a>），但对纯 SGD，以上写法与在目标中加入 L2 正则是等价的。</p><div class="notice notice-tip" role=note><span class=notice-icon aria-hidden=true><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2v2m0 16v2M4.93 4.93l1.41 1.41M17.66 17.66l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41"/><circle cx="12" cy="12" r="4"/><path d="M12 16v1a2 2 0 002 2h0a2 2 0 002-2v-1"/></svg></span><div class=notice-content><p>实践中通常不对 bias（以及如 BatchNorm 的缩放/偏移参数）使用 weight decay，以免无谓惩罚平移项。下面给出仅对权重衰减、不对偏置衰减的简易伪代码：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Split parameters: apply weight decay only to weights (not to biases)</span>
</span></span><span class=line><span class=cl><span class=n>weight_params</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>()</span> <span class=k>if</span> <span class=s2>&#34;bias&#34;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>name</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>bias_params</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span> <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>()</span> <span class=k>if</span> <span class=s2>&#34;bias&#34;</span> <span class=ow>in</span> <span class=n>name</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>SGD</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=n>weight_params</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=n>bias_params</span><span class=p>,</span> <span class=s2>&#34;weight_decay&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span><span class=o>=</span><span class=mf>1e-2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lr</span><span class=o>=</span><span class=mf>1e-2</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div></div><h3 id=动量抑制因子>动量抑制因子<a hidden class=anchor aria-hidden=true href=#动量抑制因子>#</a></h3><p>动量抑制因子（dampening）用来降低“当前梯度”注入到动量项中的权重，从而减弱瞬时梯度对速度项 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">v_t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5806em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span> 的驱动。令动量系数为 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\gamma \in [0,1)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7335em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>[</span><span class=mord>0</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord>1</span><span class=mclose>)</span></span></span></span>，抑制因子为 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\alpha\in[0,1]</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5782em;vertical-align:-.0391em></span><span class="mord mathnormal" style=margin-right:.0037em>α</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>[</span><span class=mord>0</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord>1</span><span class=mclose>]</span></span></span></span>。带抑制因子的更新可写为：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>v</mi><mi>t</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>γ</mi><mtext> </mtext><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><mtext> </mtext><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><mtext> </mtext><msub><mi>v</mi><mi>t</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
v_t &amp;= \gamma\, v_{t-1} + (1-\alpha)\,\nabla_\theta J(\theta_t),\\
\theta_{t+1} &amp;= \theta_t - \eta\, v_t.
\end{aligned}
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:3em;vertical-align:-1.25em></span><span class=mord><span class=mtable><span class=col-align-r><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.75em><span style=top:-3.91em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span><span style=top:-2.41em><span class=pstrut style=height:3em></span><span class=mord><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.25em><span></span></span></span></span></span><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.75em><span style=top:-3.91em><span class=pstrut style=height:3em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2083em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span><span class=mopen>(</span><span class=mord>1</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.0037em>α</span><span class=mclose>)</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class=mord>∇</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.02778em>θ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:.09618em>J</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>)</span><span class=mpunct>,</span></span></span><span style=top:-2.41em><span class=pstrut style=height:3em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mord><span class="mord mathnormal" style=margin-right:.02778em>θ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0278em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.2806em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mord>.</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.25em><span></span></span></span></span></span></span></span></span></span></span></span><p>当 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha=0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.4306em></span><span class="mord mathnormal" style=margin-right:.0037em>α</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>0</span></span></span></span> 时退化为标准动量；<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.4306em></span><span class="mord mathnormal" style=margin-right:.0037em>α</span></span></span></span> 越大，当前梯度的“即时贡献”越被抑制，轨迹更平滑但响应更慢。在一维恒定梯度 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>g</span></span></span></span> 的简化情形下，稳态速度</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>v</mi><mi mathvariant="normal">∞</mi></msub><mo>=</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow></mfrac><mtext> </mtext><mi>g</mi><mspace width="1em"/><mo>⇒</mo><mspace width="1em"/><mtext>有效步长</mtext><mo>≈</mo><mfrac><mrow><mi>η</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">
v_\infty=\frac{1-\alpha}{1-\gamma}\,g \quad\Rightarrow\quad \text{有效步长}\approx \frac{\eta(1-\alpha)}{1-\gamma},
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5806em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1514em><span style=top:-2.55em;margin-left:-.0359em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:2.2019em;vertical-align:-.8804em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3214em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.0037em>α</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.8804em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.03588em>g</span><span class=mspace style=margin-right:1em></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>⇒</span><span class=mspace style=margin-right:1em></span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6833em></span><span class="mord text"><span class="mord cjk_fallback">有效步长</span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>≈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:2.3074em;vertical-align:-.8804em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.427em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord mathnormal" style=margin-right:.03588em>η</span><span class=mopen>(</span><span class=mord>1</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:.2222em></span><span class="mord mathnormal" style=margin-right:.0037em>α</span><span class=mclose>)</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.8804em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct>,</span></span></span></span></span><p>表明抑制因子本质上缩小了有效步长与动量累积强度，可缓解超调（overshoot），但可能降低收敛速度。本文可视化中把 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.8</mn></mrow><annotation encoding="application/x-tex">\alpha=0.8</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.4306em></span><span class="mord mathnormal" style=margin-right:.0037em>α</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>0.8</span></span></span></span> 仅用于放大这种效应，便于观察轨迹变化。</p><div class="notice notice-note" role=note><span class=notice-icon aria-hidden=true><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M19 21l-7-5-7 5V5a2 2 0 012-2h10a2 2 0 012 2z"/></svg></span><div class=notice-content>与 Nesterov 搭配时需注意：在 PyTorch 中启用 <code>nesterov=True</code> 时要求 <code>dampening=0</code> 才对应经典 NAG（见 <a href=https://pytorch.org/docs/stable/generated/torch.optim.SGD.html>SGD 文档</a>）。实践中通常保持 <code>dampening=0</code>；若需减弱过强动量或减少超调，优先调低动量系数 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.05556em>γ</span></span></span></span> 或学习率 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.03588em>η</span></span></span></span>。</div></div><figure class=align-center><img loading=lazy src=images/fig10_gradient_descent_all_losses_momentum_dampening.svg#center alt=通过各种损失函数的梯度下降过程可视化，展示高动量和高动量抑制因子的影响。><figcaption><p>通过各种损失函数的梯度下降过程可视化，展示高动量和高动量抑制因子的影响。</p></figcaption></figure><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ol><li>G. Goh, &ldquo;Why Momentum Really Works&rdquo;, Distill, 2017. <a href=https://distill.pub/2017/momentum>https://distill.pub/2017/momentum</a></li><li>S. Ruder, &ldquo;An overview of gradient descent optimization algorithms&rdquo;, 2016. <a href=https://www.ruder.io/optimizing-gradient-descent>https://www.ruder.io/optimizing-gradient-descent</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://ehehe.cn/tags/pytorch/>PyTorch</a></li><li><a href=https://ehehe.cn/tags/sgd/>SGD</a></li><li><a href=https://ehehe.cn/tags/%E4%BC%98%E5%8C%96%E5%99%A8/>优化器</a></li><li><a href=https://ehehe.cn/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/>梯度下降</a></li><li><a href=https://ehehe.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li><li><a href=https://ehehe.cn/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/>损失函数</a></li><li><a href=https://ehehe.cn/tags/%E5%8A%A8%E9%87%8F/>动量</a></li></ul><nav class=paginav><a class=prev href=https://ehehe.cn/posts/2025/04-lejepa/><span class=title>« Prev</span><br><span>LeJEPA：可证明、可扩展的自监督学习新范式</span>
</a><a class=next href=https://ehehe.cn/posts/2025/01-flappy-bird-dqn/><span class=title>Next »</span><br><span>强化学习玩 Flappy Bird</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ehehe.cn/>Yan Tang</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){function e(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",e,{once:!0}):e()})()</script><script>(function(){var n=window.pageYOffset||document.documentElement.scrollTop||0,s=800,t=!1,e=null;function o(){if(t=!1,e||(e=document.getElementById("top-link")),!e)return;var o=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,i=o>n;n=o<0?0:o,o>s&&!i?(e.style.visibility="visible",e.style.opacity="1"):(e.style.visibility="hidden",e.style.opacity="0")}window.addEventListener("scroll",function(){t||(window.requestAnimationFrame(o),t=!0)},{passive:!0})})()</script><script>(function(){var t=10;function n(e){if(!e)return[0,20];var n,s,o=window.getComputedStyle(e),t=parseFloat(o.lineHeight);return(!t||isNaN(t))&&(t=20),n=e.getBoundingClientRect(),s=n.height||e.offsetHeight||0,[Math.round(s/t),t]}function e(){var e=document.querySelectorAll(".highlight");if(!e||!e.length)return;Array.prototype.forEach.call(e,function(e){if(e.classList.contains("expanded")||e.classList.contains("collapsible"))return;var s,o,a,r,c,l,i=e.querySelector("pre code");if(!i)return;if(r=i.className||"",r.indexOf("language-mermaid")>=0||e.classList.contains("mermaid"))return;if(a=n(i),c=a[0],l=a[1],c<=t)return;e.classList.add("collapsible"),e.style.setProperty("--code-line-height",l+"px"),e.id||(e.id="code-block-"+Math.random().toString(36).slice(2)),o=document.createElement("div"),o.className="code-expand-wrapper",s=document.createElement("button"),s.type="button",s.className="code-expand-link",s.textContent="Show more",s.setAttribute("aria-expanded","false"),s.setAttribute("aria-controls",e.id),s.addEventListener("click",function(){var n,i,a,o=e.classList.contains("expanded"),r=getComputedStyle(e).getPropertyValue("--code-line-height"),t=parseFloat(r)*(parseFloat(getComputedStyle(e).getPropertyValue("--code-max-lines"))||10);(!isFinite(t)||t<=0)&&(t=10*20),i=e.getBoundingClientRect().height,a=o?t:e.scrollHeight,e.style.maxHeight=i+"px",e.offsetHeight,o?e.classList.remove("expanded"):e.classList.add("expanded"),e.style.maxHeight=a+"px",n=function(t){if(t.propertyName!=="max-height")return;if(e.removeEventListener("transitionend",n),e.style.maxHeight="",e.classList.contains("expanded"))s.textContent="Show less",s.setAttribute("aria-expanded","true");else{s.textContent="Show more",s.setAttribute("aria-expanded","false");try{e.scrollIntoView({behavior:"smooth",block:"nearest"})}catch{}}window.dispatchEvent(new Event("resize"))},e.addEventListener("transitionend",n)}),o.appendChild(s),e.nextSibling?e.parentNode.insertBefore(o,e.nextSibling):e.parentNode.appendChild(o)})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",e,{once:!0}):e()})()</script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>