<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Vision Transformer —— 图像识别中的 Transformer 架构 | Yan Tang</title><meta name=keywords content="Vision Transformer,Transformer"><meta name=description content="本文介绍了 Vision Transformer (ViT) 的核心概念，包括如何将 Transformer 架构应用于图像识别任务，以及与传统 CNNs 的比较。"><meta name=author content="Yan Tang"><link rel=canonical href=https://ehehe.cn/posts/2023/01-vit/><link crossorigin=anonymous href=/assets/css/stylesheet.16688c28815fab2857aba83927b88054dc9027b8a2b37dd2c695d9111db01da3.css integrity="sha256-FmiMKIFfqyhXq6g5J7iAVNyQJ7iis33SxpXZER2wHaM=" rel="preload stylesheet" as=style><link rel=icon href=https://ehehe.cn/assets/images/favicon-16x16.ico><link rel=icon type=image/png sizes=16x16 href=https://ehehe.cn/assets/images/favicon-16x16.ico><link rel=icon type=image/png sizes=32x32 href=https://ehehe.cn/assets/images/favicon-32x32.ico><link rel=apple-touch-icon href=https://ehehe.cn/assets/images/apple-touch-icon.png><link rel=mask-icon href=https://ehehe.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ehehe.cn/posts/2023/01-vit/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css integrity=sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js integrity=sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ crossorigin=anonymous></script><meta property="og:url" content="https://ehehe.cn/posts/2023/01-vit/"><meta property="og:site_name" content="Yan Tang"><meta property="og:title" content="Vision Transformer —— 图像识别中的 Transformer 架构"><meta property="og:description" content="本文介绍了 Vision Transformer (ViT) 的核心概念，包括如何将 Transformer 架构应用于图像识别任务，以及与传统 CNNs 的比较。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-26T15:27:24+00:00"><meta property="article:modified_time" content="2023-08-06T10:40:34+00:00"><meta property="article:tag" content="Vision Transformer"><meta property="article:tag" content="Transformer"><meta name=twitter:card content="summary"><meta name=twitter:title content="Vision Transformer —— 图像识别中的 Transformer 架构"><meta name=twitter:description content="本文介绍了 Vision Transformer (ViT) 的核心概念，包括如何将 Transformer 架构应用于图像识别任务，以及与传统 CNNs 的比较。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ehehe.cn/posts/"},{"@type":"ListItem","position":2,"name":"Vision Transformer —— 图像识别中的 Transformer 架构","item":"https://ehehe.cn/posts/2023/01-vit/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Vision Transformer —— 图像识别中的 Transformer 架构","name":"Vision Transformer —— 图像识别中的 Transformer 架构","description":"本文介绍了 Vision Transformer (ViT) 的核心概念，包括如何将 Transformer 架构应用于图像识别任务，以及与传统 CNNs 的比较。","keywords":["Vision Transformer","Transformer"],"articleBody":" 更新日志：\n2025年9月30日： 本文相关的 ViT 实验代码已开源至 GitHub 仓库 nanoViT。该项目实现了从头训练和微调 ViT 模型，支持 CIFAR-10、CIFAR-100 和 Tiny-ImageNet-200 数据集。 基于自注意力机制的模型，尤其是 Transformer，已经成为 NLP 中的首选模型。它们通过在大型语料库上进行预训练，然后在较少的任务数据集上进行微调，可以取得最优的性能。由于 Transformer 的计算效率和可扩展性，现在已经可以训练超过 100B100B100B 参数的大模型（实际上，目前的 GPT-4 由 888 个 220B220B220B 模型组成），并且 随着模型参数和数据集的不断增长，还没有出现性能饱和的现象。\n受 Transformer 的启发，Vision Transformer（ViT）在 19 年首次提出后，就以惊人之势席卷了计算机视觉领域，在几个月内获得了数百次引用。ViT 的工作证实了 一个经过简单调整用来处理图像数据的普通 Transformer，可以与最有效的卷积神经网络达到差不多的效果。\nViT 的结构相对比较简单，相对于普通 Transformer 的改动仅限于生成 patch embedding 和可学习得 class token 部分：\n将图像划分为多个 patch（类似 NLP 中的 token），将其展平并投影到 DDD 维嵌入空间，得到 patch embedding。 将 patch embedding 与 position embedding 相加，然后拼接上一个可学习的 class token，就可以输入 Transformer 编码器进行处理。position embedding 是一组可学习的向量，让模型保留位置信息。 最后，在 class token 上使用分类头，让模型进行图像分类任务。 Vision Transformer 将图像分割成 patches 并通过 Transformer 处理\nViT 在公开 ImageNet-21k 数据集和 Google 内部 JFT-300M 数据集上预训练后，在 CIFAR-10、CIFAR-100 和 ImageNet 等数据集上进行分类，都取得了最优的精度。\n这一巨大提升是由于 ViT 没有引入 归纳偏置（inductive bias） 所导致的，即卷积的 平移不变性（translation equivariance） 和 局部性（locality）。通过减少对数据的假设，ViT 能够更好地适应给定的任务。然而，没有归纳偏置的代价是需要大规模数据集预训练，在小数据集（或没有预训练权重）的情况下，CNNs 仍是更好的选择。这说明了大规模预训练的结果是要好于归纳偏置的。\nTransformer 概述 基于循环神经网络（RNNs）和长短期记忆网络（LSTM）的架构一直是自然语言处理的主要架构，直到 2017 年 Transformer（Attention is all you need）的出现。Transformer 是一种采用自注意力机制的深度学习模型，可以不同程度地加权输入数据的每个部分的重要性（affinity）。\n1. 为什么要用 Transformer？ Transformer 最早提出用于解决序列转录（sequence transduction）问题，即 Seq2Seq。早在 Transformer 之前，基于 RNNs 和 LSTM 的神经网络被广泛用于序列建模和转录任务，比如文本预测、机器翻译、文章生成等。\n以 Seq2Seq 为例，该模型包括编码器和解码器两部分。在 encode 阶段，编码器的输入为序列当前的词和之前输出的隐藏状态（hidden state）。最终，编码器的输出为这个序列最后一个词的输出，作为上下文向量（context vector）。在 decode 阶段，将上下文向量作为解码器的输入，以每次都预测下一个词的方式（auto-regressive）得到最终的序列。\nSeq2Seq 模型由编码器和解码器组成\n基于 RNNs 的编码器和解码器的架构非常经典，然而，这种架构面临的主要问题有 如何记录长序列的上下文依赖，并且该架构 不能用于并行计算、内存占用多。\nTransformer 是第一个完全依赖于自注意力机制的深度学习模型，摒弃了循环神经网络。\nTransformer 完整架构包括编码器和解码器\n2. 编码器块 编码器部分是多个编码器块的叠加（N=6N=6N=6），每个块中的计算如下：\ngraph LR subgraph Encoder Block C[Self Attention] --\u003e M1((+)) N1 --\u003e D[Feed Forward] M1 --\u003e N1[Norm] N1 --\u003e M2((+)) D --\u003e M2 M2 --\u003e N2[Norm] end B[Input] --\u003e M1 B --\u003e C N2 --\u003e G[Output] 多头自注意力机制（Multi-Head Attention）：获取输入序列中的全局信息，然后产生一个新的词嵌入。 残差连接和层归一化（Add \u0026 Nrom）：避免深度网络训练中的梯度消失问题。 前馈神经网络（Feed Forward）：两层的 MLP，对每个词嵌入进行变换。 残差连接和层归一化（Add \u0026 Nrom）：与上面作用相同。 3. 解码器块 graph LR subgraph Decoder Block C[Masked Self Attention] --\u003e M1((+)) M1 --\u003e N1[Norm] N1 --\u003e D[Encoder-Decoder Attention] N1 --\u003e M2((+)) D --\u003e M2 M2 --\u003e N2[Norm] N2 --\u003e E[Feed Forward] N2 --\u003e M3((+)) E --\u003e M3 M3 --\u003e N3[Norm] end B[Input] --\u003e C B --\u003e M1 N3 --\u003e G[Output] 解码器与编码器的几乎是相同的，除了这里多一个 Encoder-Decoder 的注意力，用于融合编码器的输出。\n另外，这里的自注意力是 带有掩码的，为了避免当前的元素与它后面的元素产生交互。在编码器阶段，需要访问全局的信息进行交互，如果解码器还能访问全局信息那就失去意义了。掩码的作用是将当前位置之后的元素置为负无穷，这样在 Softmax 之后，就变成了 000。\nQ⋅KTdk=[9.13−∞−∞−∞5.012.37−∞−∞7.255.019.37−∞1.51.731.9710.0] \\frac{\\mathbf{Q}\\cdot \\mathbf{K}^{\\mathrm{T}}}{\\sqrt{d_k}}=\\left[ \\begin{matrix} 9.13\u0026\t-\\infty\u0026\t-\\infty\u0026\t-\\infty\\\\ 5.0\u0026\t12.37\u0026\t-\\infty\u0026\t-\\infty\\\\ 7.25\u0026\t5.0\u0026\t19.37\u0026\t-\\infty\\\\ 1.5\u0026\t1.73\u0026\t1.97\u0026\t10.0\\\\ \\end{matrix} \\right] dk​​Q⋅KT​=​9.135.07.251.5​−∞12.375.01.73​−∞−∞19.371.97​−∞−∞−∞10.0​​4. 位置编码 Transformer 的编码器接收一串词嵌入作为输入。然后，这些嵌入和 位置编码（Positional Encoding） 相加。位置编码的目的是向模型提供一些关于单词在句子中位置的信息，因为 Transformer 模型自身并不具备这种识别单词时序的能力。\n在原始的 Transformer 模型中，使用基于正弦和余弦函数的编码方式：\nPE(pos,2i)=sin⁡(pos100002i/dmodel)PE(pos,2i+1)=cos⁡(pos100002i/dmodel) \\begin{gathered} P E_{(pos, 2 i)}=\\sin \\left(\\frac{pos}{10000^{2 i / d_{\\text{model}}}}\\right) \\\\ P E_{(pos, 2 i+1)}=\\cos \\left(\\frac{pos}{10000^{2 i / d_{\\text{model}}}}\\right) \\end{gathered} PE(pos,2i)​=sin(100002i/dmodel​pos​)PE(pos,2i+1)​=cos(100002i/dmodel​pos​)​其中，pospospos 表示位置，iii 表示维度，dmodeld_{\\text{model}}dmodel​ 表示嵌入维度。\n假设处理一个长度为 505050 的序列，每个词的嵌入维度为 128128128，那么这个位置编码可视化为：\n长度为 50 的序列在 128 维嵌入空间中的位置编码可视化\n5. 自注意力机制 自注意力机制是 Transformer 架构中的核心机制，用于 计算序列中每个元素与其他元素之间的依赖关系，这种处理在文本数据中特别有用，因为每个单词通常都与其他单词存在相关性。通过这种方式，可以使网络更加关注有意义的信息，并对其余部分给予更少的关注。从实现的角度来看，注意力衡量两个向量之间的相似度，并返回加权相似度分数。\n在自注意力机制中，有三个重要概念：查询（Query），键（Key） 和 值（Value）。这些术语来自搜索引擎和推荐系统，因此下面是一个从数据库中检索的示例。\n想象一下，你输入“沙滩上的狗”，想从数据库中检索一张图片。在数据库内部，每张照片都由一组关键词所描述——“猫”、“狗”、“聚会”等。我们将这些关键词称为“键”。搜索引擎会将你的查询和数据库中的键进行对比。“狗”匹配了 111 个结果，“猫”匹配了 000 个结果。然后，它会按照匹配度（相关性）对这些键进行排序，并按相关性顺序返回前 nnn 张匹配图片。\n注意力机制类似数据库检索过程，查询（Query）与键（Key）匹配后返回对应的值（Value）\n下面是注意力机制的数学表达式：\nAttention⁡(Q,K,V)=Softmax⁡(QKTdk)V \\operatorname{Attention}(Q, K, V)=\\operatorname{Softmax}\\left(\\frac{Q K^{\\mathrm{T}}}{\\sqrt{d_k}}\\right) V Attention(Q,K,V)=Softmax(dk​​QKT​)V其中：QQQ，KKK，VVV 分别表示输入在经过投影后的查询矩阵、键矩阵和值矩阵。这一注意力机制称为 缩放点积注意力（scaled-dot product attention）。\n下面展示了单头的缩放点积注意力的计算流程及其权重矩阵：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import seaborn as sns import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.nn.functional as F # set no grad torch.set_grad_enabled(False) torch.manual_seed(1234) in_seq = [\"Hello\", \"world\", \"how\", \"are\", \"you\", \"doing\", \"today\", \"?\"] B, T, C = 1, 8, 32 # batch size, sequence length, embedding dimension x = torch.randn(B, T, C) # input head_size = 8 key = nn.Linear(C, head_size, bias=False) query = nn.Linear(C, head_size, bias=False) value = nn.Linear(C, head_size, bias=False) k = key(x) # (B, T, H) q = query(x) # (B, T, H) v = value(x) # (B, T, H) attention = q @ k.transpose(-2, -1) # (B, T, H) @ (B, H, T) -\u003e (B, T, T) attention = attention / (head_size**0.5) # scaled dot-product attention attention = F.softmax(attention, dim=-1) # (B, T, T) output = attention @ v # (B, T, T) @ (B, T, H) -\u003e (B, T, H) # visualize the attention weights plt.figure(figsize=(8, 6)) sns.heatmap( attention[0].numpy(), annot=True, cmap=\"viridis\", xticklabels=in_seq, yticklabels=in_seq, ) plt.title(\"Attention Weights\") plt.show() 自注意力机制中各 token 之间的注意力权重可视化，颜色越深表示注意力越强\n注意力机制的优势与劣势 长程依赖（Long-term Dependencies）：传统的 RNNs 容易出现随着处理序列边长，历史信息被减弱的问题。注意机制使得模型能够直接访问输入序列中的任何位置，缓解了这一问题，从而保留了完整的上下文信息。 并行性（Parallelization）：与需要顺序计算的 RNNs 不同，基于注意力的模型（如：Transformer）可以并行处理输入序列中的所有词嵌入。这使它们在计算上更高效，并且在硬件加速器上扩展性更好。 可解释性（Interpretability）：注意机制提供一定程度的可解释性，因为它突出显示模型认为对于产生特定输出最重要的输入部分。 全局上下文（Global Context）: 在卷积神经网络（CNNs）中，感受野通常是局部的，并且取决于卷积核的大小。然而，通过注意力，每个位置的元素可以考虑输入序列中的其他元素的信息，从而保留全局上下文。 更好的性能（Improved Performance）：基于注意力的模型，特别是 Transformer，在各项自然语言处理任务中都获得了最好的性能。并且也推动了其他领域的发展，如：计算机视觉、语言识别、多模态等。 尽管注意力机制存在上述优点，它们也存在一些限制。注意力机制中，随着序列长度的增加，内存消耗和计算成本都呈 O(N2)\\mathcal{O}(N^2)O(N2) 增加。这里的复杂度没考虑嵌入维度，实际上是 O(N2⋅d)\\mathcal{O}(N^2 \\cdot d)O(N2⋅d)。另外，虽然在训练时 Transformer 具有并行化的优势，在推理过程中，仍然是使用自回归的方式一个词一个词的输出。\n如果查询（Query）、键（Key）和值（Value）来自同一输入，此时的注意力被称为 自注意力（self-attention）。如果它们来自不同的输入，则称为 交叉注意力（cross-attention）。 最后，在 Transformer 中，对注意力机制还做了一点改进—— 多头注意力机制。多头注意力机制允许模型 同时关注不同位置的不同表示子空间中的信息。\nMultiHead⁡(Q,K,V)=Concat⁡(head1,…,headh)WOwhere headi=Attention⁡(QWiQ,KWiK,VWiV) \\begin{aligned} \\operatorname{MultiHead}(Q, K, V) \u0026= \\operatorname{Concat} \\left( \\text{head}_1, \\ldots, \\text{head}_{\\mathrm{h}}\\right) W^O \\\\ \\text{where} \\space \\text{head}_{i} \u0026=\\operatorname{Attention}\\left(Q W_i^Q, K W_i^K, V W_i^V\\right) \\end{aligned} MultiHead(Q,K,V)where headi​​=Concat(head1​,…,headh​)WO=Attention(QWiQ​,KWiK​,VWiV​)​6. 前馈神经网络 Transformer 中还包含一个 MLP 对特征嵌入进行变化，该 MLP 由两个线性层组成，中间使用 ReLU 激活函数。需要注意的是，该 MLP 的输入和输出维度都是 dmodeld_{\\text{model}}dmodel​，在中间层则会将特征维度扩大四倍：4×dmodel4 \\times d_{\\text{model}}4×dmodel​。在 ViT 中也延续了这一做法。\nFFN⁡(x)=max⁡(0,xW1+b1)W2+b2 \\operatorname{FFN}(x)=\\max \\left(0, x W_1+b_1\\right) W_2+b_2 FFN(x)=max(0,xW1​+b1​)W2​+b2​ViT 1. Patch Embedding ViT 的工作方式与 Transformer 类似，但是 ViT 的输入是图像块（image patch），而非词元（word token）。\n如果不将图像分成图像块，直接将像素值作为输出。那么，自注意力模块需要在每个像素间进行交互，N=224×224N=224 \\times 224N=224×224。而注意力机制的复杂度又是平方级别的，即 2244≈2.5E9224^4 \\approx 2.5E92244≈2.5E9，这样的计算复杂度是难以接受的。不过现在的大语言模型，claude 可以做到 N=100kN=100kN=100k，GPT-4 可以做到 N=32kN=32kN=32k，转换成图像的话，已经可以处理 316×316316 \\times 316316×316 和 178×178178 \\times 178178×178 大小的图像了。\n在 patch embedding 中，图像被分割成相同高度和宽度 PPP 的多个块：\nx∈RH×W×C⇒x∈RN×P2C x \\in \\mathbb{R}^{H \\times W \\times C} \\Rightarrow x \\in \\mathbb{R}^{N \\times P^2 C} x∈RH×W×C⇒x∈RN×P2C其中：\nN=HWP2N=\\frac{HW}{P^2}N=P2HW​ —— 图像块的数量； PPP —— 图像块的宽高，每个块的大小为 (P,P,C)(P,P,C)(P,P,C)； CCC —— 输入图像的通道数。 将图像分割成固定大小的 patches，每个 patch 展平后作为 Transformer 的输入\n在构建完图像块之后，使用线性层将图像块映射到嵌入空间。以上是 ViT 论文中的描述，在代码实现中，构建图像块和线性映射使用一个卷积即可完成。\n下面是 PatchEmbedding 的 PyTorch 代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class PatchEmbedding(nn.Module): def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768): super().__init__() self.img_size = img_size self.patch_size = patch_size self.n_patches = (img_size // patch_size) ** 2 self.patch_size = patch_size self.in_chans = in_chans self.embed_dim = embed_dim self.proj = nn.Conv2d( in_chans, embed_dim, kernel_size=patch_size, stride=patch_size ) def forward(self, x): # proj: [B, 3, H, W] -\u003e [B, embed, n_patches_h, n_patches_w] # flatten: [B, embed, n_patches_h, n_patches_w] -\u003e [B, embed, n_patches] # transpose: [B, embed, n_patches] -\u003e [B, n_patches, embed] x = self.proj(x).flatten(2).transpose(1, 2) return x 2. [CLS] Token 这个 [CLS] token 在自然语言处理中起源于 BERT 模型。在 BERT 中，[CLS] token 被添加到每个输入序列的开始处，并用于聚合整个序列的信息以进行分类任务。[CLS] token 的输出通常被用作下游任务的输入，例如：句子分类。\n同样，在 ViT 中，[CLS] token 在所有的 patch embedding 之前，其目的是 聚合整个图像的信息。在模型的前向传播过程中，[CLS] token 与所有的 patch embedding 一起通过 Transformer。在最后一层，我们取 [CLS] token 的输出，并使用 MLP 进行分类任务。\n3. 位置嵌入 Transformer 模型没有任何机制来考虑 token 或 patch embedding 的顺序。然而，顺序是非常关键的。在语言中，词语的顺序可以完全改变它们的意思。\n对于图像也是一样。如果给定一组乱码的拼图，对于一个人来说准确预测这一个图代表什么是非常困难甚至不可能的。因此，我们需要一种方式使得模型能够推断拼图块的顺序或位置。\n在 ViT 中，使用位置嵌入来实现顺序的机制，这些 位置嵌入是与块嵌入具有相同维度的可学习向量。\n在创建图像块嵌入并在前面添加 [CLS] token 之后，将它们和位置嵌入一起求和，作为 Transformer 的输入。这些位置嵌入在模型训练过程中一起学习。\n下图展示了每一个位置嵌入和其他位置嵌入之间的余弦相似度。可以看到，每一个位置嵌入都与它所在行和列的位置嵌入具有最高的相似度。因此位置嵌入可以学习到当前图像块在图像中的位置。\n各位置嵌入之间的余弦相似度，显示出行和列的相关性模式\n4. ViT 是否能输入不同大小的图像？ 显然 224×224224 \\times 224224×224 或者 384×384384 \\times 384384×384 大小的输入图像并不能满足很多视觉任务的需求，比如图像分割、目标检测等。\n当输入更大尺寸的图像时，我们保持图像块大小 PPP 不变，这样会导致更长的输入序列 N=HWP2N=\\frac{HW}{P^{2}}N=P2HW​，这一步对于 Patch Embedding 没有影响。但是，对于位置嵌入就会出问题，因为预训练的位置嵌入可能不再有意义了。ViT 中解决这一问题的方式是 使用双线性插值来缩放位置嵌入，来满足新的图像块大小。\n当前，这种方式并不完美，后续有的工作中放弃了位置编码或对其进行改进（SegFormer，Swin Transformer 等）。\n5. 注意力图可视化 以下是ViT注意力图的可视化对比，展示了原始注意力图与改进方法的对比效果：\n原始图像 原始注意力图 (mean) 改进注意力图 (max fusion) 6. 实验 下面是实现了一个简易版本的 ViT，并通过加载 Hugging Face 上的预训练权重，在 CIFAR10 和 CIFAR100 上对 ViT 论文中的相关实验的复现结果。\nViT 在 CIFAR10 和 CIFAR100 数据集上的实验结果对比，展示了不同预训练策略的效果\n参考资料 Investigating Vision Transformer representations Vision Transformers (ViT) Explained Exploring Explainability for Vision Transformers ","wordCount":"939","inLanguage":"en","datePublished":"2023-07-26T15:27:24Z","dateModified":"2023-08-06T10:40:34Z","author":{"@type":"Person","name":"Yan Tang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ehehe.cn/posts/2023/01-vit/"},"publisher":{"@type":"Organization","name":"Yan Tang","logo":{"@type":"ImageObject","url":"https://ehehe.cn/assets/images/favicon-16x16.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://ehehe.cn/ accesskey=h title="Yan Tang (Alt + H)">Yan Tang</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ehehe.cn/ title=Posts><span>Posts</span></a></li><li><a href=https://ehehe.cn/about/ title=About><span>About</span></a></li><li><a href=https://ehehe.cn/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://ehehe.cn/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Vision Transformer —— 图像识别中的 Transformer 架构</h1><div class=post-meta><span title='2023-07-26 15:27:24 +0000 UTC'>July 26, 2023</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>Yan Tang</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#transformer-%e6%a6%82%e8%bf%b0 aria-label="Transformer 概述">Transformer 概述</a><ul><li><a href=#1-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e7%94%a8-transformer aria-label="1. 为什么要用 Transformer？">1. 为什么要用 Transformer？</a></li><li><a href=#2-%e7%bc%96%e7%a0%81%e5%99%a8%e5%9d%97 aria-label="2. 编码器块">2. 编码器块</a></li><li><a href=#3-%e8%a7%a3%e7%a0%81%e5%99%a8%e5%9d%97 aria-label="3. 解码器块">3. 解码器块</a></li><li><a href=#4-%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81 aria-label="4. 位置编码">4. 位置编码</a></li><li><a href=#5-%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label="5. 自注意力机制">5. 自注意力机制</a><ul><li><a href=#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e4%bc%98%e5%8a%bf%e4%b8%8e%e5%8a%a3%e5%8a%bf aria-label=注意力机制的优势与劣势>注意力机制的优势与劣势</a></li></ul></li><li><a href=#6-%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="6. 前馈神经网络">6. 前馈神经网络</a></li></ul></li><li><a href=#vit aria-label=ViT>ViT</a><ul><li><a href=#1-patch-embedding aria-label="1. Patch Embedding">1. Patch Embedding</a></li><li><a href=#2-cls-token aria-label="2. [CLS] Token">2. [CLS] Token</a></li><li><a href=#3-%e4%bd%8d%e7%bd%ae%e5%b5%8c%e5%85%a5 aria-label="3. 位置嵌入">3. 位置嵌入</a></li><li><a href=#4-vit-%e6%98%af%e5%90%a6%e8%83%bd%e8%be%93%e5%85%a5%e4%b8%8d%e5%90%8c%e5%a4%a7%e5%b0%8f%e7%9a%84%e5%9b%be%e5%83%8f aria-label="4. ViT 是否能输入不同大小的图像？">4. ViT 是否能输入不同大小的图像？</a></li><li><a href=#5-%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%9b%be%e5%8f%af%e8%a7%86%e5%8c%96 aria-label="5. 注意力图可视化">5. 注意力图可视化</a></li><li><a href=#6-%e5%ae%9e%e9%aa%8c aria-label="6. 实验">6. 实验</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label=参考资料>参考资料</a></li></ul></div></details></div><div class=post-content><div class="notice notice-note" role=note><span class=notice-icon aria-hidden=true><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M19 21l-7-5-7 5V5a2 2 0 012-2h10a2 2 0 012 2z"/></svg></span><div class=notice-content><p><strong>更新日志</strong>：</p><ul><li><strong>2025年9月30日：</strong> 本文相关的 ViT 实验代码已开源至 GitHub 仓库 <a href=https://github.com/kenanking/nanoViT>nanoViT</a>。该项目实现了从头训练和微调 ViT 模型，支持 CIFAR-10、CIFAR-100 和 Tiny-ImageNet-200 数据集。</li></ul></div></div><p>基于自注意力机制的模型，尤其是 Transformer，已经成为 NLP 中的首选模型。它们通过在大型语料库上进行预训练，然后在较少的任务数据集上进行微调，可以取得最优的性能。由于 Transformer 的计算效率和可扩展性，现在已经可以训练超过 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mi>B</mi></mrow><annotation encoding="application/x-tex">100B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class=mord>100</span><span class="mord mathnormal" style=margin-right:.05017em>B</span></span></span></span> 参数的大模型（实际上，目前的 GPT-4 由 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6444em></span><span class=mord>8</span></span></span></span> 个 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>220</mn><mi>B</mi></mrow><annotation encoding="application/x-tex">220B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class=mord>220</span><span class="mord mathnormal" style=margin-right:.05017em>B</span></span></span></span> 模型组成），并且 <strong>随着模型参数和数据集的不断增长，还没有出现性能饱和的现象</strong>。</p><p>受 Transformer 的启发，Vision Transformer（ViT）在 19 年首次提出后，就以惊人之势席卷了计算机视觉领域，在几个月内获得了数百次引用。ViT 的工作证实了 <strong>一个经过简单调整用来处理图像数据的普通 Transformer，可以与最有效的卷积神经网络达到差不多的效果</strong>。</p><p>ViT 的结构相对比较简单，相对于普通 Transformer 的改动仅限于生成 patch embedding 和可学习得 class token 部分：</p><ul><li>将图像划分为多个 patch（类似 NLP 中的 token），将其展平并投影到 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.02778em>D</span></span></span></span> 维嵌入空间，得到 <strong>patch embedding</strong>。</li><li>将 patch embedding 与 position embedding 相加，然后拼接上一个可学习的 class token，就可以输入 Transformer 编码器进行处理。position embedding 是一组可学习的向量，让模型保留位置信息。</li><li>最后，在 class token 上使用分类头，让模型进行图像分类任务。</li></ul><figure><img loading=lazy src=images/vision_transformer.gif alt="Vision Transformer 架构图"><figcaption><p>Vision Transformer 将图像分割成 patches 并通过 Transformer 处理</p></figcaption></figure><p>ViT 在公开 ImageNet-21k 数据集和 Google 内部 JFT-300M 数据集上预训练后，在 CIFAR-10、CIFAR-100 和 ImageNet 等数据集上进行分类，都取得了最优的精度。</p><p>这一巨大提升是由于 ViT 没有引入 <strong>归纳偏置（inductive bias）</strong> 所导致的，即卷积的 <strong>平移不变性（translation equivariance）</strong> 和 <strong>局部性（locality）</strong>。通过减少对数据的假设，ViT 能够更好地适应给定的任务。然而，没有归纳偏置的代价是需要大规模数据集预训练，在小数据集（或没有预训练权重）的情况下，CNNs 仍是更好的选择。这说明了大规模预训练的结果是要好于归纳偏置的。</p><h2 id=transformer-概述>Transformer 概述<a hidden class=anchor aria-hidden=true href=#transformer-概述>#</a></h2><p>基于循环神经网络（RNNs）和长短期记忆网络（LSTM）的架构一直是自然语言处理的主要架构，直到 2017 年 Transformer（<a href=https://arxiv.org/abs/1706.03762>Attention is all you need</a>）的出现。Transformer 是一种采用自注意力机制的深度学习模型，可以不同程度地加权输入数据的每个部分的重要性（affinity）。</p><h3 id=1-为什么要用-transformer>1. 为什么要用 Transformer？<a hidden class=anchor aria-hidden=true href=#1-为什么要用-transformer>#</a></h3><p>Transformer 最早提出用于解决序列转录（sequence transduction）问题，即 Seq2Seq。早在 Transformer 之前，基于 RNNs 和 LSTM 的神经网络被广泛用于序列建模和转录任务，比如文本预测、机器翻译、文章生成等。</p><p>以 Seq2Seq 为例，该模型包括编码器和解码器两部分。在 encode 阶段，编码器的输入为序列当前的词和之前输出的隐藏状态（hidden state）。最终，编码器的输出为这个序列最后一个词的输出，作为上下文向量（context vector）。在 decode 阶段，将上下文向量作为解码器的输入，以每次都预测下一个词的方式（auto-regressive）得到最终的序列。</p><figure><img loading=lazy src=images/seq2seq.png alt="Seq2Seq 模型架构图"><figcaption><p>Seq2Seq 模型由编码器和解码器组成</p></figcaption></figure><p>基于 RNNs 的编码器和解码器的架构非常经典，然而，这种架构面临的主要问题有 <strong>如何记录长序列的上下文依赖</strong>，并且该架构 <strong>不能用于并行计算、内存占用多</strong>。</p><p>Transformer 是第一个完全依赖于自注意力机制的深度学习模型，摒弃了循环神经网络。</p><figure><img loading=lazy src=images/transformer.png alt="Transformer 架构图"><figcaption><p>Transformer 完整架构包括编码器和解码器</p></figcaption></figure><h3 id=2-编码器块>2. 编码器块<a hidden class=anchor aria-hidden=true href=#2-编码器块>#</a></h3><p>编码器部分是多个编码器块的叠加（<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">N=6</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.10903em>N</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>6</span></span></span></span>），每个块中的计算如下：</p><pre class=mermaid>graph LR
    subgraph Encoder Block
    C[Self Attention] --&gt; M1((+))
    N1 --&gt; D[Feed Forward]
    M1 --&gt; N1[Norm]
    N1 --&gt; M2((+))
    D --&gt; M2
    M2 --&gt; N2[Norm]
    end
    B[Input] --&gt; M1
    B --&gt; C
    N2 --&gt; G[Output]
</pre><ol><li><strong>多头自注意力机制（Multi-Head Attention）</strong>：获取输入序列中的全局信息，然后产生一个新的词嵌入。</li><li><strong>残差连接和层归一化（Add & Nrom）</strong>：避免深度网络训练中的梯度消失问题。</li><li><strong>前馈神经网络（Feed Forward）</strong>：两层的 MLP，对每个词嵌入进行变换。</li><li><strong>残差连接和层归一化（Add & Nrom）</strong>：与上面作用相同。</li></ol><h3 id=3-解码器块>3. 解码器块<a hidden class=anchor aria-hidden=true href=#3-解码器块>#</a></h3><pre class=mermaid>graph LR
    subgraph Decoder Block
    C[Masked Self Attention] --&gt; M1((+))
    M1 --&gt; N1[Norm]
    N1 --&gt; D[Encoder-Decoder Attention]
    N1 --&gt; M2((+))
    D --&gt; M2
    M2 --&gt; N2[Norm]
    N2 --&gt; E[Feed Forward]
    N2 --&gt; M3((+))
    E --&gt; M3
    M3 --&gt; N3[Norm]
    end
    B[Input] --&gt; C
    B --&gt; M1
    N3 --&gt; G[Output]
</pre><p>解码器与编码器的几乎是相同的，除了这里多一个 Encoder-Decoder 的注意力，用于融合编码器的输出。</p><p>另外，这里的自注意力是 <strong>带有掩码的</strong>，为了避免当前的元素与它后面的元素产生交互。在编码器阶段，需要访问全局的信息进行交互，如果解码器还能访问全局信息那就失去意义了。掩码的作用是将当前位置之后的元素置为负无穷，这样在 Softmax 之后，就变成了 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6444em></span><span class=mord>0</span></span></span></span>。</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="bold">Q</mi><mo>⋅</mo><msup><mi mathvariant="bold">K</mi><mi mathvariant="normal">T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9.13</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>5.0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>12.37</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>7.25</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>5.0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>19.37</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1.5</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1.73</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1.97</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>10.0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">
\frac{\mathbf{Q}\cdot \mathbf{K}^{\mathrm{T}}}{\sqrt{d_k}}=\left[ \begin{matrix}
	9.13&amp;		-\infty&amp;		-\infty&amp;		-\infty\\
	5.0&amp;		12.37&amp;		-\infty&amp;		-\infty\\
	7.25&amp;		5.0&amp;		19.37&amp;		-\infty\\
	1.5&amp;		1.73&amp;		1.97&amp;		10.0\\
\end{matrix} \right]
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:2.4483em;vertical-align:-.93em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.5183em><span style=top:-2.2528em><span class=pstrut style=height:3em></span><span class=mord><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8572em><span class=svg-align style=top:-3em><span class=pstrut style=height:3em></span><span class=mord style=padding-left:.833em><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.03148em>k</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span><span style=top:-2.8172em><span class=pstrut style=height:3em></span><span class=hide-tail style=min-width:.853em;height:1.08em><svg width="400em" height="1.08em" viewBox="0 0 4e5 1080" preserveAspectRatio="xMinYMin slice"><path d="M95 702c-2.7.0-7.17-2.7-13.5-8-5.8-5.3-9.5-10-9.5-14 0-2 .3-3.3 1-4 1.3-2.7 23.83-20.7 67.5-54 44.2-33.3 65.8-50.3 66.5-51 1.3-1.3 3-2 5-2 4.7.0 8.7 3.3 12 10s173 378 173 378c.7.0 35.3-71 104-213s137.5-285 206.5-429S812 97.3 814 94c5.3-9.3 12-14 20-14H4e5v40H845.2724s-225.272 467-225.272 467-235 486-235 486c-2.7 4.7-9 7-19 7-6 0-10-1-12-3s-194-422-194-422-65 47-65 47zM834 80h4e5v40H834z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.1828em><span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord mathbf">Q</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mord mathbf">K</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8413em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.93em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:4.8em;vertical-align:-2.15em></span><span class=minner><span class=mopen><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.65em><span style=top:-4.65em><span class=pstrut style=height:6.8em></span><span style=width:.667em;height:4.8em><svg width=".667em" height="4.8em" viewBox="0 0 667 4800"><path d="M403 1759V84H666V0H319V1759v12e2 1759h347v-84H403zm0 0V0H319V1759v12e2 1759h84z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.15em><span></span></span></span></span></span></span><span class=mord><span class=mtable><span class=col-align-c><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.65em><span style=top:-4.81em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>9.13</span></span></span><span style=top:-3.61em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>5.0</span></span></span><span style=top:-2.41em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>7.25</span></span></span><span style=top:-1.21em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1.5</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.15em><span></span></span></span></span></span><span class=arraycolsep style=width:.5em></span><span class=arraycolsep style=width:.5em></span><span class=col-align-c><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.65em><span style=top:-4.81em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>−</span><span class=mord>∞</span></span></span><span style=top:-3.61em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>12.37</span></span></span><span style=top:-2.41em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>5.0</span></span></span><span style=top:-1.21em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1.73</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.15em><span></span></span></span></span></span><span class=arraycolsep style=width:.5em></span><span class=arraycolsep style=width:.5em></span><span class=col-align-c><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.65em><span style=top:-4.81em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>−</span><span class=mord>∞</span></span></span><span style=top:-3.61em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>−</span><span class=mord>∞</span></span></span><span style=top:-2.41em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>19.37</span></span></span><span style=top:-1.21em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1.97</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.15em><span></span></span></span></span></span><span class=arraycolsep style=width:.5em></span><span class=arraycolsep style=width:.5em></span><span class=col-align-c><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.65em><span style=top:-4.81em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>−</span><span class=mord>∞</span></span></span><span style=top:-3.61em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>−</span><span class=mord>∞</span></span></span><span style=top:-2.41em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>−</span><span class=mord>∞</span></span></span><span style=top:-1.21em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>10.0</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.15em><span></span></span></span></span></span></span></span><span class=mclose><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.65em><span style=top:-4.65em><span class=pstrut style=height:6.8em></span><span style=width:.667em;height:4.8em><svg width=".667em" height="4.8em" viewBox="0 0 667 4800"><path d="M347 1759V0H0V84H263V1759v12e2 1759H0v84H347zm0 0V0H263V1759v12e2 1759h84z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:2.15em><span></span></span></span></span></span></span></span></span></span></span></span><h3 id=4-位置编码>4. 位置编码<a hidden class=anchor aria-hidden=true href=#4-位置编码>#</a></h3><p>Transformer 的编码器接收一串词嵌入作为输入。然后，这些嵌入和 <strong>位置编码（Positional Encoding）</strong> 相加。位置编码的目的是向模型提供一些关于单词在句子中位置的信息，因为 Transformer 模型自身并不具备这种识别单词时序的能力。</p><p>在原始的 Transformer 模型中，使用基于正弦和余弦函数的编码方式：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>sin</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mfrac><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>cos</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mfrac><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{gathered}
P E_{(pos, 2 i)}=\sin \left(\frac{pos}{10000^{2 i / d_{\text{model}}}}\right) \\
P E_{(pos, 2 i+1)}=\cos \left(\frac{pos}{10000^{2 i / d_{\text{model}}}}\right)
\end{gathered}
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:4.308em;vertical-align:-1.904em></span><span class=mord><span class=mtable><span class=col-align-c><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.404em><span style=top:-4.404em><span class=pstrut style=height:3.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.13889em>P</span><span class=mord><span class="mord mathnormal" style=margin-right:.05764em>E</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.5198em;margin-left:-.0576em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.3552em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mop>sin</span><span class=mspace style=margin-right:.1667em></span><span class=minner><span class="mopen delimcenter" style=top:0><span class="delimsizing size2">(</span></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.1076em><span style=top:-2.296em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1000</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.814em><span style=top:-2.989em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3488em;margin-left:0;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.1512em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.704em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style=top:0><span class="delimsizing size2">)</span></span></span></span></span><span style=top:-2.25em><span class=pstrut style=height:3.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:.13889em>P</span><span class=mord><span class="mord mathnormal" style=margin-right:.05764em>E</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.5198em;margin-left:-.0576em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.3552em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mop>cos</span><span class=mspace style=margin-right:.1667em></span><span class=minner><span class="mopen delimcenter" style=top:0><span class="delimsizing size2">(</span></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.1076em><span style=top:-2.296em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1000</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.814em><span style=top:-2.989em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3488em;margin-left:0;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.1512em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.704em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style=top:0><span class="delimsizing size2">)</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.904em><span></span></span></span></span></span></span></span></span></span></span></span><p>其中，<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.625em;vertical-align:-.1944em></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span> 表示位置，<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6595em></span><span class="mord mathnormal">i</span></span></span></span> 表示维度，<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">d_{\text{model}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span> 表示嵌入维度。</p><p>假设处理一个长度为 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn></mrow><annotation encoding="application/x-tex">50</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6444em></span><span class=mord>50</span></span></span></span> 的序列，每个词的嵌入维度为 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn></mrow><annotation encoding="application/x-tex">128</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6444em></span><span class=mord>128</span></span></span></span>，那么这个位置编码可视化为：</p><figure><img loading=lazy src=images/position_encoding.png alt=位置编码热图><figcaption><p>长度为 50 的序列在 128 维嵌入空间中的位置编码可视化</p></figcaption></figure><h3 id=5-自注意力机制>5. 自注意力机制<a hidden class=anchor aria-hidden=true href=#5-自注意力机制>#</a></h3><p>自注意力机制是 Transformer 架构中的核心机制，用于 <strong>计算序列中每个元素与其他元素之间的依赖关系</strong>，这种处理在文本数据中特别有用，因为每个单词通常都与其他单词存在相关性。通过这种方式，<strong>可以使网络更加关注有意义的信息，并对其余部分给予更少的关注</strong>。从实现的角度来看，注意力衡量两个向量之间的相似度，并返回加权相似度分数。</p><p>在自注意力机制中，有三个重要概念：<strong>查询（Query）</strong>，<strong>键（Key）</strong> 和 <strong>值（Value）</strong>。这些术语来自搜索引擎和推荐系统，因此下面是一个从数据库中检索的示例。</p><p>想象一下，你输入“沙滩上的狗”，想从数据库中检索一张图片。在数据库内部，每张照片都由一组关键词所描述——“猫”、“狗”、“聚会”等。我们将这些关键词称为“键”。搜索引擎会将你的查询和数据库中的键进行对比。“狗”匹配了 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6444em></span><span class=mord>1</span></span></span></span> 个结果，“猫”匹配了 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6444em></span><span class=mord>0</span></span></span></span> 个结果。然后，它会按照匹配度（相关性）对这些键进行排序，并按相关性顺序返回前 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.4306em></span><span class="mord mathnormal">n</span></span></span></span> 张匹配图片。</p><figure><img loading=lazy src=images/database_search.png alt=数据库检索示意图><figcaption><p>注意力机制类似数据库检索过程，查询（Query）与键（Key）匹配后返回对应的值（Value）</p></figcaption></figure><p>下面是注意力机制的数学表达式：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Attention</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">Softmax</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">
\operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{\mathrm{T}}}{\sqrt{d_k}}\right) V
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mop><span class="mord mathrm">Attention</span></span><span class=mopen>(</span><span class="mord mathnormal">Q</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.07153em>K</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.22222em>V</span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:2.4684em;vertical-align:-.95em></span><span class=mop><span class="mord mathrm">Softmax</span></span><span class=mspace style=margin-right:.1667em></span><span class=minner><span class="mopen delimcenter" style=top:0><span class="delimsizing size3">(</span></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.5183em><span style=top:-2.2528em><span class=pstrut style=height:3em></span><span class=mord><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8572em><span class=svg-align style=top:-3em><span class=pstrut style=height:3em></span><span class=mord style=padding-left:.833em><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.03148em>k</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span><span style=top:-2.8172em><span class=pstrut style=height:3em></span><span class=hide-tail style=min-width:.853em;height:1.08em><svg width="400em" height="1.08em" viewBox="0 0 4e5 1080" preserveAspectRatio="xMinYMin slice"><path d="M95 702c-2.7.0-7.17-2.7-13.5-8-5.8-5.3-9.5-10-9.5-14 0-2 .3-3.3 1-4 1.3-2.7 23.83-20.7 67.5-54 44.2-33.3 65.8-50.3 66.5-51 1.3-1.3 3-2 5-2 4.7.0 8.7 3.3 12 10s173 378 173 378c.7.0 35.3-71 104-213s137.5-285 206.5-429S812 97.3 814 94c5.3-9.3 12-14 20-14H4e5v40H845.2724s-225.272 467-225.272 467-235 486-235 486c-2.7 4.7-9 7-19 7-6 0-10-1-12-3s-194-422-194-422-65 47-65 47zM834 80h4e5v40H834z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.1828em><span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord mathnormal">Q</span><span class=mord><span class="mord mathnormal" style=margin-right:.07153em>K</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8413em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.93em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style=top:0><span class="delimsizing size3">)</span></span></span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.22222em>V</span></span></span></span></span><p>其中：<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8778em;vertical-align:-.1944em></span><span class="mord mathnormal">Q</span></span></span></span>，<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>K</span></span></span></span>，<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.22222em>V</span></span></span></span> 分别表示输入在经过投影后的查询矩阵、键矩阵和值矩阵。这一注意力机制称为 <strong>缩放点积注意力（scaled-dot product attention）</strong>。</p><p>下面展示了单头的缩放点积注意力的计算流程及其权重矩阵：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># set no grad</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>set_grad_enabled</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>1234</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>in_seq</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;Hello&#34;</span><span class=p>,</span> <span class=s2>&#34;world&#34;</span><span class=p>,</span> <span class=s2>&#34;how&#34;</span><span class=p>,</span> <span class=s2>&#34;are&#34;</span><span class=p>,</span> <span class=s2>&#34;you&#34;</span><span class=p>,</span> <span class=s2>&#34;doing&#34;</span><span class=p>,</span> <span class=s2>&#34;today&#34;</span><span class=p>,</span> <span class=s2>&#34;?&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>32</span>  <span class=c1># batch size, sequence length, embedding dimension</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span>  <span class=c1># input</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>head_size</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=n>key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>k</span> <span class=o>=</span> <span class=n>key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (B, T, H)</span>
</span></span><span class=line><span class=cl><span class=n>q</span> <span class=o>=</span> <span class=n>query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (B, T, H)</span>
</span></span><span class=line><span class=cl><span class=n>v</span> <span class=o>=</span> <span class=n>value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (B, T, H)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>attention</span> <span class=o>=</span> <span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># (B, T, H) @ (B, H, T) -&gt; (B, T, T)</span>
</span></span><span class=line><span class=cl><span class=n>attention</span> <span class=o>=</span> <span class=n>attention</span> <span class=o>/</span> <span class=p>(</span><span class=n>head_size</span><span class=o>**</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># scaled dot-product attention</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>attention</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attention</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># (B, T, T)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>attention</span> <span class=o>@</span> <span class=n>v</span>  <span class=c1># (B, T, T) @ (B, T, H) -&gt; (B, T, H)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># visualize the attention weights</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>attention</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>annot</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;viridis&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>xticklabels</span><span class=o>=</span><span class=n>in_seq</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>yticklabels</span><span class=o>=</span><span class=n>in_seq</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Attention Weights&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><figure><img loading=lazy src=images/attention_weights.png alt=注意力权重热图><figcaption><p>自注意力机制中各 token 之间的注意力权重可视化，颜色越深表示注意力越强</p></figcaption></figure><h4 id=注意力机制的优势与劣势>注意力机制的优势与劣势<a hidden class=anchor aria-hidden=true href=#注意力机制的优势与劣势>#</a></h4><ul><li><strong>长程依赖（Long-term Dependencies）</strong>：传统的 RNNs 容易出现随着处理序列边长，历史信息被减弱的问题。注意机制使得模型能够直接访问输入序列中的任何位置，缓解了这一问题，从而保留了完整的上下文信息。</li><li><strong>并行性（Parallelization）</strong>：与需要顺序计算的 RNNs 不同，基于注意力的模型（如：Transformer）可以并行处理输入序列中的所有词嵌入。这使它们在计算上更高效，并且在硬件加速器上扩展性更好。</li><li><strong>可解释性（Interpretability）</strong>：注意机制提供一定程度的可解释性，因为它突出显示模型认为对于产生特定输出最重要的输入部分。</li><li><strong>全局上下文（Global Context）</strong>: 在卷积神经网络（CNNs）中，感受野通常是局部的，并且取决于卷积核的大小。然而，通过注意力，每个位置的元素可以考虑输入序列中的其他元素的信息，从而保留全局上下文。</li><li><strong>更好的性能（Improved Performance）</strong>：基于注意力的模型，特别是 Transformer，在各项自然语言处理任务中都获得了最好的性能。并且也推动了其他领域的发展，如：计算机视觉、语言识别、多模态等。</li></ul><p>尽管注意力机制存在上述优点，它们也存在一些限制。注意力机制中，<strong>随着序列长度的增加，内存消耗和计算成本都呈 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-.25em></span><span class="mord mathcal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:.10903em>N</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span> 增加</strong>。这里的复杂度没考虑嵌入维度，实际上是 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>⋅</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(N^2 \cdot d)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-.25em></span><span class="mord mathcal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:.10903em>N</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal">d</span><span class=mclose>)</span></span></span></span>。另外，<strong>虽然在训练时 Transformer 具有并行化的优势，在推理过程中，仍然是使用自回归的方式一个词一个词的输出</strong>。</p><div class="notice notice-note" role=note><span class=notice-icon aria-hidden=true><svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M19 21l-7-5-7 5V5a2 2 0 012-2h10a2 2 0 012 2z"/></svg></span><div class=notice-content>如果查询（Query）、键（Key）和值（Value）来自同一输入，此时的注意力被称为 <strong>自注意力（self-attention）</strong>。如果它们来自不同的输入，则称为 <strong>交叉注意力（cross-attention）</strong>。</div></div><p>最后，在 Transformer 中，对注意力机制还做了一点改进—— <strong>多头注意力机制</strong>。多头注意力机制允许模型 <strong>同时关注不同位置的不同表示子空间中的信息</strong>。</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">MultiHead</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi mathvariant="normal">Concat</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mtext>head</mtext><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mtext>head</mtext><mi mathvariant="normal">h</mi></msub><mo fence="true">)</mo></mrow><msup><mi>W</mi><mi>O</mi></msup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>where </mtext><msub><mtext>head</mtext><mi>i</mi></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi mathvariant="normal">Attention</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\operatorname{MultiHead}(Q, K, V) &amp;= \operatorname{Concat} \left( \text{head}_1, \ldots, \text{head}_{\mathrm{h}}\right) W^O \\
\text{where} \space \text{head}_{i} &amp;=\operatorname{Attention}\left(Q W_i^Q, K W_i^K, V W_i^V\right)
\end{aligned}
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:3.6514em;vertical-align:-1.5757em></span><span class=mord><span class=mtable><span class=col-align-r><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.0757em><span style=top:-4.3343em><span class=pstrut style=height:3.15em></span><span class=mord><span class=mop><span class="mord mathrm">MultiHead</span></span><span class=mopen>(</span><span class="mord mathnormal">Q</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.07153em>K</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.22222em>V</span><span class=mclose>)</span></span></span><span style=top:-2.5243em><span class=pstrut style=height:3.15em></span><span class=mord><span class="mord text"><span class=mord>where</span></span><span class=mspace> </span><span class=mord><span class="mord text"><span class=mord>head</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3117em><span style=top:-2.55em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.5757em><span></span></span></span></span></span><span class=col-align-l><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:2.0757em><span style=top:-4.3343em><span class=pstrut style=height:3.15em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mop><span class="mord mathrm">Concat</span></span><span class=mspace style=margin-right:.1667em></span><span class=minner><span class="mopen delimcenter" style=top:0>(</span><span class=mord><span class="mord text"><span class=mord>head</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=minner>…</span><span class=mspace style=margin-right:.1667em></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord text"><span class=mord>head</span></span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">h</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class="mclose delimcenter" style=top:0>)</span></span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.13889em>W</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8913em><span style=top:-3.113em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.02778em>O</span></span></span></span></span></span></span></span></span></span><span style=top:-2.5243em><span class=pstrut style=height:3.15em></span><span class=mord><span class=mord></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span><span class=mop><span class="mord mathrm">Attention</span></span><span class=mspace style=margin-right:.1667em></span><span class=minner><span class="mopen delimcenter" style=top:0><span class="delimsizing size2">(</span></span><span class="mord mathnormal">Q</span><span class=mord><span class="mord mathnormal" style=margin-right:.13889em>W</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.9592em><span style=top:-2.4231em;margin-left:-.1389em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style=top:-3.1809em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2769em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.07153em>K</span><span class=mord><span class="mord mathnormal" style=margin-right:.13889em>W</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8913em><span style=top:-2.453em;margin-left:-.1389em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style=top:-3.113em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.07153em>K</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.247em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.22222em>V</span><span class=mord><span class="mord mathnormal" style=margin-right:.13889em>W</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8913em><span style=top:-2.453em;margin-left:-.1389em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style=top:-3.113em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:.22222em>V</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.247em><span></span></span></span></span></span></span><span class="mclose delimcenter" style=top:0><span class="delimsizing size2">)</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.5757em><span></span></span></span></span></span></span></span></span></span></span></span><h3 id=6-前馈神经网络>6. 前馈神经网络<a hidden class=anchor aria-hidden=true href=#6-前馈神经网络>#</a></h3><p>Transformer 中还包含一个 MLP 对特征嵌入进行变化，该 MLP 由两个线性层组成，中间使用 ReLU 激活函数。需要注意的是，该 MLP 的输入和输出维度都是 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">d_{\text{model}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span>，在中间层则会将特征维度扩大四倍：<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">4 \times d_{\text{model}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>4</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3361em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span>。在 ViT 中也延续了这一做法。</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">FFN</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo fence="true">)</mo></mrow><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">
\operatorname{FFN}(x)=\max \left(0, x W_1+b_1\right) W_2+b_2
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mop><span class="mord mathrm">FFN</span></span><span class=mopen>(</span><span class="mord mathnormal">x</span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mop>max</span><span class=mspace style=margin-right:.1667em></span><span class=minner><span class="mopen delimcenter" style=top:0>(</span><span class=mord>0</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal">x</span><span class=mord><span class="mord mathnormal" style=margin-right:.13889em>W</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.1389em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span><span class=mord><span class="mord mathnormal">b</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class="mclose delimcenter" style=top:0>)</span></span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:.13889em>W</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:-.1389em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.8444em;vertical-align:-.15em></span><span class=mord><span class="mord mathnormal">b</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3011em><span style=top:-2.55em;margin-left:0;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span></span></span></span></span><h2 id=vit>ViT<a hidden class=anchor aria-hidden=true href=#vit>#</a></h2><h3 id=1-patch-embedding>1. Patch Embedding<a hidden class=anchor aria-hidden=true href=#1-patch-embedding>#</a></h3><p>ViT 的工作方式与 Transformer 类似，但是 ViT 的输入是图像块（image patch），而非词元（word token）。</p><p>如果不将图像分成图像块，直接将像素值作为输出。那么，自注意力模块需要在每个像素间进行交互，<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">N=224 \times 224</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.10903em>N</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>224</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>224</span></span></span></span>。而注意力机制的复杂度又是平方级别的，即 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>224</mn><mn>4</mn></msup><mo>≈</mo><mn>2.5</mn><mi>E</mi><mn>9</mn></mrow><annotation encoding="application/x-tex">224^4 \approx 2.5E9</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord>22</span><span class=mord><span class=mord>4</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>≈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6833em></span><span class=mord>2.5</span><span class="mord mathnormal" style=margin-right:.05764em>E</span><span class=mord>9</span></span></span></span>，这样的计算复杂度是难以接受的。不过现在的大语言模型，claude 可以做到 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">N=100k</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.10903em>N</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6944em></span><span class=mord>100</span><span class="mord mathnormal" style=margin-right:.03148em>k</span></span></span></span>，GPT-4 可以做到 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">N=32k</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.10903em>N</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6944em></span><span class=mord>32</span><span class="mord mathnormal" style=margin-right:.03148em>k</span></span></span></span>，转换成图像的话，已经可以处理 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>316</mn><mo>×</mo><mn>316</mn></mrow><annotation encoding="application/x-tex">316 \times 316</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>316</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>316</span></span></span></span> 和 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>178</mn><mo>×</mo><mn>178</mn></mrow><annotation encoding="application/x-tex">178 \times 178</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>178</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>178</span></span></span></span> 大小的图像了。</p><p>在 patch embedding 中，图像被分割成相同高度和宽度 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.13889em>P</span></span></span></span> 的多个块：</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>C</mi></mrow></msup><mo>⇒</mo><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><msup><mi>P</mi><mn>2</mn></msup><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">
x \in \mathbb{R}^{H \times W \times C} \Rightarrow x \in \mathbb{R}^{N \times P^2 C}
</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.5782em;vertical-align:-.0391em></span><span class="mord mathnormal">x</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.8913em></span><span class=mord><span class="mord mathbb">R</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8913em><span style=top:-3.113em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:.08125em>H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style=margin-right:.13889em>W</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style=margin-right:.07153em>C</span></span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2778em></span><span class=mrel>⇒</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.5782em;vertical-align:-.0391em></span><span class="mord mathnormal">x</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1.0369em></span><span class=mord><span class="mord mathbb">R</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:1.0369em><span style=top:-3.113em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:.10903em>N</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:.13889em>P</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8913em><span style=top:-2.931em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal mtight" style=margin-right:.07153em>C</span></span></span></span></span></span></span></span></span></span></span></span></span><p>其中：</p><ul><li><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><msup><mi>P</mi><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">N=\frac{HW}{P^2}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.10903em>N</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1.2173em;vertical-align:-.345em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8723em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:.13889em>P</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7463em><span style=top:-2.786em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:.08125em>H</span><span class="mord mathnormal mtight" style=margin-right:.13889em>W</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.345em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> —— 图像块的数量；</li><li><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.13889em>P</span></span></span></span> —— 图像块的宽高，每个块的大小为 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>P</mi><mo separator="true">,</mo><mi>P</mi><mo separator="true">,</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(P,P,C)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:.13889em>P</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.13889em>P</span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span><span class=mclose>)</span></span></span></span>；</li><li><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span> —— 输入图像的通道数。</li></ul><figure><img loading=lazy src=images/patches.png alt=图像块划分示意图><figcaption><p>将图像分割成固定大小的 patches，每个 patch 展平后作为 Transformer 的输入</p></figcaption></figure><p>在构建完图像块之后，使用线性层将图像块映射到嵌入空间。以上是 ViT 论文中的描述，在代码实现中，构建图像块和线性映射使用一个卷积即可完成。</p><p>下面是 PatchEmbedding 的 PyTorch 代码：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PatchEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>img_size</span><span class=o>=</span><span class=mi>224</span><span class=p>,</span> <span class=n>patch_size</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span> <span class=n>in_chans</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>embed_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>img_size</span> <span class=o>=</span> <span class=n>img_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span> <span class=o>=</span> <span class=n>patch_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>n_patches</span> <span class=o>=</span> <span class=p>(</span><span class=n>img_size</span> <span class=o>//</span> <span class=n>patch_size</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span> <span class=o>=</span> <span class=n>patch_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>in_chans</span> <span class=o>=</span> <span class=n>in_chans</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span> <span class=o>=</span> <span class=n>embed_dim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>in_chans</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>patch_size</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># proj: [B, 3, H, W] -&gt; [B, embed, n_patches_h, n_patches_w]</span>
</span></span><span class=line><span class=cl>        <span class=c1># flatten: [B, embed, n_patches_h, n_patches_w] -&gt; [B, embed, n_patches]</span>
</span></span><span class=line><span class=cl>        <span class=c1># transpose: [B, embed, n_patches] -&gt; [B, n_patches, embed]</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=2-cls-token>2. [CLS] Token<a hidden class=anchor aria-hidden=true href=#2-cls-token>#</a></h3><p>这个 [CLS] token 在自然语言处理中起源于 BERT 模型。在 BERT 中，[CLS] token 被添加到每个输入序列的开始处，并用于聚合整个序列的信息以进行分类任务。[CLS] token 的输出通常被用作下游任务的输入，例如：句子分类。</p><p>同样，在 ViT 中，[CLS] token 在所有的 patch embedding 之前，其目的是 <strong>聚合整个图像的信息</strong>。在模型的前向传播过程中，[CLS] token 与所有的 patch embedding 一起通过 Transformer。在最后一层，我们取 [CLS] token 的输出，并使用 MLP 进行分类任务。</p><h3 id=3-位置嵌入>3. 位置嵌入<a hidden class=anchor aria-hidden=true href=#3-位置嵌入>#</a></h3><p>Transformer 模型没有任何机制来考虑 token 或 patch embedding 的顺序。然而，顺序是非常关键的。在语言中，词语的顺序可以完全改变它们的意思。</p><p>对于图像也是一样。如果给定一组乱码的拼图，对于一个人来说准确预测这一个图代表什么是非常困难甚至不可能的。因此，我们需要一种方式使得模型能够推断拼图块的顺序或位置。</p><p>在 ViT 中，使用位置嵌入来实现顺序的机制，这些 <strong>位置嵌入是与块嵌入具有相同维度的可学习向量</strong>。</p><p>在创建图像块嵌入并在前面添加 [CLS] token 之后，将它们和位置嵌入一起求和，作为 Transformer 的输入。这些位置嵌入在模型训练过程中一起学习。</p><p>下图展示了每一个位置嵌入和其他位置嵌入之间的余弦相似度。可以看到，每一个位置嵌入都与它所在行和列的位置嵌入具有最高的相似度。因此位置嵌入可以学习到当前图像块在图像中的位置。</p><figure><img loading=lazy src=images/position_embedding_similarity.png alt=位置嵌入余弦相似度热图><figcaption><p>各位置嵌入之间的余弦相似度，显示出行和列的相关性模式</p></figcaption></figure><h3 id=4-vit-是否能输入不同大小的图像>4. ViT 是否能输入不同大小的图像？<a hidden class=anchor aria-hidden=true href=#4-vit-是否能输入不同大小的图像>#</a></h3><p>显然 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224 \times 224</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>224</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>224</span></span></span></span> 或者 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>384</mn><mo>×</mo><mn>384</mn></mrow><annotation encoding="application/x-tex">384 \times 384</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7278em;vertical-align:-.0833em></span><span class=mord>384</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>384</span></span></span></span> 大小的输入图像并不能满足很多视觉任务的需求，比如图像分割、目标检测等。</p><p>当输入更大尺寸的图像时，我们保持图像块大小 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.13889em>P</span></span></span></span> 不变，这样会导致更长的输入序列 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><msup><mi>P</mi><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">N=\frac{HW}{P^{2}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.10903em>N</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1.2173em;vertical-align:-.345em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8723em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:.13889em>P</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7463em><span style=top:-2.786em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:.08125em>H</span><span class="mord mathnormal mtight" style=margin-right:.13889em>W</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.345em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，这一步对于 Patch Embedding 没有影响。但是，对于位置嵌入就会出问题，因为预训练的位置嵌入可能不再有意义了。ViT 中解决这一问题的方式是 <strong>使用双线性插值来缩放位置嵌入</strong>，来满足新的图像块大小。</p><p>当前，这种方式并不完美，后续有的工作中放弃了位置编码或对其进行改进（SegFormer，Swin Transformer 等）。</p><h3 id=5-注意力图可视化>5. 注意力图可视化<a hidden class=anchor aria-hidden=true href=#5-注意力图可视化>#</a></h3><p>以下是ViT注意力图的可视化对比，展示了原始注意力图与改进方法的对比效果：</p><table><thead><tr><th style=text-align:center>原始图像</th><th style=text-align:center>原始注意力图<br>(mean)</th><th style=text-align:center>改进注意力图<br>(max fusion)</th></tr></thead><tbody><tr><td style=text-align:center><div align=center><img src=images/both.png alt=狗和猫混合图像 width=200 height=200></div></td><td style=text-align:center><div align=center><img src=images/both_attention_rollout_mean.png alt=狗和猫原始注意力图 width=200 height=200></div></td><td style=text-align:center><div align=center><img src=images/both_attention_rollout_max.png alt=狗和猫改进注意力图 width=200 height=200></div></td></tr><tr><td style=text-align:center><div align=center><img src=images/plane.png alt=飞机图像 width=200 height=200></div></td><td style=text-align:center><div align=center><img src=images/plane_attention_rollout_mean.png alt=飞机原始注意力 width=200 height=200></div></td><td style=text-align:center><div align=center><img src=images/plane_attention_rollout_max.png alt=飞机改进注意力 width=200 height=200></div></td></tr><tr><td style=text-align:center><div align=center><img src=images/dogbird.png alt=狗鸟混合图像 width=200 height=200></div></td><td style=text-align:center><div align=center><img src=images/dogbird_attention_rollout_mean.png alt=狗鸟原始注意力 width=200 height=200></div></td><td style=text-align:center><div align=center><img src=images/dogbird_attention_rollout_max.png alt=狗鸟改进注意力 width=200 height=200></div></td></tr><tr><td style=text-align:center><div align=center><img src=images/plane2.png alt=第二张飞机图像 width=200 height=200></div></td><td style=text-align:center><div align=center><img src=images/plane2_attention_rollout_mean.png alt=第二张飞机原始注意力 width=200 height=200></div></td><td style=text-align:center><div align=center><img src=images/plane2_attention_rollout_max.png alt=第二张飞机改进注意力 width=200 height=200></div></td></tr></tbody></table><h3 id=6-实验>6. 实验<a hidden class=anchor aria-hidden=true href=#6-实验>#</a></h3><p>下面是实现了一个简易版本的 ViT，并通过加载 Hugging Face 上的预训练权重，在 CIFAR10 和 CIFAR100 上对 ViT 论文中的相关实验的复现结果。</p><figure><img loading=lazy src=images/vit_experiment_results.png alt=ViT实验复现结果><figcaption><p>ViT 在 CIFAR10 和 CIFAR100 数据集上的实验结果对比，展示了不同预训练策略的效果</p></figcaption></figure><h2 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h2><ol><li><a href=https://keras.io/examples/vision/probing_vits/>Investigating Vision Transformer representations</a></li><li><a href=https://www.pinecone.io/learn/series/image-search/vision-transformers/>Vision Transformers (ViT) Explained</a></li><li><a href=https://jacobgil.github.io/deeplearning/vision-transformer-explainability>Exploring Explainability for Vision Transformers</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://ehehe.cn/tags/vision-transformer/>Vision Transformer</a></li><li><a href=https://ehehe.cn/tags/transformer/>Transformer</a></li></ul><nav class=paginav><a class=prev href=https://ehehe.cn/posts/2023/02-swin-transformer/><span class=title>« Prev</span><br><span>Swin Transformer：层级式特征图与移动窗口注意力机制</span>
</a><a class=next href=https://ehehe.cn/posts/2023/07-deeplabv2/><span class=title>Next »</span><br><span>DeepLabv2：基于空洞卷积与 ASPP 的语义图像分割</span></a></nav></footer><script src=https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js></script><script>const elementCode=".mermaid",loadMermaid=function(e){mermaid.initialize({theme:e}),mermaid.init({theme:e},document.querySelectorAll(elementCode))},saveOriginalData=function(){return new Promise((e,t)=>{try{var n=document.querySelectorAll(elementCode),s=n.length;n.forEach(t=>{t.setAttribute("data-original-code",t.innerHTML),s--,s==0&&e()})}catch(e){t(e)}})},resetProcessed=function(){return new Promise((e,t)=>{try{var n=document.querySelectorAll(elementCode),s=n.length;n.forEach(t=>{t.getAttribute("data-original-code")!=null&&(t.removeAttribute("data-processed"),t.innerHTML=t.getAttribute("data-original-code")),s--,s==0&&e()})}catch(e){t(e)}})};saveOriginalData().catch(console.error);let isdark=document.body.className.includes("dark");isdark?resetProcessed().then(loadMermaid("dark")).catch(console.error):resetProcessed().then(loadMermaid("neutral")).catch(console.error),document.getElementById("theme-toggle").addEventListener("click",()=>{resetProcessed(),document.body.className.includes("dark")?loadMermaid("neutral"):loadMermaid("dark").catch(console.error)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://ehehe.cn/>Yan Tang</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){function e(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",e,{once:!0}):e()})()</script><script>(function(){var n=window.pageYOffset||document.documentElement.scrollTop||0,s=800,t=!1,e=null;function o(){if(t=!1,e||(e=document.getElementById("top-link")),!e)return;var o=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,i=o>n;n=o<0?0:o,o>s&&!i?(e.style.visibility="visible",e.style.opacity="1"):(e.style.visibility="hidden",e.style.opacity="0")}window.addEventListener("scroll",function(){t||(window.requestAnimationFrame(o),t=!0)},{passive:!0})})()</script><script>(function(){var t=10;function n(e){if(!e)return[0,20];var n,s,o=window.getComputedStyle(e),t=parseFloat(o.lineHeight);return(!t||isNaN(t))&&(t=20),n=e.getBoundingClientRect(),s=n.height||e.offsetHeight||0,[Math.round(s/t),t]}function e(){var e=document.querySelectorAll(".highlight");if(!e||!e.length)return;Array.prototype.forEach.call(e,function(e){if(e.classList.contains("expanded")||e.classList.contains("collapsible"))return;var s,o,a,r,c,l,i=e.querySelector("pre code");if(!i)return;if(r=i.className||"",r.indexOf("language-mermaid")>=0||e.classList.contains("mermaid"))return;if(a=n(i),c=a[0],l=a[1],c<=t)return;e.classList.add("collapsible"),e.style.setProperty("--code-line-height",l+"px"),e.id||(e.id="code-block-"+Math.random().toString(36).slice(2)),o=document.createElement("div"),o.className="code-expand-wrapper",s=document.createElement("button"),s.type="button",s.className="code-expand-link",s.textContent="Show more",s.setAttribute("aria-expanded","false"),s.setAttribute("aria-controls",e.id),s.addEventListener("click",function(){var n,i,a,o=e.classList.contains("expanded"),r=getComputedStyle(e).getPropertyValue("--code-line-height"),t=parseFloat(r)*(parseFloat(getComputedStyle(e).getPropertyValue("--code-max-lines"))||10);(!isFinite(t)||t<=0)&&(t=10*20),i=e.getBoundingClientRect().height,a=o?t:e.scrollHeight,e.style.maxHeight=i+"px",e.offsetHeight,o?e.classList.remove("expanded"):e.classList.add("expanded"),e.style.maxHeight=a+"px",n=function(t){if(t.propertyName!=="max-height")return;if(e.removeEventListener("transitionend",n),e.style.maxHeight="",e.classList.contains("expanded"))s.textContent="Show less",s.setAttribute("aria-expanded","true");else{s.textContent="Show more",s.setAttribute("aria-expanded","false");try{e.scrollIntoView({behavior:"smooth",block:"nearest"})}catch{}}window.dispatchEvent(new Event("resize"))},e.addEventListener("transitionend",n)}),o.appendChild(s),e.nextSibling?e.parentNode.insertBefore(o,e.nextSibling):e.parentNode.appendChild(o)})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",e,{once:!0}):e()})()</script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>